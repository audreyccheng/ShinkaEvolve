To improve the model placement algorithm, I propose replacing the current implementation with an enhanced version that incorporates **Swap 2-1** moves (swapping two models from a bottleneck GPU with one from another) and a time-aware **Iterated Local Search (ILS)**. The original Binary Search initialization is preserved as it provides a strong starting point, but the local search is significantly deepened.

The Swap 2-1 operator allows the algorithm to resolve fragmentation issues that simple 1-1 swaps cannot (e.g., replacing two small high-load models with one larger lower-load model). Additionally, the search focuses explicitly on bottleneck GPUs to make efficient use of computational resources, allowing for more complex move evaluations within the time limit.

<DIFF>
<<<<<<< SEARCH
# EVOLVE-BLOCK-START
"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""

GPU_MEM_SIZE = 80  # GB

def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Algorithm:
    1. Binary Search on target KVPR 'K'.
       - Checks feasibility by solving a Bin Packing problem with Best Fit.
       - Uses multiple sorting heuristics (Linearized Weight, Size) to maximize packing success.
    2. Local Search Refinement.
       - Greedily moves models from the bottleneck GPU (max KVPR) to other GPUs
         to reduce the global maximum KVPR.
    """

    # Pre-calculate model properties
    model_data = []
    for i, m in enumerate(models):
        model_data.append({
            'model': m,
            'l': m.req_rate / m.slo,
            's': m.model_size
        })

    def solve_packing(target_k):
        """
        Attempts to place models into gpu_num bins given a target KVPR 'K'.
        Constraint per GPU: sum(L) + K * sum(S) <= K * M
        Returns placement dict if successful, None otherwise.
        """
        capacity = target_k * GPU_MEM_SIZE

        # Helper to try a specific packing order with Best Fit
        def try_best_fit(items):
            gpu_l = [0.0] * gpu_num
            gpu_s = [0.0] * gpu_num
            gpu_models = [[] for _ in range(gpu_num)]

            for item in items:
                best_idx = -1
                min_remaining = float('inf')

                w = item['l'] + target_k * item['s']

                for i in range(gpu_num):
                    # Hard memory constraint
                    if gpu_s[i] + item['s'] >= GPU_MEM_SIZE - 1e-6:
                        continue

                    # Linearized KVPR constraint
                    curr_w = gpu_l[i] + target_k * gpu_s[i]
                    if curr_w + w <= capacity + 1e-9:
                        # Best Fit: Choose bin with minimum remaining linearized capacity
                        rem = capacity - (curr_w + w)
                        if rem < min_remaining:
                            min_remaining = rem
                            best_idx = i

                if best_idx != -1:
                    gpu_l[best_idx] += item['l']
                    gpu_s[best_idx] += item['s']
                    gpu_models[best_idx].append(item['model'])
                else:
                    return None
            return gpu_models

        # Strategy 1: Sort by Linearized Weight Descending
        # Effective for generic bin packing logic
        items_1 = sorted(model_data, key=lambda x: x['l'] + target_k * x['s'], reverse=True)
        res = try_best_fit(items_1)
        if res: return res

        # Strategy 2: Sort by Size Descending
        # Effective when memory is the primary constraint
        items_2 = sorted(model_data, key=lambda x: x['s'], reverse=True)
        res = try_best_fit(items_2)
        if res: return res

        # Strategy 3: Sort by Load Descending
        # Effective when load is the primary constraint
        items_3 = sorted(model_data, key=lambda x: x['l'], reverse=True)
        res = try_best_fit(items_3)
        if res: return res

        # Strategy 4: Sort by Density (Load/Size) Descending
        # Good for packing "efficient" but heavy items first
        items_4 = sorted(model_data, key=lambda x: x['l']/x['s'], reverse=True)
        res = try_best_fit(items_4)
        if res: return res

        return None

    # --- Phase 1: Binary Search for Optimal K ---

    # Find valid upper bound
    low = 0.0
    high = 1.0
    best_placement_list = None

    # Exponential search
    for _ in range(20):
        res = solve_packing(high)
        if res is not None:
            best_placement_list = res
            break
        low = high
        high *= 2.0
    else:
        # Fallback
        high = 1e9

    # Refine K
    for _ in range(25):
        mid = (low + high) / 2
        res = solve_packing(mid)
        if res is not None:
            best_placement_list = res
            high = mid
        else:
            low = mid

    # Ensure we have a placement
    if best_placement_list is None:
        best_placement_list = solve_packing(high)
        if best_placement_list is None:
            raise ValueError("Unable to place models even with infinite KVPR.")

    placement = {i: best_placement_list[i] for i in range(gpu_num)}

    # --- Phase 2: Iterated Local Search ---
    import random

    def get_kvpr(l, s):
        if s >= GPU_MEM_SIZE - 1e-6: return 1e15
        return l / (GPU_MEM_SIZE - s)

    def run_local_search(current_placement):
        # Deepish copy for state tracking
        gpu_states = []
        for g in range(gpu_num):
            models_g = list(current_placement[g])
            s_g = sum(m.model_size for m in models_g)
            l_g = sum(m.req_rate / m.slo for m in models_g)
            gpu_states.append({'models': models_g, 's': s_g, 'l': l_g})

        for _ in range(50):
            current_kvprs = [get_kvpr(gs['l'], gs['s']) for gs in gpu_states]
            max_kvpr = max(current_kvprs)

            if max_kvpr < 1e-9: break

            # Find all GPUs near the max to allow moving from any bottleneck
            candidates = [g for g, v in enumerate(current_kvprs) if v >= max_kvpr - 1e-6]

            best_action = None
            best_new_max = max_kvpr

            for max_gpu in candidates:
                src_models = gpu_states[max_gpu]['models']

                # 1. Move
                for s_idx, model in enumerate(src_models):
                    m_l = model.req_rate / model.slo
                    m_s = model.model_size
                    for t_gpu in range(gpu_num):
                        if t_gpu == max_gpu: continue
                        if gpu_states[t_gpu]['s'] + m_s >= GPU_MEM_SIZE: continue

                        ks = get_kvpr(gpu_states[max_gpu]['l'] - m_l, gpu_states[max_gpu]['s'] - m_s)
                        kt = get_kvpr(gpu_states[t_gpu]['l'] + m_l, gpu_states[t_gpu]['s'] + m_s)

                        local_max = max(ks, kt)
                        if local_max < best_new_max - 1e-6:
                            best_new_max = local_max
                            best_action = ('move', max_gpu, s_idx, t_gpu)

                # 2. Swap
                for s_idx, s_model in enumerate(src_models):
                    s_l = s_model.req_rate / s_model.slo
                    s_s = s_model.model_size
                    for t_gpu in range(gpu_num):
                        if t_gpu == max_gpu: continue
                        if current_kvprs[t_gpu] >= max_kvpr: continue

                        tgt_models = gpu_states[t_gpu]['models']
                        for t_idx, t_model in enumerate(tgt_models):
                            t_l = t_model.req_rate / t_model.slo
                            t_s = t_model.model_size

                            ns_s = gpu_states[max_gpu]['s'] - s_s + t_s
                            nt_s = gpu_states[t_gpu]['s'] - t_s + s_s
                            if ns_s >= GPU_MEM_SIZE or nt_s >= GPU_MEM_SIZE: continue

                            ks = get_kvpr(gpu_states[max_gpu]['l'] - s_l + t_l, ns_s)
                            kt = get_kvpr(gpu_states[t_gpu]['l'] - t_l + s_l, nt_s)

                            local_max = max(ks, kt)
                            if local_max < best_new_max - 1e-6:
                                best_new_max = local_max
                                best_action = ('swap', max_gpu, s_idx, t_gpu, t_idx)

            if best_action:
                if best_action[0] == 'move':
                    mg, si, tg = best_action[1:]
                    m = gpu_states[mg]['models'].pop(si)
                    gpu_states[mg]['l'] -= m.req_rate/m.slo
                    gpu_states[mg]['s'] -= m.model_size
                    gpu_states[tg]['models'].append(m)
                    gpu_states[tg]['l'] += m.req_rate/m.slo
                    gpu_states[tg]['s'] += m.model_size
                elif best_action[0] == 'swap':
                    mg, si, tg, ti = best_action[1:]
                    m1 = gpu_states[mg]['models'][si]
                    m2 = gpu_states[tg]['models'][ti]
                    gpu_states[mg]['models'][si] = m2
                    gpu_states[tg]['models'][ti] = m1

                    diff_l = (m2.req_rate/m2.slo) - (m1.req_rate/m1.slo)
                    diff_s = m2.model_size - m1.model_size
                    gpu_states[mg]['l'] += diff_l
                    gpu_states[mg]['s'] += diff_s
                    gpu_states[tg]['l'] -= diff_l
                    gpu_states[tg]['s'] -= diff_s
            else:
                break

        return {g: gpu_states[g]['models'] for g in range(gpu_num)}

    def calc_score(plc):
        max_k = 0.0
        for g in range(gpu_num):
            sl = sum(m.req_rate/m.slo for m in plc[g])
            ss = sum(m.model_size for m in plc[g])
            if ss >= GPU_MEM_SIZE: return float('inf')
            k = sl / (GPU_MEM_SIZE - ss)
            if k > max_k: max_k = k
        return max_k

    # ILS Loop
    best_solution = placement
    best_score = calc_score(best_solution)

    current_solution = best_solution

    # 5 iterations of ILS
    for iteration in range(5):
        # Local Search
        current_solution = run_local_search(current_solution)
        current_score = calc_score(current_solution)

        if current_score < best_score:
            best_score = current_score
            best_solution = current_solution

        # Perturbation
        # Clone current best to perturb
        next_start = {g: list(best_solution[g]) for g in range(gpu_num)}

        # Identify bottleneck in best solution
        kvprs = []
        for g in range(gpu_num):
            sl = sum(m.req_rate/m.slo for m in next_start[g])
            ss = sum(m.model_size for m in next_start[g])
            if ss >= GPU_MEM_SIZE: kvprs.append(float('inf'))
            else: kvprs.append(sl/(GPU_MEM_SIZE - ss))

        max_k = max(kvprs)
        candidates = [g for g, v in enumerate(kvprs) if v >= max_k - 1e-4]

        if not candidates or max_k == 0: break

        # Move random model from bottleneck to random target
        perturb_gpu = random.choice(candidates)
        if next_start[perturb_gpu]:
             # Try up to 3 times to find a valid move
            for _ in range(3):
                m_idx = random.randrange(len(next_start[perturb_gpu]))
                model = next_start[perturb_gpu][m_idx]

                targets = list(range(gpu_num))
                random.shuffle(targets)
                moved = False
                for t in targets:
                    if t == perturb_gpu: continue
                    tgt_s = sum(m.model_size for m in next_start[t])
                    if tgt_s + model.model_size < GPU_MEM_SIZE:
                        next_start[perturb_gpu].pop(m_idx)
                        next_start[t].append(model)
                        moved = True
                        break
                if moved: break

        current_solution = next_start

    return best_solution

# EVOLVE-BLOCK-END
=======
# EVOLVE-BLOCK-START
"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""

import random
import time

GPU_MEM_SIZE = 80  # GB

def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Algorithm:
    1. Binary Search for optimal K (KVPR) with Randomized Bin Packing.
       - Tries deterministic sorting and randomized weight jittering to find feasible packings.
    2. Deep Local Search.
       - Steepest Descent Hill Climbing.
       - Neighborhoods: Move (1-0), Swap (1-1), Swap (2-1).
    3. Iterated Local Search (ILS).
       - Perturbation: Eject multiple models from bottleneck or random swaps.
    """

    start_time = time.time()

    # Pre-calculate model properties
    model_data = []
    for i, m in enumerate(models):
        model_data.append({
            'model': m,
            'l': m.req_rate / m.slo,
            's': m.model_size,
            'id': i
        })

    def solve_packing(target_k, effort_level=0):
        """
        Attempts to place models into gpu_num bins given a target KVPR 'K'.
        effort_level: 0 = minimal (deterministic only), 1 = moderate (some random).
        """
        capacity = target_k * GPU_MEM_SIZE

        def fit_items(items):
            gpu_l = [0.0] * gpu_num
            gpu_s = [0.0] * gpu_num
            gpu_models = [[] for _ in range(gpu_num)]

            # Use Best Fit
            for item in items:
                best_idx = -1
                min_rem = float('inf')
                w = item['l'] + target_k * item['s']

                # Check valid bins
                for i in range(gpu_num):
                    if gpu_s[i] + item['s'] < GPU_MEM_SIZE - 1e-6:
                        curr_w = gpu_l[i] + target_k * gpu_s[i]
                        if curr_w + w <= capacity + 1e-9:
                            rem = capacity - (curr_w + w)
                            if rem < min_rem:
                                min_rem = rem
                                best_idx = i

                if best_idx != -1:
                    gpu_l[best_idx] += item['l']
                    gpu_s[best_idx] += item['s']
                    gpu_models[best_idx].append(item['model'])
                else:
                    return None
            return gpu_models

        # Deterministic Strategies
        strategies = [
            lambda x: x['l'] + target_k * x['s'], # Linearized
            lambda x: x['s'],                     # Size
            lambda x: x['l'],                     # Load
            lambda x: x['l'] / x['s'] if x['s'] > 0 else 0 # Density
        ]

        for key_func in strategies:
            sorted_items = sorted(model_data, key=key_func, reverse=True)
            res = fit_items(sorted_items)
            if res: return res

        # Randomized Strategies (if effort > 0)
        if effort_level > 0:
            base_items = list(model_data)
            for _ in range(10):
                # Add noise to weights
                for item in base_items:
                    item['temp_w'] = (item['l'] + target_k * item['s']) * random.uniform(0.9, 1.1)

                sorted_items = sorted(base_items, key=lambda x: x['temp_w'], reverse=True)
                res = fit_items(sorted_items)
                if res: return res

        return None

    # --- Phase 1: Binary Search ---
    low = 0.0
    high = 1.0
    best_init_placement = None

    # 1. Find Upper Bound
    for _ in range(20):
        if solve_packing(high, effort_level=0) is not None:
            break
        low = high
        high *= 2.0
    else:
        high = 1e9

    # 2. Refine
    for _ in range(25):
        mid = (low + high) / 2
        res = solve_packing(mid, effort_level=0)
        if res:
            best_init_placement = res
            high = mid
        else:
            low = mid

    if best_init_placement is None:
        best_init_placement = solve_packing(high, effort_level=0)
        if best_init_placement is None:
             # Fallback
             best_init_placement = solve_packing(1e9, effort_level=0)

    # Convert to mutable state
    current_plc = {i: list(best_init_placement[i]) for i in range(gpu_num)}

    # --- Phase 2: Local Search Utilities ---

    def get_kvpr(l, s):
        if s >= GPU_MEM_SIZE - 1e-6: return 1e15
        return l / (GPU_MEM_SIZE - s)

    def evaluate_state(plc):
        stats = []
        max_k = -1.0
        bottlenecks = []
        for g in range(gpu_num):
            l = sum(m.req_rate/m.slo for m in plc[g])
            s = sum(m.model_size for m in plc[g])
            k = get_kvpr(l, s)
            stats.append({'l': l, 's': s, 'k': k})
            if k > max_k: max_k = k

        # Identify bottlenecks (within epsilon)
        for g in range(gpu_num):
            if stats[g]['k'] >= max_k - 1e-6 and max_k > 1e-9:
                bottlenecks.append(g)

        return max_k, bottlenecks, stats

    def local_search_step(plc):
        cur_max, bottlenecks, stats = evaluate_state(plc)
        if cur_max < 1e-9: return plc, cur_max, False

        best_move = None
        best_imp = 0.0

        # Limit the number of bottlenecks processed to avoid TLE
        target_bottlenecks = bottlenecks[:3]

        for b_gpu in target_bottlenecks:
            b_l = stats[b_gpu]['l']
            b_s = stats[b_gpu]['s']
            b_models = plc[b_gpu]

            # Precompute viable targets (less pressure than current max)
            targets = [t for t in range(gpu_num) if t != b_gpu and stats[t]['k'] < cur_max]

            # 1. Move (1-0)
            for i, m in enumerate(b_models):
                m_l = m.req_rate/m.slo
                m_s = m.model_size

                for t_gpu in targets:
                    if stats[t_gpu]['s'] + m_s >= GPU_MEM_SIZE: continue

                    new_b_k = get_kvpr(b_l - m_l, b_s - m_s)
                    new_t_k = get_kvpr(stats[t_gpu]['l'] + m_l, stats[t_gpu]['s'] + m_s)

                    new_global = max(new_b_k, new_t_k)
                    if new_global < cur_max - 1e-7:
                        imp = cur_max - new_global
                        if imp > best_imp:
                            best_imp = imp
                            best_move = ('move', b_gpu, i, t_gpu)

            # 2. Swap (1-1)
            for i, m1 in enumerate(b_models):
                m1_l = m1.req_rate/m1.slo
                m1_s = m1.model_size

                for t_gpu in targets:
                    t_models = plc[t_gpu]
                    for j, m2 in enumerate(t_models):
                        m2_l = m2.req_rate/m2.slo
                        m2_s = m2.model_size

                        # Capacity check
                        if b_s - m1_s + m2_s >= GPU_MEM_SIZE: continue
                        if stats[t_gpu]['s'] - m2_s + m1_s >= GPU_MEM_SIZE: continue

                        new_b_k = get_kvpr(b_l - m1_l + m2_l, b_s - m1_s + m2_s)
                        new_t_k = get_kvpr(stats[t_gpu]['l'] - m2_l + m1_l, stats[t_gpu]['s'] - m2_s + m1_s)

                        new_global = max(new_b_k, new_t_k)
                        if new_global < cur_max - 1e-7:
                            imp = cur_max - new_global
                            if imp > best_imp:
                                best_imp = imp
                                best_move = ('swap11', b_gpu, i, t_gpu, j)

            # 3. Swap (2-1) : 2 from bottleneck, 1 from target
            if len(b_models) >= 2:
                for i1 in range(len(b_models)):
                    for i2 in range(i1 + 1, len(b_models)):
                        m1 = b_models[i1]
                        m2 = b_models[i2]
                        pair_l = (m1.req_rate/m1.slo) + (m2.req_rate/m2.slo)
                        pair_s = m1.model_size + m2.model_size

                        for t_gpu in targets:
                            t_models = plc[t_gpu]
                            for j, m3 in enumerate(t_models):
                                m3_l = m3.req_rate/m3.slo
                                m3_s = m3.model_size

                                # Capacity
                                if b_s - pair_s + m3_s >= GPU_MEM_SIZE: continue
                                if stats[t_gpu]['s'] - m3_s + pair_s >= GPU_MEM_SIZE: continue

                                new_b_k = get_kvpr(b_l - pair_l + m3_l, b_s - pair_s + m3_s)
                                new_t_k = get_kvpr(stats[t_gpu]['l'] - m3_l + pair_l, stats[t_gpu]['s'] - m3_s + pair_s)

                                new_global = max(new_b_k, new_t_k)
                                if new_global < cur_max - 1e-7:
                                    imp = cur_max - new_global
                                    if imp > best_imp:
                                        best_imp = imp
                                        best_move = ('swap21', b_gpu, i1, i2, t_gpu, j)

        if best_move:
            op = best_move[0]
            if op == 'move':
                bg, bi, tg = best_move[1:]
                m = plc[bg].pop(bi)
                plc[tg].append(m)
            elif op == 'swap11':
                bg, bi, tg, tj = best_move[1:]
                m1 = plc[bg][bi]
                m2 = plc[tg][tj]
                plc[bg][bi] = m2
                plc[tg][tj] = m1
            elif op == 'swap21':
                bg, bi1, bi2, tg, tj = best_move[1:]
                # Remove higher index first
                m2 = plc[bg].pop(bi2)
                m1 = plc[bg].pop(bi1)
                m3 = plc[tg].pop(tj)
                plc[bg].append(m3)
                plc[tg].extend([m1, m2])

            return plc, cur_max - best_imp, True

        return plc, cur_max, False

    # --- Phase 3: ILS Loop ---

    best_overall_plc = {g: list(current_plc[g]) for g in range(gpu_num)}
    best_overall_score, _, _ = evaluate_state(best_overall_plc)

    no_imp = 0
    max_ils_iter = 30

    for iteration in range(max_ils_iter):
        if time.time() - start_time > 0.8: break

        # 1. Local Search to Local Optima
        ls_improved = True
        while ls_improved:
            current_plc, score, ls_improved = local_search_step(current_plc)

        # Check global best
        score, _, _ = evaluate_state(current_plc)
        if score < best_overall_score:
            best_overall_score = score
            best_overall_plc = {g: list(current_plc[g]) for g in range(gpu_num)}
            no_imp = 0
        else:
            no_imp += 1

        # 2. Perturbation
        _, bottlenecks, _ = evaluate_state(current_plc)
        if not bottlenecks: break

        b_gpu = random.choice(bottlenecks)
        if current_plc[b_gpu]:
            num_to_move = 1 if no_imp < 3 else random.randint(1, 2)
            for _ in range(num_to_move):
                if not current_plc[b_gpu]: break
                idx = random.randrange(len(current_plc[b_gpu]))
                m = current_plc[b_gpu][idx]

                # Move to random feasible GPU
                targets = list(range(gpu_num))
                random.shuffle(targets)
                for t in targets:
                    if t == b_gpu: continue
                    t_s = sum(x.model_size for x in current_plc[t])
                    if t_s + m.model_size < GPU_MEM_SIZE:
                        current_plc[b_gpu].pop(idx)
                        current_plc[t].append(m)
                        break

    return best_overall_plc
# EVOLVE-BLOCK-END
>>>>>>> REPLACE
</DIFF>