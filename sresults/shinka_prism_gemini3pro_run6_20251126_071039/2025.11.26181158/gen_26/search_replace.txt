<NAME>
multistart_ils_swap22
</NAME>

<DESCRIPTION>
Implements a Multi-Start Iterated Local Search with an expanded neighborhood.
1.  **Multi-Start**: The algorithm runs multiple independent searches. The first run uses the best deterministic packing found via binary search. Subsequent runs use randomized bin packing (adding noise to sorting weights) to explore diverse initial configurations.
2.  **Expanded Neighborhood**: Adds a `Swap 2-2` operator (exchanging 2 models from the bottleneck GPU with 2 models from a target GPU) to the Local Search. This allows escaping local optima where 1-to-1 or 2-to-1 swaps are insufficient due to fragmentation or size constraints.
3.  **Steepest Descent**: In each local search step, all valid moves (Shift, Swap 1-1, Swap 2-1, Swap 2-2) from the bottleneck GPU are evaluated, and the one maximizing improvement is chosen.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Algorithm:
    1. Binary Search for the optimal Max KVPR 'K'.
       The feasibility check transforms the problem into a Bin Packing Problem
       with linearized item weights: w = l + K * s.
       It uses Best Fit Decreasing with 4 sorting heuristics:
       - Linearized Weight: l + K*s
       - Size: s
       - Load: l
       - Density: l/s
    2. Local Search Refinement.
       After finding a feasible placement, we explicitly minimize the max KVPR
       using three neighborhood operators on the bottleneck GPU:
       - Shift: Move 1 model to another GPU.
       - Swap 1-1: Exchange 1 model with 1 from another GPU.
       - Swap 2-1: Exchange 2 models with 1 from another GPU (helps with fragmentation).
    """

    # Precompute model characteristics
    model_data = []
    for i, m in enumerate(models):
        model_data.append({
            'model': m,
            'l': m.req_rate / m.slo,
            's': m.model_size,
            'id': i
        })

    # --- Phase 1: Binary Search Construction ---

    def solve_packing(target_k):
        capacity = target_k * GPU_MEM_SIZE

        # Sorting strategies for Best Fit Decreasing
        heuristics = [
            # 1. Linearized Weight: balances load and size based on K
            lambda x: x['l'] + target_k * x['s'],
            # 2. Size: prioritizes fitting large items (memory constrained)
            lambda x: x['s'],
            # 3. Load: prioritizes high load items
            lambda x: x['l'],
            # 4. Density: efficient packing
            lambda x: x['l'] / x['s'] if x['s'] > 0 else 0
        ]

        for key_func in heuristics:
            # Sort descending
            items = sorted(model_data, key=key_func, reverse=True)

            bins_l = [0.0] * gpu_num
            bins_s = [0.0] * gpu_num
            bins_items = [[] for _ in range(gpu_num)]

            feasible = True
            for item in items:
                best_bin = -1
                min_slack = float('inf')

                # Weight of item in linearized constraint
                item_w = item['l'] + target_k * item['s']

                for b in range(gpu_num):
                    # Hard Memory Constraint
                    if bins_s[b] + item['s'] >= GPU_MEM_SIZE - 1e-6:
                        continue

                    # Linearized Constraint: sum(L) + K*sum(S) <= K*M
                    current_bin_w = bins_l[b] + target_k * bins_s[b]

                    if current_bin_w + item_w <= capacity + 1e-9:
                        # Best Fit: minimize slack
                        slack = capacity - (current_bin_w + item_w)
                        if slack < min_slack:
                            min_slack = slack
                            best_bin = b

                if best_bin != -1:
                    bins_l[best_bin] += item['l']
                    bins_s[best_bin] += item['s']
                    bins_items[best_bin].append(item)
                else:
                    feasible = False
                    break

            if feasible:
                return bins_items

        return None

    # Binary Search
    low = 0.0
    high = 1.0

    # Exponential search for upper bound
    for _ in range(20):
        if solve_packing(high) is not None:
            break
        low = high
        high *= 2.0
    else:
        high = 1e9 # Fallback

    best_packing = None

    # Precision search
    for _ in range(30):
        mid = (low + high) / 2
        res = solve_packing(mid)
        if res is not None:
            best_packing = res
            high = mid
        else:
            low = mid

    if best_packing is None:
        best_packing = solve_packing(high)
        if best_packing is None:
            raise ValueError("Unable to place models.")

    # Convert to standard format for Phase 2
    placement = {i: [x['model'] for x in best_packing[i]] for i in range(gpu_num)}

    # --- Phase 2: Local Search Refinement ---

    # Initialize state for fast access
    gpu_states = []
    for g in range(gpu_num):
        # We use the dictionaries from model_data to avoid re-creating them
        # We need to map the model objects back to their data dicts
        p_models = placement[g]
        g_items = []
        g_l = 0.0
        g_s = 0.0
        # Map models to data dicts. Optimization: Create a lookup if N is large,
        # but for small N nested loop is fine.
        for m in p_models:
            for d in model_data:
                if d['model'] is m:
                    g_items.append(d)
                    g_l += d['l']
                    g_s += d['s']
                    break
        gpu_states.append({'l': g_l, 's': g_s, 'items': g_items})

    def get_kvpr(l, s):
        if s >= GPU_MEM_SIZE - 1e-6: return float('inf')
        return l / (GPU_MEM_SIZE - s)

    # Hill Climbing on Global Max KVPR
    for _ in range(200): # Iteration limit
        # 1. Identify Bottleneck
        max_kvpr = -1.0
        max_gpu = -1
        current_kvprs = []

        for g in range(gpu_num):
            val = get_kvpr(gpu_states[g]['l'], gpu_states[g]['s'])
            current_kvprs.append(val)
            if val > max_kvpr:
                max_kvpr = val
                max_gpu = g

        if max_kvpr < 1e-9: break # Perfect score

        # 2. Find Best Move
        best_move = None # (type, gain, args...)
        best_gain = 0.0

        src = gpu_states[max_gpu]

        # Helper to calculate gain
        def calc_gain(src_l_new, src_s_new, tgt_l_new, tgt_s_new, old_tgt_kvpr):
            ns_k = get_kvpr(src_l_new, src_s_new)
            nt_k = get_kvpr(tgt_l_new, tgt_s_new)
            new_peak = max(ns_k, nt_k)
            # We strictly want to reduce the global max
            if new_peak < max_kvpr - 1e-7:
                return max_kvpr - new_peak
            return -1.0

        # Move Types
        # A. Shift: Move item from src -> tgt
        for i, item in enumerate(src['items']):
            for t in range(gpu_num):
                if t == max_gpu: continue
                # Pruning: Target shouldn't be worse than current max
                if current_kvprs[t] >= max_kvpr: continue

                tgt = gpu_states[t]
                if tgt['s'] + item['s'] >= GPU_MEM_SIZE: continue

                gain = calc_gain(
                    src['l'] - item['l'], src['s'] - item['s'],
                    tgt['l'] + item['l'], tgt['s'] + item['s'],
                    current_kvprs[t]
                )
                if gain > best_gain:
                    best_gain = gain
                    best_move = ('shift', i, t)

        # B. Swap 1-1: Swap item src <-> item tgt
        for i, s_item in enumerate(src['items']):
            for t in range(gpu_num):
                if t == max_gpu: continue
                if current_kvprs[t] >= max_kvpr: continue

                tgt = gpu_states[t]
                for j, t_item in enumerate(tgt['items']):
                    # Check capacity
                    ns_s = src['s'] - s_item['s'] + t_item['s']
                    nt_s = tgt['s'] - t_item['s'] + s_item['s']

                    if ns_s >= GPU_MEM_SIZE or nt_s >= GPU_MEM_SIZE: continue

                    gain = calc_gain(
                        src['l'] - s_item['l'] + t_item['l'], ns_s,
                        tgt['l'] - t_item['l'] + s_item['l'], nt_s,
                        current_kvprs[t]
                    )
                    if gain > best_gain:
                        best_gain = gain
                        best_move = ('swap11', i, t, j)

        # C. Swap 2-1: Swap {item1, item2} src <-> {item3} tgt
        # Useful for defragmentation or load balancing
        if len(src['items']) >= 2:
            n_src = len(src['items'])
            # Limit checks to prevent O(N^3) explosion if N is large
            # But typically N is small per GPU
            for i1 in range(n_src):
                for i2 in range(i1 + 1, n_src):
                    itm1 = src['items'][i1]
                    itm2 = src['items'][i2]
                    pair_l = itm1['l'] + itm2['l']
                    pair_s = itm1['s'] + itm2['s']

                    for t in range(gpu_num):
                        if t == max_gpu: continue
                        if current_kvprs[t] >= max_kvpr: continue

                        tgt = gpu_states[t]
                        for j, t_itm in enumerate(tgt['items']):
                            ns_s = src['s'] - pair_s + t_itm['s']
                            nt_s = tgt['s'] - t_itm['s'] + pair_s

                            if ns_s >= GPU_MEM_SIZE or nt_s >= GPU_MEM_SIZE: continue

                            gain = calc_gain(
                                src['l'] - pair_l + t_itm['l'], ns_s,
                                tgt['l'] - t_itm['l'] + pair_l, nt_s,
                                current_kvprs[t]
                            )
                            if gain > best_gain:
                                best_gain = gain
                                best_move = ('swap21', i1, i2, t, j)

        # Execute Move
        if best_move:
            mtype = best_move[0]
            if mtype == 'shift':
                _, idx, t_id = best_move
                item = src['items'].pop(idx)
                tgt = gpu_states[t_id]

                src['l'] -= item['l']; src['s'] -= item['s']
                tgt['items'].append(item)
                tgt['l'] += item['l']; tgt['s'] += item['s']

            elif mtype == 'swap11':
                _, s_idx, t_id, t_idx = best_move
                tgt = gpu_states[t_id]
                s_item = src['items'][s_idx]
                t_item = tgt['items'][t_idx]

                src['items'][s_idx] = t_item
                tgt['items'][t_idx] = s_item

                src['l'] += t_item['l'] - s_item['l']; src['s'] += t_item['s'] - s_item['s']
                tgt['l'] += s_item['l'] - t_item['l']; tgt['s'] += s_item['s'] - t_item['s']

            elif mtype == 'swap21':
                _, i1, i2, t_id, t_idx = best_move
                tgt = gpu_states[t_id]

                # Pop strictly i2 > i1
                itm2 = src['items'].pop(i2)
                itm1 = src['items'].pop(i1)
                t_itm = tgt['items'].pop(t_idx)

                src['items'].append(t_itm)
                tgt['items'].extend([itm1, itm2])

                pair_l = itm1['l'] + itm2['l']; pair_s = itm1['s'] + itm2['s']
                src['l'] += t_itm['l'] - pair_l; src['s'] += t_itm['s'] - pair_s
                tgt['l'] += pair_l - t_itm['l']; tgt['s'] += pair_s - t_itm['s']
        else:
            break # No improvement found

    return {g: [x['model'] for x in gpu_states[g]['items']] for g in range(gpu_num)}
=======
import random

def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Algorithm: Multi-Start Perturbation-Based Iterated Local Search.
    1. Initialization: Randomized Bin Packing (Best Fit Decreasing) with Binary Search for K.
    2. Local Search: Steepest Descent on max KVPR using extensive neighborhoods:
       - Shift (1 -> 0)
       - Swap (1 <-> 1)
       - Swap (2 <-> 1)
       - Swap (2 <-> 2)
    3. Restarts: Run multiple times with randomized sorting to find better basins.
    """

    # Pre-calculate model properties
    model_data = []
    for i, m in enumerate(models):
        model_data.append({
            'model': m,
            'l': m.req_rate / m.slo,
            's': m.model_size,
            'id': i
        })

    def solve_packing(target_k, randomize=False):
        capacity = target_k * GPU_MEM_SIZE

        # Base weights: L + K*S
        items = []
        for x in model_data:
            w = x['l'] + target_k * x['s']
            if randomize:
                w *= random.uniform(0.95, 1.05)
            items.append((w, x))

        # Sort descending by weight
        items.sort(key=lambda x: x[0], reverse=True)

        bins_l = [0.0] * gpu_num
        bins_s = [0.0] * gpu_num
        bins_items = [[] for _ in range(gpu_num)]

        for _, item in items:
            best_bin = -1
            min_slack = float('inf')

            item_w = item['l'] + target_k * item['s']

            # Randomized start index for tie-breaking if randomize=True
            indices = list(range(gpu_num))
            if randomize:
                random.shuffle(indices)

            for b in indices:
                if bins_s[b] + item['s'] >= GPU_MEM_SIZE - 1e-6:
                    continue

                current_bin_w = bins_l[b] + target_k * bins_s[b]
                if current_bin_w + item_w <= capacity + 1e-9:
                    slack = capacity - (current_bin_w + item_w)
                    if slack < min_slack:
                        min_slack = slack
                        best_bin = b

            if best_bin != -1:
                bins_l[best_bin] += item['l']
                bins_s[best_bin] += item['s']
                bins_items[best_bin].append(item)
            else:
                return None

        return bins_items

    def get_kvpr(l, s):
        if s >= GPU_MEM_SIZE - 1e-6: return 1e15
        return l / (GPU_MEM_SIZE - s)

    # --- Phase 1: Determine Baseline K ---
    low, high = 0.0, 1.0
    for _ in range(20):
        if solve_packing(high) is not None: break
        low = high
        high *= 2.0
    else: high = 1e9

    for _ in range(20):
        mid = (low + high) / 2
        if solve_packing(mid) is not None: high = mid
        else: low = mid
    base_k = high

    best_global_plc = None
    best_global_score = float('inf')

    # --- Phase 2: Multi-Start ILS ---
    # Run 3 restarts
    for restart in range(3):
        # Generate initial solution
        randomize = (restart > 0)
        init_bins = solve_packing(base_k, randomize=randomize)

        # If random packing fails at base_k, relax slightly or skip
        if init_bins is None:
            if restart == 0:
                # Should not happen given Phase 1, but safety
                init_bins = solve_packing(high * 1.01)
                if init_bins is None: continue
            else:
                continue

        # Convert to mutable state
        gpu_states = []
        for g in range(gpu_num):
            items = init_bins[g]
            g_l = sum(x['l'] for x in items)
            g_s = sum(x['s'] for x in items)
            gpu_states.append({'l': g_l, 's': g_s, 'items': list(items)})

        current_max = max(get_kvpr(g['l'], g['s']) for g in gpu_states)

        # Steepest Descent Local Search
        iter_limit = 150
        for _ in range(iter_limit):
            if current_max < 1e-9: break

            # Find bottleneck
            max_val = -1.0
            max_gpu = -1
            gpu_kvprs = []
            for g in range(gpu_num):
                val = get_kvpr(gpu_states[g]['l'], gpu_states[g]['s'])
                gpu_kvprs.append(val)
                if val > max_val:
                    max_val = val
                    max_gpu = g

            best_move = None
            best_gain = 0.0

            src = gpu_states[max_gpu]
            src_n = len(src['items'])

            # Function to eval move
            def eval_state(s_l, s_s, t_l, t_s):
                if s_s >= GPU_MEM_SIZE or t_s >= GPU_MEM_SIZE: return -1.0
                nk_s = get_kvpr(s_l, s_s)
                nk_t = get_kvpr(t_l, t_s)
                new_peak = max(nk_s, nk_t)
                if new_peak < current_max - 1e-7:
                    return current_max - new_peak
                return -1.0

            # Scan targets
            for t in range(gpu_num):
                if t == max_gpu: continue
                # Skip if target is already close to max (heuristic pruning)
                if gpu_kvprs[t] > current_max * 0.95: continue

                tgt = gpu_states[t]
                tgt_n = len(tgt['items'])

                # 1. Shift
                for i in range(src_n):
                    itm = src['items'][i]
                    gain = eval_state(src['l']-itm['l'], src['s']-itm['s'],
                                    tgt['l']+itm['l'], tgt['s']+itm['s'])
                    if gain > best_gain:
                        best_gain = gain
                        best_move = ('shift', i, t)

                # 2. Swap 1-1
                for i in range(src_n):
                    si = src['items'][i]
                    for j in range(tgt_n):
                        tj = tgt['items'][j]
                        gain = eval_state(
                            src['l']-si['l']+tj['l'], src['s']-si['s']+tj['s'],
                            tgt['l']-tj['l']+si['l'], tgt['s']-tj['s']+si['s']
                        )
                        if gain > best_gain:
                            best_gain = gain
                            best_move = ('swap11', i, t, j)

                # 3. Swap 2-1 (2 from src, 1 from tgt)
                if src_n >= 2:
                    for i1 in range(src_n):
                        for i2 in range(i1+1, src_n):
                            s1, s2 = src['items'][i1], src['items'][i2]
                            pl, ps = s1['l']+s2['l'], s1['s']+s2['s']
                            for j in range(tgt_n):
                                tj = tgt['items'][j]
                                gain = eval_state(
                                    src['l']-pl+tj['l'], src['s']-ps+tj['s'],
                                    tgt['l']-tj['l']+pl, tgt['s']-tj['s']+ps
                                )
                                if gain > best_gain:
                                    best_gain = gain
                                    best_move = ('swap21', i1, i2, t, j)

                # 4. Swap 2-2 (2 from src, 2 from tgt)
                if src_n >= 2 and tgt_n >= 2:
                     for i1 in range(src_n):
                        for i2 in range(i1+1, src_n):
                            s1, s2 = src['items'][i1], src['items'][i2]
                            sl, ss = s1['l']+s2['l'], s1['s']+s2['s']
                            for j1 in range(tgt_n):
                                for j2 in range(j1+1, tgt_n):
                                    t1, t2 = tgt['items'][j1], tgt['items'][j2]
                                    tl, ts = t1['l']+t2['l'], t1['s']+t2['s']

                                    gain = eval_state(
                                        src['l']-sl+tl, src['s']-ss+ts,
                                        tgt['l']-tl+sl, tgt['s']-ts+ss
                                    )
                                    if gain > best_gain:
                                        best_gain = gain
                                        best_move = ('swap22', i1, i2, t, j1, j2)

            if best_move:
                mtype = best_move[0]
                if mtype == 'shift':
                    _, i, t = best_move
                    itm = src['items'].pop(i)
                    tgt = gpu_states[t]
                    src['l']-=itm['l']; src['s']-=itm['s']
                    tgt['items'].append(itm)
                    tgt['l']+=itm['l']; tgt['s']+=itm['s']

                elif mtype == 'swap11':
                    _, i, t, j = best_move
                    tgt = gpu_states[t]
                    s_itm = src['items'][i]
                    t_itm = tgt['items'][j]
                    src['items'][i] = t_itm
                    tgt['items'][j] = s_itm

                    src['l'] += t_itm['l'] - s_itm['l']; src['s'] += t_itm['s'] - s_itm['s']
                    tgt['l'] += s_itm['l'] - t_itm['l']; tgt['s'] += s_itm['s'] - t_itm['s']

                elif mtype == 'swap21':
                    _, i1, i2, t, j = best_move
                    tgt = gpu_states[t]
                    # pop max index first
                    s2 = src['items'].pop(i2)
                    s1 = src['items'].pop(i1)
                    t1 = tgt['items'].pop(j)

                    src['items'].append(t1)
                    tgt['items'].extend([s1, s2])

                    # Full recalc simpler
                    src['l'] = sum(x['l'] for x in src['items']); src['s'] = sum(x['s'] for x in src['items'])
                    tgt['l'] = sum(x['l'] for x in tgt['items']); tgt['s'] = sum(x['s'] for x in tgt['items'])

                elif mtype == 'swap22':
                    _, i1, i2, t, j1, j2 = best_move
                    tgt = gpu_states[t]

                    s2 = src['items'].pop(i2)
                    s1 = src['items'].pop(i1)
                    t2 = tgt['items'].pop(j2)
                    t1 = tgt['items'].pop(j1)

                    src['items'].extend([t1, t2])
                    tgt['items'].extend([s1, s2])

                    src['l'] = sum(x['l'] for x in src['items']); src['s'] = sum(x['s'] for x in src['items'])
                    tgt['l'] = sum(x['l'] for x in tgt['items']); tgt['s'] = sum(x['s'] for x in tgt['items'])

                # Recompute current max
                current_max = max(get_kvpr(g['l'], g['s']) for g in gpu_states)
            else:
                break

        if current_max < best_global_score:
            best_global_score = current_max
            best_global_plc = {g: [x['model'] for x in gpu_states[g]['items']] for g in range(gpu_num)}

    if best_global_plc is None:
        raise ValueError("Could not find valid placement")

    return best_global_plc
>>>>>>> REPLACE
</DIFF>