# EVOLVE-BLOCK-START
"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""

GPU_MEM_SIZE = 80.0  # GB
import math
import random

def compute_model_placement(gpu_num, models):
    """
    Algorithm:
    1. Multi-Start Initialization:
       - Binary Search Linearization with Randomized Refinement:
         First finds an approximate K balancing Load vs Size, then runs multiple
         randomized bin-packing attempts around that K to break symmetry.
       - Greedy Strategies: Size, Load, Density.
    2. Iterated Local Search (ILS):
       - Focuses on the top bottleneck GPUs.
       - Operators:
         a) Move (Steepest Descent): Evaluates all moves from bottlenecks to find the best reduction.
         b) Swap 2-1 (First Improvement): Moves 2 small models out, brings 1 back.
         c) Swap 1-1 (First Improvement).
         d) Swap 1-2 (First Improvement): Moves 1 large model out, brings 2 back.
       - Perturbation (Ruins & Recreate):
         Completely empties the worst bottleneck GPU and attempts to redistribute
         its models to other GPUs using a Best-Fit heuristic.
    """

    class GPUState:
        def __init__(self, gpu_id):
            self.id = gpu_id
            self.models = []
            self.load = 0.0
            self.used_mem = 0.0
            self._cached_kvpr = 0.0
            self._cached_rem = GPU_MEM_SIZE 

        def update_cache(self):
            self._cached_rem = GPU_MEM_SIZE - self.used_mem
            if self._cached_rem <= 1e-7:
                self._cached_kvpr = float('inf')
            else:
                self._cached_kvpr = self.load / self._cached_rem

        def can_fit(self, size):
            return self.used_mem + size <= GPU_MEM_SIZE

        def add(self, model):
            self.models.append(model)
            self.load += model.req_rate / model.slo
            self.used_mem += model.model_size
            self.update_cache()

        def remove(self, idx):
            model = self.models.pop(idx)
            self.load -= model.req_rate / model.slo
            self.used_mem -= model.model_size
            self.update_cache()
            return model
            
        def kvpr(self):
            return self._cached_kvpr

        def restore_model(self, idx, model):
            self.models.insert(idx, model)
            self.load += model.req_rate / model.slo
            self.used_mem += model.model_size
            self.update_cache()
            
        def copy_from(self, other):
            self.models = list(other.models)
            self.load = other.load
            self.used_mem = other.used_mem
            self._cached_kvpr = other._cached_kvpr
            self._cached_rem = other._cached_rem

    def get_vector(gpus):
        # Lexicographical vector: (max_kvpr, 2nd_max_kvpr, ...)
        return tuple(sorted((g.kvpr() for g in gpus), reverse=True))

    candidates = []

    # -------------------------------------------------------------------------
    # 1. Initialization
    # -------------------------------------------------------------------------
    
    # Strategy A: Binary Search Linearization + Randomized Refinement
    def solve_linearized_bp(target_k, randomness=0.0):
        bin_cap = target_k * GPU_MEM_SIZE
        items = []
        for m in models:
            # Weight = Load + K * Size
            w = (m.req_rate / m.slo) + target_k * m.model_size
            if randomness > 0:
                w *= random.uniform(1.0 - randomness, 1.0 + randomness)
            items.append((w, m))
        items.sort(key=lambda x: x[0], reverse=True)
        
        bins = [GPUState(i) for i in range(gpu_num)]
        for w, m in items:
            best_idx = -1
            min_rem_linear = float('inf')
            for i in range(gpu_num):
                if not bins[i].can_fit(m.model_size): continue
                # Linear usage check using deterministic values for constraint
                lin_use = bins[i].load + target_k * bins[i].used_mem
                item_lin = (m.req_rate / m.slo) + target_k * m.model_size
                
                if lin_use + item_lin <= bin_cap:
                    rem = bin_cap - (lin_use + item_lin)
                    if rem < min_rem_linear:
                        min_rem_linear = rem
                        best_idx = i
            if best_idx != -1:
                bins[best_idx].add(m)
            else:
                return None
        return bins

    # Binary Search for K
    low, high = 0.0, 1000.0
    if solve_linearized_bp(high) is None: high = 1e9
    
    best_k = None
    bs_res = None
    
    for _ in range(20):
        mid = (low + high) / 2
        res = solve_linearized_bp(mid)
        if res:
            bs_res = res
            best_k = mid
            high = mid
        else:
            low = mid
            
    if bs_res: 
        candidates.append(bs_res)
        # Randomized Refinement around best_k
        if best_k is not None:
            # Try around the found K with some noise to item weights
            search_k = best_k * 1.05 
            for _ in range(10):
                res_rand = solve_linearized_bp(search_k, randomness=0.05)
                if res_rand: candidates.append(res_rand)

    # Strategy B: Deterministic Greedy
    strategies = [
        lambda m: m.model_size,
        lambda m: m.req_rate / m.slo,
        lambda m: (m.req_rate / m.slo) / m.model_size if m.model_size > 1e-7 else 0
    ]
    for key_fn in strategies:
        gpus = [GPUState(i) for i in range(gpu_num)]
        valid = True
        for m in sorted(models, key=key_fn, reverse=True):
            best_idx = -1
            best_val = float('inf')
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (gpus[i].used_mem + m.model_size)
                    val = (gpus[i].load + m.req_rate/m.slo) / rem if rem > 1e-7 else float('inf')
                    if val < best_val:
                        best_val = val
                        best_idx = i
            if best_idx != -1: gpus[best_idx].add(m)
            else: valid = False; break
        if valid: candidates.append(gpus)

    # Fallback
    if not candidates:
        gpus = [GPUState(i) for i in range(gpu_num)]
        for m in sorted(models, key=lambda x: x.model_size, reverse=True):
            placed = False
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    gpus[i].add(m); placed = True; break
            if not placed: raise ValueError("Models do not fit in GPU memory.")
        candidates.append(gpus)

    # Select Best Start
    current_gpus = min(candidates, key=lambda g: get_vector(g))
    current_vector = get_vector(current_gpus)
    
    best_gpus = [GPUState(i) for i in range(gpu_num)]
    for i in range(gpu_num): best_gpus[i].copy_from(current_gpus[i])
    best_vector = current_vector

    # -------------------------------------------------------------------------
    # 2. Iterated Local Search
    # -------------------------------------------------------------------------
    iter_cnt = 0
    max_iter = 150
    
    while iter_cnt < max_iter:
        improved_step = False
        iter_cnt += 1
        
        sorted_gpus = sorted(current_gpus, key=lambda g: g.kvpr(), reverse=True)
        sources = sorted_gpus[:4] 
        destinations = sorted_gpus[::-1] 
        
        # --- Operator 1: Move (Steepest Descent on Bottlenecks) ---
        best_move = None
        best_move_gain = current_vector
        
        for source in sources:
            for i, model in enumerate(source.models):
                for dest in current_gpus:
                    if dest.id == source.id: continue
                    if dest.can_fit(model.model_size):
                        source.remove(i)
                        dest.add(model)
                        
                        new_vec = get_vector(current_gpus)
                        if new_vec < best_move_gain:
                            best_move_gain = new_vec
                            best_move = (source, i, dest, model)
                            
                        dest.remove(len(dest.models)-1)
                        source.restore_model(i, model)
        
        if best_move:
            src, idx, dst, model = best_move
            src.remove(idx)
            dst.add(model)
            current_vector = best_move_gain
            improved_step = True
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
        
        if improved_step: continue

        # --- Operator 2: Swap 2-1 (Source gives 2, Dest gives 1) ---
        for source in sources[:2]:
            if len(source.models) < 2: continue
            for i1 in range(len(source.models)):
                for i2 in range(i1+1, len(source.models)):
                    m_a1 = source.models[i1]
                    m_a2 = source.models[i2]
                    for dest in destinations:
                        if dest.id == source.id: continue
                        if dest.kvpr() >= source.kvpr(): continue
                        for j, m_b in enumerate(dest.models):
                            s_mem = source.used_mem - m_a1.model_size - m_a2.model_size + m_b.model_size
                            d_mem = dest.used_mem - m_b.model_size + m_a1.model_size + m_a2.model_size
                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                source.remove(i2) # Remove higher index first
                                source.remove(i1)
                                dest.remove(j)
                                source.add(m_b)
                                dest.add(m_a1)
                                dest.add(m_a2)
                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    if current_vector < best_vector:
                                        best_vector = current_vector
                                        for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
                                    break
                                else:
                                    dest.remove(len(dest.models)-1)
                                    dest.remove(len(dest.models)-1)
                                    source.remove(len(source.models)-1)
                                    dest.restore_model(j, m_b)
                                    source.restore_model(i1, m_a1)
                                    source.restore_model(i2, m_a2)
                        if improved_step: break
                    if improved_step: break
                if improved_step: break
            if improved_step: break
        if improved_step: continue
        
        # --- Operator 3: Swap 1-1 ---
        for source in sources:
            for i, m_a in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue
                    for j, m_b in enumerate(dest.models):
                        s_mem = source.used_mem - m_a.model_size + m_b.model_size
                        d_mem = dest.used_mem - m_b.model_size + m_a.model_size
                        if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                            source.remove(i)
                            dest.remove(j)
                            source.add(m_b)
                            dest.add(m_a)
                            new_vec = get_vector(current_gpus)
                            if new_vec < current_vector:
                                current_vector = new_vec
                                improved_step = True
                                if current_vector < best_vector:
                                    best_vector = current_vector
                                    for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
                                break
                            else:
                                dest.remove(len(dest.models)-1)
                                source.remove(len(source.models)-1)
                                dest.restore_model(j, m_b)
                                source.restore_model(i, m_a)
                    if improved_step: break
                if improved_step: break
        if improved_step: continue

        # --- Operator 4: Swap 1-2 (Source gives 1, Dest gives 2) ---
        for source in sources[:2]:
            for i, m_a in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue
                    if len(dest.models) < 2: continue
                    
                    n_d = len(dest.models)
                    for j1 in range(n_d):
                        for j2 in range(j1+1, n_d):
                            m_b1 = dest.models[j1]
                            m_b2 = dest.models[j2]
                            s_mem = source.used_mem - m_a.model_size + m_b1.model_size + m_b2.model_size
                            d_mem = dest.used_mem - m_b1.model_size - m_b2.model_size + m_a.model_size
                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                source.remove(i)
                                dest.remove(j2)
                                dest.remove(j1)
                                source.add(m_b1)
                                source.add(m_b2)
                                dest.add(m_a)
                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    if current_vector < best_vector:
                                        best_vector = current_vector
                                        for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
                                    break
                                else:
                                    dest.remove(len(dest.models)-1)
                                    source.remove(len(source.models)-1)
                                    source.remove(len(source.models)-1)
                                    dest.restore_model(j1, m_b1)
                                    dest.restore_model(j2, m_b2)
                        if improved_step: break
                    if improved_step: break
                if improved_step: break
        if improved_step: continue

        # --- Perturbation (Ruins and Recreate) ---
        if iter_cnt > max_iter - 10: break
        
        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break
        
        # Ruins: Remove all models from worst GPU
        removed_models = []
        while worst_gpu.models:
            removed_models.append(worst_gpu.remove(0))
        
        # Recreate: Try to place them elsewhere
        # Shuffle to break cycles
        random.shuffle(removed_models)
        
        for m in removed_models:
            best_dest_idx = -1
            best_dest_val = float('inf')
            
            # Find Best Fit GPU among others
            for dest in current_gpus:
                if dest.id == worst_gpu.id: continue
                if dest.can_fit(m.model_size):
                    # Check what the pressure would be
                    rem = GPU_MEM_SIZE - (dest.used_mem + m.model_size)
                    if rem > 1e-7:
                        val = (dest.load + m.req_rate/m.slo) / rem
                        if val < best_dest_val:
                            best_dest_val = val
                            best_dest_idx = dest.id
            
            if best_dest_idx != -1:
                # Add to best found
                for g in current_gpus:
                    if g.id == best_dest_idx:
                        g.add(m)
                        break
            else:
                # If no fit elsewhere, put back in worst_gpu (it was there before, so it fits)
                worst_gpu.add(m)
        
        current_vector = get_vector(current_gpus)
        # If perturbation made things better by luck
        if current_vector < best_vector:
            best_vector = current_vector
            for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])

    return {g.id: g.models for g in best_gpus}
# EVOLVE-BLOCK-END