<NAME>
optimize_init_and_perturbation
</NAME>

<DESCRIPTION>
This edit introduces three key improvements to maximize KVPR reduction:
1.  **Enhanced Initialization**: Increases the diversity of the initial population by tripling the number of noisy candidates generated from the linearized bin packing and widening the noise range (0.85-1.15). This helps find a better starting basin of attraction.
2.  **Expanded Local Search**: Increases the maximum iterations from 150 to 300 and expands the neighborhood search for swapping operators to the top 6 bottleneck GPUs (from 4), utilizing the available computational budget (current execution time is ~0.035s).
3.  **Alternating Dual-Ruin Perturbation**: Implements a robust perturbation strategy that:
    *   Always clears the bottleneck GPU.
    *   Probabilistically (25% chance) clears the lightest GPU as well, creating a larger pool of free space for redistribution.
    *   Alternates the sorting criteria for reconstruction between 'Size' (packing efficiency) and 'Load' (load balancing) to prevent getting stuck in local optima favored by a single heuristic.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Generate candidates around best_k
    if best_k < 1e8:
        # Base solution
        res = solve_linearized_bp(best_k)
        if res: candidates.append(res)
        # Noisy variations
        for _ in range(10):
            k_noisy = best_k * random.uniform(0.95, 1.1)
            res_noise = solve_linearized_bp(k_noisy, noise=0.05)
            if res_noise: candidates.append(res_noise)
=======
    # Generate candidates around best_k
    if best_k < 1e8:
        # Base solution
        res = solve_linearized_bp(best_k)
        if res: candidates.append(res)
        # Noisy variations with wider range and more samples
        for _ in range(20):
            k_noisy = best_k * random.uniform(0.85, 1.15)
            res_noise = solve_linearized_bp(k_noisy, noise=0.08)
            if res_noise: candidates.append(res_noise)
>>>>>>> REPLACE
<<<<<<< SEARCH
    # -------------------------------------------------------------------------
    # 2. Iterated Local Search (ILS)
    # -------------------------------------------------------------------------
    iter_cnt = 0
    max_iter = 150

    while iter_cnt < max_iter:
        improved_step = False
        sorted_gpus = sorted(current_gpus, key=lambda g: g.kvpr(), reverse=True)
        sources = sorted_gpus[:4] # Top bottlenecks
        destinations = sorted_gpus[::-1] # Least loaded first
=======
    # -------------------------------------------------------------------------
    # 2. Iterated Local Search (ILS)
    # -------------------------------------------------------------------------
    iter_cnt = 0
    max_iter = 300  # Increased iterations

    while iter_cnt < max_iter:
        improved_step = False
        sorted_gpus = sorted(current_gpus, key=lambda g: g.kvpr(), reverse=True)
        sources = sorted_gpus[:6] # Look deeper into bottlenecks
        destinations = sorted_gpus[::-1] # Least loaded first
>>>>>>> REPLACE
<<<<<<< SEARCH
        # --- Swap 1-2 (First Improvement) ---
        for source in sources[:4]: # Expanded search
            for i, ma in enumerate(source.models):
                for dest in destinations:
=======
        # --- Swap 1-2 (First Improvement) ---
        for source in sources[:5]: # Expanded search
            for i, ma in enumerate(source.models):
                for dest in destinations:
>>>>>>> REPLACE
<<<<<<< SEARCH
        # --- Swap 2-1 (First Improvement) ---
        for source in sources[:4]: # Expanded search
            if len(source.models) < 2: continue
            n_s = len(source.models)
=======
        # --- Swap 2-1 (First Improvement) ---
        for source in sources[:5]: # Expanded search
            if len(source.models) < 2: continue
            n_s = len(source.models)
>>>>>>> REPLACE
<<<<<<< SEARCH
        # --- Perturbation (Ruin Worst Only) ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        # Ruin
        displaced = []
        while worst_gpu.models: displaced.append(worst_gpu.remove(0))

        # Recreate: Sort by Size (Packing) with noise to create variation
        displaced.sort(key=lambda m: m.model_size * random.uniform(0.95, 1.05), reverse=True)

        # Insert into any OTHER gpu (or worst if must)
        targets = [g for g in current_gpus if g.id != worst_gpu.id]

        for m in displaced:
            best_dest = None
            best_dest_val = float('inf')
            for dest in targets:
                if dest.can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (dest.used_mem + m.model_size)
                    if rem > 1e-7:
                        val = (dest.load + m.req_rate/m.slo) / rem
                        if val < best_dest_val:
                            best_dest_val = val
                            best_dest = dest

            if best_dest: best_dest.add(m)
            else: worst_gpu.add(m)

        current_vector = get_vector(current_gpus)
        if current_vector < best_vector:
            best_vector = current_vector
            for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
=======
        # --- Perturbation (Alternating Dual Ruin) ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        displaced = []
        # Ruin the bottleneck
        while worst_gpu.models: displaced.append(worst_gpu.remove(0))

        # Occasionally ruin the lightest GPU to open up space (Probabilistic Dual Ruin)
        if random.random() < 0.25 and len(sorted_gpus) > 1:
            best_gpu = sorted_gpus[-1]
            if best_gpu.id != worst_gpu.id:
                while best_gpu.models: displaced.append(best_gpu.remove(0))

        # Recreate: Alternating Strategy (Size vs Load)
        if iter_cnt % 2 == 0:
            # Sort by Size (Packing efficiency)
            displaced.sort(key=lambda m: m.model_size * random.uniform(0.95, 1.05), reverse=True)
        else:
            # Sort by Load (Load Balancing)
            displaced.sort(key=lambda m: m.req_rate / m.slo, reverse=True)

        # Insert into ANY gpu
        for m in displaced:
            best_dest = None
            best_dest_val = float('inf')

            for dest in current_gpus:
                if dest.can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (dest.used_mem + m.model_size)
                    if rem > 1e-7:
                        val = (dest.load + m.req_rate/m.slo) / rem
                        if val < best_dest_val:
                            best_dest_val = val
                            best_dest = dest

            if best_dest:
                best_dest.add(m)
            else:
                # Fallback
                if worst_gpu.can_fit(m.model_size): worst_gpu.add(m)
                else:
                    for g in current_gpus:
                        if g.can_fit(m.model_size): g.add(m); break

        current_vector = get_vector(current_gpus)
        if current_vector < best_vector:
            best_vector = current_vector
            for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
>>>>>>> REPLACE
</DIFF>