<NAME>
robust_ils_with_time_budget
</NAME>

<DESCRIPTION>
Replaces the existing algorithm with a robust Iterated Local Search (ILS) framework that runs within a time budget (0.5s).
1. Uses Binary Search to find a tight KVPR bound.
2. Initializes with multiple randomized constructions to find a favorable starting basin.
3. Performs Steepest Descent Local Search with Move, Swap(1-1), and Swap(2-1) neighborhoods.
4. Applies a 'Ruins & Recreate' perturbation when local search stagnates, ejecting models from bottleneck GPUs and re-inserting them greedily into other GPUs to escape local optima.
5. Continues this process until the time budget is exhausted, ensuring available computational resources are utilized to refine the solution.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""

import random

GPU_MEM_SIZE = 80  # GB

def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Combines Binary Search for initialization with Iterated Local Search (ILS)
    using Move, Swap(1-1), and Swap(2-1) neighborhoods.
    """

    # Pre-calculate model properties for efficiency
    model_data = []
    for m in models:
        model_data.append({
            'model': m,
            'l': m.req_rate / m.slo,
            's': m.model_size
        })

    # --- Phase 1: Construction (Binary Search) ---
    # We look for the smallest K such that all models fit with KVPR <= K.

    def solve_packing(target_k):
        """
        Tries to pack models such that sum(L)/(M-sum(S)) <= K.
        Linearized constraint: sum(L) + K*sum(S) <= K*M.
        """
        capacity = target_k * GPU_MEM_SIZE

        # Heuristics for sorting items
        # 1. Linearized Weight: balances L and S based on K
        s1 = sorted(model_data, key=lambda x: x['l'] + target_k * x['s'], reverse=True)
        # 2. Size: Good for memory-bound
        s2 = sorted(model_data, key=lambda x: x['s'], reverse=True)
        # 3. Load: Good for load-bound
        s3 = sorted(model_data, key=lambda x: x['l'], reverse=True)
        # 4. Density: Load/Size
        s4 = sorted(model_data, key=lambda x: x['l'] / x['s'] if x['s'] > 0 else 0, reverse=True)

        strategies = [s1, s2, s3, s4]

        for items in strategies:
            gpu_l = [0.0] * gpu_num
            gpu_s = [0.0] * gpu_num
            gpu_models = [[] for _ in range(gpu_num)]

            feasible = True
            for item in items:
                # Best Fit
                best_idx = -1
                min_rem = float('inf')

                w = item['l'] + target_k * item['s']

                for i in range(gpu_num):
                    if gpu_s[i] + item['s'] >= GPU_MEM_SIZE - 1e-6:
                        continue

                    curr_w = gpu_l[i] + target_k * gpu_s[i]
                    if curr_w + w <= capacity + 1e-9:
                        # Remaining capacity in linearized terms
                        rem = capacity - (curr_w + w)
                        if rem < min_rem:
                            min_rem = rem
                            best_idx = i

                if best_idx != -1:
                    gpu_l[best_idx] += item['l']
                    gpu_s[best_idx] += item['s']
                    gpu_models[best_idx].append(item['model'])
                else:
                    feasible = False
                    break

            if feasible:
                return gpu_models
        return None

    # Binary search
    low = 0.0
    high = 1.0
    best_init = None

    # Exponential search for upper bound
    for _ in range(20):
        if solve_packing(high) is not None:
            break
        low = high
        high *= 2.0
    else:
        high = 1e9 # Should always fit if physically possible

    # Refine K
    for _ in range(30):
        mid = (low + high) / 2
        res = solve_packing(mid)
        if res:
            best_init = res
            high = mid
        else:
            low = mid

    if best_init is None:
        best_init = solve_packing(high)
        if best_init is None:
             raise ValueError("Unable to find a valid placement.")

    # Convert to mutable structure for Local Search
    placement = {i: best_init[i] for i in range(gpu_num)}

    # --- Phase 2: Iterated Local Search ---

    def get_kvpr(l, s):
        if s >= GPU_MEM_SIZE - 1e-7: return 1e15
        return l / (GPU_MEM_SIZE - s)

    def local_search(plc):
        # State: list of lists of models
        state = [list(plc[i]) for i in range(gpu_num)]
        # Track Load (l) and Size (s) for each GPU
        l_vec = [sum(m.req_rate/m.slo for m in state[g]) for g in range(gpu_num)]
        s_vec = [sum(m.model_size for m in state[g]) for g in range(gpu_num)]

        # Compute current max KVPR
        cur_max_k = max(get_kvpr(l_vec[g], s_vec[g]) for g in range(gpu_num))

        iterations = 0
        while iterations < 150: # Cap iterations
            iterations += 1
            if cur_max_k < 1e-9: break

            # Identify bottleneck GPU(s)
            candidates = [g for g in range(gpu_num) if get_kvpr(l_vec[g], s_vec[g]) >= cur_max_k - 1e-6]
            if not candidates: break

            bottleneck = candidates[0]
            src_l = l_vec[bottleneck]
            src_s = s_vec[bottleneck]

            best_move = None
            # We want to maximize the reduction of the bottleneck's pressure
            # without making another GPU worse than the current max.
            best_improvement = 0.0

            # 1. Move
            for i, m in enumerate(state[bottleneck]):
                m_l = m.req_rate/m.slo
                m_s = m.model_size

                for tgt in range(gpu_num):
                    if tgt == bottleneck: continue
                    if get_kvpr(l_vec[tgt], s_vec[tgt]) >= cur_max_k: continue # Skip if target is bad
                    if s_vec[tgt] + m_s >= GPU_MEM_SIZE: continue

                    new_src_k = get_kvpr(src_l - m_l, src_s - m_s)
                    new_tgt_k = get_kvpr(l_vec[tgt] + m_l, s_vec[tgt] + m_s)

                    new_global = max(new_src_k, new_tgt_k)
                    if new_global < cur_max_k - 1e-7:
                        imp = cur_max_k - new_global
                        if imp > best_improvement:
                            best_improvement = imp
                            best_move = ('move', bottleneck, i, tgt)

            # 2. Swap (1-1)
            for i, m1 in enumerate(state[bottleneck]):
                m1_l = m1.req_rate/m1.slo
                m1_s = m1.model_size

                for tgt in range(gpu_num):
                    if tgt == bottleneck: continue
                    if get_kvpr(l_vec[tgt], s_vec[tgt]) >= cur_max_k: continue

                    for j, m2 in enumerate(state[tgt]):
                        m2_l = m2.req_rate/m2.slo
                        m2_s = m2.model_size

                        # Verify capacity
                        if src_s - m1_s + m2_s >= GPU_MEM_SIZE: continue
                        if s_vec[tgt] - m2_s + m1_s >= GPU_MEM_SIZE: continue

                        new_src_k = get_kvpr(src_l - m1_l + m2_l, src_s - m1_s + m2_s)
                        new_tgt_k = get_kvpr(l_vec[tgt] - m2_l + m1_l, s_vec[tgt] - m2_s + m1_s)

                        new_global = max(new_src_k, new_tgt_k)
                        if new_global < cur_max_k - 1e-7:
                            imp = cur_max_k - new_global
                            if imp > best_improvement:
                                best_improvement = imp
                                best_move = ('swap', bottleneck, i, tgt, j)

            # 3. Swap (2-1): 2 from bottleneck, 1 from target
            # Only check if we have enough models
            if len(state[bottleneck]) >= 2:
                for i1 in range(len(state[bottleneck])):
                    for i2 in range(i1 + 1, len(state[bottleneck])):
                        m1 = state[bottleneck][i1]
                        m2 = state[bottleneck][i2]

                        pair_l = (m1.req_rate/m1.slo) + (m2.req_rate/m2.slo)
                        pair_s = m1.model_size + m2.model_size

                        for tgt in range(gpu_num):
                            if tgt == bottleneck: continue
                            if get_kvpr(l_vec[tgt], s_vec[tgt]) >= cur_max_k: continue

                            for j, m3 in enumerate(state[tgt]):
                                m3_l = m3.req_rate/m3.slo
                                m3_s = m3.model_size

                                # Capacity
                                if src_s - pair_s + m3_s >= GPU_MEM_SIZE: continue
                                if s_vec[tgt] - m3_s + pair_s >= GPU_MEM_SIZE: continue

                                new_src_k = get_kvpr(src_l - pair_l + m3_l, src_s - pair_s + m3_s)
                                new_tgt_k = get_kvpr(l_vec[tgt] - m3_l + pair_l, s_vec[tgt] - m3_s + pair_s)

                                new_global = max(new_src_k, new_tgt_k)
                                if new_global < cur_max_k - 1e-7:
                                    imp = cur_max_k - new_global
                                    if imp > best_improvement:
                                        best_improvement = imp
                                        best_move = ('swap21', bottleneck, i1, i2, tgt, j)

            if best_move:
                mtype = best_move[0]
                if mtype == 'move':
                    _, b, i, t = best_move
                    m = state[b].pop(i)
                    state[t].append(m)

                    l_vec[b] -= m.req_rate/m.slo
                    s_vec[b] -= m.model_size
                    l_vec[t] += m.req_rate/m.slo
                    s_vec[t] += m.model_size

                elif mtype == 'swap':
                    _, b, i, t, j = best_move
                    m1 = state[b][i]
                    m2 = state[t][j]
                    state[b][i] = m2
                    state[t][j] = m1

                    diff_l = (m2.req_rate/m2.slo) - (m1.req_rate/m1.slo)
                    diff_s = m2.model_size - m1.model_size
                    l_vec[b] += diff_l
                    s_vec[b] += diff_s
                    l_vec[t] -= diff_l
                    s_vec[t] -= diff_s

                elif mtype == 'swap21':
                    _, b, i1, i2, t, j = best_move
                    # i1 < i2, pop larger index first
                    m2 = state[b].pop(i2)
                    m1 = state[b].pop(i1)
                    m3 = state[t].pop(j)

                    state[b].append(m3)
                    state[t].append(m1)
                    state[t].append(m2)

                    pair_l = (m1.req_rate/m1.slo) + (m2.req_rate/m2.slo)
                    pair_s = m1.model_size + m2.model_size
                    m3_l = m3.req_rate/m3.slo
                    m3_s = m3.model_size

                    l_vec[b] = l_vec[b] - pair_l + m3_l
                    s_vec[b] = s_vec[b] - pair_s + m3_s
                    l_vec[t] = l_vec[t] - m3_l + pair_l
                    s_vec[t] = s_vec[t] - m3_s + pair_s

                # Recompute global max
                cur_max_k = max(get_kvpr(l_vec[g], s_vec[g]) for g in range(gpu_num))
            else:
                # Local optima reached
                break

        return {i: state[i] for i in range(gpu_num)}, cur_max_k

    # ILS Driver
    current_plc = placement
    current_score = 0
    # Calculate initial score
    init_stats = [get_kvpr(sum(m.req_rate/m.slo for m in current_plc[g]),
                          sum(m.model_size for m in current_plc[g])) for g in range(gpu_num)]
    current_score = max(init_stats)

    best_plc = current_plc
    best_score = current_score

    # 5 Restarts with perturbation
    for _ in range(5):
        # Local Search
        refined_plc, refined_score = local_search(current_plc)

        if refined_score < best_score:
            best_score = refined_score
            best_plc = refined_plc

        current_plc = refined_plc
        current_score = refined_score

        # Perturbation: Move a model from bottleneck to a random feasible GPU
        # Find bottleneck in current solution
        stats = []
        max_k = -1
        b_gpu = -1
        for g in range(gpu_num):
            l = sum(m.req_rate/m.slo for m in current_plc[g])
            s = sum(m.model_size for m in current_plc[g])
            k = get_kvpr(l, s)
            stats.append({'l':l, 's':s, 'k':k})
            if k > max_k:
                max_k = k
                b_gpu = g

        if max_k == 0: break

        # Try to move a random model from bottleneck
        if b_gpu != -1 and current_plc[b_gpu]:
            # Convert to list for mutation
            mut_state = [list(current_plc[g]) for g in range(gpu_num)]

            # Pick a model to move
            idxs = list(range(len(mut_state[b_gpu])))
            random.shuffle(idxs)
            moved = False
            for idx in idxs:
                m = mut_state[b_gpu][idx]
                # Try random targets
                targets = list(range(gpu_num))
                random.shuffle(targets)
                for t in targets:
                    if t == b_gpu: continue
                    t_s = stats[t]['s']
                    if t_s + m.model_size < GPU_MEM_SIZE:
                        # Apply perturbation
                        mut_state[b_gpu].pop(idx)
                        mut_state[t].append(m)
                        moved = True
                        break
                if moved: break

            if moved:
                current_plc = {g: mut_state[g] for g in range(gpu_num)}
            else:
                # If cannot move, maybe try random swap?
                # For now just continue, next local search might find something or just end.
                pass

    return best_plc
=======
"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""

import random
import time

GPU_MEM_SIZE = 80.0

def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Algorithm:
    1. Multi-Start Randomized Construction:
       - Uses Binary Search to find a baseline 'target K'.
       - Generates multiple initial solutions by adding random noise to sorting weights
         during Bin Packing. Selects the best start.
    2. Iterated Local Search (ILS):
       - Uses Steepest Descent (Best Improvement) on bottleneck GPUs.
       - Neighborhoods: Move, Swap(1-1), Swap(2-1).
       - Perturbation: Ruins & Recreate (LNS) on bottlenecks when stuck.
       - Runs within a 0.5s time budget.
    """
    start_time = time.time()

    # Pre-process models
    model_data = []
    for i, m in enumerate(models):
        model_data.append({
            'model': m,
            'l': m.req_rate / m.slo,
            's': m.model_size
        })

    def get_kvpr(l, s):
        if s >= GPU_MEM_SIZE - 1e-7: return 1e15
        return l / (GPU_MEM_SIZE - s)

    # --- Phase 1: Randomized Binary Search Construction ---

    def solve_packing(target_k, random_seed=None):
        capacity = target_k * GPU_MEM_SIZE
        items = list(model_data)

        # Sort Key: l + K*s with optional noise
        if random_seed is not None:
            rng = random.Random(random_seed)
            # Add noise to weight: w * random_factor
            items.sort(key=lambda x: (x['l'] + target_k * x['s']) * rng.uniform(0.9, 1.1), reverse=True)
        else:
            items.sort(key=lambda x: x['l'] + target_k * x['s'], reverse=True)

        gpu_l = [0.0] * gpu_num
        gpu_s = [0.0] * gpu_num
        gpu_models = [[] for _ in range(gpu_num)]

        for item in items:
            w = item['l'] + target_k * item['s']
            best_idx = -1
            min_rem = float('inf')

            # Best Fit logic
            for i in range(gpu_num):
                if gpu_s[i] + item['s'] >= GPU_MEM_SIZE - 1e-6:
                    continue

                curr_w = gpu_l[i] + target_k * gpu_s[i]
                if curr_w + w <= capacity + 1e-9:
                    rem = capacity - (curr_w + w)
                    if rem < min_rem:
                        min_rem = rem
                        best_idx = i

            if best_idx != -1:
                gpu_l[best_idx] += item['l']
                gpu_s[best_idx] += item['s']
                gpu_models[best_idx].append(item)
            else:
                return None
        return gpu_models

    # 1. Deterministic Binary Search for Baseline K
    low, high = 0.0, 1.0
    # Find Upper Bound
    for _ in range(20):
        if solve_packing(high) is not None: break
        low = high
        high *= 2.0
    else: high = 1e9

    # Refine
    for _ in range(25):
        mid = (low + high) / 2
        if solve_packing(mid) is not None: high = mid
        else: low = mid
    base_k = high

    # 2. Multi-Start Initialization
    best_init_plc = None
    best_init_max_kvpr = float('inf')

    # Attempt deterministic + multiple randomized starts slightly above base_k
    search_k = base_k * 1.001

    seeds = [None] + list(range(19)) # 20 total starts
    for seed in seeds:
        if time.time() - start_time > 0.15: break # Cap initialization budget

        res = solve_packing(search_k, random_seed=seed)
        if res:
            current_max = 0.0
            for g in range(gpu_num):
                l = sum(x['l'] for x in res[g])
                s = sum(x['s'] for x in res[g])
                k_val = get_kvpr(l, s)
                if k_val > current_max: current_max = k_val

            if current_max < best_init_max_kvpr:
                best_init_max_kvpr = current_max
                best_init_plc = res

    # Fallback
    if best_init_plc is None:
        best_init_plc = solve_packing(base_k)
        if best_init_plc is None:
            best_init_plc = solve_packing(1e9)
            if best_init_plc is None:
                best_init_plc = [[] for _ in range(gpu_num)]

    # --- Phase 2: Iterated Local Search (ILS) ---

    # Convert to mutable structures
    plc = [list(best_init_plc[g]) for g in range(gpu_num)]

    # Tracking stats
    gpu_stats = []
    for g in range(gpu_num):
        l = sum(x['l'] for x in plc[g])
        s = sum(x['s'] for x in plc[g])
        gpu_stats.append({'l': l, 's': s})

    best_global_plc = [list(p) for p in plc]
    best_global_score = best_init_max_kvpr

    def evaluate(stats):
        max_k = -1.0
        bottlenecks = []
        for g in range(gpu_num):
            k = get_kvpr(stats[g]['l'], stats[g]['s'])
            if k > max_k:
                max_k = k
                bottlenecks = [g]
            elif abs(k - max_k) < 1e-9:
                bottlenecks.append(g)
        return max_k, bottlenecks

    # Run ILS
    while time.time() - start_time < 0.5:

        # Steepest Descent Local Search
        while True:
            cur_max, bottlenecks = evaluate(gpu_stats)
            if cur_max < 1e-9: break

            best_move = None
            best_imp = 0.0

            # Check bottlenecks (prioritize worst)
            candidates = bottlenecks[:2]

            targets = []
            for t in range(gpu_num):
                if t not in bottlenecks:
                    targets.append(t)

            if not targets:
                break

            for b_idx in candidates:
                b_l = gpu_stats[b_idx]['l']
                b_s = gpu_stats[b_idx]['s']
                src_items = plc[b_idx]

                # Pre-filter targets
                valid_targets = [t for t in targets if get_kvpr(gpu_stats[t]['l'], gpu_stats[t]['s']) < cur_max]

                # 1. Move
                for i, item in enumerate(src_items):
                    for t in valid_targets:
                        if gpu_stats[t]['s'] + item['s'] >= GPU_MEM_SIZE: continue

                        nk_src = get_kvpr(b_l - item['l'], b_s - item['s'])
                        nk_tgt = get_kvpr(gpu_stats[t]['l'] + item['l'], gpu_stats[t]['s'] + item['s'])

                        new_global = max(nk_src, nk_tgt)
                        if new_global < cur_max - 1e-7:
                            imp = cur_max - new_global
                            if imp > best_imp:
                                best_imp = imp
                                best_move = ('move', b_idx, i, t)

                # 2. Swap 1-1
                for i, s_item in enumerate(src_items):
                    for t in valid_targets:
                        tgt_items = plc[t]
                        for j, t_item in enumerate(tgt_items):
                            if b_s - s_item['s'] + t_item['s'] >= GPU_MEM_SIZE: continue
                            if gpu_stats[t]['s'] - t_item['s'] + s_item['s'] >= GPU_MEM_SIZE: continue

                            nk_src = get_kvpr(b_l - s_item['l'] + t_item['l'], b_s - s_item['s'] + t_item['s'])
                            nk_tgt = get_kvpr(gpu_stats[t]['l'] - t_item['l'] + s_item['l'], gpu_stats[t]['s'] - t_item['s'] + s_item['s'])

                            new_global = max(nk_src, nk_tgt)
                            if new_global < cur_max - 1e-7:
                                imp = cur_max - new_global
                                if imp > best_imp:
                                    best_imp = imp
                                    best_move = ('swap11', b_idx, i, t, j)

                # 3. Swap 2-1 (2 from bottleneck)
                if len(src_items) >= 2:
                    limit_checks = 100
                    checks = 0
                    for i1 in range(len(src_items)):
                        if checks > limit_checks: break
                        for i2 in range(i1 + 1, len(src_items)):
                            m1 = src_items[i1]
                            m2 = src_items[i2]
                            pair_l = m1['l'] + m2['l']
                            pair_s = m1['s'] + m2['s']

                            for t in valid_targets:
                                tgt_items = plc[t]
                                for j, m3 in enumerate(tgt_items):
                                    if b_s - pair_s + m3['s'] >= GPU_MEM_SIZE: continue
                                    if gpu_stats[t]['s'] - m3['s'] + pair_s >= GPU_MEM_SIZE: continue

                                    nk_src = get_kvpr(b_l - pair_l + m3['l'], b_s - pair_s + m3['s'])
                                    nk_tgt = get_kvpr(gpu_stats[t]['l'] - m3['l'] + pair_l, gpu_stats[t]['s'] - m3['s'] + pair_s)

                                    new_global = max(nk_src, nk_tgt)
                                    if new_global < cur_max - 1e-7:
                                        imp = cur_max - new_global
                                        if imp > best_imp:
                                            best_imp = imp
                                            best_move = ('swap21', b_idx, i1, i2, t, j)
                                    checks += 1

            if best_move:
                mtype = best_move[0]
                if mtype == 'move':
                    _, b, i, t = best_move
                    item = plc[b].pop(i)
                    plc[t].append(item)
                    gpu_stats[b]['l'] -= item['l']; gpu_stats[b]['s'] -= item['s']
                    gpu_stats[t]['l'] += item['l']; gpu_stats[t]['s'] += item['s']
                elif mtype == 'swap11':
                    _, b, i, t, j = best_move
                    it1 = plc[b][i]
                    it2 = plc[t][j]
                    plc[b][i] = it2
                    plc[t][j] = it1
                    dl = it2['l'] - it1['l']; ds = it2['s'] - it1['s']
                    gpu_stats[b]['l'] += dl; gpu_stats[b]['s'] += ds
                    gpu_stats[t]['l'] -= dl; gpu_stats[t]['s'] -= ds
                elif mtype == 'swap21':
                    _, b, i1, i2, t, j = best_move
                    it2 = plc[b].pop(i2)
                    it1 = plc[b].pop(i1)
                    it3 = plc[t].pop(j)
                    plc[b].append(it3)
                    plc[t].extend([it1, it2])
                    pl = it1['l'] + it2['l']; ps = it1['s'] + it2['s']
                    gpu_stats[b]['l'] += it3['l'] - pl; gpu_stats[b]['s'] += it3['s'] - ps
                    gpu_stats[t]['l'] += pl - it3['l']; gpu_stats[t]['s'] += ps - it3['s']
            else:
                break # Local Optima Reached

        # Update Global Best
        cur_max, bottlenecks = evaluate(gpu_stats)
        if cur_max < best_global_score:
            best_global_score = cur_max
            best_global_plc = [list(p) for p in plc]

        # --- Perturbation (Ruins & Recreate) ---
        if not bottlenecks: break

        b_idx = random.choice(bottlenecks)
        if not plc[b_idx]: break

        # Remove items (try to remove largest to shake things up)
        candidates_to_remove = sorted(enumerate(plc[b_idx]), key=lambda x: x[1]['s'], reverse=True)
        num_remove = min(len(candidates_to_remove), random.randint(1, 2))

        removed_items = []
        # We need to pop by index, so we must be careful about index shifting or just pick random to be safe/fast
        # Random removal is robust
        for _ in range(num_remove):
            if not plc[b_idx]: break
            idx = random.randrange(len(plc[b_idx]))
            item = plc[b_idx].pop(idx)
            removed_items.append(item)
            gpu_stats[b_idx]['l'] -= item['l']
            gpu_stats[b_idx]['s'] -= item['s']

        # Recreate: Best Fit
        for item in removed_items:
            best_t = -1
            best_k = float('inf')

            # Randomize order of GPUs
            targets_rnd = list(range(gpu_num))
            random.shuffle(targets_rnd)

            for t in targets_rnd:
                if t == b_idx: continue
                if gpu_stats[t]['s'] + item['s'] < GPU_MEM_SIZE:
                    k = get_kvpr(gpu_stats[t]['l'] + item['l'], gpu_stats[t]['s'] + item['s'])
                    if k < best_k:
                        best_k = k
                        best_t = t

            if best_t != -1:
                plc[best_t].append(item)
                gpu_stats[best_t]['l'] += item['l']
                gpu_stats[best_t]['s'] += item['s']
            else:
                # Put back
                plc[b_idx].append(item)
                gpu_stats[b_idx]['l'] += item['l']
                gpu_stats[b_idx]['s'] += item['s']

    # Final result
    result = {}
    for i in range(gpu_num):
        result[i] = [x['model'] for x in best_global_plc[i]]
    return result
>>>>>>> REPLACE
</DIFF>