<NAME>
init_random_greedy
</NAME>
<DESCRIPTION>
Add a randomized greedy strategy to the initialization phase. This runs 10 iterations of shuffling the models and applying a Best-Fit heuristic. This increases the diversity of candidate solutions fed into the ILS, improving the chances of starting from a good basin of attraction, especially for instances where deterministic sorting criteria (size, load, density) are suboptimal.
</DESCRIPTION>
<DIFF>
<<<<<<< SEARCH
    # Strategy B: Deterministic Greedy
    strategies = [
        lambda m: m.model_size,
        lambda m: m.req_rate / m.slo,
        lambda m: (m.req_rate / m.slo) / m.model_size if m.model_size > 1e-7 else 0
    ]
    for key_fn in strategies:
        gpus = [GPUState(i) for i in range(gpu_num)]
        valid = True
        for m in sorted(models, key=key_fn, reverse=True):
            best_idx = -1
            best_val = float('inf')
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (gpus[i].used_mem + m.model_size)
                    val = (gpus[i].load + m.req_rate/m.slo) / rem if rem > 1e-7 else float('inf')
                    if val < best_val:
                        best_val = val
                        best_idx = i
            if best_idx != -1: gpus[best_idx].add(m)
            else: valid = False; break
        if valid: candidates.append(gpus)
=======
    # Strategy B: Deterministic Greedy
    strategies = [
        lambda m: m.model_size,
        lambda m: m.req_rate / m.slo,
        lambda m: (m.req_rate / m.slo) / m.model_size if m.model_size > 1e-7 else 0
    ]
    for key_fn in strategies:
        gpus = [GPUState(i) for i in range(gpu_num)]
        valid = True
        for m in sorted(models, key=key_fn, reverse=True):
            best_idx = -1
            best_val = float('inf')
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (gpus[i].used_mem + m.model_size)
                    val = (gpus[i].load + m.req_rate/m.slo) / rem if rem > 1e-7 else float('inf')
                    if val < best_val:
                        best_val = val
                        best_idx = i
            if best_idx != -1: gpus[best_idx].add(m)
            else: valid = False; break
        if valid: candidates.append(gpus)

    # Strategy C: Randomized Greedy (Best Fit with random order)
    for _ in range(10):
        gpus = [GPUState(i) for i in range(gpu_num)]
        shuffled_m = list(models)
        random.shuffle(shuffled_m)
        valid = True
        for m in shuffled_m:
            best_idx = -1
            best_val = float('inf')
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (gpus[i].used_mem + m.model_size)
                    val = (gpus[i].load + m.req_rate/m.slo) / rem if rem > 1e-7 else float('inf')
                    if val < best_val:
                        best_val = val
                        best_idx = i
            if best_idx != -1: gpus[best_idx].add(m)
            else: valid = False; break
        if valid: candidates.append(gpus)
>>>>>>> REPLACE
</DIFF>

<NAME>
fallback_bfd
</NAME>
<DESCRIPTION>
Upgrade the fallback initialization strategy from First-Fit Decreasing (FFD) to Best-Fit Decreasing (BFD). BFD selects the GPU with the minimum remaining space that still fits the model. This heuristic generally produces tighter packings, increasing the probability of finding a feasible placement when other strategies fail, thereby improving the success rate.
</DESCRIPTION>
<DIFF>
<<<<<<< SEARCH
    # Fallback
    if not candidates:
        gpus = [GPUState(i) for i in range(gpu_num)]
        for m in sorted(models, key=lambda x: x.model_size, reverse=True):
            placed = False
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    gpus[i].add(m); placed = True; break
            if not placed: raise ValueError("Models do not fit in GPU memory.")
        candidates.append(gpus)
=======
    # Fallback: Best Fit Decreasing (BFD)
    if not candidates:
        gpus = [GPUState(i) for i in range(gpu_num)]
        for m in sorted(models, key=lambda x: x.model_size, reverse=True):
            best_idx = -1
            min_rem = float('inf')
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (gpus[i].used_mem + m.model_size)
                    if rem < min_rem:
                        min_rem = rem
                        best_idx = i

            if best_idx != -1:
                gpus[best_idx].add(m)
            else:
                raise ValueError("Models do not fit in GPU memory.")
        candidates.append(gpus)
>>>>>>> REPLACE
</DIFF>

<NAME>
ils_swap_2_2
</NAME>
<DESCRIPTION>
Introduce a "Swap 2-2" operator to the Iterated Local Search. This operator attempts to swap two models from the bottleneck GPU with two models from a destination GPU. This allows for more complex load balancing moves that preserve memory constraints (e.g., swapping two medium models for two other medium models) which 1-1 or 1-2 swaps cannot achieve. This helps escape local optima where single model moves are blocked by capacity.
</DESCRIPTION>
<DIFF>
<<<<<<< SEARCH
        # --- Operator 4: Swap 1-2 (Source gives 1, Dest gives 2) ---
        for source in sources[:2]:
            for i, m_a in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue
                    if len(dest.models) < 2: continue

                    n_d = len(dest.models)
                    for j1 in range(n_d):
                        for j2 in range(j1+1, n_d):
                            m_b1 = dest.models[j1]
                            m_b2 = dest.models[j2]
                            s_mem = source.used_mem - m_a.model_size + m_b1.model_size + m_b2.model_size
                            d_mem = dest.used_mem - m_b1.model_size - m_b2.model_size + m_a.model_size
                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                source.remove(i)
                                dest.remove(j2)
                                dest.remove(j1)
                                source.add(m_b1)
                                source.add(m_b2)
                                dest.add(m_a)
                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    if current_vector < best_vector:
                                        best_vector = current_vector
                                        for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
                                    break
                                else:
                                    dest.remove(len(dest.models)-1)
                                    source.remove(len(source.models)-1)
                                    source.remove(len(source.models)-1)
                                    dest.restore_model(j1, m_b1)
                                    dest.restore_model(j2, m_b2)
                        if improved_step: break
                    if improved_step: break
                if improved_step: break
        if improved_step: continue
=======
        # --- Operator 4: Swap 1-2 (Source gives 1, Dest gives 2) ---
        for source in sources[:2]:
            for i, m_a in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue
                    if len(dest.models) < 2: continue

                    n_d = len(dest.models)
                    for j1 in range(n_d):
                        for j2 in range(j1+1, n_d):
                            m_b1 = dest.models[j1]
                            m_b2 = dest.models[j2]
                            s_mem = source.used_mem - m_a.model_size + m_b1.model_size + m_b2.model_size
                            d_mem = dest.used_mem - m_b1.model_size - m_b2.model_size + m_a.model_size
                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                source.remove(i)
                                dest.remove(j2)
                                dest.remove(j1)
                                source.add(m_b1)
                                source.add(m_b2)
                                dest.add(m_a)
                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    if current_vector < best_vector:
                                        best_vector = current_vector
                                        for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
                                    break
                                else:
                                    dest.remove(len(dest.models)-1)
                                    source.remove(len(source.models)-1)
                                    source.remove(len(source.models)-1)
                                    dest.restore_model(j1, m_b1)
                                    dest.restore_model(j2, m_b2)
                        if improved_step: break
                    if improved_step: break
                if improved_step: break
        if improved_step: continue

        # --- Operator 5: Swap 2-2 (Source gives 2, Dest gives 2) ---
        for source in sources[:1]: # Limit to top bottleneck
            if len(source.models) < 2: continue
            n_s = len(source.models)
            for i1 in range(n_s):
                for i2 in range(i1+1, n_s):
                    m_a1 = source.models[i1]
                    m_a2 = source.models[i2]

                    for dest in destinations:
                        if dest.id == source.id: continue
                        if dest.kvpr() >= source.kvpr(): continue
                        if len(dest.models) < 2: continue

                        n_d = len(dest.models)
                        for j1 in range(n_d):
                            for j2 in range(j1+1, n_d):
                                m_b1 = dest.models[j1]
                                m_b2 = dest.models[j2]

                                s_mem = source.used_mem - m_a1.model_size - m_a2.model_size + m_b1.model_size + m_b2.model_size
                                d_mem = dest.used_mem - m_b1.model_size - m_b2.model_size + m_a1.model_size + m_a2.model_size

                                if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                    source.remove(i2)
                                    source.remove(i1)
                                    dest.remove(j2)
                                    dest.remove(j1)
                                    source.add(m_b1)
                                    source.add(m_b2)
                                    dest.add(m_a1)
                                    dest.add(m_a2)

                                    new_vec = get_vector(current_gpus)
                                    if new_vec < current_vector:
                                        current_vector = new_vec
                                        improved_step = True
                                        if current_vector < best_vector:
                                            best_vector = current_vector
                                            for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
                                        break
                                    else:
                                        # Revert: remove added models, restore removed models
                                        dest.remove(len(dest.models)-1)
                                        dest.remove(len(dest.models)-1)
                                        source.remove(len(source.models)-1)
                                        source.remove(len(source.models)-1)

                                        # Restore in correct order: smaller index first, then larger
                                        dest.restore_model(j1, m_b1)
                                        dest.restore_model(j2, m_b2)
                                        source.restore_model(i1, m_a1)
                                        source.restore_model(i2, m_a2)
                            if improved_step: break
                        if improved_step: break
                    if improved_step: break
                if improved_step: break
        if improved_step: continue
>>>>>>> REPLACE
</DIFF>