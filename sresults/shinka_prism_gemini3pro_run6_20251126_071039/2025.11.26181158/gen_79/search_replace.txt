<NAME>
init_and_perturb_v2
</NAME>

<DESCRIPTION>
1. Improved Initialization: Instead of using only the minimum feasible K from binary search, we now scan a range of K values (multipliers of the minimum K) to find the parameter that yields the best initial KVPR vector. This balances the trade-off between packing efficiency (high K) and load balancing (low K). We also added randomized refinement around the best found K.
2. Enhanced Perturbation: The Ruin-and-Recreate phase now empties both the bottleneck GPU (worst) and the least-loaded GPU (best). This creates a larger pool of items and free space, facilitating the movement of heavy models from the bottleneck to the under-utilized GPU during the greedy reconstruction.
3. Added a Swap 2-1 pruning heuristic based on size matching to improve efficiency.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Binary Search for best K
    low, high = 0.0, 1000.0
    if solve_linearized_bp(high) is None: high = 1e9

    bs_res = None
    best_k = high
    for _ in range(20):
        mid = (low + high) / 2
        res = solve_linearized_bp(mid)
        if res:
            bs_res = res
            best_k = mid
            high = mid
        else:
            low = mid

    if bs_res: candidates.append(bs_res)

    # Generate noisy variants around best_k
    if best_k < 1e8:
        for _ in range(15):
            k_noisy = best_k * random.uniform(0.9, 1.1)
            res_noise = solve_linearized_bp(k_noisy, noise=0.05)
            if res_noise: candidates.append(res_noise)
=======
    # Binary Search to find minimum feasible K
    low, high = 0.0, 1000.0
    if solve_linearized_bp(high) is None: high = 1e9

    min_feasible_k = high
    for _ in range(16):
        mid = (low + high) / 2
        res = solve_linearized_bp(mid)
        if res:
            min_feasible_k = mid
            high = mid
        else:
            low = mid

    # Grid search for the best K starting from min_feasible_k
    # Higher K shifts priority to Size (packing), Lower K to Load (balancing).
    # We explore the trade-off.
    if min_feasible_k < 1e8:
        multipliers = [1.0, 1.05, 1.1, 1.15, 1.2, 1.3, 1.5, 2.0, 3.0]
        for mult in multipliers:
            k_try = min_feasible_k * mult
            res = solve_linearized_bp(k_try)
            if res: candidates.append(res)

        # Refine around the best candidate found so far
        if candidates:
            # Add noise around min_feasible_k * {1.0, 1.2} which are common sweet spots
            for center_k in [min_feasible_k, min_feasible_k * 1.2]:
                for _ in range(5):
                    k_noisy = center_k * random.uniform(0.95, 1.05)
                    res_noise = solve_linearized_bp(k_noisy, noise=0.03)
                    if res_noise: candidates.append(res_noise)
>>>>>>> REPLACE
<<<<<<< SEARCH
        # --- Operator 4: Swap 2-1 (First Improvement) ---
        for source in sources[:3]:
            if len(source.models) < 2: continue
            n_s = len(source.models)
            for i1 in range(n_s):
                for i2 in range(i1+1, n_s):
                    m_a1 = source.models[i1]
                    m_a2 = source.models[i2]

                    for dest in destinations:
                        if dest.id == source.id: continue
                        if dest.kvpr() >= source.kvpr(): continue

                        for j, m_b in enumerate(dest.models):
                            s_mem = source.used_mem - m_a1.model_size - m_a2.model_size + m_b.model_size
                            d_mem = dest.used_mem - m_b.model_size + m_a1.model_size + m_a2.model_size

                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                source.remove(i2) # higher idx first
                                source.remove(i1)
                                dest.remove(j)
                                source.add(m_b)
                                dest.add(m_a1)
                                dest.add(m_a2)

                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    break
                                else:
                                    dest.remove(len(dest.models)-1)
                                    dest.remove(len(dest.models)-1)
                                    source.remove(len(source.models)-1)
                                    dest.restore_model(j, m_b)
                                    source.restore_model(i1, m_a1)
                                    source.restore_model(i2, m_a2)
                        if improved_step: break
                    if improved_step: break
                if improved_step: break
            if improved_step: break
=======
        # --- Operator 4: Swap 2-1 (First Improvement) ---
        for source in sources[:3]:
            if len(source.models) < 2: continue
            n_s = len(source.models)
            for i1 in range(n_s):
                for i2 in range(i1+1, n_s):
                    m_a1 = source.models[i1]
                    m_a2 = source.models[i2]
                    combined_size_a = m_a1.model_size + m_a2.model_size

                    for dest in destinations:
                        if dest.id == source.id: continue
                        if dest.kvpr() >= source.kvpr(): continue

                        for j, m_b in enumerate(dest.models):
                            # Optimization: Size compatibility check
                            # Only swap if incoming model size is somewhat comparable to outgoing sum
                            # This avoids trying to fit huge items into small holes or vice versa
                            if not (0.5 * m_b.model_size <= combined_size_a <= 2.5 * m_b.model_size):
                                continue

                            s_mem = source.used_mem - combined_size_a + m_b.model_size
                            d_mem = dest.used_mem - m_b.model_size + combined_size_a

                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                source.remove(i2) # higher idx first
                                source.remove(i1)
                                dest.remove(j)
                                source.add(m_b)
                                dest.add(m_a1)
                                dest.add(m_a2)

                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    break
                                else:
                                    dest.remove(len(dest.models)-1)
                                    dest.remove(len(dest.models)-1)
                                    source.remove(len(source.models)-1)
                                    dest.restore_model(j, m_b)
                                    source.restore_model(i1, m_a1)
                                    source.restore_model(i2, m_a2)
                        if improved_step: break
                    if improved_step: break
                if improved_step: break
            if improved_step: break
>>>>>>> REPLACE
<<<<<<< SEARCH
        # --- Perturbation (Ruins and Recreate) ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        # Ruin: Remove all models from the bottleneck
        evicted = []
        while worst_gpu.models:
            evicted.append(worst_gpu.remove(0))

        # Optional: Ruin a random second GPU to allow swaps?
        # Let's stick to simple ruin of worst to be safe and robust

        # Recreate: Best Fit Descending (by load)
        evicted.sort(key=lambda m: m.req_rate/m.slo, reverse=True)

        other_gpus = [g for g in current_gpus if g.id != worst_gpu.id]

        for m in evicted:
            best_dest = None
            best_dest_val = float('inf')

            for dest in other_gpus:
                if dest.can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (dest.used_mem + m.model_size)
                    if rem > 1e-7:
                        # Heuristic: minimize resulting KVPR
                        val = (dest.load + m.req_rate/m.slo) / rem
                        if val < best_dest_val:
                            best_dest_val = val
                            best_dest = dest

            if best_dest:
                best_dest.add(m)
            else:
                worst_gpu.add(m)
=======
        # --- Perturbation (Ruins and Recreate) ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        # Ruin: worst GPU and best GPU (to facilitate flow from worst -> best)
        worst_gpu = sorted_gpus[0]
        best_gpu = sorted_gpus[-1] if len(sorted_gpus) > 1 else None

        if not worst_gpu.models: break

        evicted = []

        # Empty worst GPU
        while worst_gpu.models:
            evicted.append(worst_gpu.remove(0))

        # Empty best GPU (if distinct)
        if best_gpu and best_gpu.id != worst_gpu.id:
            while best_gpu.models:
                evicted.append(best_gpu.remove(0))

        # Recreate:
        # Vary the sorting strategy to escape attractors
        if iter_cnt % 2 == 0:
            # Sort by Load Descending
            evicted.sort(key=lambda m: m.req_rate/m.slo, reverse=True)
        else:
            # Sort by Size Descending (Packing)
            evicted.sort(key=lambda m: m.model_size, reverse=True)

        # Targets are all GPUs (worst and best are now empty and available)
        targets = current_gpus

        for m in evicted:
            best_dest = None
            best_dest_val = float('inf')

            for dest in targets:
                if dest.can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (dest.used_mem + m.model_size)
                    if rem > 1e-7:
                        # Heuristic: minimize resulting KVPR
                        val = (dest.load + m.req_rate/m.slo) / rem
                        if val < best_dest_val:
                            best_dest_val = val
                            best_dest = dest

            if best_dest:
                best_dest.add(m)
            else:
                # Fallback: Put back in worst_gpu (or best_gpu) if fits.
                # Since they came from the system, they should fit somewhere.
                if worst_gpu.can_fit(m.model_size):
                    worst_gpu.add(m)
                elif best_gpu and best_gpu.can_fit(m.model_size):
                    best_gpu.add(m)
                else:
                    # Should practically not happen if fragmentation isn't extreme
                    worst_gpu.add(m)
>>>>>>> REPLACE
</DIFF>