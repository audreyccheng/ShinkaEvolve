<NAME>
hybrid_ils_dual_ruin
</NAME>

<DESCRIPTION>
1.  **Dual Ruin Perturbation**: Modified the perturbation phase to ruin both the worst (bottleneck) and best (least loaded) GPUs. This increases the pool of models available for redistribution, helping to balance load more effectively than single-GPU ruin. The reconstruction alternates between sorting by Size (packing efficiency) and Load (balancing) to escape local optima. A fallback mechanism reverts to the best known solution if reconstruction fails.
2.  **Relaxed Swap 1-1 Heuristic**: Relaxed the pruning condition in the Swap 1-1 operator. Previously, it strictly prevented swaps that increased load on the bottleneck. Now, it only prevents swaps that worsen *both* load and size. This allows swaps that improve KVPR by reducing size (increasing free memory) even if load marginally increases, or vice versa.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    """
    Hybrid Memetic Algorithm for Model Placement:
    1. Stochastic Initialization:
       - Binary Search Linearization with noise (Weighted Bin Packing) for diverse starting candidates.
       - Deterministic and Randomized Greedy strategies.

    2. Iterated Local Search (ILS):
       - Focused on top bottleneck GPUs.
       - Operators: Move, Swap 1-1, Swap 2-1, Swap 1-2.
       - Uses 'Best Improvement' for the primary bottleneck to maximize gradient descent.
       - Uses 'First Improvement' on sorted destinations (low load first) for speed.

    3. Perturbation:
       - Ruin-and-Recreate: Completely empties the worst bottleneck GPU and redistributes models
         to escape local optima.
    """
=======
    """
    Hybrid Memetic Algorithm for Model Placement:
    1. Stochastic Initialization:
       - Binary Search Linearization with noise (Weighted Bin Packing) for diverse starting candidates.
       - Deterministic and Randomized Greedy strategies.

    2. Iterated Local Search (ILS):
       - Focused on top bottleneck GPUs.
       - Operators: Move, Swap 1-1 (Relaxed), Swap 2-1, Swap 1-2.
       - Uses 'Best Improvement' for the primary bottleneck to maximize gradient descent.

    3. Perturbation:
       - Dual Ruin-and-Recreate: Empties the worst bottleneck GPU AND the best GPU to
         redistribute load and capacity. Alternates sorting strategies (Size vs Load).
    """
>>>>>>> REPLACE
<<<<<<< SEARCH
        # --- Operator 2: Swap 1-1 ---
        # Scan for swap
        for source in sources:
            for i, m_a in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue
                    for j, m_b in enumerate(dest.models):
                        # Skip if swap doesn't help load
                        if (m_b.req_rate/m_b.slo) >= (m_a.req_rate/m_a.slo): continue

                        s_mem = source.used_mem - m_a.model_size + m_b.model_size
                        d_mem = dest.used_mem - m_b.model_size + m_a.model_size

                        if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                            source.remove(i)
                            dest.remove(j)
                            source.add(m_b)
                            dest.add(m_a)
                            new_vec = get_vector(current_gpus)

                            if new_vec < current_vector:
                                current_vector = new_vec
                                improved_step = True
                                break
                            else:
                                dest.remove(len(dest.models)-1)
                                source.remove(len(source.models)-1)
                                dest.restore_model(j, m_b)
                                source.restore_model(i, m_a)
                    if improved_step: break
                if improved_step: break
            if improved_step: break
=======
        # --- Operator 2: Swap 1-1 ---
        # Scan for swap
        for source in sources:
            for i, m_a in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue
                    for j, m_b in enumerate(dest.models):
                        # Optimization: Relaxed check. Allow swap if it helps load OR size significantly.
                        # Strict check (m_b.load < m_a.load) prevents size-reducing swaps.
                        # We use a combined heuristic: skip only if both load and size are worse/same.
                        if (m_b.req_rate/m_b.slo) >= (m_a.req_rate/m_a.slo) and m_b.model_size >= m_a.model_size:
                            continue

                        s_mem = source.used_mem - m_a.model_size + m_b.model_size
                        d_mem = dest.used_mem - m_b.model_size + m_a.model_size

                        if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                            source.remove(i)
                            dest.remove(j)
                            source.add(m_b)
                            dest.add(m_a)
                            new_vec = get_vector(current_gpus)

                            if new_vec < current_vector:
                                current_vector = new_vec
                                improved_step = True
                                break
                            else:
                                dest.remove(len(dest.models)-1)
                                source.remove(len(source.models)-1)
                                dest.restore_model(j, m_b)
                                source.restore_model(i, m_a)
                    if improved_step: break
                if improved_step: break
            if improved_step: break
>>>>>>> REPLACE
<<<<<<< SEARCH
        # --- Perturbation (Ruin and Recreate) ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        # Ruin: Evict all
        evicted = []
        while worst_gpu.models:
            evicted.append(worst_gpu.remove(0))

        # Shuffle for randomness
        random.shuffle(evicted)

        # Recreate: Best Fit
        for m in evicted:
            best_dest = None
            best_val = float('inf')
            # Consider all including worst_gpu (which is now empty/lighter)
            # Actually, let's try to put them elsewhere first.
            # If worst_gpu is empty, its load is 0, so it will be a candidate.

            for dest in current_gpus:
                if dest.can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (dest.used_mem + m.model_size)
                    val = (dest.load + m.req_rate/m.slo) / rem if rem > 1e-7 else float('inf')
                    if val < best_val:
                        best_val = val
                        best_dest = dest

            if best_dest:
                best_dest.add(m)
            else:
                # Must put back in original if no fit (should not happen if it came from there)
                worst_gpu.add(m)

        current_vector = get_vector(current_gpus)
        if current_vector < best_vector:
            best_vector = current_vector
            for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
=======
        # --- Perturbation (Dual Ruin and Recreate) ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        # Pick the best GPU (least utilized) to participate in the ruin for redistribution
        best_gpu = sorted_gpus[-1] if len(sorted_gpus) > 1 and sorted_gpus[-1].id != worst_gpu.id else None

        if not worst_gpu.models: break

        evicted = []
        # Ruin worst
        while worst_gpu.models:
            evicted.append(worst_gpu.remove(0))
        # Ruin best
        if best_gpu:
            while best_gpu.models:
                evicted.append(best_gpu.remove(0))

        # Alternating Sort Strategy
        if iter_cnt % 2 == 0:
            # Sort by Size Descending (Packing)
            evicted.sort(key=lambda m: m.model_size, reverse=True)
        else:
            # Sort by Load Descending (Balancing)
            evicted.sort(key=lambda m: m.req_rate / m.slo, reverse=True)

        # Recreate: Best Fit into ANY GPU
        rebuild_success = True
        for m in evicted:
            best_dest = None
            best_val = float('inf')

            for dest in current_gpus:
                if dest.can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (dest.used_mem + m.model_size)
                    val = (dest.load + m.req_rate/m.slo) / rem if rem > 1e-7 else float('inf')
                    if val < best_val:
                        best_val = val
                        best_dest = dest

            if best_dest:
                best_dest.add(m)
            else:
                # Failed to place model (fragmentation). Revert perturbation.
                rebuild_success = False
                break

        if not rebuild_success:
            # Revert to best solution
            for k in range(gpu_num): current_gpus[k].copy_from(best_gpus[k])
        else:
            current_vector = get_vector(current_gpus)
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
>>>>>>> REPLACE
</DIFF>