<NAME>
optimize_ils_operators
</NAME>

<DESCRIPTION>
1.  **Swap 1-1 Steepest Descent**: Change the Swap 1-1 operator from "First Improvement" to "Best Improvement" (Steepest Descent). This ensures that for the bottleneck GPUs, we select the swap that provides the maximal reduction in global pressure, avoiding shallow local optima. Added a heuristic optimization to skip swaps that increase the load on the bottleneck source.
2.  **Expanded Swap Scopes**: Increase the number of bottleneck GPUs considered for Swap 1-2 (from top 2 to top 3) and Swap 2-1 (from top 1 to top 2). This helps in resolving secondary bottlenecks that might block future improvements.
3.  **Targeted Perturbation**: Improve the perturbation operator to preferentially target high-load models (top 50% by load) from the worst GPU for relocation. This makes the kick more effective at reducing the max KVPR compared to moving a random (potentially tiny) model.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        # --- Operator 2: Swap 1-1 ---
        for source in sources:
            for i, m_a in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    # Only swap if dest is better off than source
                    if dest.kvpr() >= source.kvpr(): continue

                    for j, m_b in enumerate(dest.models):
                        s_mem = source.used_mem - m_a.model_size + m_b.model_size
                        d_mem = dest.used_mem - m_b.model_size + m_a.model_size
                        if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                            source.remove(i)
                            dest.remove(j)
                            source.add(m_b)
                            dest.add(m_a)

                            new_vec = get_vector(current_gpus)
                            if new_vec < current_vector:
                                current_vector = new_vec
                                improved_step = True
                                break
                            else:
                                dest.remove(len(dest.models)-1)
                                source.remove(len(source.models)-1)
                                dest.restore_model(j, m_b)
                                source.restore_model(i, m_a)
                    if improved_step: break
                if improved_step: break
            if improved_step: break

        if improved_step:
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- Operator 3: Swap 1-2 (Source gives 1, Dest gives 2) ---
        for source in sources[:2]:
=======
        # --- Operator 2: Swap 1-1 (Steepest Descent) ---
        best_swap = None
        best_swap_val = current_vector

        for source in sources:
            for i, m_a in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue

                    for j, m_b in enumerate(dest.models):
                        # Optimization: Skip if swap increases load on bottleneck source
                        diff_load = (m_b.req_rate/m_b.slo) - (m_a.req_rate/m_a.slo)
                        if diff_load >= 0: continue

                        s_mem = source.used_mem - m_a.model_size + m_b.model_size
                        d_mem = dest.used_mem - m_b.model_size + m_a.model_size

                        if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                            source.remove(i)
                            dest.remove(j)
                            source.add(m_b)
                            dest.add(m_a)

                            new_vec = get_vector(current_gpus)
                            if new_vec < best_swap_val:
                                best_swap_val = new_vec
                                best_swap = (source, i, dest, j, m_a, m_b)

                            # Revert
                            dest.remove(len(dest.models)-1)
                            source.remove(len(source.models)-1)
                            dest.restore_model(j, m_b)
                            source.restore_model(i, m_a)

        if best_swap:
            src, i, dst, j, ma, mb = best_swap
            src.remove(i)
            dst.remove(j)
            src.add(mb)
            dst.add(ma)
            current_vector = best_swap_val
            improved_step = True

        if improved_step:
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- Operator 3: Swap 1-2 (Source gives 1, Dest gives 2) ---
        # Expanded to top 3 bottlenecks
        for source in sources[:3]:
>>>>>>> REPLACE
<<<<<<< SEARCH
        # --- Operator 4: Swap 2-1 (Source gives 2, Dest gives 1) ---
        # Useful when source is cluttered with small models
        for source in sources[:1]: # Expensive, limit to top bottleneck
            if len(source.models) < 2: continue

            n_s = len(source.models)
=======
        # --- Operator 4: Swap 2-1 (Source gives 2, Dest gives 1) ---
        # Expanded to top 2 bottlenecks
        for source in sources[:2]:
            if len(source.models) < 2: continue

            n_s = len(source.models)
>>>>>>> REPLACE
<<<<<<< SEARCH
        # --- Perturbation ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        m_idx = random.randint(0, len(worst_gpu.models)-1)
        model_to_move = worst_gpu.models[m_idx]

        feasible_dests = [g for g in current_gpus if g.id != worst_gpu.id and g.can_fit(model_to_move.model_size)]

        if feasible_dests:
            # Guided: Prefer good GPUs to avoid moving to another bottleneck
            feasible_dests.sort(key=lambda g: g.kvpr())
            target_candidates = feasible_dests[:3]
            dest = random.choice(target_candidates)

            worst_gpu.remove(m_idx)
            dest.add(model_to_move)
            current_vector = get_vector(current_gpus)
        else:
            break
=======
        # --- Perturbation ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        # Select model to move: Prefer high load models
        # Sort models by load descending
        sorted_models_idx = sorted(range(len(worst_gpu.models)),
                                   key=lambda k: worst_gpu.models[k].req_rate/worst_gpu.models[k].slo,
                                   reverse=True)
        # Pick from top 50%
        n_candidates = max(1, len(sorted_models_idx) // 2)
        m_idx_idx = random.randint(0, n_candidates - 1)
        m_idx = sorted_models_idx[m_idx_idx]

        model_to_move = worst_gpu.models[m_idx]

        feasible_dests = [g for g in current_gpus if g.id != worst_gpu.id and g.can_fit(model_to_move.model_size)]

        if feasible_dests:
            # Guided: Prefer good GPUs to avoid moving to another bottleneck
            feasible_dests.sort(key=lambda g: g.kvpr())
            target_candidates = feasible_dests[:4]
            dest = random.choice(target_candidates)

            worst_gpu.remove(m_idx)
            dest.add(model_to_move)
            current_vector = get_vector(current_gpus)
        else:
            break
>>>>>>> REPLACE
</DIFF>