think silently if needed.
The previous search block failed because I copied the text from the "Current Program" display in my thought block but likely got some indentation or spacing wrong compared to the actual file content, or the display had trailing spaces I missed/added.

The error message shows the exact search pattern it was looking for. It seems I included:
```python
        # --- Operator 3: Swap 2-1 and 1-2 ---
        for s_gpu in sources[:1]: # Top bottleneck only
```
But in the actual file, maybe there is a small difference.

Let's look at the "Current program" again.
```python
    while iter_cnt < max_iter:
        improved_step = False

        sorted_gpus = sorted(current_gpus, key=lambda g: g.kvpr(), reverse=True)
        # Focus on the absolute bottleneck
        source = sorted_gpus[0]
```
Wait, the error message `SEARCH text not found` implies that I should be very careful.

I will try to match a smaller block to anchor it, but I need to replace the *entire* while loop content.
The loop starts at `while iter_cnt < max_iter:`.
The loop ends right before `return {g.id: g.models for g in best_gpus}`.

I will verify line by line from the provided "Current program".

```python
    # -------------------------------------------------------------------------
    # 2. Iterated Local Search
    # -------------------------------------------------------------------------
    iter_cnt = 0
    max_iter = 150

    while iter_cnt < max_iter:
```

Lines match.

```python
        improved_step = False

        sorted_gpus = sorted(current_gpus, key=lambda g: g.kvpr(), reverse=True)
        # Focus on the absolute bottleneck
        source = sorted_gpus[0]
```

Lines match.

The problematic part might be around comments or whitespace.
I will try to construct the SEARCH block by copying *exactly* from the prompt's "Current program" block.

Wait, looking at the previous diff attempt, I might have missed a blank line or something.

Let's try to match from `iter_cnt = 0` down to the end of the perturbation block.

One detail: In the "Current program", inside `Operator 3`, it says:
```python
        # --- Operator 3: Swap 2-1 and 1-2 ---
        for s_gpu in sources[:1]: # Top bottleneck only
            # Try 1-2 (Source gives 1, Dest gives 2)
```

In my failed search block:
```python
        # --- Operator 3: Swap 2-1 and 1-2 ---
        for s_gpu in sources[:1]: # Top bottleneck only
            # Try 1-2 (Source gives 1, Dest gives 2)
```
They look identical.

Maybe the issue is `sorted_gpus = sorted(current_gpus, key=lambda g: g.kvpr(), reverse=True)`.

The error message provided by the system shows the "Full search pattern" which matches what I submitted.
It says "Found similar text (but not exact match)". This implies slight difference.

The safest way is to use a slightly less huge block if possible, but I need to replace the whole logic.

Let's try to include `iter_cnt = 0` and `max_iter = 150` lines to be sure where we start.

I will reproduce the SEARCH block carefully.

```python
    # -------------------------------------------------------------------------
    # 2. Iterated Local Search
    # -------------------------------------------------------------------------
    iter_cnt = 0
    max_iter = 150

    while iter_cnt < max_iter:
        improved_step = False

        sorted_gpus = sorted(current_gpus, key=lambda g: g.kvpr(), reverse=True)
        # Focus on the absolute bottleneck
        source = sorted_gpus[0]

        # --- Operator 1: Move (Steepest Descent / Best Improvement) ---
        # Look for the single best move from the bottleneck to any other GPU
        best_move_indices = None # (model_idx, dest_gpu)
        best_move_vec = current_vector

        for i, model in enumerate(source.models):
            for dest in current_gpus:
                if dest.id == source.id: continue
                if dest.can_fit(model.model_size):
                    # Try move
                    source.remove(i)
                    dest.add(model)

                    new_vec = get_vector(current_gpus)
                    if new_vec < best_move_vec:
                        best_move_vec = new_vec
                        best_move_indices = (i, dest)

                    # Revert
                    dest.remove(len(dest.models)-1)
                    source.restore_model(i, model)

        if best_move_indices:
            # Apply best move
            idx, dest_gpu = best_move_indices
            model = source.remove(idx)
            dest_gpu.add(model)
            current_vector = best_move_vec
            improved_step = True

            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])

            iter_cnt += 1
            continue # Restart loop to re-evaluate bottleneck

        # --- Operator 2: Swap 1-1 (First Improvement) ---
        sources = sorted_gpus[:2]
        destinations = sorted_gpus[::-1]

        for s_gpu in sources:
            for i, m_a in enumerate(s_gpu.models):
                for d_gpu in destinations:
                    if d_gpu.id == s_gpu.id: continue
                    if d_gpu.kvpr() >= s_gpu.kvpr(): continue

                    for j, m_b in enumerate(d_gpu.models):
                        s_mem = s_gpu.used_mem - m_a.model_size + m_b.model_size
                        d_mem = d_gpu.used_mem - m_b.model_size + m_a.model_size
                        if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                            s_gpu.remove(i)
                            d_gpu.remove(j)
                            s_gpu.add(m_b)
                            d_gpu.add(m_a)

                            new_vec = get_vector(current_gpus)
                            if new_vec < current_vector:
                                current_vector = new_vec
                                improved_step = True
                                break
                            else:
                                d_gpu.remove(len(d_gpu.models)-1)
                                s_gpu.remove(len(s_gpu.models)-1)
                                d_gpu.restore_model(j, m_b)
                                s_gpu.restore_model(i, m_a)
                    if improved_step: break
                if improved_step: break
            if improved_step: break

        if improved_step:
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- Operator 3: Swap 2-1 and 1-2 ---
        for s_gpu in sources[:1]: # Top bottleneck only
            # Try 1-2 (Source gives 1, Dest gives 2)
            for i, m_a in enumerate(s_gpu.models):
                for d_gpu in destinations:
                    if d_gpu.id == s_gpu.id: continue
                    if d_gpu.kvpr() >= s_gpu.kvpr(): continue
                    if len(d_gpu.models) < 2: continue

                    n_d = len(d_gpu.models)
                    pair_found = False
                    for j1 in range(n_d):
                        for j2 in range(j1+1, n_d):
                            m_b1 = d_gpu.models[j1]
                            m_b2 = d_gpu.models[j2]

                            s_mem = s_gpu.used_mem - m_a.model_size + m_b1.model_size + m_b2.model_size
                            d_mem = d_gpu.used_mem - m_b1.model_size - m_b2.model_size + m_a.model_size

                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                s_gpu.remove(i)
                                d_gpu.remove(j2)
                                d_gpu.remove(j1)
                                s_gpu.add(m_b1)
                                s_gpu.add(m_b2)
                                d_gpu.add(m_a)

                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    pair_found = True
                                    break
                                else:
                                    d_gpu.remove(len(d_gpu.models)-1)
                                    s_gpu.remove(len(s_gpu.models)-1)
                                    s_gpu.remove(len(s_gpu.models)-1)
                                    d_gpu.restore_model(j1, m_b1)
                                    d_gpu.restore_model(j2, m_b2)
                        if pair_found: break
                    if pair_found: break
                if improved_step: break
            if improved_step: break

            # Try 2-1 (Source gives 2, Dest gives 1)
            if len(s_gpu.models) >= 2:
                n_s = len(s_gpu.models)
                pair_found = False
                for i1 in range(n_s):
                    for i2 in range(i1+1, n_s):
                        m_a1 = s_gpu.models[i1]
                        m_a2 = s_gpu.models[i2]

                        for d_gpu in destinations:
                            if d_gpu.id == s_gpu.id: continue
                            if d_gpu.kvpr() >= s_gpu.kvpr(): continue

                            for j, m_b in enumerate(d_gpu.models):
                                s_mem = s_gpu.used_mem - m_a1.model_size - m_a2.model_size + m_b.model_size
                                d_mem = d_gpu.used_mem - m_b.model_size + m_a1.model_size + m_a2.model_size

                                if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                    s_gpu.remove(i2)
                                    s_gpu.remove(i1)
                                    d_gpu.remove(j)
                                    s_gpu.add(m_b)
                                    d_gpu.add(m_a1)
                                    d_gpu.add(m_a2)

                                    new_vec = get_vector(current_gpus)
                                    if new_vec < current_vector:
                                        current_vector = new_vec
                                        improved_step = True
                                        pair_found = True
                                        break
                                    else:
                                        d_gpu.remove(len(d_gpu.models)-1)
                                        d_gpu.remove(len(d_gpu.models)-1)
                                        s_gpu.remove(len(s_gpu.models)-1)
                                        d_gpu.restore_model(j, m_b)
                                        s_gpu.restore_model(i1, m_a1)
                                        s_gpu.restore_model(i2, m_a2)
                            if pair_found: break
                        if pair_found: break
                    if pair_found: break
            if improved_step: break

        if improved_step:
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- PERTURBATION: Ruins and Recreate ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        # Ruin: Remove all models from worst GPU
        displaced_models = list(worst_gpu.models)
        while worst_gpu.models:
            worst_gpu.remove(0)

        random.shuffle(displaced_models)

        # Recreate: Distribute to others if possible
        other_gpus = [g for g in current_gpus if g.id != worst_gpu.id]

        for m in displaced_models:
            # Try to place in other GPUs minimizing local pressure increase
            best_dest = None
            best_dest_val = float('inf')

            for g in other_gpus:
                if g.can_fit(m.model_size):
                    # Metric: New KVPR if added
                    rem = GPU_MEM_SIZE - (g.used_mem + m.model_size)
                    if rem > 1e-7:
                        val = (g.load + m.req_rate/m.slo) / rem
                        if val < best_dest_val:
                            best_dest_val = val
                            best_dest = g

            if best_dest:
                best_dest.add(m)
            else:
                # Must put back in source
                worst_gpu.add(m)

        current_vector = get_vector(current_gpus)
```
Wait, in the previous attempt I noticed one potential issue: I might have had a space or tab difference in the indentation of `# --- Operator 3: ...`.

In the current code dump:
```python
        # --- Operator 3: Swap 2-1 and 1-2 ---
        for s_gpu in sources[:1]: # Top bottleneck only
```

I will try again with the text I just extracted from the "Current program" display in the prompt, which is definitely what is expected.

I will verify the indentation of `improved_step = False` is 8 spaces. Yes.

One possible culprit:
```python
            continue # Restart loop to re-evaluate bottleneck
```
versus
```python
            continue
```
in some versions.

In the provided "Current program":
```python
            iter_cnt += 1
            continue # Restart loop to re-evaluate bottleneck
```
(Line 257)

In my SEARCH block:
```python
            iter_cnt += 1
            continue # Restart loop to re-evaluate bottleneck
```

It matches.

I will attempt the edit again. I'm using the exact text from the prompt's `Current program`.

<NAME>
ils_steepest_descent_with_tabu
</NAME>

<DESCRIPTION>
Replaces the sequential Iterated Local Search (ILS) loop with a unified "Steepest Descent" approach that simultaneously evaluates Move, Swap 1-1, Swap 1-2, and Swap 2-1 operators. This ensures that in every step, the algorithm chooses the single best improvement available.

Additionally, a Tabu Search mechanism is introduced to prevent cycling (A->B->A moves). The perturbation strategy is also adjusted to be a targeted "guided kick" that moves a single model to a promising destination, rather than a destructive "Ruins and Recreate".
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # -------------------------------------------------------------------------
    # 2. Iterated Local Search
    # -------------------------------------------------------------------------
    iter_cnt = 0
    max_iter = 150

    while iter_cnt < max_iter:
        improved_step = False

        sorted_gpus = sorted(current_gpus, key=lambda g: g.kvpr(), reverse=True)
        # Focus on the absolute bottleneck
        source = sorted_gpus[0]

        # --- Operator 1: Move (Steepest Descent / Best Improvement) ---
        # Look for the single best move from the bottleneck to any other GPU
        best_move_indices = None # (model_idx, dest_gpu)
        best_move_vec = current_vector

        for i, model in enumerate(source.models):
            for dest in current_gpus:
                if dest.id == source.id: continue
                if dest.can_fit(model.model_size):
                    # Try move
                    source.remove(i)
                    dest.add(model)

                    new_vec = get_vector(current_gpus)
                    if new_vec < best_move_vec:
                        best_move_vec = new_vec
                        best_move_indices = (i, dest)

                    # Revert
                    dest.remove(len(dest.models)-1)
                    source.restore_model(i, model)

        if best_move_indices:
            # Apply best move
            idx, dest_gpu = best_move_indices
            model = source.remove(idx)
            dest_gpu.add(model)
            current_vector = best_move_vec
            improved_step = True

            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])

            iter_cnt += 1
            continue # Restart loop to re-evaluate bottleneck

        # --- Operator 2: Swap 1-1 (First Improvement) ---
        sources = sorted_gpus[:2]
        destinations = sorted_gpus[::-1]

        for s_gpu in sources:
            for i, m_a in enumerate(s_gpu.models):
                for d_gpu in destinations:
                    if d_gpu.id == s_gpu.id: continue
                    if d_gpu.kvpr() >= s_gpu.kvpr(): continue

                    for j, m_b in enumerate(d_gpu.models):
                        s_mem = s_gpu.used_mem - m_a.model_size + m_b.model_size
                        d_mem = d_gpu.used_mem - m_b.model_size + m_a.model_size
                        if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                            s_gpu.remove(i)
                            d_gpu.remove(j)
                            s_gpu.add(m_b)
                            d_gpu.add(m_a)

                            new_vec = get_vector(current_gpus)
                            if new_vec < current_vector:
                                current_vector = new_vec
                                improved_step = True
                                break
                            else:
                                d_gpu.remove(len(d_gpu.models)-1)
                                s_gpu.remove(len(s_gpu.models)-1)
                                d_gpu.restore_model(j, m_b)
                                s_gpu.restore_model(i, m_a)
                    if improved_step: break
                if improved_step: break
            if improved_step: break

        if improved_step:
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- Operator 3: Swap 2-1 and 1-2 ---
        for s_gpu in sources[:1]: # Top bottleneck only
            # Try 1-2 (Source gives 1, Dest gives 2)
            for i, m_a in enumerate(s_gpu.models):
                for d_gpu in destinations:
                    if d_gpu.id == s_gpu.id: continue
                    if d_gpu.kvpr() >= s_gpu.kvpr(): continue
                    if len(d_gpu.models) < 2: continue

                    n_d = len(d_gpu.models)
                    pair_found = False
                    for j1 in range(n_d):
                        for j2 in range(j1+1, n_d):
                            m_b1 = d_gpu.models[j1]
                            m_b2 = d_gpu.models[j2]

                            s_mem = s_gpu.used_mem - m_a.model_size + m_b1.model_size + m_b2.model_size
                            d_mem = d_gpu.used_mem - m_b1.model_size - m_b2.model_size + m_a.model_size

                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                s_gpu.remove(i)
                                d_gpu.remove(j2)
                                d_gpu.remove(j1)
                                s_gpu.add(m_b1)
                                s_gpu.add(m_b2)
                                d_gpu.add(m_a)

                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    pair_found = True
                                    break
                                else:
                                    d_gpu.remove(len(d_gpu.models)-1)
                                    s_gpu.remove(len(s_gpu.models)-1)
                                    s_gpu.remove(len(s_gpu.models)-1)
                                    d_gpu.restore_model(j1, m_b1)
                                    d_gpu.restore_model(j2, m_b2)
                        if pair_found: break
                    if pair_found: break
                if improved_step: break
            if improved_step: break

            # Try 2-1 (Source gives 2, Dest gives 1)
            if len(s_gpu.models) >= 2:
                n_s = len(s_gpu.models)
                pair_found = False
                for i1 in range(n_s):
                    for i2 in range(i1+1, n_s):
                        m_a1 = s_gpu.models[i1]
                        m_a2 = s_gpu.models[i2]

                        for d_gpu in destinations:
                            if d_gpu.id == s_gpu.id: continue
                            if d_gpu.kvpr() >= s_gpu.kvpr(): continue

                            for j, m_b in enumerate(d_gpu.models):
                                s_mem = s_gpu.used_mem - m_a1.model_size - m_a2.model_size + m_b.model_size
                                d_mem = d_gpu.used_mem - m_b.model_size + m_a1.model_size + m_a2.model_size

                                if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                    s_gpu.remove(i2)
                                    s_gpu.remove(i1)
                                    d_gpu.remove(j)
                                    s_gpu.add(m_b)
                                    d_gpu.add(m_a1)
                                    d_gpu.add(m_a2)

                                    new_vec = get_vector(current_gpus)
                                    if new_vec < current_vector:
                                        current_vector = new_vec
                                        improved_step = True
                                        pair_found = True
                                        break
                                    else:
                                        d_gpu.remove(len(d_gpu.models)-1)
                                        d_gpu.remove(len(d_gpu.models)-1)
                                        s_gpu.remove(len(s_gpu.models)-1)
                                        d_gpu.restore_model(j, m_b)
                                        s_gpu.restore_model(i1, m_a1)
                                        s_gpu.restore_model(i2, m_a2)
                            if pair_found: break
                        if pair_found: break
                    if pair_found: break
            if improved_step: break

        if improved_step:
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- PERTURBATION: Ruins and Recreate ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        # Ruin: Remove all models from worst GPU
        displaced_models = list(worst_gpu.models)
        while worst_gpu.models:
            worst_gpu.remove(0)

        random.shuffle(displaced_models)

        # Recreate: Distribute to others if possible
        other_gpus = [g for g in current_gpus if g.id != worst_gpu.id]

        for m in displaced_models:
            # Try to place in other GPUs minimizing local pressure increase
            best_dest = None
            best_dest_val = float('inf')

            for g in other_gpus:
                if g.can_fit(m.model_size):
                    # Metric: New KVPR if added
                    rem = GPU_MEM_SIZE - (g.used_mem + m.model_size)
                    if rem > 1e-7:
                        val = (g.load + m.req_rate/m.slo) / rem
                        if val < best_dest_val:
                            best_dest_val = val
                            best_dest = g

            if best_dest:
                best_dest.add(m)
            else:
                # Must put back in source
                worst_gpu.add(m)

        current_vector = get_vector(current_gpus)
=======
    # -------------------------------------------------------------------------
    # 2. Iterated Local Search (Steepest Descent with Tabu)
    # -------------------------------------------------------------------------
    iter_cnt = 0
    max_iter = 150

    # Tabu list: stores ((model, source_gpu_id), expiry_iter)
    tabu_list = {}
    tabu_tenure = 5

    def is_tabu(model, gpu_id, iter_curr):
        return (model, gpu_id) in tabu_list and tabu_list[(model, gpu_id)] > iter_curr

    def add_tabu(model, gpu_id, iter_curr):
        tabu_list[(model, gpu_id)] = iter_curr + tabu_tenure

    while iter_cnt < max_iter:
        improved_step = False

        sorted_gpus = sorted(current_gpus, key=lambda g: g.kvpr(), reverse=True)
        sources = sorted_gpus[:4]
        destinations = sorted_gpus[::-1]

        # Global Steepest Descent: Find the BEST move across all operators
        best_op_gain = current_vector
        best_op_action = None # (type, data...)

        # 1. Operator: Move
        for src in sources:
            for i, m in enumerate(src.models):
                if is_tabu(m, src.id, iter_cnt): continue
                for dst in destinations:
                    if dst.id == src.id: continue
                    if dst.can_fit(m.model_size):
                        src.remove(i)
                        dst.add(m)

                        new_vec = get_vector(current_gpus)
                        if new_vec < best_op_gain:
                            best_op_gain = new_vec
                            best_op_action = ('move', src, i, dst, m)

                        dst.remove(len(dst.models)-1)
                        src.restore_model(i, m)

        # 2. Operator: Swap 1-1
        for src in sources:
            for i, m_a in enumerate(src.models):
                if is_tabu(m_a, src.id, iter_cnt): continue
                for dst in destinations:
                    if dst.id == src.id: continue
                    if dst.kvpr() >= src.kvpr(): continue

                    for j, m_b in enumerate(dst.models):
                        if is_tabu(m_b, dst.id, iter_cnt): continue

                        s_mem = src.used_mem - m_a.model_size + m_b.model_size
                        d_mem = dst.used_mem - m_b.model_size + m_a.model_size
                        if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                            src.remove(i)
                            dst.remove(j)
                            src.add(m_b)
                            dst.add(m_a)

                            new_vec = get_vector(current_gpus)
                            if new_vec < best_op_gain:
                                best_op_gain = new_vec
                                best_op_action = ('swap11', src, i, m_a, dst, j, m_b)

                            dst.remove(len(dst.models)-1)
                            src.remove(len(src.models)-1)
                            dst.restore_model(j, m_b)
                            src.restore_model(i, m_a)

        # 3. Operator: Swap 1-2 (Source gives 1, Dest gives 2)
        for src in sources:
            for i, m_a in enumerate(src.models):
                if is_tabu(m_a, src.id, iter_cnt): continue
                for dst in destinations:
                    if dst.id == src.id: continue
                    if dst.kvpr() >= src.kvpr(): continue
                    if len(dst.models) < 2: continue

                    n_d = len(dst.models)
                    for j1 in range(n_d):
                        for j2 in range(j1+1, n_d):
                            m_b1 = dst.models[j1]
                            m_b2 = dst.models[j2]
                            if is_tabu(m_b1, dst.id, iter_cnt) or is_tabu(m_b2, dst.id, iter_cnt): continue

                            s_mem = src.used_mem - m_a.model_size + m_b1.model_size + m_b2.model_size
                            d_mem = dst.used_mem - m_b1.model_size - m_b2.model_size + m_a.model_size

                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                src.remove(i)
                                dst.remove(j2) # Higher index first
                                dst.remove(j1)
                                src.add(m_b1)
                                src.add(m_b2)
                                dst.add(m_a)

                                new_vec = get_vector(current_gpus)
                                if new_vec < best_op_gain:
                                    best_op_gain = new_vec
                                    best_op_action = ('swap12', src, i, m_a, dst, j1, m_b1, j2, m_b2)

                                dst.remove(len(dst.models)-1)
                                src.remove(len(src.models)-1)
                                src.remove(len(src.models)-1)
                                dst.restore_model(j1, m_b1)
                                dst.restore_model(j2, m_b2)
                                src.restore_model(i, m_a)

        # 4. Operator: Swap 2-1 (Source gives 2, Dest gives 1)
        for src in sources:
            if len(src.models) < 2: continue
            n_s = len(src.models)
            for i1 in range(n_s):
                for i2 in range(i1+1, n_s):
                    m_a1 = src.models[i1]
                    m_a2 = src.models[i2]
                    if is_tabu(m_a1, src.id, iter_cnt) or is_tabu(m_a2, src.id, iter_cnt): continue

                    for dst in destinations:
                        if dst.id == src.id: continue
                        if dst.kvpr() >= src.kvpr(): continue

                        for j, m_b in enumerate(dst.models):
                             if is_tabu(m_b, dst.id, iter_cnt): continue

                             s_mem = src.used_mem - m_a1.model_size - m_a2.model_size + m_b.model_size
                             d_mem = dst.used_mem - m_b.model_size + m_a1.model_size + m_a2.model_size

                             if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                 src.remove(i2) # Remove higher index first
                                 src.remove(i1)
                                 dst.remove(j)
                                 src.add(m_b)
                                 dst.add(m_a1)
                                 dst.add(m_a2)

                                 new_vec = get_vector(current_gpus)
                                 if new_vec < best_op_gain:
                                    best_op_gain = new_vec
                                    best_op_action = ('swap21', src, i1, m_a1, i2, m_a2, dst, j, m_b)

                                 dst.remove(len(dst.models)-1)
                                 dst.remove(len(dst.models)-1)
                                 src.remove(len(src.models)-1)
                                 dst.restore_model(j, m_b)
                                 src.restore_model(i1, m_a1)
                                 src.restore_model(i2, m_a2)

        # Apply Best Action
        if best_op_action:
            type_op = best_op_action[0]
            improved_step = True
            current_vector = best_op_gain

            if type_op == 'move':
                _, src, i, dst, m = best_op_action
                src.remove(i)
                dst.add(m)
                add_tabu(m, dst.id, iter_cnt)

            elif type_op == 'swap11':
                _, src, i, m_a, dst, j, m_b = best_op_action
                src.remove(i)
                dst.remove(j)
                src.add(m_b)
                dst.add(m_a)
                add_tabu(m_a, dst.id, iter_cnt)
                add_tabu(m_b, src.id, iter_cnt)

            elif type_op == 'swap12':
                _, src, i, m_a, dst, j1, m_b1, j2, m_b2 = best_op_action
                src.remove(i)
                dst.remove(j2)
                dst.remove(j1)
                src.add(m_b1)
                src.add(m_b2)
                dst.add(m_a)
                add_tabu(m_a, dst.id, iter_cnt)
                add_tabu(m_b1, src.id, iter_cnt)
                add_tabu(m_b2, src.id, iter_cnt)

            elif type_op == 'swap21':
                _, src, i1, m_a1, i2, m_a2, dst, j, m_b = best_op_action
                src.remove(i2)
                src.remove(i1)
                dst.remove(j)
                src.add(m_b)
                dst.add(m_a1)
                dst.add(m_a2)
                add_tabu(m_b, src.id, iter_cnt)
                add_tabu(m_a1, dst.id, iter_cnt)
                add_tabu(m_a2, dst.id, iter_cnt)

            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])

            iter_cnt += 1
            continue

        # --- PERTURBATION: Guided Kick ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        m_idx = random.randint(0, len(worst_gpu.models)-1)
        model_to_move = worst_gpu.models[m_idx]

        candidates = [g for g in current_gpus if g.id != worst_gpu.id and g.can_fit(model_to_move.model_size)]
        if candidates:
            # Pick from top 3 least loaded
            candidates.sort(key=lambda g: g.kvpr())
            target = random.choice(candidates[:3]) if len(candidates) >= 3 else candidates[0]

            worst_gpu.remove(m_idx)
            target.add(model_to_move)
            current_vector = get_vector(current_gpus)
            add_tabu(model_to_move, target.id, iter_cnt)
>>>>>>> REPLACE
</DIFF>