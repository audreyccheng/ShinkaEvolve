# EVOLVE-BLOCK-START
import math

GPU_MEM_SIZE = 80  # GB

def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.
    
    Algorithm:
    1. Calculate a theoretical lower bound for KVPR assuming fluid models.
    2. Binary Search for the optimal Max KVPR 'K'.
    3. The feasibility check 'can_pack(K)' attempts to fit models into GPUs
       subject to: sum(req/slo) + K * sum(size) <= K * MEM_SIZE.
       It uses Best Fit Decreasing with multiple sorting keys (Linearized Weight, Size, Load)
       to robustly find a packing in different constraint regimes.
    """
    
    # Precompute model characteristics
    # l = req_rate / slo, s = model_size
    model_data = []
    total_l = 0.0
    total_s = 0.0
    
    for m in models:
        l = m.req_rate / m.slo
        s = m.model_size
        model_data.append({
            'model': m,
            'l': l,
            's': s
        })
        total_l += l
        total_s += s

    # 1. Theoretical Lower Bound
    # If models were fluid, we could perfectly balance:
    # K_min = Total_L / (Total_Capacity - Total_S)
    remaining_mem_global = (gpu_num * GPU_MEM_SIZE) - total_s
    if remaining_mem_global <= 0:
         # This implies total model size > total GPU memory, strictly impossible.
         # We'll let the packing logic handle the failure or raise here.
         lower_bound_k = 0.0
    else:
         lower_bound_k = total_l / remaining_mem_global
    
    # 2. Feasibility Check with Multi-Strategy Best Fit
    def can_pack(target_k):
        capacity = target_k * GPU_MEM_SIZE
        
        # Define sorting strategies
        # Strategy A: Linearized Weight (L + K*S). 
        # Adapts to K: High K -> sort by Size; Low K -> sort by Load.
        strat_weight = sorted(
            model_data, 
            key=lambda x: x['l'] + target_k * x['s'], 
            reverse=True
        )
        
        # Strategy B: Size Decreasing.
        # Good for tight memory constraints.
        strat_size = sorted(
            model_data, 
            key=lambda x: x['s'], 
            reverse=True
        )
        
        # Strategy C: Load Decreasing.
        # Good for loose memory but tight load constraints.
        strat_load = sorted(
            model_data, 
            key=lambda x: x['l'], 
            reverse=True
        )
        
        # Try each strategy until one works
        for items in [strat_weight, strat_size, strat_load]:
            bins_l = [0.0] * gpu_num
            bins_s = [0.0] * gpu_num
            bins_models = [[] for _ in range(gpu_num)]
            possible = True
            
            for item in items:
                # Best Fit: Choose valid bin with minimal remaining slack
                # Slack in linearized constraint: (Capacity) - (Bin_W + Item_W)
                best_idx = -1
                min_slack = float('inf')
                
                w_item = item['l'] + target_k * item['s']
                
                for i in range(gpu_num):
                    # Hard Memory Constraint
                    # Use slightly larger epsilon for safety against float precision
                    if bins_s[i] + item['s'] >= GPU_MEM_SIZE - 1e-6:
                        continue
                    
                    # Linearized Capacity Constraint
                    w_bin = bins_l[i] + target_k * bins_s[i]
                    if w_bin + w_item <= capacity + 1e-9:
                        slack = capacity - (w_bin + w_item)
                        if slack < min_slack:
                            min_slack = slack
                            best_idx = i
                
                if best_idx != -1:
                    bins_l[best_idx] += item['l']
                    bins_s[best_idx] += item['s']
                    bins_models[best_idx].append(item['model'])
                else:
                    possible = False
                    break
            
            if possible:
                return bins_models
                
        return None

    # 3. Binary Search
    low = lower_bound_k
    high = max(lower_bound_k * 2, 1.0)
    best_placement = None
    
    # Exponential expansion to find valid upper bound
    # (Starting from a reasoned guess saves steps)
    found_high = False
    for _ in range(20):
        res = can_pack(high)
        if res is not None:
            best_placement = res
            found_high = True
            break
        low = high
        high *= 2.0
        
    if not found_high:
        # Last ditch attempt with massive K (effectively just memory packing)
        high = 1e12
        best_placement = can_pack(high)
        if best_placement is None:
            raise ValueError("Unable to fit models into GPU memory.")

    # Refine K with Binary Search
    for _ in range(40):
        mid = (low + high) / 2
        res = can_pack(mid)
        if res is not None:
            best_placement = res
            high = mid
        else:
            low = mid
            
    return {i: best_placement[i] for i in range(gpu_num)}

# EVOLVE-BLOCK-END