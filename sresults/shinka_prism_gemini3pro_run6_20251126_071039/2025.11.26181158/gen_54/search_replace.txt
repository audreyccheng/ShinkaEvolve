<NAME>
ils_best_improvement_and_stochastic_init
</NAME>

<DESCRIPTION>
1.  **Stochastic Multi-Start Initialization**: Modifies the `solve_linearized_bp` function to accept a `noise` parameter. This allows item weights to be perturbed (weighted sum of load and size) during bin packing, enabling the exploration of multiple basins of attraction. A loop is added to perform randomized refinements around the optimal `K` found by binary search.
2.  **Best Improvement Strategy for Move & Swap 1-1**: Transitions the "Move" and "Swap 1-1" operators from a "First Improvement" (taking the first valid better move) to a "Best Improvement" strategy. The algorithm now scans all valid moves/swaps involving the bottleneck GPUs and selects the single one that yields the lexicographically best improvement. This helps avoid getting trapped in shallow local optima.
3.  **Expanded Scope for Asymmetric Swaps**: Increases the number of source GPUs considered for "Swap 1-2" (from 2 to 3) and "Swap 2-1" (from 1 to 2) to better handle fragmentation on secondary bottlenecks.
4.  **Targeted Perturbation**: Improves the perturbation mechanism to preferentially move high-load models from the worst GPU, rather than random models, to more effectively relieve pressure when local search stagnates.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # A. Binary Search Linearization
    def solve_linearized_bp(target_k):
        bin_cap = target_k * GPU_MEM_SIZE
        items = []
        for m in models:
            w = (m.req_rate / m.slo) + target_k * m.model_size
            items.append((w, m))
        items.sort(key=lambda x: x[0], reverse=True)

        bins = [GPUState(i) for i in range(gpu_num)]
        for w, m in items:
            best_idx = -1
            min_rem_linear = float('inf')
            for i in range(gpu_num):
                if not bins[i].can_fit(m.model_size): continue
                lin_use = bins[i].load + target_k * bins[i].used_mem
                # Constraint: load + k*size <= k*cap
                if lin_use + w <= bin_cap:
                    # Best fit on linearized remainder
                    rem = bin_cap - (lin_use + w)
                    if rem < min_rem_linear:
                        min_rem_linear = rem
                        best_idx = i
            if best_idx != -1:
                bins[best_idx].add(m)
            else:
                return None
        return bins

    low, high = 0.0, 1000.0
    # Quick check for high
    if solve_linearized_bp(high) is None: high = 1e9

    bs_res = None
    for _ in range(20):
        mid = (low + high) / 2
        res = solve_linearized_bp(mid)
        if res:
            bs_res = res
            high = mid
        else:
            low = mid
    if bs_res: candidates.append(bs_res)
=======
    # A. Binary Search Linearization
    def solve_linearized_bp(target_k, noise=0.0):
        bin_cap = target_k * GPU_MEM_SIZE
        items = []
        for m in models:
            # Base weight: Load + K * Size
            base_w = (m.req_rate / m.slo) + target_k * m.model_size
            w = base_w
            if noise > 0:
                w *= random.uniform(1.0 - noise, 1.0 + noise)
            items.append((w, m, base_w))
        items.sort(key=lambda x: x[0], reverse=True)

        bins = [GPUState(i) for i in range(gpu_num)]
        for w, m, base_w in items:
            best_idx = -1
            min_rem_linear = float('inf')
            for i in range(gpu_num):
                if not bins[i].can_fit(m.model_size): continue
                lin_use = bins[i].load + target_k * bins[i].used_mem
                # Constraint: load + k*size <= k*cap (use deterministic weight for capacity check)
                if lin_use + base_w <= bin_cap:
                    # Best fit on linearized remainder
                    rem = bin_cap - (lin_use + base_w)
                    if rem < min_rem_linear:
                        min_rem_linear = rem
                        best_idx = i
            if best_idx != -1:
                bins[best_idx].add(m)
            else:
                return None
        return bins

    low, high = 0.0, 1000.0
    # Quick check for high
    if solve_linearized_bp(high) is None: high = 1e9

    bs_res = None
    best_k = high
    for _ in range(20):
        mid = (low + high) / 2
        res = solve_linearized_bp(mid)
        if res:
            bs_res = res
            best_k = mid
            high = mid
        else:
            low = mid
    if bs_res:
        candidates.append(bs_res)
        # Randomized Refinement around best_k
        for _ in range(15):
            # Vary K slightly and add item noise
            k_noisy = best_k * random.uniform(0.9, 1.1)
            res_noise = solve_linearized_bp(k_noisy, noise=0.05)
            if res_noise: candidates.append(res_noise)
>>>>>>> REPLACE
<<<<<<< SEARCH
    while iter_cnt < max_iter:
        improved_step = False

        # Sort GPUs: Sources are bottlenecks, Destinations are least loaded
        sorted_gpus = sorted(current_gpus, key=lambda g: g.kvpr(), reverse=True)
        sources = sorted_gpus[:4]
        destinations = sorted_gpus[::-1] # Check least loaded GPUs first

        # --- Operator 1: Move ---
        for source in sources:
            for i, model in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.can_fit(model.model_size):
                        source.remove(i)
                        dest.add(model)

                        new_vec = get_vector(current_gpus)
                        if new_vec < current_vector:
                            current_vector = new_vec
                            improved_step = True
                            break
                        else:
                            dest.remove(len(dest.models)-1)
                            source.restore_model(i, model)
                if improved_step: break
            if improved_step: break

        if improved_step:
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- Operator 2: Swap 1-1 ---
        for source in sources:
            for i, m_a in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue

                    for j, m_b in enumerate(dest.models):
                        s_mem = source.used_mem - m_a.model_size + m_b.model_size
                        d_mem = dest.used_mem - m_b.model_size + m_a.model_size
                        if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                            source.remove(i)
                            dest.remove(j)
                            source.add(m_b)
                            dest.add(m_a)

                            new_vec = get_vector(current_gpus)
                            if new_vec < current_vector:
                                current_vector = new_vec
                                improved_step = True
                                break
                            else:
                                dest.remove(len(dest.models)-1)
                                source.remove(len(source.models)-1)
                                dest.restore_model(j, m_b)
                                source.restore_model(i, m_a)
                    if improved_step: break
                if improved_step: break
            if improved_step: break

        if improved_step:
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- Operator 3: Swap 1-2 (Source gives 1, Dest gives 2) ---
        for source in sources[:2]:
            for i, m_a in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue
                    if len(dest.models) < 2: continue

                    n_d = len(dest.models)
                    pair_found = False
                    for j1 in range(n_d):
                        for j2 in range(j1+1, n_d):
                            m_b1 = dest.models[j1]
                            m_b2 = dest.models[j2]

                            s_mem = source.used_mem - m_a.model_size + m_b1.model_size + m_b2.model_size
                            d_mem = dest.used_mem - m_b1.model_size - m_b2.model_size + m_a.model_size

                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                source.remove(i)
                                dest.remove(j2)
                                dest.remove(j1)
                                source.add(m_b1)
                                source.add(m_b2)
                                dest.add(m_a)

                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    pair_found = True
                                    break
                                else:
                                    dest.remove(len(dest.models)-1) # m_a
                                    source.remove(len(source.models)-1) # m_b2
                                    source.remove(len(source.models)-1) # m_b1
                                    dest.restore_model(j1, m_b1)
                                    dest.restore_model(j2, m_b2)
                        if pair_found: break
                    if pair_found: break
                if improved_step: break
            if improved_step: break

        if improved_step:
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- Operator 4: Swap 2-1 (Source gives 2, Dest gives 1) ---
        for source in sources[:1]: # Expensive, check only most loaded
            if len(source.models) < 2: continue

            n_s = len(source.models)
            pair_found = False
            for i1 in range(n_s):
                for i2 in range(i1+1, n_s):
                    m_a1 = source.models[i1]
                    m_a2 = source.models[i2]

                    for dest in destinations:
                        if dest.id == source.id: continue
                        if dest.kvpr() >= source.kvpr(): continue

                        for j, m_b in enumerate(dest.models):
                            s_mem = source.used_mem - m_a1.model_size - m_a2.model_size + m_b.model_size
                            d_mem = dest.used_mem - m_b.model_size + m_a1.model_size + m_a2.model_size

                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                # Remove indices, higher first
                                source.remove(i2)
                                source.remove(i1)
                                dest.remove(j)

                                source.add(m_b)
                                dest.add(m_a1)
                                dest.add(m_a2)

                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    pair_found = True
                                    break
                                else:
                                    dest.remove(len(dest.models)-1) # m_a2
                                    dest.remove(len(dest.models)-1) # m_a1
                                    source.remove(len(source.models)-1) # m_b

                                    dest.restore_model(j, m_b)
                                    source.restore_model(i1, m_a1)
                                    source.restore_model(i2, m_a2)
                        if pair_found: break
                    if pair_found: break
                if pair_found: break
            if improved_step: break

        if improved_step:
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- Perturbation ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        m_idx = random.randint(0, len(worst_gpu.models)-1)
        model_to_move = worst_gpu.models[m_idx]

        # Guided Kick: Prefer better GPUs
        feasible_dests = [g for g in current_gpus if g.id != worst_gpu.id and g.can_fit(model_to_move.model_size)]

        if feasible_dests:
            feasible_dests.sort(key=lambda g: g.kvpr())
            # Pick from top 3 to avoid determinism loop
            target_candidates = feasible_dests[:3]
            dest = random.choice(target_candidates)

            worst_gpu.remove(m_idx)
            dest.add(model_to_move)
            current_vector = get_vector(current_gpus)
        else:
            break
=======
    while iter_cnt < max_iter:
        improved_step = False

        # Sort GPUs: Sources are bottlenecks, Destinations are least loaded
        sorted_gpus = sorted(current_gpus, key=lambda g: g.kvpr(), reverse=True)
        sources = sorted_gpus[:4]
        destinations = sorted_gpus[::-1]

        # --- Operator 1: Move (Best Improvement) ---
        best_move = None
        best_move_gain = current_vector

        for source in sources:
            for i, model in enumerate(source.models):
                for dest in current_gpus:
                    if dest.id == source.id: continue
                    if dest.can_fit(model.model_size):
                        source.remove(i)
                        dest.add(model)

                        new_vec = get_vector(current_gpus)
                        if new_vec < best_move_gain:
                            best_move_gain = new_vec
                            best_move = (source, i, dest, model)

                        dest.remove(len(dest.models)-1)
                        source.restore_model(i, model)

        if best_move:
            src, idx, dst, mdl = best_move
            src.remove(idx)
            dst.add(mdl)
            current_vector = best_move_gain
            improved_step = True

            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- Operator 2: Swap 1-1 (Best Improvement) ---
        best_swap = None
        best_swap_gain = current_vector

        for source in sources:
            for i, m_a in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue

                    for j, m_b in enumerate(dest.models):
                        # Optimization: Skip if swap likely bad (load diff check optional but helps speed)
                        if (m_b.req_rate/m_b.slo) >= (m_a.req_rate/m_a.slo): continue

                        s_mem = source.used_mem - m_a.model_size + m_b.model_size
                        d_mem = dest.used_mem - m_b.model_size + m_a.model_size

                        if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                            source.remove(i)
                            dest.remove(j)
                            source.add(m_b)
                            dest.add(m_a)

                            new_vec = get_vector(current_gpus)
                            if new_vec < best_swap_gain:
                                best_swap_gain = new_vec
                                best_swap = (source, i, m_a, dest, j, m_b)

                            dest.remove(len(dest.models)-1)
                            source.remove(len(source.models)-1)
                            dest.restore_model(j, m_b)
                            source.restore_model(i, m_a)

        if best_swap:
            src, i, ma, dst, j, mb = best_swap
            src.remove(i)
            dst.remove(j)
            src.add(mb)
            dst.add(ma)
            current_vector = best_swap_gain
            improved_step = True

            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- Operator 3: Swap 1-2 (Source gives 1, Dest gives 2) ---
        # Expanded to top 3 bottlenecks
        for source in sources[:3]:
            for i, m_a in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue
                    if len(dest.models) < 2: continue

                    n_d = len(dest.models)
                    pair_found = False
                    for j1 in range(n_d):
                        for j2 in range(j1+1, n_d):
                            m_b1 = dest.models[j1]
                            m_b2 = dest.models[j2]

                            s_mem = source.used_mem - m_a.model_size + m_b1.model_size + m_b2.model_size
                            d_mem = dest.used_mem - m_b1.model_size - m_b2.model_size + m_a.model_size

                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                source.remove(i)
                                dest.remove(j2)
                                dest.remove(j1)
                                source.add(m_b1)
                                source.add(m_b2)
                                dest.add(m_a)

                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    pair_found = True
                                    break
                                else:
                                    dest.remove(len(dest.models)-1) # m_a
                                    source.remove(len(source.models)-1) # m_b2
                                    source.remove(len(source.models)-1) # m_b1
                                    dest.restore_model(j1, m_b1)
                                    dest.restore_model(j2, m_b2)
                        if pair_found: break
                    if pair_found: break
                if improved_step: break
            if improved_step: break

        if improved_step:
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- Operator 4: Swap 2-1 (Source gives 2, Dest gives 1) ---
        # Expanded to top 2 bottlenecks
        for source in sources[:2]:
            if len(source.models) < 2: continue

            n_s = len(source.models)
            pair_found = False
            for i1 in range(n_s):
                for i2 in range(i1+1, n_s):
                    m_a1 = source.models[i1]
                    m_a2 = source.models[i2]

                    for dest in destinations:
                        if dest.id == source.id: continue
                        if dest.kvpr() >= source.kvpr(): continue

                        for j, m_b in enumerate(dest.models):
                            s_mem = source.used_mem - m_a1.model_size - m_a2.model_size + m_b.model_size
                            d_mem = dest.used_mem - m_b.model_size + m_a1.model_size + m_a2.model_size

                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                # Remove indices, higher first
                                source.remove(i2)
                                source.remove(i1)
                                dest.remove(j)

                                source.add(m_b)
                                dest.add(m_a1)
                                dest.add(m_a2)

                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    pair_found = True
                                    break
                                else:
                                    dest.remove(len(dest.models)-1) # m_a2
                                    dest.remove(len(dest.models)-1) # m_a1
                                    source.remove(len(source.models)-1) # m_b

                                    dest.restore_model(j, m_b)
                                    source.restore_model(i1, m_a1)
                                    source.restore_model(i2, m_a2)
                        if pair_found: break
                    if pair_found: break
                if pair_found: break
            if improved_step: break

        if improved_step:
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- Perturbation ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        # Guided Kick: Prefer moving high-load models
        sorted_models_idx = sorted(range(len(worst_gpu.models)),
                                   key=lambda k: worst_gpu.models[k].req_rate/worst_gpu.models[k].slo,
                                   reverse=True)
        # Pick from top 50%
        n_candidates = max(1, len(sorted_models_idx) // 2)
        m_idx_idx = random.randint(0, n_candidates - 1)
        m_idx = sorted_models_idx[m_idx_idx]

        model_to_move = worst_gpu.models[m_idx]

        feasible_dests = [g for g in current_gpus if g.id != worst_gpu.id and g.can_fit(model_to_move.model_size)]

        if feasible_dests:
            feasible_dests.sort(key=lambda g: g.kvpr())
            target_candidates = feasible_dests[:4]
            dest = random.choice(target_candidates)

            worst_gpu.remove(m_idx)
            dest.add(model_to_move)
            current_vector = get_vector(current_gpus)
        else:
            break
>>>>>>> REPLACE
</DIFF>