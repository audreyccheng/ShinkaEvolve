<NAME>
improved_packing_and_local_search
</NAME>

<DESCRIPTION>
This edit introduces two improvements to the model placement algorithm:

1.  **Enhanced Bin Packing Heuristics:** Adds a third sorting strategy (sorting by Load 'l' descending) to the `solve_packing` function. This increases the likelihood of finding a feasible packing during the binary search phase, especially in scenarios where load balancing is tighter than memory constraints.
2.  **Advanced Local Search with Swaps:** Replaces the simple greedy local search (which only moved models) with a Hill Climbing approach that attempts both **Moves** and **Swaps**. It maintains GPU states efficiently to perform multiple iterations (up to 100). This allows the algorithm to rebalance the load more effectively by swapping models between the highest-pressure GPU and others, potentially reducing the maximum KVPR further than simple moves could.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        # Strategy 2: Sort by Size Descending
        # Effective when memory is the primary constraint
        items_2 = sorted(model_data, key=lambda x: x['s'], reverse=True)
        res = try_best_fit(items_2)
        if res: return res

        return None
=======
        # Strategy 2: Sort by Size Descending
        # Effective when memory is the primary constraint
        items_2 = sorted(model_data, key=lambda x: x['s'], reverse=True)
        res = try_best_fit(items_2)
        if res: return res

        # Strategy 3: Sort by Load Descending
        # Effective when load is the primary constraint
        items_3 = sorted(model_data, key=lambda x: x['l'], reverse=True)
        res = try_best_fit(items_3)
        if res: return res

        return None
>>>>>>> REPLACE
<<<<<<< SEARCH
    # --- Phase 2: Local Search Refinement ---

    def calculate_gpu_kvpr(p_models):
        sl = sum(m.req_rate / m.slo for m in p_models)
        ss = sum(m.model_size for m in p_models)
        if ss >= GPU_MEM_SIZE: return float('inf')
        return sl / (GPU_MEM_SIZE - ss)

    # Greedily improve the worst GPU
    for _ in range(50):
        # Find current max KVPR GPU
        kvprs = {g: calculate_gpu_kvpr(placement[g]) for g in range(gpu_num)}
        max_gpu = max(kvprs, key=kvprs.get)
        max_val = kvprs[max_gpu]

        if max_val == 0: break # Optimal

        best_move = None
        best_improvement = 0.0

        # Try moving each model from max_gpu to any other GPU
        src_models = placement[max_gpu]

        for idx, model in enumerate(src_models):
            for tgt_gpu in range(gpu_num):
                if tgt_gpu == max_gpu: continue

                tgt_models = placement[tgt_gpu]

                # Check memory fit
                if sum(m.model_size for m in tgt_models) + model.model_size >= GPU_MEM_SIZE:
                    continue

                # Predict new KVPRs
                ns_l = sum(m.req_rate / m.slo for m in src_models) - (model.req_rate / model.slo)
                ns_s = sum(m.model_size for m in src_models) - model.model_size
                ns_kvpr = ns_l / (GPU_MEM_SIZE - ns_s) if ns_s < GPU_MEM_SIZE else float('inf')

                nt_l = sum(m.req_rate / m.slo for m in tgt_models) + (model.req_rate / model.slo)
                nt_s = sum(m.model_size for m in tgt_models) + model.model_size
                nt_kvpr = nt_l / (GPU_MEM_SIZE - nt_s) if nt_s < GPU_MEM_SIZE else float('inf')

                # We accept if the new local max is better than the old global max
                new_local_max = max(ns_kvpr, nt_kvpr)

                if new_local_max < max_val - 1e-5:
                    improv = max_val - new_local_max
                    if improv > best_improvement:
                        best_improvement = improv
                        best_move = (idx, tgt_gpu)

        if best_move:
            m_idx, t_gpu = best_move
            model_to_move = placement[max_gpu].pop(m_idx)
            placement[t_gpu].append(model_to_move)
        else:
            # No move from the bottleneck GPU improved the situation
            break

    return placement
=======
    # --- Phase 2: Local Search Refinement with Moves and Swaps ---

    # Initialize state for faster computation
    gpu_states = []
    for g in range(gpu_num):
        models_g = placement[g]
        s_g = sum(m.model_size for m in models_g)
        l_g = sum(m.req_rate / m.slo for m in models_g)
        gpu_states.append({'models': models_g, 's': s_g, 'l': l_g})

    def get_kvpr(l, s):
        if s >= GPU_MEM_SIZE - 1e-6: return 1e15 # Penalty for full/overflow
        return l / (GPU_MEM_SIZE - s)

    # Iterative improvement
    for _ in range(100):
        # Identify bottleneck GPU
        current_kvprs = [get_kvpr(gs['l'], gs['s']) for gs in gpu_states]
        max_kvpr = max(current_kvprs)
        max_gpu = current_kvprs.index(max_kvpr)

        if max_kvpr < 1e-9: break

        best_action = None
        best_new_max = max_kvpr

        # 1. Try Moving a model from max_gpu to any other GPU
        src_models = gpu_states[max_gpu]['models']
        for s_idx, model in enumerate(src_models):
            m_l = model.req_rate / model.slo
            m_s = model.model_size

            for t_gpu in range(gpu_num):
                if t_gpu == max_gpu: continue

                # Quick capacity check
                if gpu_states[t_gpu]['s'] + m_s >= GPU_MEM_SIZE: continue

                # Calc projected KVPRs
                new_src_l = gpu_states[max_gpu]['l'] - m_l
                new_src_s = gpu_states[max_gpu]['s'] - m_s
                new_tgt_l = gpu_states[t_gpu]['l'] + m_l
                new_tgt_s = gpu_states[t_gpu]['s'] + m_s

                ks = get_kvpr(new_src_l, new_src_s)
                kt = get_kvpr(new_tgt_l, new_tgt_s)

                local_max = max(ks, kt)

                # We want to reduce the max KVPR. Strict improvement required.
                if local_max < best_new_max - 1e-6:
                    best_new_max = local_max
                    best_action = ('move', s_idx, t_gpu)

        # 2. Try Swapping a model from max_gpu with a model from another GPU
        for s_idx, s_model in enumerate(src_models):
            s_l = s_model.req_rate / s_model.slo
            s_s = s_model.model_size

            for t_gpu in range(gpu_num):
                if t_gpu == max_gpu: continue

                # Optimization: Don't swap with a GPU that is already worse or equal
                if current_kvprs[t_gpu] >= max_kvpr: continue

                tgt_models = gpu_states[t_gpu]['models']
                for t_idx, t_model in enumerate(tgt_models):
                    t_l = t_model.req_rate / t_model.slo
                    t_s = t_model.model_size

                    # Capacity check
                    new_src_s = gpu_states[max_gpu]['s'] - s_s + t_s
                    new_tgt_s = gpu_states[t_gpu]['s'] - t_s + s_s

                    if new_src_s >= GPU_MEM_SIZE or new_tgt_s >= GPU_MEM_SIZE:
                        continue

                    new_src_l = gpu_states[max_gpu]['l'] - s_l + t_l
                    new_tgt_l = gpu_states[t_gpu]['l'] - t_l + s_l

                    ks = get_kvpr(new_src_l, new_src_s)
                    kt = get_kvpr(new_tgt_l, new_tgt_s)

                    local_max = max(ks, kt)

                    if local_max < best_new_max - 1e-6:
                        best_new_max = local_max
                        best_action = ('swap', s_idx, t_gpu, t_idx)

        # Apply best action
        if best_action:
            if best_action[0] == 'move':
                _, s_idx, t_gpu = best_action
                model = gpu_states[max_gpu]['models'].pop(s_idx)

                gpu_states[max_gpu]['l'] -= model.req_rate / model.slo
                gpu_states[max_gpu]['s'] -= model.model_size

                gpu_states[t_gpu]['models'].append(model)
                gpu_states[t_gpu]['l'] += model.req_rate / model.slo
                gpu_states[t_gpu]['s'] += model.model_size

            elif best_action[0] == 'swap':
                _, s_idx, t_gpu, t_idx = best_action
                s_model = gpu_states[max_gpu]['models'][s_idx]
                t_model = gpu_states[t_gpu]['models'][t_idx]

                # Swap in lists
                gpu_states[max_gpu]['models'][s_idx] = t_model
                gpu_states[t_gpu]['models'][t_idx] = s_model

                # Update stats
                sl_diff = (t_model.req_rate/t_model.slo) - (s_model.req_rate/s_model.slo)
                ss_diff = t_model.model_size - s_model.model_size

                gpu_states[max_gpu]['l'] += sl_diff
                gpu_states[max_gpu]['s'] += ss_diff

                gpu_states[t_gpu]['l'] -= sl_diff
                gpu_states[t_gpu]['s'] -= ss_diff
        else:
            break

    return {g: state['models'] for g, state in enumerate(gpu_states)}
>>>>>>> REPLACE
</DIFF>