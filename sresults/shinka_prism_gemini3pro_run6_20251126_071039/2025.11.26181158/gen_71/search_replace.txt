<NAME>
restore_best_ils_features_v2
</NAME>

<DESCRIPTION>
This edit restores and refines several features from the best-performing historical program that were lost in the current version.
1.  **Initialization**: In the randomized refinement of Binary Search Linearization, we now vary the target `K` (the weight balancing factor between load and size) in addition to adding noise to item weights.
2.  **ILS Operators**:
    *   The search depth for complex swaps (Swap 1-2 and Swap 2-1) is expanded to the top 4 bottleneck GPUs.
    *   The `destinations` list is iterated in reverse order of KVPR (least loaded first) for all operators.
    *   Swap 1-2 is prioritized before Swap 2-1.
3.  **Perturbation**: The "Alternating Ruin & Recreate" strategy is reinstated. It alternates between sorting by size (with noise) and sorting by load during the recreation phase.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    candidates = []

    # Strategy A: Binary Search Linearization + Randomized Refinement
    def solve_linearized_bin_packing(target_k, noise_factor=0.0):
        bin_cap = target_k * GPU_MEM_SIZE
        items = []
        for m in models:
            # Base weight
            w = (m.req_rate / m.slo) + target_k * m.model_size
            # Add noise if requested
            if noise_factor > 0:
                w *= random.uniform(1.0 - noise_factor, 1.0 + noise_factor)
            items.append((w, m))
        # Best Fit Decreasing based on linearized weight
        items.sort(key=lambda x: x[0], reverse=True)

        bins = [GPUState(i) for i in range(gpu_num)]
        for w, m in items:
            best_idx = -1
            min_rem_linear = float('inf')
            for i in range(gpu_num):
                if not bins[i].can_fit(m.model_size): continue
                lin_use = bins[i].load + target_k * bins[i].used_mem
                # Recalculate actual linear usage for constraint check (without noise)
                real_w = (m.req_rate / m.slo) + target_k * m.model_size
                if lin_use + real_w <= bin_cap:
                    rem = bin_cap - (lin_use + real_w)
                    if rem < min_rem_linear:
                        min_rem_linear = rem
                        best_idx = i
            if best_idx != -1: bins[best_idx].add(m)
            else: return None
        return bins

    # Binary Search for K
    low, high = 0.0, 1000.0
    if solve_linearized_bin_packing(high) is None: high = 1e9

    best_k = float('inf')
    bs_res = None
    for _ in range(20):
        mid = (low + high) / 2
        res = solve_linearized_bin_packing(mid)
        if res:
            bs_res = res
            best_k = mid
            high = mid
        else:
            low = mid

    if bs_res:
        candidates.append(bs_res)
        # Randomized Refinement around best_k
        # Run multiple times with noise to find potentially better packings
        for _ in range(10):
            res_noise = solve_linearized_bin_packing(best_k, noise_factor=0.05)
            if res_noise: candidates.append(res_noise)

    # Strategy B, C, D: Greedy heuristics
    strategies = [
        ('size', lambda m: m.model_size),
        ('load', lambda m: m.req_rate / m.slo),
        ('density', lambda m: (m.req_rate / m.slo) / m.model_size if m.model_size > 1e-7 else 0)
    ]

    for _, key_fn in strategies:
        gpus = [GPUState(i) for i in range(gpu_num)]
        valid = True
        for m in sorted(models, key=key_fn, reverse=True):
            best_idx = -1
            best_val = float('inf')
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (gpus[i].used_mem + m.model_size)
                    val = (gpus[i].load + m.req_rate/m.slo) / rem if rem > 1e-7 else float('inf')
                    if val < best_val:
                        best_val = val
                        best_idx = i
            if best_idx != -1:
                gpus[best_idx].add(m)
            else:
                valid = False
                break
        if valid: candidates.append(gpus)

    # Strategy E: Randomized Greedy (5 attempts) to increase diversity
    for _ in range(5):
        gpus = [GPUState(i) for i in range(gpu_num)]
        shuffled_models = list(models)
        random.shuffle(shuffled_models)
        valid = True
        for m in shuffled_models:
            best_idx = -1
            best_val = float('inf')
            # Try to place minimizing local KVPR increase
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (gpus[i].used_mem + m.model_size)
                    val = (gpus[i].load + m.req_rate/m.slo) / rem if rem > 1e-7 else float('inf')
                    if val < best_val:
                        best_val = val
                        best_idx = i
            if best_idx != -1:
                gpus[best_idx].add(m)
            else:
                valid = False
                break
        if valid: candidates.append(gpus)

    if not candidates:
        gpus = [GPUState(i) for i in range(gpu_num)]
        for m in sorted(models, key=lambda x: x.model_size, reverse=True):
            placed = False
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    gpus[i].add(m)
                    placed = True
                    break
            if not placed: raise ValueError("Models do not fit in GPU memory.")
        candidates.append(gpus)

    # Pick best start
    current_gpus = min(candidates, key=lambda g: get_vector(g))
    current_vector = get_vector(current_gpus)

    best_gpus = [GPUState(i) for i in range(gpu_num)]
    for i in range(gpu_num): best_gpus[i].copy_from(current_gpus[i])
    best_vector = current_vector

    # -------------------------------------------------------------------------
    # 2. Iterated Local Search (ILS) with Best Improvement
    # -------------------------------------------------------------------------

    iter_cnt = 0
    max_iter = 200

    while iter_cnt < max_iter:
        improved_step = False

        # Identify bottlenecks
        sorted_gpus = sorted(current_gpus, key=lambda g: g.kvpr(), reverse=True)
        sources = sorted_gpus[:4] # Top bottlenecks
        destinations = sorted_gpus[::-1] # Least loaded first

        # --- Operator 1: MOVE (Best Improvement) ---
        best_move = None
        best_move_gain = current_vector

        for source in sources:
            for i, model in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.can_fit(model.model_size):
                        source.remove(i)
                        dest.add(model)

                        new_vec = get_vector(current_gpus)
                        if new_vec < best_move_gain:
                            best_move_gain = new_vec
                            best_move = (source, i, dest, model)

                        dest.remove(len(dest.models)-1)
                        source.restore_model(i, model)

        if best_move:
            src, idx, dst, mdl = best_move
            src.remove(idx)
            dst.add(mdl)
            current_vector = best_move_gain
            improved_step = True

            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- Operator 2: SWAP 1-1 (Best Improvement) ---
        best_swap = None
        best_swap_gain = current_vector

        for source in sources:
            for i, m_a in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue

                    for j, m_b in enumerate(dest.models):
                        # Optimization: source must reduce load
                        if (m_b.req_rate/m_b.slo) >= (m_a.req_rate/m_a.slo): continue

                        # Capacity check
                        s_mem = source.used_mem - m_a.model_size + m_b.model_size
                        d_mem = dest.used_mem - m_b.model_size + m_a.model_size

                        if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                            source.remove(i)
                            dest.remove(j)
                            source.add(m_b)
                            dest.add(m_a)

                            new_vec = get_vector(current_gpus)
                            if new_vec < best_swap_gain:
                                best_swap_gain = new_vec
                                best_swap = (source, i, m_a, dest, j, m_b)

                            dest.remove(len(dest.models)-1)
                            source.remove(len(source.models)-1)
                            dest.restore_model(j, m_b)
                            source.restore_model(i, m_a)

        if best_swap:
            src, i, ma, dst, j, mb = best_swap
            src.remove(i)
            dst.remove(j)
            src.add(mb)
            dst.add(ma)
            current_vector = best_swap_gain
            improved_step = True

            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- Operator 3: SWAP 2-1 (Source gives 2, Dest gives 1, First Improvement) ---
        # Expanded to top 3 bottlenecks
        for source in sources[:3]:
            if len(source.models) < 2: continue

            n_s = len(source.models)
            pair_found = False
            for i1 in range(n_s):
                for i2 in range(i1+1, n_s):
                    m_a1 = source.models[i1]
                    m_a2 = source.models[i2]

                    for dest in destinations:
                        if dest.id == source.id: continue
                        if dest.kvpr() >= source.kvpr(): continue

                        for j, m_b in enumerate(dest.models):
                            s_mem = source.used_mem - m_a1.model_size - m_a2.model_size + m_b.model_size
                            d_mem = dest.used_mem - m_b.model_size + m_a1.model_size + m_a2.model_size

                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                source.remove(i2) # Larger idx first
                                source.remove(i1)
                                dest.remove(j)
                                source.add(m_b)
                                dest.add(m_a1)
                                dest.add(m_a2)

                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    pair_found = True
                                    break
                                else:
                                    dest.remove(len(dest.models)-1)
                                    dest.remove(len(dest.models)-1)
                                    source.remove(len(source.models)-1)
                                    dest.restore_model(j, m_b)
                                    source.restore_model(i1, m_a1)
                                    source.restore_model(i2, m_a2)
                        if pair_found: break
                    if pair_found: break
                if pair_found: break
            if improved_step: break

        if improved_step:
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- Operator 4: SWAP 1-2 (Source gives 1, Dest gives 2, First Improvement) ---
        for source in sources[:3]:
            for i, m_a in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue
                    if len(dest.models) < 2: continue

                    n_d = len(dest.models)
                    pair_found = False
                    for j1 in range(n_d):
                        for j2 in range(j1 + 1, n_d):
                            m_b1 = dest.models[j1]
                            m_b2 = dest.models[j2]

                            s_mem = source.used_mem - m_a.model_size + m_b1.model_size + m_b2.model_size
                            d_mem = dest.used_mem - m_b1.model_size - m_b2.model_size + m_a.model_size

                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                source.remove(i)
                                dest.remove(j2) # Larger idx first
                                dest.remove(j1)
                                source.add(m_b1)
                                source.add(m_b2)
                                dest.add(m_a)

                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    pair_found = True
                                    break
                                else:
                                    dest.remove(len(dest.models)-1) # m_a
                                    source.remove(len(source.models)-1) # m_b2
                                    source.remove(len(source.models)-1) # m_b1
                                    dest.restore_model(j1, m_b1)
                                    dest.restore_model(j2, m_b2)
                        if pair_found: break
                    if pair_found: break
                if improved_step: break
            if improved_step: break

        if improved_step:
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- PERTURBATION (Ruins and Recreate) ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        # Ruin: Remove all models from the bottleneck GPU
        displaced_models = []
        while worst_gpu.models:
            displaced_models.append(worst_gpu.remove(0))

        # Recreate: Try to distribute models to other GPUs
        # Sort by load to put heavy items first
        displaced_models.sort(key=lambda m: m.req_rate/m.slo, reverse=True)

        # Candidate GPUs (excluding source initially to force movement)
        other_gpus = [g for g in current_gpus if g.id != worst_gpu.id]

        for m in displaced_models:
            best_dest = None
            best_dest_val = float('inf')

            # Find Best Fit destination
            for dest in other_gpus:
                if dest.can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (dest.used_mem + m.model_size)
                    if rem > 1e-7:
                        val = (dest.load + m.req_rate/m.slo) / rem
                        if val < best_dest_val:
                            best_dest_val = val
                            best_dest = dest

            if best_dest:
                best_dest.add(m)
            else:
                # Fallback: put back in source
                worst_gpu.add(m)

        current_vector = get_vector(current_gpus)
        if current_vector < best_vector:
            best_vector = current_vector
            for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])

    return {g.id: g.models for g in best_gpus}
=======
    candidates = []

    # Strategy A: Binary Search Linearization + Randomized Refinement
    def solve_linearized_bin_packing(target_k, noise_factor=0.0):
        bin_cap = target_k * GPU_MEM_SIZE
        items = []
        for m in models:
            # Base weight: Load + K * Size
            base_w = (m.req_rate / m.slo) + target_k * m.model_size
            w = base_w
            if noise_factor > 0:
                w *= random.uniform(1.0 - noise_factor, 1.0 + noise_factor)
            items.append((w, m, base_w))
        items.sort(key=lambda x: x[0], reverse=True)

        bins = [GPUState(i) for i in range(gpu_num)]
        for w, m, base_w in items:
            best_idx = -1
            min_rem_linear = float('inf')
            for i in range(gpu_num):
                if not bins[i].can_fit(m.model_size): continue
                lin_use = bins[i].load + target_k * bins[i].used_mem
                # Constraint
                if lin_use + base_w <= bin_cap:
                    rem = bin_cap - (lin_use + base_w)
                    if rem < min_rem_linear:
                        min_rem_linear = rem
                        best_idx = i
            if best_idx != -1: bins[best_idx].add(m)
            else: return None
        return bins

    low, high = 0.0, 1000.0
    if solve_linearized_bin_packing(high) is None: high = 1e9

    bs_res = None
    best_k = high
    for _ in range(20):
        mid = (low + high) / 2
        res = solve_linearized_bin_packing(mid)
        if res:
            bs_res = res
            best_k = mid
            high = mid
        else:
            low = mid

    if bs_res:
        candidates.append(bs_res)
        # Randomized Refinement: vary K and add noise
        for _ in range(15):
            k_noisy = best_k * random.uniform(0.9, 1.1)
            res_noise = solve_linearized_bin_packing(k_noisy, noise_factor=0.05)
            if res_noise: candidates.append(res_noise)

    # Strategy B, C, D: Greedy heuristics
    strategies = [
        ('size', lambda m: m.model_size),
        ('load', lambda m: m.req_rate / m.slo),
        ('density', lambda m: (m.req_rate / m.slo) / m.model_size if m.model_size > 1e-7 else 0)
    ]

    for _, key_fn in strategies:
        gpus = [GPUState(i) for i in range(gpu_num)]
        valid = True
        for m in sorted(models, key=key_fn, reverse=True):
            best_idx = -1
            best_val = float('inf')
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (gpus[i].used_mem + m.model_size)
                    val = (gpus[i].load + m.req_rate/m.slo) / rem if rem > 1e-7 else float('inf')
                    if val < best_val:
                        best_val = val
                        best_idx = i
            if best_idx != -1: gpus[best_idx].add(m)
            else: valid = False; break
        if valid: candidates.append(gpus)

    # Strategy E: Randomized Greedy (for diversity)
    for _ in range(5):
        gpus = [GPUState(i) for i in range(gpu_num)]
        shuffled = list(models)
        random.shuffle(shuffled)
        valid = True
        for m in shuffled:
            best_idx = -1
            best_val = float('inf')
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (gpus[i].used_mem + m.model_size)
                    val = (gpus[i].load + m.req_rate/m.slo) / rem if rem > 1e-7 else float('inf')
                    if val < best_val:
                        best_val = val
                        best_idx = i
            if best_idx != -1: gpus[best_idx].add(m)
            else: valid = False; break
        if valid: candidates.append(gpus)

    if not candidates:
        gpus = [GPUState(i) for i in range(gpu_num)]
        for m in sorted(models, key=lambda x: x.model_size, reverse=True):
            placed = False
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    gpus[i].add(m)
                    placed = True
                    break
            if not placed: raise ValueError("Models do not fit in GPU memory.")
        candidates.append(gpus)

    # Pick best start
    current_gpus = min(candidates, key=lambda g: get_vector(g))
    current_vector = get_vector(current_gpus)

    best_gpus = [GPUState(i) for i in range(gpu_num)]
    for i in range(gpu_num): best_gpus[i].copy_from(current_gpus[i])
    best_vector = current_vector

    # -------------------------------------------------------------------------
    # 2. Iterated Local Search (ILS) with Best Improvement
    # -------------------------------------------------------------------------

    iter_cnt = 0
    max_iter = 200

    while iter_cnt < max_iter:
        improved_step = False

        # Identify bottlenecks
        sorted_gpus = sorted(current_gpus, key=lambda g: g.kvpr(), reverse=True)
        sources = sorted_gpus[:4] # Top bottlenecks
        destinations = sorted_gpus[::-1] # Least loaded first

        # --- Operator 1: MOVE (Best Improvement) ---
        best_move = None
        best_move_gain = current_vector

        for source in sources:
            for i, model in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.can_fit(model.model_size):
                        source.remove(i)
                        dest.add(model)

                        new_vec = get_vector(current_gpus)
                        if new_vec < best_move_gain:
                            best_move_gain = new_vec
                            best_move = (source, i, dest, model)

                        dest.remove(len(dest.models)-1)
                        source.restore_model(i, model)

        if best_move:
            src, idx, dst, mdl = best_move
            src.remove(idx)
            dst.add(mdl)
            current_vector = best_move_gain
            improved_step = True

            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1
            continue

        # --- Operator 2: SWAP 1-1 (Best Improvement) ---
        best_swap = None
        best_swap_gain = current_vector

        for source in sources:
            for i, m_a in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue

                    for j, m_b in enumerate(dest.models):
                        # Optimization: source must reduce load
                        if (m_b.req_rate/m_b.slo) >= (m_a.req_rate/m_a.slo): continue

                        # Capacity check
                        s_mem