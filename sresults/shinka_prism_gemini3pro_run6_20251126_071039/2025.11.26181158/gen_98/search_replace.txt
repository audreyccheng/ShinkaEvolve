<NAME>
improved_local_search_swaps
</NAME>

<DESCRIPTION>
Enhances the local search phase by adding support for (2,1) swaps (moving two models from the bottleneck GPU in exchange for one from another GPU). This helps in resolving fragmentation and balancing the load more effectively than simple 1-1 swaps or moves.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # --- Phase 2: Local Search Refinement with Moves and Swaps ---

    # Initialize state for faster computation
    gpu_states = []
    for g in range(gpu_num):
        models_g = placement[g]
        s_g = sum(m.model_size for m in models_g)
        l_g = sum(m.req_rate / m.slo for m in models_g)
        gpu_states.append({'models': models_g, 's': s_g, 'l': l_g})

    def get_kvpr(l, s):
        if s >= GPU_MEM_SIZE - 1e-6: return 1e15 # Penalty for full/overflow
        return l / (GPU_MEM_SIZE - s)

    # Iterative improvement
    for _ in range(100):
        # Identify bottleneck GPU
        current_kvprs = [get_kvpr(gs['l'], gs['s']) for gs in gpu_states]
        max_kvpr = max(current_kvprs)
        max_gpu = current_kvprs.index(max_kvpr)

        if max_kvpr < 1e-9: break

        best_action = None
        best_new_max = max_kvpr

        # 1. Try Moving a model from max_gpu to any other GPU
        src_models = gpu_states[max_gpu]['models']
        for s_idx, model in enumerate(src_models):
            m_l = model.req_rate / model.slo
            m_s = model.model_size

            for t_gpu in range(gpu_num):
                if t_gpu == max_gpu: continue

                # Quick capacity check
                if gpu_states[t_gpu]['s'] + m_s >= GPU_MEM_SIZE: continue

                # Calc projected KVPRs
                new_src_l = gpu_states[max_gpu]['l'] - m_l
                new_src_s = gpu_states[max_gpu]['s'] - m_s
                new_tgt_l = gpu_states[t_gpu]['l'] + m_l
                new_tgt_s = gpu_states[t_gpu]['s'] + m_s

                ks = get_kvpr(new_src_l, new_src_s)
                kt = get_kvpr(new_tgt_l, new_tgt_s)

                local_max = max(ks, kt)

                # We want to reduce the max KVPR. Strict improvement required.
                if local_max < best_new_max - 1e-6:
                    best_new_max = local_max
                    best_action = ('move', s_idx, t_gpu)

        # 2. Try Swapping a model from max_gpu with a model from another GPU
        for s_idx, s_model in enumerate(src_models):
            s_l = s_model.req_rate / s_model.slo
            s_s = s_model.model_size

            for t_gpu in range(gpu_num):
                if t_gpu == max_gpu: continue

                # Optimization: Don't swap with a GPU that is already worse or equal
                if current_kvprs[t_gpu] >= max_kvpr: continue

                tgt_models = gpu_states[t_gpu]['models']
                for t_idx, t_model in enumerate(tgt_models):
                    t_l = t_model.req_rate / t_model.slo
                    t_s = t_model.model_size

                    # Capacity check
                    new_src_s = gpu_states[max_gpu]['s'] - s_s + t_s
                    new_tgt_s = gpu_states[t_gpu]['s'] - t_s + s_s

                    if new_src_s >= GPU_MEM_SIZE or new_tgt_s >= GPU_MEM_SIZE:
                        continue

                    new_src_l = gpu_states[max_gpu]['l'] - s_l + t_l
                    new_tgt_l = gpu_states[t_gpu]['l'] - t_l + s_l

                    ks = get_kvpr(new_src_l, new_src_s)
                    kt = get_kvpr(new_tgt_l, new_tgt_s)

                    local_max = max(ks, kt)

                    if local_max < best_new_max - 1e-6:
                        best_new_max = local_max
                        best_action = ('swap', s_idx, t_gpu, t_idx)

        # Apply best action
        if best_action:
            if best_action[0] == 'move':
                _, s_idx, t_gpu = best_action
                model = gpu_states[max_gpu]['models'].pop(s_idx)

                gpu_states[max_gpu]['l'] -= model.req_rate / model.slo
                gpu_states[max_gpu]['s'] -= model.model_size

                gpu_states[t_gpu]['models'].append(model)
                gpu_states[t_gpu]['l'] += model.req_rate / model.slo
                gpu_states[t_gpu]['s'] += model.model_size

            elif best_action[0] == 'swap':
                _, s_idx, t_gpu, t_idx = best_action
                s_model = gpu_states[max_gpu]['models'][s_idx]
                t_model = gpu_states[t_gpu]['models'][t_idx]

                # Swap in lists
                gpu_states[max_gpu]['models'][s_idx] = t_model
                gpu_states[t_gpu]['models'][t_idx] = s_model

                # Update stats
                sl_diff = (t_model.req_rate/t_model.slo) - (s_model.req_rate/s_model.slo)
                ss_diff = t_model.model_size - s_model.model_size

                gpu_states[max_gpu]['l'] += sl_diff
                gpu_states[max_gpu]['s'] += ss_diff

                gpu_states[t_gpu]['l'] -= sl_diff
                gpu_states[t_gpu]['s'] -= ss_diff
        else:
            break

    return {g: state['models'] for g, state in enumerate(gpu_states)}
=======
    # --- Phase 2: Local Search Refinement with Moves and Swaps ---

    # Initialize state for faster computation
    gpu_states = []
    for g in range(gpu_num):
        models_g = placement[g]
        s_g = sum(m.model_size for m in models_g)
        l_g = sum(m.req_rate / m.slo for m in models_g)
        gpu_states.append({'models': models_g, 's': s_g, 'l': l_g})

    def get_kvpr(l, s):
        if s >= GPU_MEM_SIZE - 1e-6: return 1e15 # Penalty for full/overflow
        return l / (GPU_MEM_SIZE - s)

    # Iterative improvement
    for _ in range(200):
        # Identify bottleneck GPU
        current_kvprs = [get_kvpr(gs['l'], gs['s']) for gs in gpu_states]
        max_kvpr = max(current_kvprs)
        max_gpu = current_kvprs.index(max_kvpr)

        if max_kvpr < 1e-9: break

        best_action = None
        best_new_max = max_kvpr

        # Helper to check if a move/swap improves global state
        def check_improvement(new_src_l, new_src_s, new_tgt_l, new_tgt_s, action):
            nonlocal best_new_max, best_action
            if new_src_s >= GPU_MEM_SIZE or new_tgt_s >= GPU_MEM_SIZE:
                return
            ks = get_kvpr(new_src_l, new_src_s)
            kt = get_kvpr(new_tgt_l, new_tgt_s)
            local_max = max(ks, kt)
            if local_max < best_new_max - 1e-7:
                best_new_max = local_max
                best_action = action

        src = gpu_states[max_gpu]
        src_data = [(m.req_rate/m.slo, m.model_size) for m in src['models']]
        n_src = len(src_data)

        for t_gpu in range(gpu_num):
            if t_gpu == max_gpu: continue
            if current_kvprs[t_gpu] >= max_kvpr: continue # Optimization

            tgt = gpu_states[t_gpu]
            tgt_data = [(m.req_rate/m.slo, m.model_size) for m in tgt['models']]
            n_tgt = len(tgt_data)

            # 1. Move: src -> tgt
            for i in range(n_src):
                l_i, s_i = src_data[i]
                check_improvement(
                    src['l'] - l_i, src['s'] - s_i,
                    tgt['l'] + l_i, tgt['s'] + s_i,
                    ('move', i, t_gpu)
                )

            # 2. Swap 1-1: src[i] <-> tgt[j]
            for i in range(n_src):
                l_i, s_i = src_data[i]
                for j in range(n_tgt):
                    l_j, s_j = tgt_data[j]
                    check_improvement(
                        src['l'] - l_i + l_j, src['s'] - s_i + s_j,
                        tgt['l'] - l_j + l_i, tgt['s'] - s_j + s_i,
                        ('swap11', i, t_gpu, j)
                    )

            # 3. Swap 2-1: src[i1, i2] <-> tgt[j]
            # Moving two small items out of bottleneck, taking one large item?
            # Or just moving mass out.
            for i1 in range(n_src):
                for i2 in range(i1 + 1, n_src):
                    l_out = src_data[i1][0] + src_data[i2][0]
                    s_out = src_data[i1][1] + src_data[i2][1]
                    for j in range(n_tgt):
                        l_j, s_j = tgt_data[j]
                        check_improvement(
                            src['l'] - l_out + l_j, src['s'] - s_out + s_j,
                            tgt['l'] - l_j + l_out, tgt['s'] - s_j + s_out,
                            ('swap21', i1, i2, t_gpu, j)
                        )

        # Apply best action
        if best_action:
            if best_action[0] == 'move':
                _, i, t_gpu = best_action
                m = gpu_states[max_gpu]['models'].pop(i)
                gpu_states[t_gpu]['models'].append(m)

                # Update stats manually to avoid drift/recalc
                l_m, s_m = m.req_rate/m.slo, m.model_size
                gpu_states[max_gpu]['l'] -= l_m; gpu_states[max_gpu]['s'] -= s_m
                gpu_states[t_gpu]['l'] += l_m; gpu_states[t_gpu]['s'] += s_m

            elif best_action[0] == 'swap11':
                _, i, t_gpu, j = best_action
                m_src = gpu_states[max_gpu]['models'][i]
                m_tgt = gpu_states[t_gpu]['models'][j]

                gpu_states[max_gpu]['models'][i] = m_tgt
                gpu_states[t_gpu]['models'][j] = m_src

                l_src, s_src = m_src.req_rate/m_src.slo, m_src.model_size
                l_tgt, s_tgt = m_tgt.req_rate/m_tgt.slo, m_tgt.model_size

                diff_l = l_tgt - l_src
                diff_s = s_tgt - s_src

                gpu_states[max_gpu]['l'] += diff_l; gpu_states[max_gpu]['s'] += diff_s
                gpu_states[t_gpu]['l'] -= diff_l; gpu_states[t_gpu]['s'] -= diff_s

            elif best_action[0] == 'swap21':
                _, i1, i2, t_gpu, j = best_action
                # Pop indices carefully (larger first)
                m1 = gpu_states[max_gpu]['models'].pop(i2)
                m2 = gpu_states[max_gpu]['models'].pop(i1)
                m_tgt = gpu_states[t_gpu]['models'][j]

                gpu_states[max_gpu]['models'].append(m_tgt)
                gpu_states[t_gpu]['models'][j] = m1
                gpu_states[t_gpu]['models'].append(m2)

                l_out = (m1.req_rate/m1.slo) + (m2.req_rate/m2.slo)
                s_out = m1.model_size + m2.model_size
                l_in = m_tgt.req_rate/m_tgt.slo
                s_in = m_tgt.model_size

                gpu_states[max_gpu]['l'] += l_in - l_out
                gpu_states[max_gpu]['s'] += s_in - s_out
                gpu_states[t_gpu]['l'] += l_out - l_in
                gpu_states[t_gpu]['s'] += s_out - s_in
        else:
            break

    return {g: state['models'] for g, state in enumerate(gpu_states)}
>>>>>>> REPLACE
</DIFF>