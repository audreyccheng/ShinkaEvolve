<NAME>
packing_density_heuristic
</NAME>

<DESCRIPTION>
Add a fourth sorting heuristic to the bin packing feasibility check: sorting by Load/Size (density). This helps in scenarios where items have high load but small size, allowing them to be packed efficiently into remaining space gaps.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        # Strategy 3: Sort by Load Descending
        items_3 = sorted(model_data, key=lambda x: x['l'], reverse=True)
        res = try_best_fit(items_3)
        if res: return res

        return None
=======
        # Strategy 3: Sort by Load Descending
        items_3 = sorted(model_data, key=lambda x: x['l'], reverse=True)
        res = try_best_fit(items_3)
        if res: return res

        # Strategy 4: Sort by Density (Load / Size) Descending
        items_4 = sorted(model_data, key=lambda x: x['l'] / x['s'] if x['s'] > 0 else 0.0, reverse=True)
        res = try_best_fit(items_4)
        if res: return res

        return None
>>>>>>> REPLACE
</DIFF>

<NAME>
local_search_2_for_1
</NAME>

<DESCRIPTION>
Enhance the local search with "2-for-1 Swaps". This allows swapping two models from the bottleneck GPU with one model from a target GPU. This is crucial for overcoming memory capacity constraints where 1-for-1 swaps fail due to size fragmentation (e.g., swapping two 10GB models for one 20GB model). It also optimizes the standard 1-for-1 swap with index assignment for speed and adds a prune condition to skip targets that are already bottlenecks.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        # 2. Try Swapping a model from max_gpu <-> model from tgt_gpu
        for m1_idx, m1 in enumerate(src_st['models']):
            m1_l = m1.req_rate / m1.slo
            m1_s = m1.model_size

            for tgt_idx in range(gpu_num):
                if tgt_idx == max_gpu: continue
                tgt_st = gpu_state[tgt_idx]

                for m2_idx, m2 in enumerate(tgt_st['models']):
                    m2_l = m2.req_rate / m2.slo
                    m2_s = m2.model_size

                    # Verify capacity
                    n_src_s = src_st['s'] - m1_s + m2_s
                    if n_src_s >= GPU_MEM_SIZE: continue

                    n_tgt_s = tgt_st['s'] - m2_s + m1_s
                    if n_tgt_s >= GPU_MEM_SIZE: continue

                    # Verify KVPR improvement
                    n_src_l = src_st['l'] - m1_l + m2_l
                    n_src_kvpr = get_kvpr(n_src_l, n_src_s)

                    n_tgt_l = tgt_st['l'] - m2_l + m1_l
                    n_tgt_kvpr = get_kvpr(n_tgt_l, n_tgt_s)

                    new_max = max(n_src_kvpr, n_tgt_kvpr)
                    if new_max < max_kvpr - 1e-6:
                        gain = max_kvpr - new_max
                        if gain > best_gain:
                            best_gain = gain
                            best_action = ('swap', m1_idx, tgt_idx, m2_idx)

        # Execute best action
        if best_action:
            if best_action[0] == 'move':
                _, m_idx, tgt_idx = best_action
                model = gpu_state[max_gpu]['models'].pop(m_idx)
                m_l = model.req_rate / model.slo
                m_s = model.model_size

                gpu_state[max_gpu]['l'] -= m_l
                gpu_state[max_gpu]['s'] -= m_s

                gpu_state[tgt_idx]['models'].append(model)
                gpu_state[tgt_idx]['l'] += m_l
                gpu_state[tgt_idx]['s'] += m_s

            elif best_action[0] == 'swap':
                _, m1_idx, tgt_idx, m2_idx = best_action
                # We need to pop strictly but we have indices.
                # Pop from different lists is safe.
                m1 = gpu_state[max_gpu]['models'].pop(m1_idx)
                m2 = gpu_state[tgt_idx]['models'].pop(m2_idx)

                gpu_state[max_gpu]['models'].append(m2)
                gpu_state[tgt_idx]['models'].append(m1)

                m1_l = m1.req_rate / m1.slo
                m1_s = m1.model_size
                m2_l = m2.req_rate / m2.slo
                m2_s = m2.model_size

                gpu_state[max_gpu]['l'] += (m2_l - m1_l)
                gpu_state[max_gpu]['s'] += (m2_s - m1_s)

                gpu_state[tgt_idx]['l'] += (m1_l - m2_l)
                gpu_state[tgt_idx]['s'] += (m1_s - m2_s)
=======
        # 2. Try Swapping a model from max_gpu <-> model from tgt_gpu
        for m1_idx, m1 in enumerate(src_st['models']):
            m1_l = m1.req_rate / m1.slo
            m1_s = m1.model_size

            for tgt_idx in range(gpu_num):
                if tgt_idx == max_gpu: continue
                tgt_st = gpu_state[tgt_idx]

                # Optimization: Skip if target is already near max
                if get_kvpr(tgt_st['l'], tgt_st['s']) >= max_kvpr: continue

                for m2_idx, m2 in enumerate(tgt_st['models']):
                    m2_l = m2.req_rate / m2.slo
                    m2_s = m2.model_size

                    # Verify capacity
                    n_src_s = src_st['s'] - m1_s + m2_s
                    if n_src_s >= GPU_MEM_SIZE: continue

                    n_tgt_s = tgt_st['s'] - m2_s + m1_s
                    if n_tgt_s >= GPU_MEM_SIZE: continue

                    # Verify KVPR improvement
                    n_src_l = src_st['l'] - m1_l + m2_l
                    n_src_kvpr = get_kvpr(n_src_l, n_src_s)

                    n_tgt_l = tgt_st['l'] - m2_l + m1_l
                    n_tgt_kvpr = get_kvpr(n_tgt_l, n_tgt_s)

                    new_max = max(n_src_kvpr, n_tgt_kvpr)
                    if new_max < max_kvpr - 1e-6:
                        gain = max_kvpr - new_max
                        if gain > best_gain:
                            best_gain = gain
                            best_action = ('swap', m1_idx, tgt_idx, m2_idx)

        # 3. Try 2-for-1 Swaps: 2 models from max_gpu <-> 1 model from tgt_gpu
        src_models = src_st['models']
        n_src = len(src_models)
        if n_src >= 2:
            for i in range(n_src):
                m1 = src_models[i]
                for j in range(i + 1, n_src):
                    m2 = src_models[j]

                    pair_l = (m1.req_rate/m1.slo) + (m2.req_rate/m2.slo)
                    pair_s = m1.model_size + m2.model_size

                    for tgt_idx in range(gpu_num):
                        if tgt_idx == max_gpu: continue
                        tgt_st = gpu_state[tgt_idx]

                        if get_kvpr(tgt_st['l'], tgt_st['s']) >= max_kvpr: continue

                        for m3_idx, m3 in enumerate(tgt_st['models']):
                            m3_l = m3.req_rate / m3.slo
                            m3_s = m3.model_size

                            # Check Capacity
                            n_src_s = src_st['s'] - pair_s + m3_s
                            if n_src_s >= GPU_MEM_SIZE: continue

                            n_tgt_s = tgt_st['s'] - m3_s + pair_s
                            if n_tgt_s >= GPU_MEM_SIZE: continue

                            # Check KVPR
                            n_src_l = src_st['l'] - pair_l + m3_l
                            n_src_kvpr = get_kvpr(n_src_l, n_src_s)

                            n_tgt_l = tgt_st['l'] - m3_l + pair_l
                            n_tgt_kvpr = get_kvpr(n_tgt_l, n_tgt_s)

                            new_max = max(n_src_kvpr, n_tgt_kvpr)
                            if new_max < max_kvpr - 1e-6:
                                gain = max_kvpr - new_max
                                if gain > best_gain:
                                    best_gain = gain
                                    best_action = ('swap21', i, j, tgt_idx, m3_idx)

        # Execute best action
        if best_action:
            if best_action[0] == 'move':
                _, m_idx, tgt_idx = best_action
                model = gpu_state[max_gpu]['models'].pop(m_idx)
                m_l = model.req_rate / model.slo
                m_s = model.model_size

                gpu_state[max_gpu]['l'] -= m_l
                gpu_state[max_gpu]['s'] -= m_s

                gpu_state[tgt_idx]['models'].append(model)
                gpu_state[tgt_idx]['l'] += m_l
                gpu_state[tgt_idx]['s'] += m_s

            elif best_action[0] == 'swap':
                _, m1_idx, tgt_idx, m2_idx = best_action
                m1 = gpu_state[max_gpu]['models'][m1_idx]
                m2 = gpu_state[tgt_idx]['models'][m2_idx]

                # In-place swap
                gpu_state[max_gpu]['models'][m1_idx] = m2
                gpu_state[tgt_idx]['models'][m2_idx] = m1

                m1_l = m1.req_rate / m1.slo
                m1_s = m1.model_size
                m2_l = m2.req_rate / m2.slo
                m2_s = m2.model_size

                gpu_state[max_gpu]['l'] += (m2_l - m1_l)
                gpu_state[max_gpu]['s'] += (m2_s - m1_s)

                gpu_state[tgt_idx]['l'] += (m1_l - m2_l)
                gpu_state[tgt_idx]['s'] += (m1_s - m2_s)

            elif best_action[0] == 'swap21':
                _, idx1, idx2, tgt_idx, m3_idx = best_action
                # Pop strictly: idx2 > idx1 so pop idx2 first
                m2 = gpu_state[max_gpu]['models'].pop(idx2)
                m1 = gpu_state[max_gpu]['models'].pop(idx1)

                m3 = gpu_state[tgt_idx]['models'].pop(m3_idx)

                gpu_state[max_gpu]['models'].append(m3)
                gpu_state[tgt_idx]['models'].extend([m1, m2])

                pair_l = (m1.req_rate/m1.slo) + (m2.req_rate/m2.slo)
                pair_s = m1.model_size + m2.model_size
                m3_l = m3.req_rate/m3.slo
                m3_s = m3.model_size

                gpu_state[max_gpu]['l'] += (m3_l - pair_l)
                gpu_state[max_gpu]['s'] += (m3_s - pair_s)

                gpu_state[tgt_idx]['l'] += (pair_l - m3_l)
                gpu_state[tgt_idx]['s'] += (pair_s - m3_s)
>>>>>>> REPLACE
</DIFF>