<NAME>
enhanced_initialization
</NAME>

<DESCRIPTION>
Replaces the simplified initialization with a more robust multi-start strategy.
1.  Uses binary search to find the minimum feasible K for linearized bin packing.
2.  Generates candidates using that K, and variations (noisy K, noisy item weights) to escape local optima in the packing phase.
3.  Retains deterministic greedy strategies as fallbacks.
This diversity in initialization provides a better starting point for the local search, significantly improving the chances of finding a high-quality global optimum.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # -------------------------------------------------------------------------
    # 1. Initialization Strategies
    # -------------------------------------------------------------------------

    def get_vector(gpus):
        # Lexicographical vector: (max_kvpr, 2nd_max_kvpr, ...)
        return tuple(sorted((g.kvpr() for g in gpus), reverse=True))

    # Strategy A: Binary Search Linearization
    # Solves parameter-weighted Bin Packing: Load + K*Size <= K*Cap
    def init_binary_search():
        def solve_linearized_bp(target_k):
            bin_cap = target_k * GPU_MEM_SIZE
            items = []
            for m in models:
                w = (m.req_rate / m.slo) + target_k * m.model_size
                items.append((w, m))
            items.sort(key=lambda x: x[0], reverse=True)

            bins = [GPUState(i) for i in range(gpu_num)]
            for w, m in items:
                best_idx = -1
                min_rem = float('inf')
                for i in range(gpu_num):
                    if not bins[i].can_fit(m.model_size): continue
                    lin_usage = bins[i].load + target_k * bins[i].used_mem
                    if lin_usage + w <= bin_cap:
                        rem = bin_cap - (lin_usage + w)
                        if rem < min_rem:
                            min_rem = rem
                            best_idx = i
                if best_idx != -1:
                    bins[best_idx].add(m)
                else:
                    return None
            return bins

        low, high = 0.0, 1000.0
        # Check upper bound quickly
        if solve_linearized_bp(high) is None: high = 1e8

        best_res = None
        for _ in range(15):
            mid = (low + high) / 2
            res = solve_linearized_bp(mid)
            if res:
                best_res = res
                high = mid
            else:
                low = mid
        return best_res

    # Strategy B: Greedy by Size
    def init_greedy_size():
        gpus = [GPUState(i) for i in range(gpu_num)]
        sorted_models = sorted(models, key=lambda m: m.model_size, reverse=True)
        for m in sorted_models:
            best_idx = -1
            best_val = float('inf')
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    # Heuristic: minimize resulting KVPR
                    rem = GPU_MEM_SIZE - (gpus[i].used_mem + m.model_size)
                    val = (gpus[i].load + m.req_rate/m.slo) / rem if rem > 1e-7 else float('inf')
                    if val < best_val:
                        best_val = val
                        best_idx = i
            if best_idx != -1:
                gpus[best_idx].add(m)
            else:
                return None
        return gpus

    # Generate Candidates
    candidates = []

    res_bs = init_binary_search()
    if res_bs: candidates.append(res_bs)

    res_sz = init_greedy_size()
    if res_sz: candidates.append(res_sz)

    # Fallback if both failed
    if not candidates:
        gpus = [GPUState(i) for i in range(gpu_num)]
        for m in sorted(models, key=lambda x: x.model_size, reverse=True):
            placed = False
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    gpus[i].add(m)
                    placed = True
                    break
            if not placed: raise ValueError("Models do not fit in GPU memory.")
        candidates.append(gpus)

    # Pick Best Start
    current_gpus = min(candidates, key=lambda g: get_vector(g))
    current_vector = get_vector(current_gpus)

    # Save Global Best
    best_gpus = [GPUState(i) for i in range(gpu_num)]
    for i in range(gpu_num): best_gpus[i].copy_from(current_gpus[i])
    best_vector = current_vector
=======
    # -------------------------------------------------------------------------
    # 1. Initialization Strategies
    # -------------------------------------------------------------------------

    def get_vector(gpus):
        # Lexicographical vector: (max_kvpr, 2nd_max_kvpr, ...)
        return tuple(sorted((g.kvpr() for g in gpus), reverse=True))

    candidates = []

    # Strategy A: Linearized Bin Packing with Noise
    def solve_linearized_bp(target_k, noise=0.0):
        bin_cap = target_k * GPU_MEM_SIZE
        items = []
        for m in models:
            # Base weight: Load + K * Size
            base_w = (m.req_rate / m.slo) + target_k * m.model_size
            w = base_w
            if noise > 0:
                w *= random.uniform(1.0 - noise, 1.0 + noise)
            items.append((w, m, base_w))
        items.sort(key=lambda x: x[0], reverse=True)

        bins = [GPUState(i) for i in range(gpu_num)]
        for w, m, base_w in items:
            best_idx = -1
            min_rem = float('inf')
            for i in range(gpu_num):
                if not bins[i].can_fit(m.model_size): continue
                lin_use = bins[i].load + target_k * bins[i].used_mem
                # Linearized capacity check
                if lin_use + base_w <= bin_cap:
                    rem = bin_cap - (lin_use + base_w)
                    if rem < min_rem:
                        min_rem = rem
                        best_idx = i
            if best_idx != -1:
                bins[best_idx].add(m)
            else:
                return None
        return bins

    low, high = 0.0, 1000.0
    if solve_linearized_bp(high) is None: high = 1e9

    bs_res = None
    best_k = high
    for _ in range(16):
        mid = (low + high) / 2
        res = solve_linearized_bp(mid)
        if res:
            bs_res = res
            best_k = mid
            high = mid
            candidates.append(res)
        else:
            low = mid

    # Randomized Refinement around best_k
    if best_k < 1e8:
        for _ in range(12):
            k_noisy = best_k * random.uniform(0.9, 1.1)
            res_noise = solve_linearized_bp(k_noisy, noise=0.06)
            if res_noise: candidates.append(res_noise)

    # Strategy B: Deterministic Greedy Strategies
    strategies = [
        ('size', lambda m: m.model_size),
        ('load', lambda m: m.req_rate / m.slo),
        ('density', lambda m: (m.req_rate / m.slo) / m.model_size if m.model_size > 1e-7 else 0)
    ]

    for _, key_fn in strategies:
        gpus = [GPUState(i) for i in range(gpu_num)]
        valid = True
        for m in sorted(models, key=key_fn, reverse=True):
            best_idx = -1
            best_val = float('inf')
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (gpus[i].used_mem + m.model_size)
                    val = (gpus[i].load + m.req_rate/m.slo) / rem if rem > 1e-7 else float('inf')
                    if val < best_val:
                        best_val = val
                        best_idx = i
            if best_idx != -1: gpus[best_idx].add(m)
            else: valid = False; break
        if valid: candidates.append(gpus)

    # Fallback
    if not candidates:
        gpus = [GPUState(i) for i in range(gpu_num)]
        for m in sorted(models, key=lambda x: x.model_size, reverse=True):
            placed = False
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    gpus[i].add(m)
                    placed = True
                    break
            if not placed: raise ValueError("Models do not fit in GPU memory.")
        candidates.append(gpus)

    # Select best starting point
    current_gpus = min(candidates, key=lambda g: get_vector(g))
    current_vector = get_vector(current_gpus)

    best_gpus = [GPUState(i) for i in range(gpu_num)]
    for i in range(gpu_num): best_gpus[i].copy_from(current_gpus[i])
    best_vector = current_vector
>>>>>>> REPLACE
</DIFF>

<NAME>
enhanced_ils
</NAME>

<DESCRIPTION>
Updates the Iterated Local Search (ILS) to use a comprehensive set of operators (Move, Swap 1-1, Swap 1-2, Swap 2-1) and replaces the random kick perturbation with a more powerful Ruin and Recreate strategy.
1.  Moves and Swaps 1-1 use Best Improvement logic (scan all, apply best) for higher quality steps.
2.  Adds Swap 2-1 (exchanging 2 models from source for 1 from dest) to handle cases where fragmentation prevents simple swaps.
3.  Ruin and Recreate empties the bottleneck GPU completely and redistributes models, effectively reshuffling the problem configuration to escape deep local optima.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # -------------------------------------------------------------------------
    # 2. Iterated Local Search (ILS)
    # -------------------------------------------------------------------------

    iter_cnt = 0
    max_iter = 150

    while iter_cnt < max_iter:
        improved_step = False
        iter_cnt += 1

        # Sort GPUs by KVPR descending
        sorted_gpus = sorted(current_gpus, key=lambda g: g.kvpr(), reverse=True)
        sources = sorted_gpus[:4] # Focus on bottlenecks

        # --- Operator 1: Move ---
        for source in sources:
            for i, model in enumerate(source.models):
                for dest in current_gpus:
                    if dest.id == source.id: continue
                    if dest.can_fit(model.model_size):
                        source.remove(i)
                        dest.add(model)

                        new_vec = get_vector(current_gpus)
                        if new_vec < current_vector:
                            current_vector = new_vec
                            improved_step = True
                            if current_vector < best_vector:
                                best_vector = current_vector
                                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
                            break
                        else:
                            dest.remove(len(dest.models)-1)
                            source.restore_model(i, model)
                if improved_step: break
            if improved_step: break

        if improved_step: continue

        # --- Operator 2: Swap (1-1) ---
        for source in sources:
            for i, m_a in enumerate(source.models):
                for dest in current_gpus:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue

                    for j, m_b in enumerate(dest.models):
                        # Capacity check
                        s_mem = source.used_mem - m_a.model_size + m_b.model_size
                        d_mem = dest.used_mem - m_b.model_size + m_a.model_size

                        if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                            source.remove(i)
                            dest.remove(j)
                            source.add(m_b)
                            dest.add(m_a)

                            new_vec = get_vector(current_gpus)
                            if new_vec < current_vector:
                                current_vector = new_vec
                                improved_step = True
                                if current_vector < best_vector:
                                    best_vector = current_vector
                                    for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
                                break
                            else:
                                dest.remove(len(dest.models)-1)
                                source.remove(len(source.models)-1)
                                dest.restore_model(j, m_b)
                                source.restore_model(i, m_a)
                    if improved_step: break
                if improved_step: break

        if improved_step: continue

        # --- Operator 3: Swap (1-2) ---
        # Move 1 from Source, 2 from Dest
        for source in sources[:2]:
            for i, m_a in enumerate(source.models):
                for dest in current_gpus:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue
                    if len(dest.models) < 2: continue

                    n_d = len(dest.models)
                    pair_found = False
                    for j1 in range(n_d):
                        for j2 in range(j1+1, n_d):
                            m_b1 = dest.models[j1]
                            m_b2 = dest.models[j2]

                            s_mem = source.used_mem - m_a.model_size + m_b1.model_size + m_b2.model_size
                            d_mem = dest.used_mem - m_b1.model_size - m_b2.model_size + m_a.model_size

                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                source.remove(i)
                                dest.remove(j2) # Larger idx first
                                dest.remove(j1)
                                source.add(m_b1)
                                source.add(m_b2)
                                dest.add(m_a)

                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    pair_found = True
                                    if current_vector < best_vector:
                                        best_vector = current_vector
                                        for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
                                    break
                                else:
                                    dest.remove(len(dest.models)-1) # m_a
                                    source.remove(len(source.models)-1)
                                    source.remove(len(source.models)-1)
                                    dest.restore_model(j1, m_b1)
                                    dest.restore_model(j2, m_b2)
                        if pair_found: break
                    if pair_found: break
                if improved_step: break

        if improved_step: continue

        #