--- a/original.py
+++ b/original.py
@@ -1,203 +1,357 @@
 # EVOLVE-BLOCK-START
 import math
 
 GPU_MEM_SIZE = 80  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
     
     Algorithm:
-    1. Calculate a theoretical lower bound for KVPR assuming fluid models.
-    2. Binary Search for the optimal Max KVPR 'K'.
-    3. The feasibility check 'can_pack(K)' attempts to fit models into GPUs
-       subject to: sum(req/slo) + K * sum(size) <= K * MEM_SIZE.
-       It uses Best Fit Decreasing with multiple sorting keys (Linearized Weight, Size, Load)
-       to robustly find a packing in different constraint regimes.
+    1. Binary Search for the optimal Max KVPR 'K'.
+       The feasibility check transforms the problem into a Bin Packing Problem
+       with linearized item weights: w = l + K * s.
+       It uses Best Fit Decreasing with 4 sorting heuristics:
+       - Linearized Weight: l + K*s
+       - Size: s
+       - Load: l
+       - Density: l/s
+    2. Local Search Refinement.
+       After finding a feasible placement, we explicitly minimize the max KVPR
+       using three neighborhood operators on the bottleneck GPU:
+       - Shift: Move 1 model to another GPU.
+       - Swap 1-1: Exchange 1 model with 1 from another GPU.
+       - Swap 2-1: Exchange 2 models with 1 from another GPU (helps with fragmentation).
     """
     
     # Precompute model characteristics
-    # l = req_rate / slo, s = model_size
     model_data = []
-    total_l = 0.0
-    total_s = 0.0
-    
-    for m in models:
-        l = m.req_rate / m.slo
-        s = m.model_size
+    for i, m in enumerate(models):
         model_data.append({
             'model': m,
-            'l': l,
-            's': s
+            'l': m.req_rate / m.slo,
+            's': m.model_size,
+            'id': i
         })
-        total_l += l
-        total_s += s
-
-    # 1. Theoretical Lower Bound
-    # If models were fluid, we could perfectly balance:
-    # K_min = Total_L / (Total_Capacity - Total_S)
-    remaining_mem_global = (gpu_num * GPU_MEM_SIZE) - total_s
-    if remaining_mem_global <= 0:
-         # This implies total model size > total GPU memory, strictly impossible.
-         # We'll let the packing logic handle the failure or raise here.
-         lower_bound_k = 0.0
-    else:
-         lower_bound_k = total_l / remaining_mem_global
-    
-    # 2. Feasibility Check with Multi-Strategy Best Fit
-    def can_pack(target_k):
+
+    # --- Phase 1: Binary Search Construction ---
+    
+    def solve_packing(target_k):
         capacity = target_k * GPU_MEM_SIZE
         
-        # Define sorting strategies
-        # Strategy A: Linearized Weight (L + K*S). 
-        # Adapts to K: High K -> sort by Size; Low K -> sort by Load.
-        strat_weight = sorted(
-            model_data, 
-            key=lambda x: x['l'] + target_k * x['s'], 
-            reverse=True
-        )
-        
-        # Strategy B: Size Decreasing.
-        # Good for tight memory constraints.
-        strat_size = sorted(
-            model_data, 
-            key=lambda x: x['s'], 
-            reverse=True
-        )
-        
-        # Strategy C: Load Decreasing.
-        # Good for loose memory but tight load constraints.
-        strat_load = sorted(
-            model_data, 
-            key=lambda x: x['l'], 
-            reverse=True
-        )
-        
-        # Try each strategy until one works
-        for items in [strat_weight, strat_size, strat_load]:
+        # Sorting strategies for Best Fit Decreasing
+        heuristics = [
+            # 1. Linearized Weight: balances load and size based on K
+            lambda x: x['l'] + target_k * x['s'],
+            # 2. Size: prioritizes fitting large items (memory constrained)
+            lambda x: x['s'],
+            # 3. Load: prioritizes high load items
+            lambda x: x['l'],
+            # 4. Density: efficient packing
+            lambda x: x['l'] / x['s'] if x['s'] > 0 else 0
+        ]
+        
+        for key_func in heuristics:
+            # Sort descending
+            items = sorted(model_data, key=key_func, reverse=True)
+            
             bins_l = [0.0] * gpu_num
             bins_s = [0.0] * gpu_num
-            bins_models = [[] for _ in range(gpu_num)]
-            possible = True
-            
+            bins_items = [[] for _ in range(gpu_num)]
+            
+            feasible = True
             for item in items:
-                # Best Fit: Choose valid bin with minimal remaining slack
-                # Slack in linearized constraint: (Capacity) - (Bin_W + Item_W)
-                best_idx = -1
+                best_bin = -1
                 min_slack = float('inf')
                 
-                w_item = item['l'] + target_k * item['s']
-                
-                for i in range(gpu_num):
+                # Weight of item in linearized constraint
+                item_w = item['l'] + target_k * item['s']
+                
+                for b in range(gpu_num):
                     # Hard Memory Constraint
-                    # Use slightly larger epsilon for safety against float precision
-                    if bins_s[i] + item['s'] >= GPU_MEM_SIZE - 1e-6:
+                    if bins_s[b] + item['s'] >= GPU_MEM_SIZE - 1e-6:
                         continue
                     
-                    # Linearized Capacity Constraint
-                    w_bin = bins_l[i] + target_k * bins_s[i]
-                    if w_bin + w_item <= capacity + 1e-9:
-                        slack = capacity - (w_bin + w_item)
+                    # Linearized Constraint: sum(L) + K*sum(S) <= K*M
+                    current_bin_w = bins_l[b] + target_k * bins_s[b]
+                    
+                    if current_bin_w + item_w <= capacity + 1e-9:
+                        # Best Fit: minimize slack
+                        slack = capacity - (current_bin_w + item_w)
                         if slack < min_slack:
                             min_slack = slack
-                            best_idx = i
-                
-                if best_idx != -1:
-                    bins_l[best_idx] += item['l']
-                    bins_s[best_idx] += item['s']
-                    bins_models[best_idx].append(item['model'])
+                            best_bin = b
+                            
+                if best_bin != -1:
+                    bins_l[best_bin] += item['l']
+                    bins_s[best_bin] += item['s']
+                    bins_items[best_bin].append(item)
                 else:
-                    possible = False
+                    feasible = False
                     break
             
-            if possible:
-                return bins_models
+            if feasible:
+                return bins_items
                 
         return None
 
-    # 3. Binary Search
-    low = lower_bound_k
-    high = max(lower_bound_k * 2, 1.0)
-    best_placement = None
-    
-    # Exponential expansion to find valid upper bound
-    # (Starting from a reasoned guess saves steps)
-    found_high = False
+    # Binary Search
+    low = 0.0
+    high = 1.0
+    
+    # Exponential search for upper bound
     for _ in range(20):
-        res = can_pack(high)
-        if res is not None:
-            best_placement = res
-            found_high = True
+        if solve_packing(high) is not None:
             break
         low = high
         high *= 2.0
-        
-    if not found_high:
-        # Last ditch attempt with massive K (effectively just memory packing)
-        high = 1e12
-        best_placement = can_pack(high)
-        if best_placement is None:
-            raise ValueError("Unable to fit models into GPU memory.")
-
-    # Refine K with Binary Search
-    for _ in range(40):
+    else:
+        high = 1e9 # Fallback
+        
+    best_packing = None
+    
+    # Precision search
+    for _ in range(30):
         mid = (low + high) / 2
-        res = can_pack(mid)
+        res = solve_packing(mid)
         if res is not None:
-            best_placement = res
+            best_packing = res
             high = mid
         else:
             low = mid
             
-    return {i: best_placement[i] for i in range(gpu_num)}
-
+    if best_packing is None:
+        best_packing = solve_packing(high)
+        if best_packing is None:
+            raise ValueError("Unable to place models.")
+
+    # Convert to standard format for Phase 2
+    placement = {i: [x['model'] for x in best_packing[i]] for i in range(gpu_num)}
+
+    # --- Phase 2: Local Search Refinement ---
+    
+    # Initialize state for fast access
+    gpu_states = []
+    for g in range(gpu_num):
+        # We use the dictionaries from model_data to avoid re-creating them
+        # We need to map the model objects back to their data dicts
+        p_models = placement[g]
+        g_items = []
+        g_l = 0.0
+        g_s = 0.0
+        # Map models to data dicts. Optimization: Create a lookup if N is large, 
+        # but for small N nested loop is fine.
+        for m in p_models:
+            for d in model_data:
+                if d['model'] is m:
+                    g_items.append(d)
+                    g_l += d['l']
+                    g_s += d['s']
+                    break
+        gpu_states.append({'l': g_l, 's': g_s, 'items': g_items})
+
+    def get_kvpr(l, s):
+        if s >= GPU_MEM_SIZE - 1e-6: return float('inf')
+        return l / (GPU_MEM_SIZE - s)
+
+    # Hill Climbing on Global Max KVPR
+    for _ in range(200): # Iteration limit
+        # 1. Identify Bottleneck
+        max_kvpr = -1.0
+        max_gpu = -1
+        current_kvprs = []
+        
+        for g in range(gpu_num):
+            val = get_kvpr(gpu_states[g]['l'], gpu_states[g]['s'])
+            current_kvprs.append(val)
+            if val > max_kvpr:
+                max_kvpr = val
+                max_gpu = g
+        
+        if max_kvpr < 1e-9: break # Perfect score
+        
+        # 2. Find Best Move
+        best_move = None # (type, gain, args...)
+        best_gain = 0.0
+        
+        src = gpu_states[max_gpu]
+        
+        # Helper to calculate gain
+        def calc_gain(src_l_new, src_s_new, tgt_l_new, tgt_s_new, old_tgt_kvpr):
+            ns_k = get_kvpr(src_l_new, src_s_new)
+            nt_k = get_kvpr(tgt_l_new, tgt_s_new)
+            new_peak = max(ns_k, nt_k)
+            # We strictly want to reduce the global max
+            if new_peak < max_kvpr - 1e-7:
+                return max_kvpr - new_peak
+            return -1.0
+
+        # Move Types
+        # A. Shift: Move item from src -> tgt
+        for i, item in enumerate(src['items']):
+            for t in range(gpu_num):
+                if t == max_gpu: continue
+                # Pruning: Target shouldn't be worse than current max
+                if current_kvprs[t] >= max_kvpr: continue
+                
+                tgt = gpu_states[t]
+                if tgt['s'] + item['s'] >= GPU_MEM_SIZE: continue
+                
+                gain = calc_gain(
+                    src['l'] - item['l'], src['s'] - item['s'],
+                    tgt['l'] + item['l'], tgt['s'] + item['s'],
+                    current_kvprs[t]
+                )
+                if gain > best_gain:
+                    best_gain = gain
+                    best_move = ('shift', i, t)
+
+        # B. Swap 1-1: Swap item src <-> item tgt
+        for i, s_item in enumerate(src['items']):
+            for t in range(gpu_num):
+                if t == max_gpu: continue
+                if current_kvprs[t] >= max_kvpr: continue
+                
+                tgt = gpu_states[t]
+                for j, t_item in enumerate(tgt['items']):
+                    # Check capacity
+                    ns_s = src['s'] - s_item['s'] + t_item['s']
+                    nt_s = tgt['s'] - t_item['s'] + s_item['s']
+                    
+                    if ns_s >= GPU_MEM_SIZE or nt_s >= GPU_MEM_SIZE: continue
+                    
+                    gain = calc_gain(
+                        src['l'] - s_item['l'] + t_item['l'], ns_s,
+                        tgt['l'] - t_item['l'] + s_item['l'], nt_s,
+                        current_kvprs[t]
+                    )
+                    if gain > best_gain:
+                        best_gain = gain
+                        best_move = ('swap11', i, t, j)
+                        
+        # C. Swap 2-1: Swap {item1, item2} src <-> {item3} tgt
+        # Useful for defragmentation or load balancing
+        if len(src['items']) >= 2:
+            n_src = len(src['items'])
+            # Limit checks to prevent O(N^3) explosion if N is large
+            # But typically N is small per GPU
+            for i1 in range(n_src):
+                for i2 in range(i1 + 1, n_src):
+                    itm1 = src['items'][i1]
+                    itm2 = src['items'][i2]
+                    pair_l = itm1['l'] + itm2['l']
+                    pair_s = itm1['s'] + itm2['s']
+                    
+                    for t in range(gpu_num):
+                        if t == max_gpu: continue
+                        if current_kvprs[t] >= max_kvpr: continue
+                        
+                        tgt = gpu_states[t]
+                        for j, t_itm in enumerate(tgt['items']):
+                            ns_s = src['s'] - pair_s + t_itm['s']
+                            nt_s = tgt['s'] - t_itm['s'] + pair_s
+                            
+                            if ns_s >= GPU_MEM_SIZE or nt_s >= GPU_MEM_SIZE: continue
+                            
+                            gain = calc_gain(
+                                src['l'] - pair_l + t_itm['l'], ns_s,
+                                tgt['l'] - t_itm['l'] + pair_l, nt_s,
+                                current_kvprs[t]
+                            )
+                            if gain > best_gain:
+                                best_gain = gain
+                                best_move = ('swap21', i1, i2, t, j)
+
+        # Execute Move
+        if best_move:
+            mtype = best_move[0]
+            if mtype == 'shift':
+                _, idx, t_id = best_move
+                item = src['items'].pop(idx)
+                tgt = gpu_states[t_id]
+                
+                src['l'] -= item['l']; src['s'] -= item['s']
+                tgt['items'].append(item)
+                tgt['l'] += item['l']; tgt['s'] += item['s']
+                
+            elif mtype == 'swap11':
+                _, s_idx, t_id, t_idx = best_move
+                tgt = gpu_states[t_id]
+                s_item = src['items'][s_idx]
+                t_item = tgt['items'][t_idx]
+                
+                src['items'][s_idx] = t_item
+                tgt['items'][t_idx] = s_item
+                
+                src['l'] += t_item['l'] - s_item['l']; src['s'] += t_item['s'] - s_item['s']
+                tgt['l'] += s_item['l'] - t_item['l']; tgt['s'] += s_item['s'] - t_item['s']
+                
+            elif mtype == 'swap21':
+                _, i1, i2, t_id, t_idx = best_move
+                tgt = gpu_states[t_id]
+                
+                # Pop strictly i2 > i1
+                itm2 = src['items'].pop(i2)
+                itm1 = src['items'].pop(i1)
+                t_itm = tgt['items'].pop(t_idx)
+                
+                src['items'].append(t_itm)
+                tgt['items'].extend([itm1, itm2])
+                
+                pair_l = itm1['l'] + itm2['l']; pair_s = itm1['s'] + itm2['s']
+                src['l'] += t_itm['l'] - pair_l; src['s'] += t_itm['s'] - pair_s
+                tgt['l'] += pair_l - t_itm['l']; tgt['s'] += pair_s - t_itm['s']
+        else:
+            break # No improvement found
+            
+    return {g: [x['model'] for x in gpu_states[g]['items']] for g in range(gpu_num)}
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")
