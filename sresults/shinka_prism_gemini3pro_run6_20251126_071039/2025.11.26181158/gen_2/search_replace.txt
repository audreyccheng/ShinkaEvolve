<NAME>
greedy_resulting_kvpr_bfd
</NAME>

<DESCRIPTION>
This edit improves the model placement algorithm by switching from a greedy strategy that minimizes the *current* KV cache pressure to one that minimizes the *resulting* KV cache pressure after placement. This look-ahead prevents the algorithm from placing models on GPUs that have low current pressure but would end up with very high pressure (due to reduced remaining memory). Additionally, the sorting order of models is changed to Best Fit Decreasing (BFD) based on model size, then load. Placing larger models first is generally more effective for capacity-constrained packing problems, ensuring that large models find space before memory becomes fragmented or too scarce, which helps minimize the maximum KVPR.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Greedy KVPR-minimizing placement based on Algorithm 1 (without Ï„ check)
    # 1) Sort models by r_j / s_j in descending order
    sorted_models = sorted(models, key=lambda m: (m.req_rate / m.slo), reverse=True)

    # 2) Initialize per-GPU states
    placement = {gpu_id: [] for gpu_id in range(gpu_num)}
    shared_kv = [GPU_MEM_SIZE for _ in range(gpu_num)]  # remaining memory per GPU
    weighted_req_rate = [0.0 for _ in range(gpu_num)]   # sum of r_j / s_j per GPU

    # 3) Assign each model to the GPU that minimizes current KVPR while fitting in memory
    for model in sorted_models:
        best_idx = None
        best_ratio = float('inf')

        for gpu_id in range(gpu_num):
            if model.model_size <= shared_kv[gpu_id] and shared_kv[gpu_id] > 0:
                current_ratio = weighted_req_rate[gpu_id] / shared_kv[gpu_id]
                if current_ratio < best_ratio:
                    best_ratio = current_ratio
                    best_idx = gpu_id

        # Failure: if no GPU can fit, raise an error instead of overcommitting
        if best_idx is None:
            raise ValueError(
                f"Unable to place model of size {model.model_size} GB on any GPU. "
                f"Remaining per-GPU memory: {shared_kv}"
            )

        placement[best_idx].append(model)
        weighted_req_rate[best_idx] += model.req_rate / model.slo
        shared_kv[best_idx] -= model.model_size
=======
    # Improved Greedy Placement: Minimize Resulting KVPR
    # 1) Sort models by size (descending) primarily, then by load (descending)
    # This helps placing large rocks first to ensure they fit, which is crucial for
    # minimizing the denominator of KVPR (remaining memory).
    sorted_models = sorted(models, key=lambda m: (m.model_size, m.req_rate / m.slo), reverse=True)

    # 2) Initialize per-GPU states
    placement = {gpu_id: [] for gpu_id in range(gpu_num)}
    shared_kv = [GPU_MEM_SIZE for _ in range(gpu_num)]  # remaining memory per GPU
    weighted_req_rate = [0.0 for _ in range(gpu_num)]   # sum of r_j / s_j per GPU

    # 3) Assign each model to the GPU that minimizes RESULTING KVPR
    for model in sorted_models:
        best_idx = None
        best_kvpr = float('inf')
        model_load = model.req_rate / model.slo

        for gpu_id in range(gpu_num):
            remaining = shared_kv[gpu_id] - model.model_size
            if remaining < 0:
                continue

            # Calculate resulting KVPR if we place this model here
            # Use a small epsilon to handle full GPU case gracefully
            new_load = weighted_req_rate[gpu_id] + model_load
            new_kvpr = new_load / max(remaining, 1e-6)

            if new_kvpr < best_kvpr:
                best_kvpr = new_kvpr
                best_idx = gpu_id

        # Failure: if no GPU can fit, raise an error instead of overcommitting
        if best_idx is None:
            raise ValueError(
                f"Unable to place model of size {model.model_size} GB on any GPU. "
                f"Remaining per-GPU memory: {shared_kv}"
            )

        placement[best_idx].append(model)
        weighted_req_rate[best_idx] += model_load
        shared_kv[best_idx] -= model.model_size
>>>>>>> REPLACE
</DIFF>