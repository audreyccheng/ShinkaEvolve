# EVOLVE-BLOCK-START
"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""

GPU_MEM_SIZE = 80.0  # GB
import math
import random

def compute_model_placement(gpu_num, models):
    """
    Algorithm:
    1. Hybrid Initialization:
       - Binary Search Linearized Bin Packing (Load + K*Size <= K*Cap).
         Finds minimum feasible K (min_k) representing the tightest KVPR bound.
       - Generates candidates using min_k and relaxed factors (1.05, 1.1, etc.) to 
         balance packing density vs load distribution.
       - Includes deterministic and randomized greedy strategies.
       - Robust Fallback (FFD with retries) to guarantee 100% success rate.
    
    2. Iterated Local Search (ILS):
       - Focuses on the "Bottleneck" GPUs (highest KVPR).
       - Operators: Move, Swap 1-1, Swap 2-1 (Consolidate), Swap 1-2 (Split).
       - Uses efficient pruning and 'Best Improvement' for simple moves.
       
    3. Perturbation:
       - Single Ruin: Completely empties the bottleneck GPU to force redistribution.
       - Re-insertion alternates between Size-based (packing) and Load-based (balancing).
    """

    class GPUState:
        __slots__ = ['id', 'models', 'load', 'used_mem', '_cached_kvpr', '_cached_rem']
        
        def __init__(self, gpu_id):
            self.id = gpu_id
            self.models = []
            self.load = 0.0
            self.used_mem = 0.0
            self._cached_kvpr = 0.0
            self._cached_rem = GPU_MEM_SIZE

        def update_cache(self):
            self._cached_rem = GPU_MEM_SIZE - self.used_mem
            if self._cached_rem <= 1e-7:
                self._cached_kvpr = float('inf')
            else:
                self._cached_kvpr = self.load / self._cached_rem

        def can_fit(self, size):
            return self.used_mem + size <= GPU_MEM_SIZE

        def add(self, model):
            self.models.append(model)
            self.load += model.req_rate / model.slo
            self.used_mem += model.model_size
            self.update_cache()

        def remove(self, idx):
            model = self.models.pop(idx)
            self.load -= model.req_rate / model.slo
            self.used_mem -= model.model_size
            self.update_cache()
            return model

        def kvpr(self):
            return self._cached_kvpr

        def restore_model(self, idx, model):
            self.models.insert(idx, model)
            self.load += model.req_rate / model.slo
            self.used_mem += model.model_size
            self.update_cache()

        def copy_from(self, other):
            self.models = list(other.models)
            self.load = other.load
            self.used_mem = other.used_mem
            self._cached_kvpr = other._cached_kvpr
            self._cached_rem = other._cached_rem

    def get_vector(gpus):
        # Lexicographical vector of KVPRs (descending)
        return tuple(sorted((g.kvpr() for g in gpus), reverse=True))

    candidates = []

    # -------------------------------------------------------------------------
    # 1. Initialization
    # -------------------------------------------------------------------------

    # A. Linearized Bin Packing
    # Constraint: Load + K*Size <= K*Cap  <=>  Load/(Cap-Size) <= K
    def solve_linearized_bp(target_k, noise=0.0):
        bin_cap = target_k * GPU_MEM_SIZE
        items = []
        for m in models:
            base_w = (m.req_rate / m.slo) + target_k * m.model_size
            w = base_w
            if noise > 0:
                w *= random.uniform(1.0 - noise, 1.0 + noise)
            items.append((w, m, base_w))
        
        # Best Fit Decreasing
        items.sort(key=lambda x: x[0], reverse=True)
        
        bins = [GPUState(i) for i in range(gpu_num)]
        for w, m, base_w in items:
            best_idx = -1
            min_rem = float('inf')
            
            for i in range(gpu_num):
                if not bins[i].can_fit(m.model_size): continue
                
                lin_use = bins[i].load + target_k * bins[i].used_mem
                # Check linearized capacity constraint
                if lin_use + base_w <= bin_cap:
                    rem = bin_cap - (lin_use + base_w)
                    if rem < min_rem:
                        min_rem = rem
                        best_idx = i
            
            if best_idx != -1:
                bins[best_idx].add(m)
            else:
                return None
        return bins

    # Binary search for min feasible K
    low, high = 0.0, 2000.0
    if solve_linearized_bp(high) is None: high = 1e9
    
    best_k = high
    for _ in range(18):
        mid = (low + high) / 2
        if solve_linearized_bp(mid):
            best_k = mid
            high = mid
        else:
            low = mid

    # Generate candidates from K range
    # Relaxation factors allow finding solutions that aren't packed as tightly (spreads load)
    if best_k < 1e8:
        for factor in [1.0, 1.05, 1.1, 1.2, 1.5]:
            k_val = best_k * factor
            res = solve_linearized_bp(k_val)
            if res: candidates.append(res)
            # Add noise for diversity
            res_noise = solve_linearized_bp(k_val, noise=0.05)
            if res_noise: candidates.append(res_noise)

    # B. Greedy Strategies
    strategies = [
        ('size', lambda m: m.model_size),
        ('load', lambda m: m.req_rate / m.slo),
        ('density', lambda m: (m.req_rate / m.slo) / m.model_size if m.model_size > 0 else 0)
    ]
    for _, key_fn in strategies:
        gpus = [GPUState(i) for i in range(gpu_num)]
        valid = True
        for m in sorted(models, key=key_fn, reverse=True):
            best_idx = -1
            best_val = float('inf')
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (gpus[i].used_mem + m.model_size)
                    val = (gpus[i].load + m.req_rate/m.slo) / rem if rem > 1e-7 else float('inf')
                    if val < best_val:
                        best_val = val
                        best_idx = i
            if best_idx != -1: gpus[best_idx].add(m)
            else: valid = False; break
        if valid: candidates.append(gpus)

    # C. Robust Safety Net (Randomized First Fit)
    # Essential for 100% success rate on hard instances
    for _ in range(20):
        gpus = [GPUState(i) for i in range(gpu_num)]
        shuffled = list(models)
        random.shuffle(shuffled)
        valid = True
        for m in shuffled:
            placed = False
            # Try Best Fit first
            best_i = -1
            best_v = float('inf')
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (gpus[i].used_mem + m.model_size)
                    if rem > 1e-7:
                         # Score based on KVPR
                         score = (gpus[i].load + m.req_rate/m.slo) / rem
                         if score < best_v:
                             best_v = score
                             best_i = i
            if best_i != -1:
                gpus[best_i].add(m)
                placed = True
            else:
                # Fallback to First Fit
                for i in range(gpu_num):
                    if gpus[i].can_fit(m.model_size):
                        gpus[i].add(m)
                        placed = True
                        break
            if not placed:
                valid = False
                break
        if valid: candidates.append(gpus)

    # Absolute fallback
    if not candidates:
        gpus = [GPUState(i) for i in range(gpu_num)]
        # Sort by size to pack efficiently
        for m in sorted(models, key=lambda x: x.model_size, reverse=True):
            placed = False
            for i in range(gpu_num):
                if gpus[i].can_fit(m.model_size):
                    gpus[i].add(m)
                    placed = True
                    break
            if not placed: raise ValueError("Models do not fit.")
        candidates.append(gpus)

    # Selection
    current_gpus = min(candidates, key=lambda g: get_vector(g))
    current_vector = get_vector(current_gpus)
    
    best_gpus = [GPUState(i) for i in range(gpu_num)]
    for i in range(gpu_num): best_gpus[i].copy_from(current_gpus[i])
    best_vector = current_vector

    # --- 2. Iterated Local Search ---
    iter_cnt = 0
    max_iter = 150

    while iter_cnt < max_iter:
        improved_step = False
        
        # Sort by KVPR descending
        sorted_gpus = sorted(current_gpus, key=lambda g: g.kvpr(), reverse=True)
        sources = sorted_gpus[:4] # Top bottlenecks
        destinations = sorted_gpus[::-1] # Least loaded first

        # Operator 1: Move (Best Improvement)
        best_move = None
        best_move_gain = current_vector
        for source in sources:
            for i, m in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id: continue
                    if dest.can_fit(m.model_size):
                        source.remove(i); dest.add(m)
                        new_vec = get_vector(current_gpus)
                        if new_vec < best_move_gain:
                            best_move_gain = new_vec
                            best_move = (source, i, dest, m)
                        dest.remove(len(dest.models)-1); source.restore_model(i, m)
        if best_move:
            src, idx, dst, m = best_move
            src.remove(idx); dst.add(m)
            current_vector = best_move_gain
            improved_step = True
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1; continue

        # Operator 2: Swap 1-1 (Best Improvement)
        best_swap = None
        best_swap_gain = current_vector
        for source in sources:
            for i, ma in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id or dest.kvpr() >= source.kvpr(): continue
                    for j, mb in enumerate(dest.models):
                        if (mb.req_rate/mb.slo) >= (ma.req_rate/ma.slo): continue
                        if (source.used_mem - ma.model_size + mb.model_size <= GPU_MEM_SIZE and 
                            dest.used_mem - mb.model_size + ma.model_size <= GPU_MEM_SIZE):
                            source.remove(i); dest.remove(j)
                            source.add(mb); dest.add(ma)
                            new_vec = get_vector(current_gpus)
                            if new_vec < best_swap_gain:
                                best_swap_gain = new_vec
                                best_swap = (source, i, ma, dest, j, mb)
                            dest.remove(len(dest.models)-1); source.remove(len(source.models)-1)
                            dest.restore_model(j, mb); source.restore_model(i, ma)
        if best_swap:
            src, i, ma, dst, j, mb = best_swap
            src.remove(i); dst.remove(j)
            src.add(mb); dst.add(ma)
            current_vector = best_swap_gain
            improved_step = True
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1; continue

        # Operator 3: Swap 2-1 (Source gives 2, Dest gives 1)
        for source in sources[:3]:
            if len(source.models) < 2: continue
            n_s = len(source.models)
            for i1 in range(n_s):
                for i2 in range(i1+1, n_s):
                    ma1, ma2 = source.models[i1], source.models[i2]
                    for dest in destinations:
                        if dest.id == source.id or dest.kvpr() >= source.kvpr(): continue
                        for j, mb in enumerate(dest.models):
                            # Capacity check
                            s_rem = GPU_MEM_SIZE - (source.used_mem - ma1.model_size - ma2.model_size)
                            d_rem = GPU_MEM_SIZE - (dest.used_mem - mb.model_size)
                            if mb.model_size > s_rem or (ma1.model_size + ma2.model_size) > d_rem: continue

                            source.remove(i2); source.remove(i1); dest.remove(j)
                            source.add(mb); dest.add(ma1); dest.add(ma2)
                            
                            new_vec = get_vector(current_gpus)
                            if new_vec < current_vector:
                                current_vector = new_vec
                                improved_step = True
                            else:
                                dest.remove(len(dest.models)-1); dest.remove(len(dest.models)-1)
                                source.remove(len(source.models)-1)
                                dest.restore_model(j, mb)
                                source.restore_model(i1, ma1); source.restore_model(i2, ma2)
                            if improved_step: break
                        if improved_step: break
                    if improved_step: break
                if improved_step: break
            if improved_step: break
        if improved_step:
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1; continue

        # Operator 4: Swap 1-2
        for source in sources[:3]:
            for i, ma in enumerate(source.models):
                for dest in destinations:
                    if dest.id == source.id or dest.kvpr() >= source.kvpr(): continue
                    if len(dest.models) < 2: continue
                    n_d = len(dest.models)
                    for j1 in range(n_d):
                        for j2 in range(j1+1, n_d):
                            mb1, mb2 = dest.models[j1], dest.models[j2]
                            s_rem = GPU_MEM_SIZE - (source.used_mem - ma.model_size)
                            d_rem = GPU_MEM_SIZE - (dest.used_mem - mb1.model_size - mb2.model_size)
                            if (mb1.model_size + mb2.model_size) > s_rem or ma.model_size > d_rem: continue
                            
                            source.remove(i); dest.remove(j2); dest.remove(j1)
                            source.add(mb1); source.add(mb2); dest.add(ma)
                            new_vec = get_vector(current_gpus)
                            if new_vec < current_vector:
                                current_vector = new_vec
                                improved_step = True
                            else:
                                dest.remove(len(dest.models)-1)
                                source.remove(len(source.models)-1); source.remove(len(source.models)-1)
                                dest.restore_model(j1, mb1); dest.restore_model(j2, mb2)
                            if improved_step: break
                        if improved_step: break
                    if improved_step: break
                if improved_step: break
            if improved_step: break
        if improved_step:
            if current_vector < best_vector:
                best_vector = current_vector
                for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
            iter_cnt += 1; continue

        # --- Perturbation ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break
        
        displaced = []
        while worst_gpu.models: displaced.append(worst_gpu.remove(0))
        
        # Alternating sort strategies for re-insertion
        if iter_cnt % 2 == 0:
            displaced.sort(key=lambda m: m.model_size * random.uniform(0.9, 1.1), reverse=True)
        else:
            displaced.sort(key=lambda m: m.req_rate / m.slo, reverse=True)
            
        targets = [g for g in current_gpus if g.id != worst_gpu.id]
        
        for m in displaced:
            best_dest = None
            best_dest_val = float('inf')
            
            for dest in targets:
                if dest.can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (dest.used_mem + m.model_size)
                    if rem > 1e-7:
                        val = (dest.load + m.req_rate/m.slo) / rem
                        if val < best_dest_val:
                            best_dest_val = val
                            best_dest = dest
            
            if best_dest: best_dest.add(m)
            else: worst_gpu.add(m)

        current_vector = get_vector(current_gpus)
        if current_vector < best_vector:
            best_vector = current_vector
            for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])

    return {g.id: g.models for g in best_gpus}
# EVOLVE-BLOCK-END