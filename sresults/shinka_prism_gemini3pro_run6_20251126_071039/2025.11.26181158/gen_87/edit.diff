--- a/original.py
+++ b/original.py
@@ -1,520 +1,459 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 GPU_MEM_SIZE = 80.0  # GB
 import math
 import random
 
 def compute_model_placement(gpu_num, models):
     """
     Algorithm:
-    1. Binary Search Linearization: Solves a parameter-weighted Bin Packing problem
-       to find a layout satisfying Sum(load + K*size) <= K*Cap.
+    1. Hybrid Initialization:
+       - Binary Search Linearized Bin Packing (Load + K*Size <= K*Cap).
+         Finds minimum feasible K (min_k) representing the tightest KVPR bound.
+       - Generates candidates using min_k and relaxed factors (1.05, 1.1, etc.) to 
+         balance packing density vs load distribution.
+       - Includes deterministic and randomized greedy strategies.
+       - Robust Fallback (FFD with retries) to guarantee 100% success rate.
+    
     2. Iterated Local Search (ILS):
-       - Variable Neighborhood Descent with Move, Swap(1-1), Swap(1-2).
-       - Perturbation: If local search stagnates, forcefully moves a model from
-         the bottleneck GPU to a random feasible GPU to escape local optima.
+       - Focuses on the "Bottleneck" GPUs (highest KVPR).
+       - Operators: Move, Swap 1-1, Swap 2-1 (Consolidate), Swap 1-2 (Split).
+       - Uses efficient pruning and 'Best Improvement' for simple moves.
+       
+    3. Perturbation:
+       - Single Ruin: Completely empties the bottleneck GPU to force redistribution.
+       - Re-insertion alternates between Size-based (packing) and Load-based (balancing).
     """
 
     class GPUState:
+        __slots__ = ['id', 'models', 'load', 'used_mem', '_cached_kvpr', '_cached_rem']
+        
         def __init__(self, gpu_id):
             self.id = gpu_id
             self.models = []
             self.load = 0.0
             self.used_mem = 0.0
             self._cached_kvpr = 0.0
             self._cached_rem = GPU_MEM_SIZE
 
         def update_cache(self):
             self._cached_rem = GPU_MEM_SIZE - self.used_mem
             if self._cached_rem <= 1e-7:
                 self._cached_kvpr = float('inf')
             else:
                 self._cached_kvpr = self.load / self._cached_rem
 
         def can_fit(self, size):
             return self.used_mem + size <= GPU_MEM_SIZE
 
         def add(self, model):
             self.models.append(model)
             self.load += model.req_rate / model.slo
             self.used_mem += model.model_size
             self.update_cache()
 
         def remove(self, idx):
             model = self.models.pop(idx)
             self.load -= model.req_rate / model.slo
             self.used_mem -= model.model_size
             self.update_cache()
             return model
 
         def kvpr(self):
             return self._cached_kvpr
 
         def restore_model(self, idx, model):
             self.models.insert(idx, model)
             self.load += model.req_rate / model.slo
             self.used_mem += model.model_size
             self.update_cache()
 
         def copy_from(self, other):
             self.models = list(other.models)
             self.load = other.load
             self.used_mem = other.used_mem
             self._cached_kvpr = other._cached_kvpr
             self._cached_rem = other._cached_rem
 
+    def get_vector(gpus):
+        # Lexicographical vector of KVPRs (descending)
+        return tuple(sorted((g.kvpr() for g in gpus), reverse=True))
+
+    candidates = []
+
     # -------------------------------------------------------------------------
-    # 1. Initialization Strategies (Multi-Start)
+    # 1. Initialization
     # -------------------------------------------------------------------------
-    def get_vector(gs):
-        return tuple(sorted((g.kvpr() for g in gs), reverse=True))
-
-    candidates = []
-
-    # Strategy A: Binary Search Linearization + Randomized Refinement
-    def solve_linearized_bin_packing(target_k, noise_factor=0.0):
+
+    # A. Linearized Bin Packing
+    # Constraint: Load + K*Size <= K*Cap  <=>  Load/(Cap-Size) <= K
+    def solve_linearized_bp(target_k, noise=0.0):
         bin_cap = target_k * GPU_MEM_SIZE
         items = []
         for m in models:
-            # Base weight
-            w = (m.req_rate / m.slo) + target_k * m.model_size
-            # Add noise if requested
-            if noise_factor > 0:
-                w *= random.uniform(1.0 - noise_factor, 1.0 + noise_factor)
-            items.append((w, m))
-        # Best Fit Decreasing based on linearized weight
+            base_w = (m.req_rate / m.slo) + target_k * m.model_size
+            w = base_w
+            if noise > 0:
+                w *= random.uniform(1.0 - noise, 1.0 + noise)
+            items.append((w, m, base_w))
+        
+        # Best Fit Decreasing
         items.sort(key=lambda x: x[0], reverse=True)
-
+        
         bins = [GPUState(i) for i in range(gpu_num)]
-        for w, m in items:
+        for w, m, base_w in items:
             best_idx = -1
-            min_rem_linear = float('inf')
+            min_rem = float('inf')
+            
             for i in range(gpu_num):
                 if not bins[i].can_fit(m.model_size): continue
+                
                 lin_use = bins[i].load + target_k * bins[i].used_mem
-                # Recalculate actual linear usage for constraint check (without noise)
-                real_w = (m.req_rate / m.slo) + target_k * m.model_size
-                if lin_use + real_w <= bin_cap:
-                    rem = bin_cap - (lin_use + real_w)
-                    if rem < min_rem_linear:
-                        min_rem_linear = rem
+                # Check linearized capacity constraint
+                if lin_use + base_w <= bin_cap:
+                    rem = bin_cap - (lin_use + base_w)
+                    if rem < min_rem:
+                        min_rem = rem
                         best_idx = i
-            if best_idx != -1: bins[best_idx].add(m)
-            else: return None
+            
+            if best_idx != -1:
+                bins[best_idx].add(m)
+            else:
+                return None
         return bins
 
-    # Binary Search for K (min feasible K)
-    low, high = 0.0, 1000.0
-    if solve_linearized_bin_packing(high) is None: high = 1e9
-
-    min_feasible_k = high
-    bs_res = None
-    for _ in range(20):
+    # Binary search for min feasible K
+    low, high = 0.0, 2000.0
+    if solve_linearized_bp(high) is None: high = 1e9
+    
+    best_k = high
+    for _ in range(18):
         mid = (low + high) / 2
-        res = solve_linearized_bin_packing(mid)
-        if res:
-            bs_res = res
-            min_feasible_k = mid
+        if solve_linearized_bp(mid):
+            best_k = mid
             high = mid
         else:
             low = mid
 
-    if bs_res:
-        candidates.append(bs_res)
-        # Scan range of K values starting from min_feasible_k
-        # Higher K prioritizes size packing, Lower K prioritizes load balancing
-        multipliers = [1.05, 1.2, 1.5, 2.0, 3.0, 5.0]
-        for mult in multipliers:
-            k_val = min_feasible_k * mult
-            res = solve_linearized_bin_packing(k_val)
+    # Generate candidates from K range
+    # Relaxation factors allow finding solutions that aren't packed as tightly (spreads load)
+    if best_k < 1e8:
+        for factor in [1.0, 1.05, 1.1, 1.2, 1.5]:
+            k_val = best_k * factor
+            res = solve_linearized_bp(k_val)
             if res: candidates.append(res)
-            # Add noise for robustness
-            res_noise = solve_linearized_bin_packing(k_val, noise_factor=0.05)
+            # Add noise for diversity
+            res_noise = solve_linearized_bp(k_val, noise=0.05)
             if res_noise: candidates.append(res_noise)
 
-    # Strategy B, C, D: Greedy heuristics
+    # B. Greedy Strategies
     strategies = [
         ('size', lambda m: m.model_size),
         ('load', lambda m: m.req_rate / m.slo),
-        ('density', lambda m: (m.req_rate / m.slo) / m.model_size if m.model_size > 1e-7 else 0)
+        ('density', lambda m: (m.req_rate / m.slo) / m.model_size if m.model_size > 0 else 0)
     ]
-
     for _, key_fn in strategies:
         gpus = [GPUState(i) for i in range(gpu_num)]
         valid = True
         for m in sorted(models, key=key_fn, reverse=True):
             best_idx = -1
             best_val = float('inf')
             for i in range(gpu_num):
                 if gpus[i].can_fit(m.model_size):
                     rem = GPU_MEM_SIZE - (gpus[i].used_mem + m.model_size)
                     val = (gpus[i].load + m.req_rate/m.slo) / rem if rem > 1e-7 else float('inf')
                     if val < best_val:
                         best_val = val
                         best_idx = i
-            if best_idx != -1:
-                gpus[best_idx].add(m)
+            if best_idx != -1: gpus[best_idx].add(m)
+            else: valid = False; break
+        if valid: candidates.append(gpus)
+
+    # C. Robust Safety Net (Randomized First Fit)
+    # Essential for 100% success rate on hard instances
+    for _ in range(20):
+        gpus = [GPUState(i) for i in range(gpu_num)]
+        shuffled = list(models)
+        random.shuffle(shuffled)
+        valid = True
+        for m in shuffled:
+            placed = False
+            # Try Best Fit first
+            best_i = -1
+            best_v = float('inf')
+            for i in range(gpu_num):
+                if gpus[i].can_fit(m.model_size):
+                    rem = GPU_MEM_SIZE - (gpus[i].used_mem + m.model_size)
+                    if rem > 1e-7:
+                         # Score based on KVPR
+                         score = (gpus[i].load + m.req_rate/m.slo) / rem
+                         if score < best_v:
+                             best_v = score
+                             best_i = i
+            if best_i != -1:
+                gpus[best_i].add(m)
+                placed = True
             else:
+                # Fallback to First Fit
+                for i in range(gpu_num):
+                    if gpus[i].can_fit(m.model_size):
+                        gpus[i].add(m)
+                        placed = True
+                        break
+            if not placed:
                 valid = False
                 break
         if valid: candidates.append(gpus)
 
-    # Strategy E: Randomized Greedy (5 attempts) to increase diversity
-    for _ in range(5):
-        gpus = [GPUState(i) for i in range(gpu_num)]
-        shuffled_models = list(models)
-        random.shuffle(shuffled_models)
-        valid = True
-        for m in shuffled_models:
-            best_idx = -1
-            best_val = float('inf')
-            # Try to place minimizing local KVPR increase
-            for i in range(gpu_num):
-                if gpus[i].can_fit(m.model_size):
-                    rem = GPU_MEM_SIZE - (gpus[i].used_mem + m.model_size)
-                    val = (gpus[i].load + m.req_rate/m.slo) / rem if rem > 1e-7 else float('inf')
-                    if val < best_val:
-                        best_val = val
-                        best_idx = i
-            if best_idx != -1:
-                gpus[best_idx].add(m)
-            else:
-                valid = False
-                break
-        if valid: candidates.append(gpus)
-
+    # Absolute fallback
     if not candidates:
         gpus = [GPUState(i) for i in range(gpu_num)]
+        # Sort by size to pack efficiently
         for m in sorted(models, key=lambda x: x.model_size, reverse=True):
             placed = False
             for i in range(gpu_num):
                 if gpus[i].can_fit(m.model_size):
                     gpus[i].add(m)
                     placed = True
                     break
-            if not placed: raise ValueError("Models do not fit in GPU memory.")
+            if not placed: raise ValueError("Models do not fit.")
         candidates.append(gpus)
 
-    # Pick best start
+    # Selection
     current_gpus = min(candidates, key=lambda g: get_vector(g))
     current_vector = get_vector(current_gpus)
-
+    
     best_gpus = [GPUState(i) for i in range(gpu_num)]
     for i in range(gpu_num): best_gpus[i].copy_from(current_gpus[i])
     best_vector = current_vector
 
-    # -------------------------------------------------------------------------
-    # 2. Iterated Local Search (ILS) with Best Improvement
-    # -------------------------------------------------------------------------
-
+    # --- 2. Iterated Local Search ---
     iter_cnt = 0
-    max_iter = 200
+    max_iter = 150
 
     while iter_cnt < max_iter:
         improved_step = False
-
-        # Identify bottlenecks
+        
+        # Sort by KVPR descending
         sorted_gpus = sorted(current_gpus, key=lambda g: g.kvpr(), reverse=True)
         sources = sorted_gpus[:4] # Top bottlenecks
         destinations = sorted_gpus[::-1] # Least loaded first
 
-        # --- Operator 1: MOVE (Best Improvement) ---
+        # Operator 1: Move (Best Improvement)
         best_move = None
         best_move_gain = current_vector
-
         for source in sources:
-            for i, model in enumerate(source.models):
+            for i, m in enumerate(source.models):
                 for dest in destinations:
                     if dest.id == source.id: continue
-                    if dest.can_fit(model.model_size):
-                        source.remove(i)
-                        dest.add(model)
-
+                    if dest.can_fit(m.model_size):
+                        source.remove(i); dest.add(m)
                         new_vec = get_vector(current_gpus)
                         if new_vec < best_move_gain:
                             best_move_gain = new_vec
-                            best_move = (source, i, dest, model)
-
-                        dest.remove(len(dest.models)-1)
-                        source.restore_model(i, model)
-
+                            best_move = (source, i, dest, m)
+                        dest.remove(len(dest.models)-1); source.restore_model(i, m)
         if best_move:
-            src, idx, dst, mdl = best_move
-            src.remove(idx)
-            dst.add(mdl)
+            src, idx, dst, m = best_move
+            src.remove(idx); dst.add(m)
             current_vector = best_move_gain
             improved_step = True
-
             if current_vector < best_vector:
                 best_vector = current_vector
                 for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
-            iter_cnt += 1
-            continue
-
-        # --- Operator 2: SWAP 1-1 (Best Improvement) ---
+            iter_cnt += 1; continue
+
+        # Operator 2: Swap 1-1 (Best Improvement)
         best_swap = None
         best_swap_gain = current_vector
-
         for source in sources:
-            for i, m_a in enumerate(source.models):
+            for i, ma in enumerate(source.models):
                 for dest in destinations:
-                    if dest.id == source.id: continue
-                    if dest.kvpr() >= source.kvpr(): continue
-
-                    for j, m_b in enumerate(dest.models):
-                        # Optimization: source must reduce load
-                        if (m_b.req_rate/m_b.slo) >= (m_a.req_rate/m_a.slo): continue
-
-                        # Capacity check
-                        s_mem = source.used_mem - m_a.model_size + m_b.model_size
-                        d_mem = dest.used_mem - m_b.model_size + m_a.model_size
-
-                        if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
-                            source.remove(i)
-                            dest.remove(j)
-                            source.add(m_b)
-                            dest.add(m_a)
-
+                    if dest.id == source.id or dest.kvpr() >= source.kvpr(): continue
+                    for j, mb in enumerate(dest.models):
+                        if (mb.req_rate/mb.slo) >= (ma.req_rate/ma.slo): continue
+                        if (source.used_mem - ma.model_size + mb.model_size <= GPU_MEM_SIZE and 
+                            dest.used_mem - mb.model_size + ma.model_size <= GPU_MEM_SIZE):
+                            source.remove(i); dest.remove(j)
+                            source.add(mb); dest.add(ma)
                             new_vec = get_vector(current_gpus)
                             if new_vec < best_swap_gain:
                                 best_swap_gain = new_vec
-                                best_swap = (source, i, m_a, dest, j, m_b)
-
-                            dest.remove(len(dest.models)-1)
-                            source.remove(len(source.models)-1)
-                            dest.restore_model(j, m_b)
-                            source.restore_model(i, m_a)
-
+                                best_swap = (source, i, ma, dest, j, mb)
+                            dest.remove(len(dest.models)-1); source.remove(len(source.models)-1)
+                            dest.restore_model(j, mb); source.restore_model(i, ma)
         if best_swap:
             src, i, ma, dst, j, mb = best_swap
-            src.remove(i)
-            dst.remove(j)
-            src.add(mb)
-            dst.add(ma)
+            src.remove(i); dst.remove(j)
+            src.add(mb); dst.add(ma)
             current_vector = best_swap_gain
             improved_step = True
-
             if current_vector < best_vector:
                 best_vector = current_vector
                 for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
-            iter_cnt += 1
-            continue
-
-        # --- Operator 3: SWAP 2-1 (Source gives 2, Dest gives 1, First Improvement) ---
-        # Expanded to top 3 bottlenecks
+            iter_cnt += 1; continue
+
+        # Operator 3: Swap 2-1 (Source gives 2, Dest gives 1)
         for source in sources[:3]:
             if len(source.models) < 2: continue
-
             n_s = len(source.models)
-            pair_found = False
             for i1 in range(n_s):
                 for i2 in range(i1+1, n_s):
-                    m_a1 = source.models[i1]
-                    m_a2 = source.models[i2]
-
+                    ma1, ma2 = source.models[i1], source.models[i2]
                     for dest in destinations:
-                        if dest.id == source.id: continue
-                        if dest.kvpr() >= source.kvpr(): continue
-
-                        for j, m_b in enumerate(dest.models):
-                            s_mem = source.used_mem - m_a1.model_size - m_a2.model_size + m_b.model_size
-                            d_mem = dest.used_mem - m_b.model_size + m_a1.model_size + m_a2.model_size
-
-                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
-                                source.remove(i2) # Larger idx first
-                                source.remove(i1)
-                                dest.remove(j)
-                                source.add(m_b)
-                                dest.add(m_a1)
-                                dest.add(m_a2)
-
-                                new_vec = get_vector(current_gpus)
-                                if new_vec < current_vector:
-                                    current_vector = new_vec
-                                    improved_step = True
-                                    pair_found = True
-                                    break
-                                else:
-                                    dest.remove(len(dest.models)-1)
-                                    dest.remove(len(dest.models)-1)
-                                    source.remove(len(source.models)-1)
-                                    dest.restore_model(j, m_b)
-                                    source.restore_model(i1, m_a1)
-                                    source.restore_model(i2, m_a2)
-                        if pair_found: break
-                    if pair_found: break
-                if pair_found: break
+                        if dest.id == source.id or dest.kvpr() >= source.kvpr(): continue
+                        for j, mb in enumerate(dest.models):
+                            # Capacity check
+                            s_rem = GPU_MEM_SIZE - (source.used_mem - ma1.model_size - ma2.model_size)
+                            d_rem = GPU_MEM_SIZE - (dest.used_mem - mb.model_size)
+                            if mb.model_size > s_rem or (ma1.model_size + ma2.model_size) > d_rem: continue
+
+                            source.remove(i2); source.remove(i1); dest.remove(j)
+                            source.add(mb); dest.add(ma1); dest.add(ma2)
+                            
+                            new_vec = get_vector(current_gpus)
+                            if new_vec < current_vector:
+                                current_vector = new_vec
+                                improved_step = True
+                            else:
+                                dest.remove(len(dest.models)-1); dest.remove(len(dest.models)-1)
+                                source.remove(len(source.models)-1)
+                                dest.restore_model(j, mb)
+                                source.restore_model(i1, ma1); source.restore_model(i2, ma2)
+                            if improved_step: break
+                        if improved_step: break
+                    if improved_step: break
+                if improved_step: break
             if improved_step: break
-
         if improved_step:
             if current_vector < best_vector:
                 best_vector = current_vector
                 for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
-            iter_cnt += 1
-            continue
-
-        # --- Operator 4: SWAP 1-2 (Source gives 1, Dest gives 2, First Improvement) ---
+            iter_cnt += 1; continue
+
+        # Operator 4: Swap 1-2
         for source in sources[:3]:
-            for i, m_a in enumerate(source.models):
+            for i, ma in enumerate(source.models):
                 for dest in destinations:
-                    if dest.id == source.id: continue
-                    if dest.kvpr() >= source.kvpr(): continue
+                    if dest.id == source.id or dest.kvpr() >= source.kvpr(): continue
                     if len(dest.models) < 2: continue
-
                     n_d = len(dest.models)
-                    pair_found = False
                     for j1 in range(n_d):
-                        for j2 in range(j1 + 1, n_d):
-                            m_b1 = dest.models[j1]
-                            m_b2 = dest.models[j2]
-
-                            s_mem = source.used_mem - m_a.model_size + m_b1.model_size + m_b2.model_size
-                            d_mem = dest.used_mem - m_b1.model_size - m_b2.model_size + m_a.model_size
-
-                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
-                                source.remove(i)
-                                dest.remove(j2) # Larger idx first
-                                dest.remove(j1)
-                                source.add(m_b1)
-                                source.add(m_b2)
-                                dest.add(m_a)
-
-                                new_vec = get_vector(current_gpus)
-                                if new_vec < current_vector:
-                                    current_vector = new_vec
-                                    improved_step = True
-                                    pair_found = True
-                                    break
-                                else:
-                                    dest.remove(len(dest.models)-1) # m_a
-                                    source.remove(len(source.models)-1) # m_b2
-                                    source.remove(len(source.models)-1) # m_b1
-                                    dest.restore_model(j1, m_b1)
-                                    dest.restore_model(j2, m_b2)
-                        if pair_found: break
-                    if pair_found: break
+                        for j2 in range(j1+1, n_d):
+                            mb1, mb2 = dest.models[j1], dest.models[j2]
+                            s_rem = GPU_MEM_SIZE - (source.used_mem - ma.model_size)
+                            d_rem = GPU_MEM_SIZE - (dest.used_mem - mb1.model_size - mb2.model_size)
+                            if (mb1.model_size + mb2.model_size) > s_rem or ma.model_size > d_rem: continue
+                            
+                            source.remove(i); dest.remove(j2); dest.remove(j1)
+                            source.add(mb1); source.add(mb2); dest.add(ma)
+                            new_vec = get_vector(current_gpus)
+                            if new_vec < current_vector:
+                                current_vector = new_vec
+                                improved_step = True
+                            else:
+                                dest.remove(len(dest.models)-1)
+                                source.remove(len(source.models)-1); source.remove(len(source.models)-1)
+                                dest.restore_model(j1, mb1); dest.restore_model(j2, mb2)
+                            if improved_step: break
+                        if improved_step: break
+                    if improved_step: break
                 if improved_step: break
             if improved_step: break
-
         if improved_step:
             if current_vector < best_vector:
                 best_vector = current_vector
                 for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
-            iter_cnt += 1
-            continue
-
-        # --- PERTURBATION (Ruins and Recreate) ---
+            iter_cnt += 1; continue
+
+        # --- Perturbation ---
         iter_cnt += 1
         if iter_cnt > max_iter - 10: break
 
         worst_gpu = sorted_gpus[0]
         if not worst_gpu.models: break
-
-        # Dual Ruin: Clear worst GPU and best GPU (to create space for redistribution)
-        best_gpu = sorted_gpus[-1]
-        second_gpu = None
-        if best_gpu.id != worst_gpu.id:
-            second_gpu = best_gpu
-        elif len(sorted_gpus) > 1:
-            second_gpu = sorted_gpus[-2]
-
-        displaced_models = []
-        # Clear worst
-        while worst_gpu.models:
-            displaced_models.append(worst_gpu.remove(0))
-        # Clear second
-        if second_gpu:
-            while second_gpu.models:
-                displaced_models.append(second_gpu.remove(0))
-
-        # Recreate: Distribute models to ANY GPU (including emptied ones)
-        # Alternating sort strategy for diversity
+        
+        displaced = []
+        while worst_gpu.models: displaced.append(worst_gpu.remove(0))
+        
+        # Alternating sort strategies for re-insertion
         if iter_cnt % 2 == 0:
-            displaced_models.sort(key=lambda m: m.req_rate/m.slo, reverse=True)
+            displaced.sort(key=lambda m: m.model_size * random.uniform(0.9, 1.1), reverse=True)
         else:
-            displaced_models.sort(key=lambda m: m.model_size, reverse=True)
-
-        for m in displaced_models:
+            displaced.sort(key=lambda m: m.req_rate / m.slo, reverse=True)
+            
+        targets = [g for g in current_gpus if g.id != worst_gpu.id]
+        
+        for m in displaced:
             best_dest = None
             best_dest_val = float('inf')
-
-            # Find Best Fit destination (Minimizing local KVPR)
-            for dest in current_gpus:
+            
+            for dest in targets:
                 if dest.can_fit(m.model_size):
                     rem = GPU_MEM_SIZE - (dest.used_mem + m.model_size)
                     if rem > 1e-7:
                         val = (dest.load + m.req_rate/m.slo) / rem
                         if val < best_dest_val:
                             best_dest_val = val
                             best_dest = dest
-
-            if best_dest:
-                best_dest.add(m)
-            else:
-                # Fallback: Try to put back in one of the emptied ones
-                if worst_gpu.can_fit(m.model_size): worst_gpu.add(m)
-                elif second_gpu and second_gpu.can_fit(m.model_size): second_gpu.add(m)
-                else:
-                    # Last resort: any GPU that fits
-                    for g in current_gpus:
-                        if g.can_fit(m.model_size):
-                            g.add(m)
-                            break
+            
+            if best_dest: best_dest.add(m)
+            else: worst_gpu.add(m)
 
         current_vector = get_vector(current_gpus)
         if current_vector < best_vector:
             best_vector = current_vector
             for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
 
     return {g.id: g.models for g in best_gpus}
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")