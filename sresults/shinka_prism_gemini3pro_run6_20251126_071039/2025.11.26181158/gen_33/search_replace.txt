<NAME>
add_steepest_descent_local_search
</NAME>

<DESCRIPTION>
Augment the binary search initialization with a Steepest Descent Local Search phase.
The binary search provides a robust and feasible starting configuration.
The local search then iteratively identifies the bottleneck GPU (highest KVPR) and evaluates all possible Move and Swap operations involving that GPU. It selects the operation that maximizes the reduction of the bottleneck's pressure (steepest descent), ensuring monotonically improving solutions. This hybrid approach combines the speed of constructive heuristics with the fine-tuning capability of local search, addressing the recommendation to use steepest descent move selection.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Refine K with Binary Search
    for _ in range(40):
        mid = (low + high) / 2
        res = can_pack(mid)
        if res is not None:
            best_placement = res
            high = mid
        else:
            low = mid

    return {i: best_placement[i] for i in range(gpu_num)}
=======
    # Refine K with Binary Search
    for _ in range(40):
        mid = (low + high) / 2
        res = can_pack(mid)
        if res is not None:
            best_placement = res
            high = mid
        else:
            low = mid

    # --- Phase 3: Steepest Descent Local Search ---
    # Convert to mutable state with precomputed values
    placement = []
    # Cache model info: (model_obj, l, s)
    for g in range(gpu_num):
        gpu_models = []
        for m in best_placement[g]:
            gpu_models.append((m, m.req_rate / m.slo, m.model_size))
        placement.append(gpu_models)

    # Track sums
    gpu_l = [sum(x[1] for x in p) for p in placement]
    gpu_s = [sum(x[2] for x in p) for p in placement]

    def get_kvpr(l, s):
        if s >= GPU_MEM_SIZE - 1e-6: return 1e15
        return l / (GPU_MEM_SIZE - s)

    for _ in range(100): # Limit iterations
        # Find bottleneck
        current_kvprs = [get_kvpr(gpu_l[g], gpu_s[g]) for g in range(gpu_num)]
        max_kvpr = max(current_kvprs)
        if max_kvpr < 1e-9: break

        # Identify bottleneck GPU
        b_idx = -1
        for g in range(gpu_num):
            if current_kvprs[g] >= max_kvpr - 1e-9:
                b_idx = g
                break

        best_move = None
        best_new_max = max_kvpr

        # Try Moves: b_idx -> t_idx
        for i, (m, m_l, m_s) in enumerate(placement[b_idx]):
            for t_idx in range(gpu_num):
                if t_idx == b_idx: continue
                # Capacity check
                if gpu_s[t_idx] + m_s >= GPU_MEM_SIZE - 1e-6: continue

                # Pruning: if target load is high, unlikely to help
                if current_kvprs[t_idx] >= max_kvpr: continue

                # Predict
                new_b_l = gpu_l[b_idx] - m_l
                new_b_s = gpu_s[b_idx] - m_s
                new_t_l = gpu_l[t_idx] + m_l
                new_t_s = gpu_s[t_idx] + m_s

                k_b = get_kvpr(new_b_l, new_b_s)
                k_t = get_kvpr(new_t_l, new_t_s)

                if max(k_b, k_t) < best_new_max - 1e-9:
                    best_new_max = max(k_b, k_t)
                    best_move = ('move', b_idx, i, t_idx)

        # Try Swaps: b_idx[i] <-> t_idx[j]
        for i, (m1, m1_l, m1_s) in enumerate(placement[b_idx]):
            for t_idx in range(gpu_num):
                if t_idx == b_idx: continue
                if current_kvprs[t_idx] >= max_kvpr: continue

                for j, (m2, m2_l, m2_s) in enumerate(placement[t_idx]):
                    # Capacity check
                    if gpu_s[b_idx] - m1_s + m2_s >= GPU_MEM_SIZE - 1e-6: continue
                    if gpu_s[t_idx] - m2_s + m1_s >= GPU_MEM_SIZE - 1e-6: continue

                    new_b_l = gpu_l[b_idx] - m1_l + m2_l
                    new_b_s = gpu_s[b_idx] - m1_s + m2_s
                    new_t_l = gpu_l[t_idx] - m2_l + m1_l
                    new_t_s = gpu_s[t_idx] - m2_s + m1_s

                    k_b = get_kvpr(new_b_l, new_b_s)
                    k_t = get_kvpr(new_t_l, new_t_s)

                    if max(k_b, k_t) < best_new_max - 1e-9:
                        best_new_max = max(k_b, k_t)
                        best_move = ('swap', b_idx, i, t_idx, j)

        if best_move:
            type_ = best_move[0]
            if type_ == 'move':
                _, s, si, t = best_move
                item = placement[s].pop(si)
                placement[t].append(item)

                # Update sums
                _, m_l, m_s = item
                gpu_l[s] -= m_l; gpu_s[s] -= m_s
                gpu_l[t] += m_l; gpu_s[t] += m_s

            elif type_ == 'swap':
                _, s, si, t, ti = best_move
                item1 = placement[s][si]
                item2 = placement[t][ti]
                placement[s][si] = item2
                placement[t][ti] = item1

                _, m1_l, m1_s = item1
                _, m2_l, m2_s = item2

                gpu_l[s] += (m2_l - m1_l); gpu_s[s] += (m2_s - m1_s)
                gpu_l[t] += (m1_l - m2_l); gpu_s[t] += (m1_s - m2_s)
        else:
            break

    # Reconstruct result format
    final_result = {}
    for i in range(gpu_num):
        final_result[i] = [x[0] for x in placement[i]]

    return final_result
>>>>>>> REPLACE
</DIFF>