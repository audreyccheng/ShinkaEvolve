--- a/original.py
+++ b/original.py
@@ -1,427 +1,429 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 import random
 import time
 
 GPU_MEM_SIZE = 80.0
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
 
     Algorithm:
     1. Multi-Start Randomized Construction:
        - Uses Binary Search to find a baseline 'target K'.
        - Generates multiple initial solutions by adding random noise to sorting weights
          during Bin Packing. Selects the best start.
     2. Iterated Local Search (ILS):
        - Uses Steepest Descent (Best Improvement) on bottleneck GPUs.
        - Neighborhoods: Move, Swap(1-1), Swap(2-1).
        - Perturbation: Ruins & Recreate (LNS) on bottlenecks when stuck.
        - Runs within a 0.5s time budget.
     """
     start_time = time.time()
 
     # Pre-process models
     model_data = []
     for i, m in enumerate(models):
         model_data.append({
             'model': m,
             'l': m.req_rate / m.slo,
             's': m.model_size
         })
 
     def get_kvpr(l, s):
         if s >= GPU_MEM_SIZE - 1e-7: return 1e15
         return l / (GPU_MEM_SIZE - s)
 
     # --- Phase 1: Randomized Binary Search Construction ---
 
     def solve_packing(target_k, random_seed=None):
         capacity = target_k * GPU_MEM_SIZE
         items = list(model_data)
 
         # Sort Key: l + K*s with optional noise
         if random_seed is not None:
             rng = random.Random(random_seed)
             # Add noise to weight: w * random_factor
             items.sort(key=lambda x: (x['l'] + target_k * x['s']) * rng.uniform(0.9, 1.1), reverse=True)
         else:
             items.sort(key=lambda x: x['l'] + target_k * x['s'], reverse=True)
 
         gpu_l = [0.0] * gpu_num
         gpu_s = [0.0] * gpu_num
         gpu_models = [[] for _ in range(gpu_num)]
 
         for item in items:
             w = item['l'] + target_k * item['s']
             best_idx = -1
             min_rem = float('inf')
 
             # Best Fit logic
             for i in range(gpu_num):
                 if gpu_s[i] + item['s'] >= GPU_MEM_SIZE - 1e-6:
                     continue
 
                 curr_w = gpu_l[i] + target_k * gpu_s[i]
                 if curr_w + w <= capacity + 1e-9:
                     rem = capacity - (curr_w + w)
                     if rem < min_rem:
                         min_rem = rem
                         best_idx = i
 
             if best_idx != -1:
                 gpu_l[best_idx] += item['l']
                 gpu_s[best_idx] += item['s']
                 gpu_models[best_idx].append(item)
             else:
                 return None
         return gpu_models
 
     # 1. Deterministic Binary Search for Baseline K
     low, high = 0.0, 1.0
     # Find Upper Bound
     for _ in range(20):
         if solve_packing(high) is not None: break
         low = high
         high *= 2.0
     else: high = 1e9
 
     # Refine
     for _ in range(25):
         mid = (low + high) / 2
         if solve_packing(mid) is not None: high = mid
         else: low = mid
     base_k = high
 
     # 2. Multi-Start Initialization
     best_init_plc = None
     best_init_max_kvpr = float('inf')
 
     # Attempt deterministic + multiple randomized starts with varying K
     # Varying K helps find packing that balances load vs size better
-    k_multipliers = [1.0, 1.05, 1.1, 1.15, 1.2]
-    seeds = list(range(25))
+    k_multipliers = [1.0, 1.02, 1.05, 1.08, 1.1, 1.15, 1.2, 1.25, 1.3]
+    seeds = list(range(40))
     random.shuffle(seeds)
 
     for i, seed in enumerate(seeds):
         if time.time() - start_time > 0.20: break # Increased budget slightly
 
         # Cycle K multipliers
         mult = k_multipliers[i % len(k_multipliers)]
         curr_k = base_k * mult
 
         res = solve_packing(curr_k, random_seed=seed)
         if res:
             current_max = 0.0
             for g in range(gpu_num):
                 l = sum(x['l'] for x in res[g])
                 s = sum(x['s'] for x in res[g])
                 k_val = get_kvpr(l, s)
                 if k_val > current_max: current_max = k_val
 
             if current_max < best_init_max_kvpr:
                 best_init_max_kvpr = current_max
                 best_init_plc = res
 
     # Fallback
     if best_init_plc is None:
         best_init_plc = solve_packing(base_k)
         if best_init_plc is None:
             best_init_plc = solve_packing(1e9)
             if best_init_plc is None:
                 best_init_plc = [[] for _ in range(gpu_num)]
 
     # --- Phase 2: Iterated Local Search (ILS) ---
 
     # Convert to mutable structures
     plc = [list(best_init_plc[g]) for g in range(gpu_num)]
 
     # Tracking stats
     gpu_stats = []
     for g in range(gpu_num):
         l = sum(x['l'] for x in plc[g])
         s = sum(x['s'] for x in plc[g])
         gpu_stats.append({'l': l, 's': s})
 
     best_global_plc = [list(p) for p in plc]
     best_global_score = best_init_max_kvpr
 
     def evaluate(stats):
         max_k = -1.0
         bottlenecks = []
         for g in range(gpu_num):
             k = get_kvpr(stats[g]['l'], stats[g]['s'])
             if k > max_k:
                 max_k = k
                 bottlenecks = [g]
             elif abs(k - max_k) < 1e-9:
                 bottlenecks.append(g)
         return max_k, bottlenecks
 
     # Run ILS
     while time.time() - start_time < 0.5:
 
         # Steepest Descent Local Search
         while True:
             cur_max, bottlenecks = evaluate(gpu_stats)
             if cur_max < 1e-9: break
 
             best_move = None
             best_imp = 0.0
 
             # Check bottlenecks (prioritize worst)
             candidates = bottlenecks[:2]
 
             targets = []
             for t in range(gpu_num):
                 if t not in bottlenecks:
                     targets.append(t)
 
             if not targets:
                 break
 
             for b_idx in candidates:
                 b_l = gpu_stats[b_idx]['l']
                 b_s = gpu_stats[b_idx]['s']
                 src_items = plc[b_idx]
 
                 # Pre-filter targets
                 valid_targets = [t for t in targets if get_kvpr(gpu_stats[t]['l'], gpu_stats[t]['s']) < cur_max]
 
                 # 1. Move
                 for i, item in enumerate(src_items):
                     for t in valid_targets:
                         if gpu_stats[t]['s'] + item['s'] >= GPU_MEM_SIZE: continue
 
                         nk_src = get_kvpr(b_l - item['l'], b_s - item['s'])
                         nk_tgt = get_kvpr(gpu_stats[t]['l'] + item['l'], gpu_stats[t]['s'] + item['s'])
 
                         new_global = max(nk_src, nk_tgt)
                         if new_global < cur_max - 1e-7:
                             imp = cur_max - new_global
                             if imp > best_imp:
                                 best_imp = imp
                                 best_move = ('move', b_idx, i, t)
 
                 # 2. Swap 1-1
                 for i, s_item in enumerate(src_items):
                     for t in valid_targets:
                         tgt_items = plc[t]
                         for j, t_item in enumerate(tgt_items):
                             if b_s - s_item['s'] + t_item['s'] >= GPU_MEM_SIZE: continue
                             if gpu_stats[t]['s'] - t_item['s'] + s_item['s'] >= GPU_MEM_SIZE: continue
 
                             nk_src = get_kvpr(b_l - s_item['l'] + t_item['l'], b_s - s_item['s'] + t_item['s'])
                             nk_tgt = get_kvpr(gpu_stats[t]['l'] - t_item['l'] + s_item['l'], gpu_stats[t]['s'] - t_item['s'] + s_item['s'])
 
                             new_global = max(nk_src, nk_tgt)
                             if new_global < cur_max - 1e-7:
                                 imp = cur_max - new_global
                                 if imp > best_imp:
                                     best_imp = imp
                                     best_move = ('swap11', b_idx, i, t, j)
 
                 # 3. Swap 2-1 (2 from bottleneck)
                 if len(src_items) >= 2:
                     # Sort indices by size descending to prioritize removing large items
                     sorted_indices = sorted(range(len(src_items)), key=lambda k: src_items[k]['s'], reverse=True)
-                    limit_checks = 150
+                    limit_checks = 600
                     checks = 0
 
                     for idx1 in range(len(sorted_indices)):
                         if checks > limit_checks: break
                         i1 = sorted_indices[idx1]
                         # Only check pairs
                         for idx2 in range(idx1 + 1, len(sorted_indices)):
                             i2 = sorted_indices[idx2]
 
                             m1 = src_items[i1]
                             m2 = src_items[i2]
                             pair_l = m1['l'] + m2['l']
                             pair_s = m1['s'] + m2['s']
 
                             for t in valid_targets:
                                 tgt_items = plc[t]
                                 for j, m3 in enumerate(tgt_items):
                                     # Capacity checks
                                     if b_s - pair_s + m3['s'] >= GPU_MEM_SIZE - 1e-7: continue
                                     if gpu_stats[t]['s'] - m3['s'] + pair_s >= GPU_MEM_SIZE - 1e-7: continue
 
                                     nk_src = get_kvpr(b_l - pair_l + m3['l'], b_s - pair_s + m3['s'])
                                     nk_tgt = get_kvpr(gpu_stats[t]['l'] - m3['l'] + pair_l, gpu_stats[t]['s'] - m3['s'] + pair_s)
 
                                     new_global = max(nk_src, nk_tgt)
                                     if new_global < cur_max - 1e-7:
                                         imp = cur_max - new_global
                                         if imp > best_imp:
                                             best_imp = imp
                                             best_move = ('swap21', b_idx, i1, i2, t, j)
                                     checks += 1
 
             if best_move:
                 mtype = best_move[0]
                 if mtype == 'move':
                     _, b, i, t = best_move
                     item = plc[b].pop(i)
                     plc[t].append(item)
                     gpu_stats[b]['l'] -= item['l']; gpu_stats[b]['s'] -= item['s']
                     gpu_stats[t]['l'] += item['l']; gpu_stats[t]['s'] += item['s']
                 elif mtype == 'swap11':
                     _, b, i, t, j = best_move
                     it1 = plc[b][i]
                     it2 = plc[t][j]
                     plc[b][i] = it2
                     plc[t][j] = it1
                     dl = it2['l'] - it1['l']; ds = it2['s'] - it1['s']
                     gpu_stats[b]['l'] += dl; gpu_stats[b]['s'] += ds
                     gpu_stats[t]['l'] -= dl; gpu_stats[t]['s'] -= ds
                 elif mtype == 'swap21':
                     _, b, i1, i2, t, j = best_move
                     # Pop larger index first to avoid shifting
                     idx_first, idx_second = sorted((i1, i2), reverse=True)
                     it_first = plc[b].pop(idx_first)
                     it_second = plc[b].pop(idx_second)
 
                     it3 = plc[t].pop(j)
                     plc[b].append(it3)
                     plc[t].extend([it_first, it_second])
 
                     pl = it_first['l'] + it_second['l']; ps = it_first['s'] + it_second['s']
                     gpu_stats[b]['l'] += it3['l'] - pl; gpu_stats[b]['s'] += it3['s'] - ps
                     gpu_stats[t]['l'] += pl - it3['l']; gpu_stats[t]['s'] += ps - it3['s']
             else:
                 break # Local Optima Reached
 
         # Update Global Best
         cur_max, bottlenecks = evaluate(gpu_stats)
         if cur_max < best_global_score:
             best_global_score = cur_max
             best_global_plc = [list(p) for p in plc]
 
         # --- Perturbation (Ruins & Recreate) ---
         if not bottlenecks: break
 
         # 1. Select Bottleneck
         b_idx = random.choice(bottlenecks)
 
         # 2. Select Benefactor (GPU with lowest KVPR to dump load into)
         min_k = float('inf')
         min_idx = -1
         for g in range(gpu_num):
             if g == b_idx: continue
             k_val = get_kvpr(gpu_stats[g]['l'], gpu_stats[g]['s'])
             if k_val < min_k:
                 min_k = k_val
                 min_idx = g
 
         removed_items = []
 
-        # Remove from bottleneck (Randomly, 1-2 items)
+        # Remove from bottleneck (Aggressive: 2-4 items)
         if plc[b_idx]:
-            num_remove = min(len(plc[b_idx]), random.randint(1, 2))
+            n_b = len(plc[b_idx])
+            num_remove = min(n_b, random.randint(2, 4))
             for _ in range(num_remove):
                 if not plc[b_idx]: break
                 idx = random.randrange(len(plc[b_idx]))
                 item = plc[b_idx].pop(idx)
                 removed_items.append(item)
                 gpu_stats[b_idx]['l'] -= item['l']
                 gpu_stats[b_idx]['s'] -= item['s']
 
-        # Remove from benefactor (Create space, 1 item)
+        # Remove from benefactor (Aggressive: 2-4 items to create space)
         if min_idx != -1 and plc[min_idx]:
-            # Try to remove a smaller item to make space for a bigger one from bottleneck?
-            # Or just random. Random is safer.
-            if plc[min_idx]:
+            n_m = len(plc[min_idx])
+            num_remove = min(n_m, random.randint(2, 4))
+            for _ in range(num_remove):
+                if not plc[min_idx]: break
                 idx = random.randrange(len(plc[min_idx]))
                 item = plc[min_idx].pop(idx)
                 removed_items.append(item)
                 gpu_stats[min_idx]['l'] -= item['l']
                 gpu_stats[min_idx]['s'] -= item['s']
 
         random.shuffle(removed_items)
 
         # Recreate: Best Fit (Greedy K-minimization)
         for item in removed_items:
             best_t = -1
             best_k = float('inf')
 
             # Randomize order of GPUs
             targets_rnd = list(range(gpu_num))
             random.shuffle(targets_rnd)
 
             for t in targets_rnd:
                 if t == b_idx: continue
                 if gpu_stats[t]['s'] + item['s'] < GPU_MEM_SIZE:
                     k = get_kvpr(gpu_stats[t]['l'] + item['l'], gpu_stats[t]['s'] + item['s'])
                     if k < best_k:
                         best_k = k
                         best_t = t
 
             if best_t != -1:
                 plc[best_t].append(item)
                 gpu_stats[best_t]['l'] += item['l']
                 gpu_stats[best_t]['s'] += item['s']
             else:
                 # Put back
                 plc[b_idx].append(item)
                 gpu_stats[b_idx]['l'] += item['l']
                 gpu_stats[b_idx]['s'] += item['s']
 
     # Final result
     result = {}
     for i in range(gpu_num):
         result[i] = [x['model'] for x in best_global_plc[i]]
     return result
 
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")