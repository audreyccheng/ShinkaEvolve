--- a/original.py
+++ b/original.py
@@ -1,336 +1,367 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 import random
+import time
+import math
 
 GPU_MEM_SIZE = 80  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
     
-    Algorithm: Multi-Start Perturbation-Based Iterated Local Search.
-    
-    1. Initialization: Randomized Bin Packing based on Linearized Weight (L + K*S).
-       Runs multiple independent restarts to explore diverse starting basins.
-    2. Local Search: Steepest Descent Hill Climbing.
-       - Moves: Transfer, Swap(1-1), Swap(2-1).
-       - Selection: Evaluates ALL valid moves from the bottleneck GPU and selects
-         the one yielding the best immediate reduction in KVPR.
-    3. Perturbation: targeted 'kick' that moves a model from the bottleneck GPU
-       to the least-loaded feasible GPU to escape local optima while maintaining balance.
+    Algorithm: Multi-Start Randomized Packing + Steepest Descent ILS
+    
+    1. Multi-Start Initialization:
+       - Runs binary search to find feasible KVPR 'K'.
+       - Generates multiple initial solutions using Randomized Best Fit Decreasing 
+         sorted by Linearized Weight (Load + K*Size).
+       - Selects the start with the lowest max KVPR.
+       
+    2. Steepest Descent Local Search:
+       - Identifies top bottleneck GPUs (not just the single worst).
+       - Evaluates all valid moves (Transfer, Swap 1-1, Swap 2-1) from bottlenecks.
+       - Greedily selects the move that minimizes the global max KVPR (or local max of affected GPUs).
+       
+    3. Perturbation:
+       - Kicks the system by forcing moves from the worst bottleneck to feasible targets.
     """
     
-    # Pre-calculate model properties
+    start_time = time.time()
+    
+    # --- Pre-processing ---
     model_data = []
-    for m in models:
+    for i, m in enumerate(models):
         model_data.append({
             'model': m,
             'l': m.req_rate / m.slo,
-            's': m.model_size
+            's': m.model_size,
+            'id': i
         })
 
+    # --- Helper Functions ---
+
     def solve_packing(target_k, randomize=False):
-        """Generates a placement. Randomize=True adds noise to weights for diversity."""
+        """
+        Tries to pack models with target KVPR constraint.
+        randomize: applies noise to sorting weights to generate diverse packings.
+        """
         capacity = target_k * GPU_MEM_SIZE
+        
+        # Linearized Weight: w = L + K*S
         items = []
         for d in model_data:
             w = d['l'] + target_k * d['s']
             if randomize:
-                w *= random.uniform(0.9, 1.1)
+                w *= random.uniform(0.95, 1.05)
             items.append((w, d))
-            
+        
+        # Sort Descending (Best Fit Decreasing)
         items.sort(key=lambda x: x[0], reverse=True)
         
         gpu_l = [0.0] * gpu_num
         gpu_s = [0.0] * gpu_num
         gpu_models = [[] for _ in range(gpu_num)]
         
-        # Helper indices for random tie-breaking
-        indices = list(range(gpu_num))
+        # Helper for randomizing bin selection order in case of ties (though Best Fit is deterministic usually)
+        # We stick to deterministic Best Fit logic on the randomized order of items
         
         for _, item in items:
             best_idx = -1
             min_rem = float('inf')
             w_item = item['l'] + target_k * item['s']
             
-            if randomize: random.shuffle(indices)
-            
-            for i in indices:
+            # Check all bins
+            for i in range(gpu_num):
                 if gpu_s[i] + item['s'] >= GPU_MEM_SIZE - 1e-6: continue
                 
                 curr_w = gpu_l[i] + target_k * gpu_s[i]
                 if curr_w + w_item <= capacity + 1e-9:
                     rem = capacity - (curr_w + w_item)
                     if rem < min_rem:
                         min_rem = rem
                         best_idx = i
             
             if best_idx != -1:
                 gpu_l[best_idx] += item['l']
                 gpu_s[best_idx] += item['s']
                 gpu_models[best_idx].append(item['model'])
             else:
                 return None
         return gpu_models
 
-    def get_kvpr(l, s):
-        if s >= GPU_MEM_SIZE - 1e-7: return 1e15
+    def get_k(l, s):
+        if s >= GPU_MEM_SIZE - 1e-6: return 1e15
         return l / (GPU_MEM_SIZE - s)
 
-    # --- Phase 1: Determine Baseline K ---
+    # --- Phase 1: Binary Search for K ---
     low, high = 0.0, 1.0
     for _ in range(20):
         if solve_packing(high) is not None: break
         low, high = high, high * 2.0
     else: high = 1e9
     
+    # Refine K
     for _ in range(20):
         mid = (low + high) / 2
         if solve_packing(mid) is not None: high = mid
         else: low = mid
     base_k = high
 
-    # --- Phase 2: Multi-Start ILS ---
-    best_global_plc = None
-    best_global_score = float('inf')
-
-    # Run 5 restarts (Time permitting, this is very fast)
-    for restart_idx in range(5):
-        # Generate initial solution (Randomize subsequent starts)
-        init_plc_list = solve_packing(base_k, randomize=(restart_idx > 0))
-        if init_plc_list is None: 
-            # Fallback if base_k is too tight for random packing
-            init_plc_list = solve_packing(base_k * 1.5, randomize=False)
-            if init_plc_list is None: continue
-
-        current_plc = {i: list(init_plc_list[i]) for i in range(gpu_num)}
-        
-        # Calculate stats
-        l_vec = [sum(m.req_rate/m.slo for m in current_plc[g]) for g in range(gpu_num)]
-        s_vec = [sum(m.model_size for m in current_plc[g]) for g in range(gpu_num)]
-        
-        cur_max_k = max(get_kvpr(l_vec[g], s_vec[g]) for g in range(gpu_num))
-        
-        no_imp_iter = 0
-        iter_limit = 100
-        
-        for _ in range(iter_limit):
-            if cur_max_k < 1e-9: break
-            
-            # Find bottleneck
-            bottleneck = -1
-            max_val = -1.0
-            for g in range(gpu_num):
-                val = get_kvpr(l_vec[g], s_vec[g])
-                if val > max_val:
-                    max_val = val
-                    bottleneck = g
-            
-            # Steepest Descent: Find BEST move from bottleneck
-            best_move = None
-            best_imp = 0.0
-            
-            src_l = l_vec[bottleneck]
-            src_s = s_vec[bottleneck]
-            
-            # Helper to check validity and improvement
-            def check_update(new_src_l, new_src_s, new_tgt_l, new_tgt_s):
-                if new_src_s >= GPU_MEM_SIZE or new_tgt_s >= GPU_MEM_SIZE: return -1.0
-                nk_src = get_kvpr(new_src_l, new_src_s)
-                nk_tgt = get_kvpr(new_tgt_l, new_tgt_s)
-                new_local_max = max(nk_src, nk_tgt)
-                if new_local_max < cur_max_k - 1e-7:
-                    return cur_max_k - new_local_max
-                return -1.0
-
-            src_models = current_plc[bottleneck]
-            n_src = len(src_models)
-
-            # Iterate targets
-            for t in range(gpu_num):
-                if t == bottleneck: continue
-                # Pruning: if target is already heavily loaded, skip
-                if get_kvpr(l_vec[t], s_vec[t]) > cur_max_k * 0.95: continue
-
-                tgt_models = current_plc[t]
-                n_tgt = len(tgt_models)
-                tgt_l = l_vec[t]
-                tgt_s = s_vec[t]
-
-                # 1. Move
-                for i in range(n_src):
-                    m = src_models[i]
-                    ml, ms = m.req_rate/m.slo, m.model_size
-                    imp = check_update(src_l - ml, src_s - ms, tgt_l + ml, tgt_s + ms)
-                    if imp > best_imp:
-                        best_imp = imp
-                        best_move = ('move', bottleneck, i, t)
-
-                # 2. Swap 1-1
-                for i in range(n_src):
-                    m1 = src_models[i]
-                    m1l, m1s = m1.req_rate/m1.slo, m1.model_size
-                    for j in range(n_tgt):
-                        m2 = tgt_models[j]
-                        m2l, m2s = m2.req_rate/m2.slo, m2.model_size
-                        imp = check_update(src_l - m1l + m2l, src_s - m1s + m2s, 
-                                         tgt_l - m2l + m1l, tgt_s - m2s + m1s)
+    # --- Phase 2: Multi-Start Selection ---
+    best_init_plc = None
+    best_init_score = float('inf')
+    
+    # 1 Deterministic + 19 Randomized starts
+    for i in range(20):
+        if time.time() - start_time > 0.5: break # Time guard
+        
+        res = solve_packing(base_k, randomize=(i > 0))
+        if res is None and i == 0:
+            # Fallback if base_k is tight
+            res = solve_packing(base_k * 1.05, randomize=False)
+            if res is None: res = solve_packing(1e9)
+        
+        if res:
+            # Calculate score
+            max_k = 0
+            for g_m in res:
+                l = sum(m.req_rate/m.slo for m in g_m)
+                s = sum(m.model_size for m in g_m)
+                k = get_k(l, s)
+                if k > max_k: max_k = k
+            
+            if max_k < best_init_score:
+                best_init_score = max_k
+                best_init_plc = res
+
+    if best_init_plc is None:
+        best_init_plc = solve_packing(1e9)
+        if best_init_plc is None: raise ValueError("Infeasible")
+
+    # Initialize State
+    # List of dicts for fast access
+    state = []
+    for g_list in best_init_plc:
+        l = sum(m.req_rate/m.slo for m in g_list)
+        s = sum(m.model_size for m in g_list)
+        state.append({'l': l, 's': s, 'models': list(g_list)})
+
+    current_max_k = best_init_score
+
+    # --- Phase 3: Steepest Descent ILS ---
+    
+    iter_limit = 200
+    
+    for iteration in range(iter_limit):
+        if time.time() - start_time > 0.9: break
+        
+        # 1. Identify Bottlenecks
+        kvprs = [(get_k(g['l'], g['s']), i) for i, g in enumerate(state)]
+        kvprs.sort(key=lambda x: x[0], reverse=True)
+        
+        current_max_k = kvprs[0][0]
+        if current_max_k < 1e-9: break
+        
+        # Focus on top bottlenecks (top 4 or those within 90% of max)
+        bottlenecks = [x[1] for x in kvprs if x[0] > current_max_k * 0.9][:4]
+        
+        best_move = None
+        best_imp = 0.0
+        
+        # 2. Evaluate Neighbors
+        for b_idx in bottlenecks:
+            src = state[b_idx]
+            # Valid targets are those with KVPR < current_max (strict improvement potential)
+            valid_targets = [i for i in range(gpu_num) if i != b_idx and get_k(state[i]['l'], state[i]['s']) < current_max_k]
+            
+            # Sort targets by KVPR ascending (try empty/low load first)
+            valid_targets.sort(key=lambda i: get_k(state[i]['l'], state[i]['s']))
+            # Limit targets to speed up? Top 10 best targets?
+            valid_targets = valid_targets[:10]
+            
+            # A. Move
+            for m_idx, m in enumerate(src['models']):
+                m_l = m.req_rate/m.slo
+                m_s = m.model_size
+                
+                for t_idx in valid_targets:
+                    tgt = state[t_idx]
+                    if tgt['s'] + m_s >= GPU_MEM_SIZE: continue
+                    
+                    new_src_k = get_k(src['l'] - m_l, src['s'] - m_s)
+                    new_tgt_k = get_k(tgt['l'] + m_l, tgt['s'] + m_s)
+                    
+                    local_max = max(new_src_k, new_tgt_k)
+                    if local_max < current_max_k - 1e-7:
+                        imp = current_max_k - local_max
                         if imp > best_imp:
                             best_imp = imp
-                            best_move = ('swap11', bottleneck, i, t, j)
-
-                # 3. Swap 2-1 (2 from Bottleneck, 1 from Target)
-                if n_src >= 2:
-                    for i1 in range(n_src):
-                        for i2 in range(i1 + 1, n_src):
-                            m1, m2 = src_models[i1], src_models[i2]
-                            pl = (m1.req_rate/m1.slo) + (m2.req_rate/m2.slo)
-                            ps = m1.model_size + m2.model_size
-                            
-                            for j in range(n_tgt):
-                                m3 = tgt_models[j]
-                                m3l, m3s = m3.req_rate/m3.slo, m3.model_size
-                                imp = check_update(src_l - pl + m3l, src_s - ps + m3s,
-                                                 tgt_l - m3l + pl, tgt_s - m3s + ps)
-                                if imp > best_imp:
-                                    best_imp = imp
-                                    best_move = ('swap21', bottleneck, i1, i2, t, j)
-
-            # Execute best move
-            if best_move:
-                mtype = best_move[0]
-                if mtype == 'move':
-                    _, b, i, t = best_move
-                    m = current_plc[b].pop(i)
-                    current_plc[t].append(m)
-                    
-                    ml, ms = m.req_rate/m.slo, m.model_size
-                    l_vec[b] -= ml; s_vec[b] -= ms
-                    l_vec[t] += ml; s_vec[t] += ms
-                    
-                elif mtype == 'swap11':
-                    _, b, i, t, j = best_move
-                    m1 = current_plc[b][i]
-                    m2 = current_plc[t][j]
-                    current_plc[b][i] = m2
-                    current_plc[t][j] = m1
-                    
-                    diff_l = (m2.req_rate/m2.slo) - (m1.req_rate/m1.slo)
-                    diff_s = m2.model_size - m1.model_size
-                    l_vec[b] += diff_l; s_vec[b] += diff_s
-                    l_vec[t] -= diff_l; s_vec[t] -= diff_s
-                    
-                elif mtype == 'swap21':
-                    _, b, i1, i2, t, j = best_move
-                    # Pop carefully: larger index first
-                    m2 = current_plc[b].pop(i2)
-                    m1 = current_plc[b].pop(i1)
-                    m3 = current_plc[t].pop(j)
-                    current_plc[b].append(m3)
-                    current_plc[t].append(m1)
-                    current_plc[t].append(m2)
-                    
-                    # Full recalc for safety on multi-item moves
-                    l_vec[b] = sum(m.req_rate/m.slo for m in current_plc[b])
-                    s_vec[b] = sum(m.model_size for m in current_plc[b])
-                    l_vec[t] = sum(m.req_rate/m.slo for m in current_plc[t])
-                    s_vec[t] = sum(m.model_size for m in current_plc[t])
-
-                cur_max_k = max(get_kvpr(l_vec[g], s_vec[g]) for g in range(gpu_num))
-                no_imp_iter = 0
-            else:
-                # Perturbation
-                if no_imp_iter > 3: break
-                
-                # Move a model from bottleneck to LEAST LOADED feasible GPU
-                min_k = float('inf')
-                min_g = -1
-                for g in range(gpu_num):
-                    if g == bottleneck: continue
-                    k_val = get_kvpr(l_vec[g], s_vec[g])
-                    if k_val < min_k:
-                        min_k = k_val
-                        min_g = g
+                            best_move = ('move', b_idx, m_idx, t_idx)
+
+            # B. Swap 1-1
+            for m_idx, m1 in enumerate(src['models']):
+                m1_l = m1.req_rate/m1.slo
+                m1_s = m1.model_size
+                
+                for t_idx in valid_targets:
+                    tgt = state[t_idx]
+                    for tm_idx, m2 in enumerate(tgt['models']):
+                        m2_l = m2.req_rate/m2.slo
+                        m2_s = m2.model_size
+                        
+                        if src['s'] - m1_s + m2_s >= GPU_MEM_SIZE: continue
+                        if tgt['s'] - m2_s + m1_s >= GPU_MEM_SIZE: continue
+                        
+                        new_src_k = get_k(src['l'] - m1_l + m2_l, src['s'] - m1_s + m2_s)
+                        new_tgt_k = get_k(tgt['l'] - m2_l + m1_l, tgt['s'] - m2_s + m1_s)
+                        
+                        local_max = max(new_src_k, new_tgt_k)
+                        if local_max < current_max_k - 1e-7:
+                            imp = current_max_k - local_max
+                            if imp > best_imp:
+                                best_imp = imp
+                                best_move = ('swap11', b_idx, m_idx, t_idx, tm_idx)
+            
+            # C. Swap 2-1 (2 from bottleneck, 1 from target)
+            if len(src['models']) >= 2:
+                for i1 in range(len(src['models'])):
+                    for i2 in range(i1 + 1, len(src['models'])):
+                        m1 = src['models'][i1]
+                        m2 = src['models'][i2]
+                        pair_l = (m1.req_rate/m1.slo) + (m2.req_rate/m2.slo)
+                        pair_s = m1.model_size + m2.model_size
+                        
+                        for t_idx in valid_targets:
+                            tgt = state[t_idx]
+                            for tm_idx, m3 in enumerate(tgt['models']):
+                                m3_l = m3.req_rate/m3.slo
+                                m3_s = m3.model_size
+                                
+                                if src['s'] - pair_s + m3_s >= GPU_MEM_SIZE: continue
+                                if tgt['s'] - m3_s + pair_s >= GPU_MEM_SIZE: continue
+                                
+                                new_src_k = get_k(src['l'] - pair_l + m3_l, src['s'] - pair_s + m3_s)
+                                new_tgt_k = get_k(tgt['l'] - m3_l + pair_l, tgt['s'] - m3_s + pair_s)
+                                
+                                local_max = max(new_src_k, new_tgt_k)
+                                if local_max < current_max_k - 1e-7:
+                                    imp = current_max_k - local_max
+                                    if imp > best_imp:
+                                        best_imp = imp
+                                        best_move = ('swap21', b_idx, i1, i2, t_idx, tm_idx)
+
+        # 3. Apply Move or Perturb
+        if best_move:
+            op = best_move[0]
+            if op == 'move':
+                s, si, t = best_move[1:]
+                m = state[s]['models'].pop(si)
+                state[t]['models'].append(m)
+                m_l = m.req_rate/m.slo; m_s = m.model_size
+                state[s]['l'] -= m_l; state[s]['s'] -= m_s
+                state[t]['l'] += m_l; state[t]['s'] += m_s
+            elif op == 'swap11':
+                s, si, t, ti = best_move[1:]
+                m1 = state[s]['models'][si]
+                m2 = state[t]['models'][ti]
+                state[s]['models'][si] = m2
+                state[t]['models'][ti] = m1
+                diff_l = (m2.req_rate/m2.slo) - (m1.req_rate/m1.slo)
+                diff_s = m2.model_size - m1.model_size
+                state[s]['l'] += diff_l; state[s]['s'] += diff_s
+                state[t]['l'] -= diff_l; state[t]['s'] -= diff_s
+            elif op == 'swap21':
+                s, si1, si2, t, ti = best_move[1:]
+                m2 = state[s]['models'].pop(si2)
+                m1 = state[s]['models'].pop(si1)
+                m3 = state[t]['models'].pop(ti)
+                state[s]['models'].append(m3)
+                state[t]['models'].extend([m1, m2])
+                
+                # Full recalc
+                state[s]['l'] = sum(x.req_rate/x.slo for x in state[s]['models'])
+                state[s]['s'] = sum(x.model_size for x in state[s]['models'])
+                state[t]['l'] = sum(x.req_rate/x.slo for x in state[t]['models'])
+                state[t]['s'] = sum(x.model_size for x in state[t]['models'])
+        else:
+            # Perturbation: Move items from worst GPU to valid targets
+            worst_idx = kvprs[0][1]
+            if not state[worst_idx]['models']: break
+            
+            # Attempt to move up to 2 items to diversify
+            for _ in range(2):
+                if not state[worst_idx]['models']: break
+                
+                ridx = random.randint(0, len(state[worst_idx]['models'])-1)
+                m = state[worst_idx]['models'][ridx]
+                
+                # Find best fit target (lowest KVPR) that accepts it
+                candidates = [i for i in range(gpu_num) if i != worst_idx]
+                candidates.sort(key=lambda i: get_k(state[i]['l'], state[i]['s']))
                 
                 moved = False
-                if min_g != -1 and current_plc[bottleneck]:
-                    # Try first model (simple heuristic)
-                    m = current_plc[bottleneck][0]
-                    if s_vec[min_g] + m.model_size < GPU_MEM_SIZE:
-                        current_plc[bottleneck].pop(0)
-                        current_plc[min_g].append(m)
-                        
-                        l_vec[bottleneck] = sum(m.req_rate/m.slo for m in current_plc[bottleneck])
-                        s_vec[bottleneck] = sum(m.model_size for m in current_plc[bottleneck])
-                        l_vec[min_g] = sum(m.req_rate/m.slo for m in current_plc[min_g])
-                        s_vec[min_g] = sum(m.model_size for m in current_plc[min_g])
-                        
-                        cur_max_k = max(get_kvpr(l_vec[g], s_vec[g]) for g in range(gpu_num))
+                for tidx in candidates:
+                    if state[tidx]['s'] + m.model_size < GPU_MEM_SIZE:
+                        state[worst_idx]['models'].pop(ridx)
+                        state[tidx]['models'].append(m)
+                        
+                        m_l = m.req_rate/m.slo; m_s = m.model_size
+                        state[worst_idx]['l'] -= m_l; state[worst_idx]['s'] -= m_s
+                        state[tidx]['l'] += m_l; state[tidx]['s'] += m_s
                         moved = True
-                
+                        break
                 if not moved: break
-                no_imp_iter += 1
-
-        if cur_max_k < best_global_score:
-            best_global_score = cur_max_k
-            best_global_plc = {i: list(current_plc[i]) for i in range(gpu_num)}
-
-    # Fallback guarantees
-    if best_global_plc is None:
-        init = solve_packing(base_k)
-        if init is None: init = solve_packing(1e9)
-        if init is None: raise ValueError("No valid placement found.")
-        best_global_plc = {i: init[i] for i in range(gpu_num)}
-
-    return best_global_plc
+
+    return {i: state[i]['models'] for i in range(gpu_num)}
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")
