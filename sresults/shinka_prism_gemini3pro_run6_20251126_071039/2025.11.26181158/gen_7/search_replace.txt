<NAME>
improved_packing_heuristics
</NAME>

<DESCRIPTION>
Replace the single-strategy First Fit packing check with a multi-strategy Best Fit packing check.
By trying multiple sorting orders (Linearized Weight, Size, Load) and using Best Fit, we increase the likelihood of finding a valid packing for a given KVPR 'K'.
This allows the binary search to converge to a smaller 'K', directly reducing the maximum KVPR.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Binary search for the optimal max KVPR (K)
    # Check if a KVPR 'K' is feasible by transforming to Bin Packing:
    # Constraint: sum(l) / (M - sum(s)) <= K
    #          => sum(l) + K * sum(s) <= K * M
    # Item weight: w = l + K * s
    # Bin capacity: C = K * M

    def can_pack(target_k):
        # Calculate weights for current K
        items = []
        for d in model_data:
            items.append((d['l'] + target_k * d['s'], d['model']))

        # Sort by weight descending (First Fit Decreasing)
        items.sort(key=lambda x: x[0], reverse=True)

        capacity = target_k * GPU_MEM_SIZE
        bins_load = [0.0] * gpu_num
        bins_models = [[] for _ in range(gpu_num)]

        for w, model in items:
            placed = False
            for i in range(gpu_num):
                if bins_load[i] + w <= capacity:
                    bins_load[i] += w
                    bins_models[i].append(model)
                    placed = True
                    break
            if not placed:
                return None
        return bins_models
=======
    # Multi-strategy packing check
    # Check if a KVPR 'K' is feasible using Best Fit Decreasing with multiple sort criteria

    # Pre-sorted lists for static criteria
    # Sort by Size descending
    models_by_s = sorted(range(len(model_data)), key=lambda i: model_data[i]['s'], reverse=True)
    # Sort by Load descending
    models_by_l = sorted(range(len(model_data)), key=lambda i: model_data[i]['l'], reverse=True)

    def can_pack(target_k):
        capacity = target_k * GPU_MEM_SIZE

        # We try multiple sorting strategies. If any succeeds, K is feasible.
        strategies = []

        # Strategy 1: Sort by Linearized Weight (L + K*S) descending
        # This dynamic sort is crucial as K changes
        strat_w = sorted(range(len(model_data)),
                         key=lambda i: model_data[i]['l'] + target_k * model_data[i]['s'],
                         reverse=True)
        strategies.append(strat_w)

        # Strategy 2: Sort by Size descending (useful when memory is bottleneck)
        strategies.append(models_by_s)

        # Strategy 3: Sort by Load descending (useful when load is bottleneck)
        strategies.append(models_by_l)

        for indices in strategies:
            bins_weight = [0.0] * gpu_num
            bins_size = [0.0] * gpu_num # Track size explicitly for safety
            bins_models = [[] for _ in range(gpu_num)]
            possible = True

            for idx in indices:
                item = model_data[idx]
                w = item['l'] + target_k * item['s']
                s = item['s']

                # Best Fit: Find bin with min remaining capacity that fits item
                best_bin = -1
                min_rem = float('inf')

                for i in range(gpu_num):
                    # Check explicit memory safety
                    if bins_size[i] + s >= GPU_MEM_SIZE - 1e-6:
                        continue

                    # Check linearized constraint
                    if bins_weight[i] + w <= capacity + 1e-9:
                        rem = capacity - (bins_weight[i] + w)
                        if rem < min_rem:
                            min_rem = rem
                            best_bin = i

                if best_bin != -1:
                    bins_weight[best_bin] += w
                    bins_size[best_bin] += s
                    bins_models[best_bin].append(item['model'])
                else:
                    possible = False
                    break

            if possible:
                return bins_models

        return None
>>>>>>> REPLACE
</DIFF>