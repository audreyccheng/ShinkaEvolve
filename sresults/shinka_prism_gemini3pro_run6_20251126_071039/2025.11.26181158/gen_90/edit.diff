--- a/original.py
+++ b/original.py
@@ -1,232 +1,363 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
+import math
 
 GPU_MEM_SIZE = 80  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
     
     Algorithm:
-    1. Binary Search on target KVPR 'K'.
-       - Checks feasibility by solving a Bin Packing problem with Best Fit.
-       - Uses multiple sorting heuristics (Linearized Weight, Size) to maximize packing success.
-    2. Local Search Refinement.
-       - Greedily moves models from the bottleneck GPU (max KVPR) to other GPUs
-         to reduce the global maximum KVPR.
+    1. Initialization: Binary Search on 'K' (Max KVPR target).
+       - Solves a Bin Packing problem where item size depends on K.
+       - Uses multiple sorting heuristics (Linearized Weight, Size, Load) to find valid packing.
+    2. Local Search Refinement:
+       - Iteratively improves the solution by targeting the bottleneck GPU.
+       - Operators: 
+         a. Dual Ruin & Recreate: Redistributes models between the max-load and min-load GPUs.
+         b. Move: Shifts models from bottleneck to other GPUs.
+         c. Swap: Exchanges models to balance load.
     """
     
-    # Pre-calculate model properties
-    model_data = []
+    # Pre-calculate model properties for efficiency
+    # Items stores: {model, l (load), s (size), id}
+    items = []
     for i, m in enumerate(models):
-        model_data.append({
+        items.append({
             'model': m,
             'l': m.req_rate / m.slo,
-            's': m.model_size
+            's': m.model_size,
+            'id': i
         })
 
-    def solve_packing(target_k):
+    # --- Phase 1: Initialization via Binary Search on Linearized Constraint ---
+    
+    def try_pack(target_k, item_list):
         """
-        Attempts to place models into gpu_num bins given a target KVPR 'K'.
-        Constraint per GPU: sum(L) + K * sum(S) <= K * M
-        Returns placement dict if successful, None otherwise.
+        Attempts to pack items into gpu_num bins satisfying:
+        Sum(L) / (Capacity - Sum(S)) <= K
+        Equivalent to: Sum(L) + K * Sum(S) <= K * Capacity
         """
-        capacity = target_k * GPU_MEM_SIZE
-        
-        # Helper to try a specific packing order with Best Fit
-        def try_best_fit(items):
-            gpu_l = [0.0] * gpu_num
-            gpu_s = [0.0] * gpu_num
-            gpu_models = [[] for _ in range(gpu_num)]
-            
-            for item in items:
+        lin_capacity = target_k * GPU_MEM_SIZE
+        
+        def solve_bf(sorted_items):
+            # State: list of [current_lin_weight, current_size, items_list]
+            bins = [[0.0, 0.0, []] for _ in range(gpu_num)]
+            
+            for item in sorted_items:
+                item_lin = item['l'] + target_k * item['s']
+                item_s = item['s']
+                
                 best_idx = -1
-                min_remaining = float('inf')
-                
-                w = item['l'] + target_k * item['s']
+                min_rem_cap = float('inf')
                 
                 for i in range(gpu_num):
+                    b_lin, b_s, _ = bins[i]
+                    
                     # Hard memory constraint
-                    if gpu_s[i] + item['s'] >= GPU_MEM_SIZE - 1e-6:
+                    if b_s + item_s >= GPU_MEM_SIZE - 1e-6:
                         continue
                         
-                    # Linearized KVPR constraint
-                    curr_w = gpu_l[i] + target_k * gpu_s[i]
-                    if curr_w + w <= capacity + 1e-9:
-                        # Best Fit: Choose bin with minimum remaining linearized capacity
-                        rem = capacity - (curr_w + w)
-                        if rem < min_remaining:
-                            min_remaining = rem
+                    # Linearized constraint check
+                    if b_lin + item_lin <= lin_capacity + 1e-9:
+                        # Best Fit: minimize remaining capacity (tightest fit)
+                        rem = lin_capacity - (b_lin + item_lin)
+                        if rem < min_rem_cap:
+                            min_rem_cap = rem
                             best_idx = i
-                
+                            
                 if best_idx != -1:
-                    gpu_l[best_idx] += item['l']
-                    gpu_s[best_idx] += item['s']
-                    gpu_models[best_idx].append(item['model'])
+                    bins[best_idx][0] += item_lin
+                    bins[best_idx][1] += item_s
+                    bins[best_idx][2].append(item['model'])
                 else:
                     return None
-            return gpu_models
-
-        # Strategy 1: Sort by Linearized Weight Descending
-        # Effective for generic bin packing logic
-        items_1 = sorted(model_data, key=lambda x: x['l'] + target_k * x['s'], reverse=True)
-        res = try_best_fit(items_1)
+            return bins
+
+        # Heuristic 1: Sort by Linearized Weight (L + K*S) Descending
+        # Most accurate for the specific target K
+        s1 = sorted(item_list, key=lambda x: x['l'] + target_k * x['s'], reverse=True)
+        res = solve_bf(s1)
         if res: return res
         
-        # Strategy 2: Sort by Size Descending
-        # Effective when memory is the primary constraint
-        items_2 = sorted(model_data, key=lambda x: x['s'], reverse=True)
-        res = try_best_fit(items_2)
+        # Heuristic 2: Sort by Size Descending
+        # Good when memory constraints are dominant (high K)
+        s2 = sorted(item_list, key=lambda x: x['s'], reverse=True)
+        res = solve_bf(s2)
         if res: return res
         
+        # Heuristic 3: Sort by Load Descending
+        # Good for balancing pure load
+        s3 = sorted(item_list, key=lambda x: x['l'], reverse=True)
+        res = solve_bf(s3)
+        if res: return res
+        
         return None
 
-    # --- Phase 1: Binary Search for Optimal K ---
-    
-    # Find valid upper bound
+    # Binary Search
     low = 0.0
     high = 1.0
-    best_placement_list = None
-    
-    # Exponential search
-    for _ in range(20):
-        res = solve_packing(high)
-        if res is not None:
-            best_placement_list = res
+    best_init = None
+    
+    # Exponential expansion to find valid upper bound
+    for _ in range(15):
+        if try_pack(high, items):
             break
         low = high
         high *= 2.0
     else:
-        # Fallback
-        high = 1e9
+        high = 1e9 # Fallback
         
     # Refine K
-    for _ in range(25):
+    for _ in range(20):
         mid = (low + high) / 2
-        res = solve_packing(mid)
-        if res is not None:
-            best_placement_list = res
+        res = try_pack(mid, items)
+        if res:
+            best_init = res
             high = mid
         else:
             low = mid
             
-    # Ensure we have a placement
-    if best_placement_list is None:
-        best_placement_list = solve_packing(high)
-        if best_placement_list is None:
-            raise ValueError("Unable to place models even with infinite KVPR.")
-
-    placement = {i: best_placement_list[i] for i in range(gpu_num)}
+    # Ensure placement exists
+    if best_init is None:
+        best_init = try_pack(high, items)
+        if best_init is None:
+            # Emergency fallback: First Fit Decreasing on Size (ignoring KVPR balance)
+            bins = [[] for _ in range(gpu_num)]
+            bin_s = [0.0] * gpu_num
+            for item in sorted(items, key=lambda x: x['s'], reverse=True):
+                placed = False
+                for i in range(gpu_num):
+                    if bin_s[i] + item['s'] < GPU_MEM_SIZE:
+                        bins[i].append(item['model'])
+                        bin_s[i] += item['s']
+                        placed = True
+                        break
+                if not placed:
+                    # Overfill min bin (should not happen in valid inputs)
+                    idx = min(range(gpu_num), key=lambda i: bin_s[i])
+                    bins[idx].append(item['model'])
+                    bin_s[idx] += item['s']
+            placement = {i: bins[i] for i in range(gpu_num)}
+        else:
+            placement = {i: b[2] for i, b in enumerate(best_init)}
+    else:
+        placement = {i: b[2] for i, b in enumerate(best_init)}
 
     # --- Phase 2: Local Search Refinement ---
     
-    def calculate_gpu_kvpr(p_models):
+    def get_kvpr(p_models):
         sl = sum(m.req_rate / m.slo for m in p_models)
         ss = sum(m.model_size for m in p_models)
         if ss >= GPU_MEM_SIZE: return float('inf')
+        if ss == 0: return 0.0 # Empty GPU
         return sl / (GPU_MEM_SIZE - ss)
 
-    # Greedily improve the worst GPU
-    for _ in range(50):
-        # Find current max KVPR GPU
-        kvprs = {g: calculate_gpu_kvpr(placement[g]) for g in range(gpu_num)}
-        max_gpu = max(kvprs, key=kvprs.get)
-        max_val = kvprs[max_gpu]
-        
-        if max_val == 0: break # Optimal
-        
-        best_move = None
-        best_improvement = 0.0
-        
-        # Try moving each model from max_gpu to any other GPU
-        src_models = placement[max_gpu]
-        
-        for idx, model in enumerate(src_models):
-            for tgt_gpu in range(gpu_num):
-                if tgt_gpu == max_gpu: continue
-                
-                tgt_models = placement[tgt_gpu]
-                
-                # Check memory fit
-                if sum(m.model_size for m in tgt_models) + model.model_size >= GPU_MEM_SIZE:
-                    continue
-                
-                # Predict new KVPRs
-                ns_l = sum(m.req_rate / m.slo for m in src_models) - (model.req_rate / model.slo)
-                ns_s = sum(m.model_size for m in src_models) - model.model_size
-                ns_kvpr = ns_l / (GPU_MEM_SIZE - ns_s) if ns_s < GPU_MEM_SIZE else float('inf')
-                
-                nt_l = sum(m.req_rate / m.slo for m in tgt_models) + (model.req_rate / model.slo)
-                nt_s = sum(m.model_size for m in tgt_models) + model.model_size
-                nt_kvpr = nt_l / (GPU_MEM_SIZE - nt_s) if nt_s < GPU_MEM_SIZE else float('inf')
-                
-                # We accept if the new local max is better than the old global max
-                new_local_max = max(ns_kvpr, nt_kvpr)
-                
-                if new_local_max < max_val - 1e-5:
-                    improv = max_val - new_local_max
-                    if improv > best_improvement:
-                        best_improvement = improv
-                        best_move = (idx, tgt_gpu)
-        
-        if best_move:
-            m_idx, t_gpu = best_move
-            model_to_move = placement[max_gpu].pop(m_idx)
-            placement[t_gpu].append(model_to_move)
-        else:
-            # No move from the bottleneck GPU improved the situation
-            break
-            
+    current_kvpr = {g: get_kvpr(placement[g]) for g in range(gpu_num)}
+    
+    # Helper to update internal state
+    def update_gpu(g, new_models):
+        placement[g] = new_models
+        current_kvpr[g] = get_kvpr(new_models)
+
+    # Iterative Improvement
+    for _ in range(200):
+        # Identify Bottleneck and Slack
+        sorted_gpus = sorted(current_kvpr.keys(), key=lambda k: current_kvpr[k], reverse=True)
+        max_g = sorted_gpus[0]
+        max_val = current_kvpr[max_g]
+        
+        if max_val < 1e-9: break
+        
+        min_g = sorted_gpus[-1]
+        improved = False
+        
+        # 1. Dual Ruin & Recreate (Rebalance max_g and min_g)
+        if max_g != min_g:
+            pool = placement[max_g] + placement[min_g]
+            
+            # Strategies to re-partition pool
+            sort_strategies = [
+                lambda m: m.model_size,  # Size Desc
+                lambda m: m.req_rate / m.slo  # Load Desc
+            ]
+            
+            best_local_split = None
+            # Current max over these two is the target to beat
+            current_local_max = max(max_val, current_kvpr[min_g])
+            best_iter_max = current_local_max
+            
+            for key_func in sort_strategies:
+                sorted_pool = sorted(pool, key=key_func, reverse=True)
+                
+                b1 = {'l': 0.0, 's': 0.0, 'm': []}
+                b2 = {'l': 0.0, 's': 0.0, 'm': []}
+                possible = True
+                
+                for m in sorted_pool:
+                    l = m.req_rate / m.slo
+                    s = m.model_size
+                    
+                    # Projections
+                    c1 = float('inf')
+                    if b1['s'] + s < GPU_MEM_SIZE:
+                        c1 = (b1['l'] + l) / (GPU_MEM_SIZE - (b1['s'] + s))
+                        
+                    c2 = float('inf')
+                    if b2['s'] + s < GPU_MEM_SIZE:
+                        c2 = (b2['l'] + l) / (GPU_MEM_SIZE - (b2['s'] + s))
+                    
+                    if c1 == float('inf') and c2 == float('inf'):
+                        possible = False; break
+                        
+                    # Greedy choice: Minimize resulting max of the two bins
+                    # We must consider the current state of the bins
+                    cur_c1 = b1['l'] / (GPU_MEM_SIZE - b1['s']) if b1['s'] < GPU_MEM_SIZE else float('inf')
+                    cur_c2 = b2['l'] / (GPU_MEM_SIZE - b2['s']) if b2['s'] < GPU_MEM_SIZE else float('inf')
+                    
+                    # If we add to 1, max is max(c1, cur_c2)
+                    # If we add to 2, max is max(cur_c1, c2)
+                    if max(c1, cur_c2) < max(cur_c1, c2):
+                        b1['l']+=l; b1['s']+=s; b1['m'].append(m)
+                    else:
+                        b2['l']+=l; b2['s']+=s; b2['m'].append(m)
+                
+                if possible:
+                    fk1 = b1['l']/(GPU_MEM_SIZE - b1['s'])
+                    fk2 = b2['l']/(GPU_MEM_SIZE - b2['s'])
+                    peak = max(fk1, fk2)
+                    
+                    if peak < best_iter_max - 1e-6:
+                        best_iter_max = peak
+                        best_local_split = (b1['m'], b2['m'])
+            
+            if best_local_split:
+                update_gpu(max_g, best_local_split[0])
+                update_gpu(min_g, best_local_split[1])
+                improved = True
+                
+        if improved: continue
+
+        # 2. Move Operator (Shift item from max_g)
+        src_models = placement[max_g]
+        for i, m in enumerate(src_models):
+            m_l = m.req_rate / m.slo
+            m_s = m.model_size
+            
+            # New src state
+            rem_l = sum(x.req_rate/x.slo for x in src_models) - m_l
+            rem_s = sum(x.model_size for x in src_models) - m_s
+            new_src_val = rem_l / (GPU_MEM_SIZE - rem_s)
+            
+            # Try better targets first (lowest KVPR)
+            for tgt_g in reversed(sorted_gpus):
+                if tgt_g == max_g: continue
+                
+                tgt_models = placement[tgt_g]
+                tgt_s = sum(x.model_size for x in tgt_models)
+                
+                if tgt_s + m_s >= GPU_MEM_SIZE: continue
+                
+                tgt_l = sum(x.req_rate/x.slo for x in tgt_models)
+                new_tgt_val = (tgt_l + m_l) / (GPU_MEM_SIZE - (tgt_s + m_s))
+                
+                if max(new_src_val, new_tgt_val) < max_val - 1e-6:
+                    new_src = list(src_models); new_src.pop(i)
+                    new_tgt = list(tgt_models); new_tgt.append(m)
+                    update_gpu(max_g, new_src)
+                    update_gpu(tgt_g, new_tgt)
+                    improved = True
+                    break
+            if improved: break
+        
+        if improved: continue
+
+        # 3. Swap Operator
+        # Try swapping with 3 best candidates
+        candidates = sorted_gpus[-3:]
+        if max_g in candidates: candidates.remove(max_g)
+        
+        for tgt_g in candidates:
+            tgt_models = placement[tgt_g]
+            for i_src, m_src in enumerate(src_models):
+                s_l = m_src.req_rate/m_src.slo; s_s = m_src.model_size
+                for i_tgt, m_tgt in enumerate(tgt_models):
+                    t_l = m_tgt.req_rate/m_tgt.slo; t_s = m_tgt.model_size
+                    
+                    if abs(s_s - t_s) < 0.1 and abs(s_l - t_l) < 0.1: continue
+                    
+                    # Calc new values
+                    ns_l = sum(x.req_rate/x.slo for x in src_models) - s_l + t_l
+                    ns_s = sum(x.model_size for x in src_models) - s_s + t_s
+                    if ns_s >= GPU_MEM_SIZE: continue
+                    val_src = ns_l / (GPU_MEM_SIZE - ns_s)
+                    
+                    nt_l = sum(x.req_rate/x.slo for x in tgt_models) - t_l + s_l
+                    nt_s = sum(x.model_size for x in tgt_models) - t_s + s_s
+                    if nt_s >= GPU_MEM_SIZE: continue
+                    val_tgt = nt_l / (GPU_MEM_SIZE - nt_s)
+                    
+                    if max(val_src, val_tgt) < max_val - 1e-6:
+                        new_src = list(src_models); new_src[i_src] = m_tgt
+                        new_tgt = list(tgt_models); new_tgt[i_tgt] = m_src
+                        update_gpu(max_g, new_src)
+                        update_gpu(tgt_g, new_tgt)
+                        improved = True; break
+                if improved: break
+            if improved: break
+            
+        if not improved: break
+
     return placement
-
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
     
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
     
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
     
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
     
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
     
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")
 
