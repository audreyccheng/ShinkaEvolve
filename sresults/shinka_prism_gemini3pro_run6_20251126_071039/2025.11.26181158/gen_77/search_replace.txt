<NAME>
Update_Docstring
</NAME>

<DESCRIPTION>
Update the docstring to reflect the addition of Swap 2-1 and the new Ruin and Recreate perturbation strategy.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    2. Iterated Local Search (ILS):
       a) Variable Neighborhood Descent (VND) with Move, Swap(1-1), Swap(1-2).
       b) Perturbation: If local search stagnates, applies a "Kick" by forcing a move
          from the bottleneck GPU to a random feasible GPU to escape local optima.
=======
    2. Iterated Local Search (ILS):
       a) Variable Neighborhood Descent (VND) with Move, Swap(1-1), Swap(1-2), Swap(2-1).
       b) Perturbation: Ruin and Recreate strategy that redistributes models from the
          bottleneck and the lightest GPU to escape local optima.
>>>>>>> REPLACE
</DIFF>

<NAME>
Add_Swap21_And_RuinRecreate
</NAME>

<DESCRIPTION>
1. Introduces a "Swap 2-1" operator to the VND loop (First Improvement). This symmetric operator to Swap 1-2 attempts to consolidate two smaller models from a bottleneck source into a destination GPU, exchanging them for one larger model. This helps resolve fragmentation where a bottleneck is caused by multiple small models.
2. Replaces the single-move perturbation with a "Ruin and Recreate" strategy. When local search stagnates, this method empties the highest-load (bottleneck) GPU and the lowest-load GPU, then greedily reinserts their models into the system (targeting any GPU) using a Best-Fit heuristic. This larger disruption allows the algorithm to escape deep local optima by rebalancing extreme load disparities.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        # --- Operator 3: Swap (1-2) ---
        # Move 1 from Source, 2 from Dest
        for source in sources[:2]:
            for i, m_a in enumerate(source.models):
                for dest in current_gpus:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue
                    if len(dest.models) < 2: continue

                    n_d = len(dest.models)
                    pair_found = False
                    for j1 in range(n_d):
                        for j2 in range(j1+1, n_d):
                            m_b1 = dest.models[j1]
                            m_b2 = dest.models[j2]

                            s_mem = source.used_mem - m_a.model_size + m_b1.model_size + m_b2.model_size
                            d_mem = dest.used_mem - m_b1.model_size - m_b2.model_size + m_a.model_size

                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                source.remove(i)
                                dest.remove(j2) # Larger idx first
                                dest.remove(j1)
                                source.add(m_b1)
                                source.add(m_b2)
                                dest.add(m_a)

                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    pair_found = True
                                    if current_vector < best_vector:
                                        best_vector = current_vector
                                        for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
                                    break
                                else:
                                    dest.remove(len(dest.models)-1) # m_a
                                    source.remove(len(source.models)-1)
                                    source.remove(len(source.models)-1)
                                    dest.restore_model(j1, m_b1)
                                    dest.restore_model(j2, m_b2)
                        if pair_found: break
                    if pair_found: break
                if improved_step: break

        if improved_step: continue

        # --- Perturbation ---
        # If no improvement found (Local Optimum), apply a kick.
        # Don't perturb if we are near the end of budget.
        if iter_cnt > max_iter - 15: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        # Move a random model from worst_gpu to a random feasible destination
        m_idx = random.randint(0, len(worst_gpu.models)-1)
        model_to_move = worst_gpu.models[m_idx]

        feasible_dests = [g for g in current_gpus if g.id != worst_gpu.id and g.can_fit(model_to_move.model_size)]

        if feasible_dests:
            dest = random.choice(feasible_dests)
            worst_gpu.remove(m_idx)
            dest.add(model_to_move)
            current_vector = get_vector(current_gpus)
            # We accept the worse state to explore new areas
        else:
            break
=======
        # --- Operator 3: Swap (1-2) ---
        # Move 1 from Source, 2 from Dest
        for source in sources[:2]:
            for i, m_a in enumerate(source.models):
                for dest in current_gpus:
                    if dest.id == source.id: continue
                    if dest.kvpr() >= source.kvpr(): continue
                    if len(dest.models) < 2: continue

                    n_d = len(dest.models)
                    pair_found = False
                    for j1 in range(n_d):
                        for j2 in range(j1+1, n_d):
                            m_b1 = dest.models[j1]
                            m_b2 = dest.models[j2]

                            s_mem = source.used_mem - m_a.model_size + m_b1.model_size + m_b2.model_size
                            d_mem = dest.used_mem - m_b1.model_size - m_b2.model_size + m_a.model_size

                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                source.remove(i)
                                dest.remove(j2) # Larger idx first
                                dest.remove(j1)
                                source.add(m_b1)
                                source.add(m_b2)
                                dest.add(m_a)

                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    pair_found = True
                                    if current_vector < best_vector:
                                        best_vector = current_vector
                                        for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
                                    break
                                else:
                                    dest.remove(len(dest.models)-1) # m_a
                                    source.remove(len(source.models)-1)
                                    source.remove(len(source.models)-1)
                                    dest.restore_model(j1, m_b1)
                                    dest.restore_model(j2, m_b2)
                        if pair_found: break
                    if pair_found: break
                if improved_step: break

        if improved_step: continue

        # --- Operator 4: Swap (2-1) ---
        # Move 2 from Source, 1 from Dest
        for source in sources[:2]:
            if len(source.models) < 2: continue
            n_s = len(source.models)
            pair_found = False
            for i1 in range(n_s):
                for i2 in range(i1+1, n_s):
                    m_a1 = source.models[i1]
                    m_a2 = source.models[i2]

                    for dest in current_gpus:
                        if dest.id == source.id: continue
                        if dest.kvpr() >= source.kvpr(): continue

                        for j, m_b in enumerate(dest.models):
                            s_mem = source.used_mem - m_a1.model_size - m_a2.model_size + m_b.model_size
                            d_mem = dest.used_mem - m_b.model_size + m_a1.model_size + m_a2.model_size

                            if s_mem <= GPU_MEM_SIZE and d_mem <= GPU_MEM_SIZE:
                                source.remove(i2) # Remove higher index first
                                source.remove(i1)
                                dest.remove(j)
                                source.add(m_b)
                                dest.add(m_a1)
                                dest.add(m_a2)

                                new_vec = get_vector(current_gpus)
                                if new_vec < current_vector:
                                    current_vector = new_vec
                                    improved_step = True
                                    pair_found = True
                                    if current_vector < best_vector:
                                        best_vector = current_vector
                                        for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
                                    break
                                else:
                                    dest.remove(len(dest.models)-1) # m_a2
                                    dest.remove(len(dest.models)-1) # m_a1
                                    source.remove(len(source.models)-1) # m_b
                                    dest.restore_model(j, m_b)
                                    source.restore_model(i1, m_a1)
                                    source.restore_model(i2, m_a2)
                        if pair_found: break
                    if pair_found: break
                if pair_found: break
            if improved_step: break

        if improved_step: continue

        # --- Perturbation: Ruin and Recreate ---
        # If no improvement found (Local Optimum), destroy bottleneck and lightest GPU
        # and redistribute their models.
        if iter_cnt > max_iter - 15: break

        # Refresh sorted list to be sure we identify current bottleneck
        current_gpus.sort(key=lambda g: g.kvpr(), reverse=True)
        worst_gpu = current_gpus[0]
        if not worst_gpu.models: break

        best_gpu = current_gpus[-1]
        if best_gpu.id == worst_gpu.id:
            if len(current_gpus) > 1: best_gpu = current_gpus[-2]
            else: break

        floating_models = []
        while worst_gpu.models: floating_models.append(worst_gpu.remove(0))
        while best_gpu.models: floating_models.append(best_gpu.remove(0))

        # Sort by size descending for packing
        floating_models.sort(key=lambda m: m.model_size, reverse=True)

        # Recreate into ANY GPU
        for m in floating_models:
            best_dest = None
            best_val = float('inf')

            for g in current_gpus:
                if g.can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (g.used_mem + m.model_size)
                    if rem > 1e-7:
                        val = (g.load + m.req_rate/m.slo) / rem
                        if val < best_val:
                            best_val = val
                            best_dest = g

            if best_dest:
                best_dest.add(m)
            else:
                # Force fit back if possible
                if worst_gpu.can_fit(m.model_size): worst_gpu.add(m)
                elif best_gpu.can_fit(m.model_size): best_gpu.add(m)

        current_vector = get_vector(current_gpus)
        if current_vector < best_vector:
             best_vector = current_vector
             for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
>>>>>>> REPLACE
</DIFF>