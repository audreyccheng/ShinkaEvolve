<NAME>
init_scan_and_dual_ruin
</NAME>

<DESCRIPTION>
1.  **Objective-Aware Initialization:** Improves the initialization phase by scanning a range of 'K' values (multiples of the minimum feasible K found by binary search). This generates a diverse set of initial solutions ranging from tight load balancing (low K) to efficient size packing (high K), increasing the probability of finding a high-quality starting basin.
2.  **Dual-Ruin Perturbation:** Enhances the escape mechanism from local optima. Instead of only ruining the bottleneck GPU, it also ruins the best (lowest load) GPU. This creates a larger pool of models and available space, allowing the "Recreate" phase to effectively redistribute high-load models from the bottleneck to the underutilized GPU, balancing the cluster more effectively.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Binary Search for K
    low, high = 0.0, 1000.0
    if solve_linearized_bin_packing(high) is None: high = 1e9

    best_k = float('inf')
    bs_res = None
    for _ in range(20):
        mid = (low + high) / 2
        res = solve_linearized_bin_packing(mid)
        if res:
            bs_res = res
            best_k = mid
            high = mid
        else:
            low = mid

    if bs_res:
        candidates.append(bs_res)
        # Randomized Refinement around best_k
        # Run multiple times with noise to find potentially better packings
        for _ in range(10):
            res_noise = solve_linearized_bin_packing(best_k, noise_factor=0.05)
            if res_noise: candidates.append(res_noise)
=======
    # Binary Search for K (min feasible K)
    low, high = 0.0, 1000.0
    if solve_linearized_bin_packing(high) is None: high = 1e9

    min_feasible_k = high
    bs_res = None
    for _ in range(20):
        mid = (low + high) / 2
        res = solve_linearized_bin_packing(mid)
        if res:
            bs_res = res
            min_feasible_k = mid
            high = mid
        else:
            low = mid

    if bs_res:
        candidates.append(bs_res)
        # Scan range of K values starting from min_feasible_k
        # Higher K prioritizes size packing, Lower K prioritizes load balancing
        multipliers = [1.05, 1.2, 1.5, 2.0, 3.0, 5.0]
        for mult in multipliers:
            k_val = min_feasible_k * mult
            res = solve_linearized_bin_packing(k_val)
            if res: candidates.append(res)
            # Add noise for robustness
            res_noise = solve_linearized_bin_packing(k_val, noise_factor=0.05)
            if res_noise: candidates.append(res_noise)
>>>>>>> REPLACE
<<<<<<< SEARCH
        # --- PERTURBATION (Ruins and Recreate) ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        # Ruin: Remove all models from the bottleneck GPU
        displaced_models = []
        while worst_gpu.models:
            displaced_models.append(worst_gpu.remove(0))

        # Recreate: Try to distribute models to other GPUs
        # Sort by load to put heavy items first
        displaced_models.sort(key=lambda m: m.req_rate/m.slo, reverse=True)

        # Candidate GPUs (excluding source initially to force movement)
        other_gpus = [g for g in current_gpus if g.id != worst_gpu.id]

        for m in displaced_models:
            best_dest = None
            best_dest_val = float('inf')

            # Find Best Fit destination
            for dest in other_gpus:
                if dest.can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (dest.used_mem + m.model_size)
                    if rem > 1e-7:
                        val = (dest.load + m.req_rate/m.slo) / rem
                        if val < best_dest_val:
                            best_dest_val = val
                            best_dest = dest

            if best_dest:
                best_dest.add(m)
            else:
                # Fallback: put back in source
                worst_gpu.add(m)
=======
        # --- PERTURBATION (Ruins and Recreate) ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        # Dual Ruin: Clear worst GPU and best GPU (to create space for redistribution)
        best_gpu = sorted_gpus[-1]
        second_gpu = None
        if best_gpu.id != worst_gpu.id:
            second_gpu = best_gpu
        elif len(sorted_gpus) > 1:
            second_gpu = sorted_gpus[-2]

        displaced_models = []
        # Clear worst
        while worst_gpu.models:
            displaced_models.append(worst_gpu.remove(0))
        # Clear second
        if second_gpu:
            while second_gpu.models:
                displaced_models.append(second_gpu.remove(0))

        # Recreate: Distribute models to ANY GPU (including emptied ones)
        # Alternating sort strategy for diversity
        if iter_cnt % 2 == 0:
            displaced_models.sort(key=lambda m: m.req_rate/m.slo, reverse=True)
        else:
            displaced_models.sort(key=lambda m: m.model_size, reverse=True)

        for m in displaced_models:
            best_dest = None
            best_dest_val = float('inf')

            # Find Best Fit destination (Minimizing local KVPR)
            for dest in current_gpus:
                if dest.can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (dest.used_mem + m.model_size)
                    if rem > 1e-7:
                        val = (dest.load + m.req_rate/m.slo) / rem
                        if val < best_dest_val:
                            best_dest_val = val
                            best_dest = dest

            if best_dest:
                best_dest.add(m)
            else:
                # Fallback: Try to put back in one of the emptied ones
                if worst_gpu.can_fit(m.model_size): worst_gpu.add(m)
                elif second_gpu and second_gpu.can_fit(m.model_size): second_gpu.add(m)
                else:
                    # Last resort: any GPU that fits
                    for g in current_gpus:
                        if g.can_fit(m.model_size):
                            g.add(m)
                            break
>>>>>>> REPLACE
</DIFF>