<NAME>
randomized_multistart_ils_swap22
</NAME>

<DESCRIPTION>
Replaces the deterministic construction and limited local search with a Multi-Start Randomized ILS.
1. Finds a baseline K using strict Binary Search.
2. Performs 5 restarts where the initial packing is randomized (noise in weights) and K is slightly relaxed to encourage diverse basins of attraction.
3. Applies an enhanced Local Search with Steepest Descent using Shift, Swap(1-1), Swap(2-1), and a new Swap(2-2) operator to better handle fragmentation and tight packing.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Algorithm:
    1. Binary Search for the optimal Max KVPR 'K'.
       The feasibility check transforms the problem into a Bin Packing Problem
       with linearized item weights: w = l + K * s.
       It uses Best Fit Decreasing with 4 sorting heuristics:
       - Linearized Weight: l + K*s
       - Size: s
       - Load: l
       - Density: l/s
    2. Local Search Refinement.
       After finding a feasible placement, we explicitly minimize the max KVPR
       using three neighborhood operators on the bottleneck GPU:
       - Shift: Move 1 model to another GPU.
       - Swap 1-1: Exchange 1 model with 1 from another GPU.
       - Swap 2-1: Exchange 2 models with 1 from another GPU (helps with fragmentation).
    """

    # Precompute model characteristics
    model_data = []
    for i, m in enumerate(models):
        model_data.append({
            'model': m,
            'l': m.req_rate / m.slo,
            's': m.model_size,
            'id': i
        })

    # --- Phase 1: Binary Search Construction ---

    def solve_packing(target_k):
        capacity = target_k * GPU_MEM_SIZE

        # Sorting strategies for Best Fit Decreasing
        heuristics = [
            # 1. Linearized Weight: balances load and size based on K
            lambda x: x['l'] + target_k * x['s'],
            # 2. Size: prioritizes fitting large items (memory constrained)
            lambda x: x['s'],
            # 3. Load: prioritizes high load items
            lambda x: x['l'],
            # 4. Density: efficient packing
            lambda x: x['l'] / x['s'] if x['s'] > 0 else 0
        ]

        for key_func in heuristics:
            # Sort descending
            items = sorted(model_data, key=key_func, reverse=True)

            bins_l = [0.0] * gpu_num
            bins_s = [0.0] * gpu_num
            bins_items = [[] for _ in range(gpu_num)]

            feasible = True
            for item in items:
                best_bin = -1
                min_slack = float('inf')

                # Weight of item in linearized constraint
                item_w = item['l'] + target_k * item['s']

                for b in range(gpu_num):
                    # Hard Memory Constraint
                    if bins_s[b] + item['s'] >= GPU_MEM_SIZE - 1e-6:
                        continue

                    # Linearized Constraint: sum(L) + K*sum(S) <= K*M
                    current_bin_w = bins_l[b] + target_k * bins_s[b]

                    if current_bin_w + item_w <= capacity + 1e-9:
                        # Best Fit: minimize slack
                        slack = capacity - (current_bin_w + item_w)
                        if slack < min_slack:
                            min_slack = slack
                            best_bin = b

                if best_bin != -1:
                    bins_l[best_bin] += item['l']
                    bins_s[best_bin] += item['s']
                    bins_items[best_bin].append(item)
                else:
                    feasible = False
                    break

            if feasible:
                return bins_items

        return None

    # Binary Search
    low = 0.0
    high = 1.0

    # Exponential search for upper bound
    for _ in range(20):
        if solve_packing(high) is not None:
            break
        low = high
        high *= 2.0
    else:
        high = 1e9 # Fallback

    best_packing = None

    # Precision search
    for _ in range(30):
        mid = (low + high) / 2
        res = solve_packing(mid)
        if res is not None:
            best_packing = res
            high = mid
        else:
            low = mid

    if best_packing is None:
        best_packing = solve_packing(high)
        if best_packing is None:
            raise ValueError("Unable to place models.")

    # Convert to standard format for Phase 2
    placement = {i: [x['model'] for x in best_packing[i]] for i in range(gpu_num)}

    # --- Phase 2: Local Search Refinement ---

    # Initialize state for fast access
    gpu_states = []
    for g in range(gpu_num):
        # We use the dictionaries from model_data to avoid re-creating them
        # We need to map the model objects back to their data dicts
        p_models = placement[g]
        g_items = []
        g_l = 0.0
        g_s = 0.0
        # Map models to data dicts. Optimization: Create a lookup if N is large,
        # but for small N nested loop is fine.
        for m in p_models:
            for d in model_data:
                if d['model'] is m:
                    g_items.append(d)
                    g_l += d['l']
                    g_s += d['s']
                    break
        gpu_states.append({'l': g_l, 's': g_s, 'items': g_items})

    def get_kvpr(l, s):
        if s >= GPU_MEM_SIZE - 1e-6: return float('inf')
        return l / (GPU_MEM_SIZE - s)

    # Hill Climbing on Global Max KVPR
    for _ in range(200): # Iteration limit
        # 1. Identify Bottleneck
        max_kvpr = -1.0
        max_gpu = -1
        current_kvprs = []

        for g in range(gpu_num):
            val = get_kvpr(gpu_states[g]['l'], gpu_states[g]['s'])
            current_kvprs.append(val)
            if val > max_kvpr:
                max_kvpr = val
                max_gpu = g

        if max_kvpr < 1e-9: break # Perfect score

        # 2. Find Best Move
        best_move = None # (type, gain, args...)
        best_gain = 0.0

        src = gpu_states[max_gpu]

        # Helper to calculate gain
        def calc_gain(src_l_new, src_s_new, tgt_l_new, tgt_s_new, old_tgt_kvpr):
            ns_k = get_kvpr(src_l_new, src_s_new)
            nt_k = get_kvpr(tgt_l_new, tgt_s_new)
            new_peak = max(ns_k, nt_k)
            # We strictly want to reduce the global max
            if new_peak < max_kvpr - 1e-7:
                return max_kvpr - new_peak
            return -1.0

        # Move Types
        # A. Shift: Move item from src -> tgt
        for i, item in enumerate(src['items']):
            for t in range(gpu_num):
                if t == max_gpu: continue
                # Pruning: Target shouldn't be worse than current max
                if current_kvprs[t] >= max_kvpr: continue

                tgt = gpu_states[t]
                if tgt['s'] + item['s'] >= GPU_MEM_SIZE: continue

                gain = calc_gain(
                    src['l'] - item['l'], src['s'] - item['s'],
                    tgt['l'] + item['l'], tgt['s'] + item['s'],
                    current_kvprs[t]
                )
                if gain > best_gain:
                    best_gain = gain
                    best_move = ('shift', i, t)

        # B. Swap 1-1: Swap item src <-> item tgt
        for i, s_item in enumerate(src['items']):
            for t in range(gpu_num):
                if t == max_gpu: continue
                if current_kvprs[t] >= max_kvpr: continue

                tgt = gpu_states[t]
                for j, t_item in enumerate(tgt['items']):
                    # Check capacity
                    ns_s = src['s'] - s_item['s'] + t_item['s']
                    nt_s = tgt['s'] - t_item['s'] + s_item['s']

                    if ns_s >= GPU_MEM_SIZE or nt_s >= GPU_MEM_SIZE: continue

                    gain = calc_gain(
                        src['l'] - s_item['l'] + t_item['l'], ns_s,
                        tgt['l'] - t_item['l'] + s_item['l'], nt_s,
                        current_kvprs[t]
                    )
                    if gain > best_gain:
                        best_gain = gain
                        best_move = ('swap11', i, t, j)

        # C. Swap 2-1: Swap {item1, item2} src <-> {item3} tgt
        # Useful for defragmentation or load balancing
        if len(src['items']) >= 2:
            n_src = len(src['items'])
            # Limit checks to prevent O(N^3) explosion if N is large
            # But typically N is small per GPU
            for i1 in range(n_src):
                for i2 in range(i1 + 1, n_src):
                    itm1 = src['items'][i1]
                    itm2 = src['items'][i2]
                    pair_l = itm1['l'] + itm2['l']
                    pair_s = itm1['s'] + itm2['s']

                    for t in range(gpu_num):
                        if t == max_gpu: continue
                        if current_kvprs[t] >= max_kvpr: continue

                        tgt = gpu_states[t]
                        for j, t_itm in enumerate(tgt['items']):
                            ns_s = src['s'] - pair_s + t_itm['s']
                            nt_s = tgt['s'] - t_itm['s'] + pair_s

                            if ns_s >= GPU_MEM_SIZE or nt_s >= GPU_MEM_SIZE: continue

                            gain = calc_gain(
                                src['l'] - pair_l + t_itm['l'], ns_s,
                                tgt['l'] - t_itm['l'] + pair_l, nt_s,
                                current_kvprs[t]
                            )
                            if gain > best_gain:
                                best_gain = gain
                                best_move = ('swap21', i1, i2, t, j)

        # Execute Move
        if best_move:
            mtype = best_move[0]
            if mtype == 'shift':
                _, idx, t_id = best_move
                item = src['items'].pop(idx)
                tgt = gpu_states[t_id]

                src['l'] -= item['l']; src['s'] -= item['s']
                tgt['items'].append(item)
                tgt['l'] += item['l']; tgt['s'] += item['s']

            elif mtype == 'swap11':
                _, s_idx, t_id, t_idx = best_move
                tgt = gpu_states[t_id]
                s_item = src['items'][s_idx]
                t_item = tgt['items'][t_idx]

                src['items'][s_idx] = t_item
                tgt['items'][t_idx] = s_item

                src['l'] += t_item['l'] - s_item['l']; src['s'] += t_item['s'] - s_item['s']
                tgt['l'] += s_item['l'] - t_item['l']; tgt['s'] += s_item['s'] - t_item['s']

            elif mtype == 'swap21':
                _, i1, i2, t_id, t_idx = best_move
                tgt = gpu_states[t_id]

                # Pop strictly i2 > i1
                itm2 = src['items'].pop(i2)
                itm1 = src['items'].pop(i1)
                t_itm = tgt['items'].pop(t_idx)

                src['items'].append(t_itm)
                tgt['items'].extend([itm1, itm2])

                pair_l = itm1['l'] + itm2['l']; pair_s = itm1['s'] + itm2['s']
                src['l'] += t_itm['l'] - pair_l; src['s'] += t_itm['s'] - pair_s
                tgt['l'] += pair_l - t_itm['l']; tgt['s'] += pair_s - t_itm['s']
        else:
            break # No improvement found

    return {g: [x['model'] for x in gpu_states[g]['items']] for g in range(gpu_num)}
=======
def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Algorithm: Multi-Start Randomized ILS with Heavy Local Search.

    1. Binary Search Baseline: Determine a 'base_k' using strict binary search.
    2. Multi-Start Loop (5 restarts):
       - Initialization: Randomized Bin Packing.
         Uses base_k but adds random noise to sorting weights to explore different configurations.
       - Local Search: Steepest Descent on max KVPR.
         Neighborhoods:
         - Shift (Move 1)
         - Swap 1-1
         - Swap 2-1 (2 from bottleneck, 1 from target)
         - Swap 2-2 (2 from bottleneck, 2 from target) - Critical for resolving complex deadlocks.
    """
    import random

    model_data = []
    for m in models:
        model_data.append({
            'model': m,
            'l': m.req_rate / m.slo,
            's': m.model_size
        })

    def get_kvpr(l, s):
        if s >= GPU_MEM_SIZE - 1e-6: return 1e15
        return l / (GPU_MEM_SIZE - s)

    # --- Phase 1: Determine Baseline K ---
    # Standard deterministic packing check
    def can_pack_deterministic(target_k):
        capacity = target_k * GPU_MEM_SIZE
        # Sort by linearized weight
        items = sorted(model_data, key=lambda x: x['l'] + target_k * x['s'], reverse=True)

        bins_l = [0.0] * gpu_num
        bins_s = [0.0] * gpu_num

        for item in items:
            best_idx = -1
            min_slack = float('inf')
            w = item['l'] + target_k * item['s']

            for i in range(gpu_num):
                if bins_s[i] + item['s'] >= GPU_MEM_SIZE - 1e-6: continue
                curr_w = bins_l[i] + target_k * bins_s[i]

                if curr_w + w <= capacity + 1e-9:
                    slack = capacity - (curr_w + w)
                    if slack < min_slack:
                        min_slack = slack
                        best_idx = i

            if best_idx != -1:
                bins_l[best_idx] += item['l']
                bins_s[best_idx] += item['s']
            else:
                return False
        return True

    low, high = 0.0, 1.0
    for _ in range(20):
        if can_pack_deterministic(high): break
        low = high; high *= 2.0
    else: high = 1e9

    for _ in range(25):
        mid = (low + high) / 2
        if can_pack_deterministic(mid): high = mid
        else: low = mid
    base_k = high

    # --- Phase 2: Multi-Start Local Search ---

    def solve_randomized_packing(target_k, randomize=False):
        capacity = target_k * GPU_MEM_SIZE
        # Create weighted items
        items = []
        for d in model_data:
            weight = d['l'] + target_k * d['s']
            if randomize:
                # Add +/- 5% noise
                weight *= random.uniform(0.95, 1.05)
            items.append((weight, d))

        items.sort(key=lambda x: x[0], reverse=True)

        gpu_state = [{'l': 0.0, 's': 0.0, 'items': []} for _ in range(gpu_num)]

        # Tie-breaking randomization
        indices = list(range(gpu_num))

        for _, item in items:
            best_idx = -1
            min_slack = float('inf')

            w_real = item['l'] + target_k * item['s']

            if randomize: random.shuffle(indices)

            for i in indices:
                st = gpu_state[i]
                if st['s'] + item['s'] >= GPU_MEM_SIZE - 1e-6: continue

                curr_w = st['l'] + target_k * st['s']
                if curr_w + w_real <= capacity + 1e-9:
                    slack = capacity - (curr_w + w_real)
                    if slack < min_slack:
                        min_slack = slack
                        best_idx = i

            if best_idx != -1:
                st = gpu_state[best_idx]
                st['l'] += item['l']
                st['s'] += item['s']
                st['items'].append(item)
            else:
                return None
        return gpu_state

    best_global_score = float('inf')
    best_global_state = None

    # Run restarts
    # 0: Deterministic (baseline)
    # 1-4: Randomized with slightly relaxed K to allow different packings
    for restart in range(5):
        k_factor = 1.0 if restart == 0 else 1.05
        init_state = solve_randomized_packing(base_k * k_factor, randomize=(restart > 0))

        if init_state is None:
            # If relaxed packing fails (rare if base_k is correct), try finding a looser K
            if restart == 0:
                init_state = solve_randomized_packing(base_k * 1.01, False)
            if init_state is None: continue

        # Local Search
        curr_state = init_state

        # Calculate initial max
        max_kvpr = 0.0
        for g in range(gpu_num):
            k = get_kvpr(curr_state[g]['l'], curr_state[g]['s'])
            if k > max_kvpr: max_kvpr = k

        for _ in range(100): # Iterations per restart
            if max_kvpr < 1e-9: break

            bottleneck = -1
            highest_k = -1.0

            # Identify bottleneck
            kvpr_map = []
            for g in range(gpu_num):
                val = get_kvpr(curr_state[g]['l'], curr_state[g]['s'])
                kvpr_map.append(val)
                if val > highest_k:
                    highest_k = val
                    bottleneck = g

            best_move = None # (type, improvement, args)
            best_imp = 0.0

            src = curr_state[bottleneck]

            # Helper to evaluate improvement
            def eval_imp(nl_src, ns_src, nl_tgt, ns_tgt):
                k1 = get_kvpr(nl_src, ns_src)
                k2 = get_kvpr(nl_tgt, ns_tgt)
                new_peak = max(k1, k2)
                if new_peak < highest_k - 1e-7:
                    return highest_k - new_peak
                return -1.0

            # 1. Shift
            for i, m in enumerate(src['items']):
                for t in range(gpu_num):
                    if t == bottleneck or kvpr_map[t] >= highest_k: continue
                    tgt = curr_state[t]
                    if tgt['s'] + m['s'] >= GPU_MEM_SIZE: continue

                    imp = eval_imp(
                        src['l'] - m['l'], src['s'] - m['s'],
                        tgt['l'] + m['l'], tgt['s'] + m['s']
                    )
                    if imp > best_imp:
                        best_imp = imp
                        best_move = ('shift', i, t)

            # 2. Swap 1-1
            for i, m1 in enumerate(src['items']):
                for t in range(gpu_num):
                    if t == bottleneck or kvpr_map[t] >= highest_k: continue
                    tgt = curr_state[t]
                    for j, m2 in enumerate(tgt['items']):
                        ns_s = src['s'] - m1['s'] + m2['s']
                        nt_s = tgt['s'] - m2['s'] + m1['s']
                        if ns_s >= GPU_MEM_SIZE or nt_s >= GPU_MEM_SIZE: continue

                        imp = eval_imp(
                            src['l'] - m1['l'] + m2['l'], ns_s,
                            tgt['l'] - m2['l'] + m1['l'], nt_s
                        )
                        if imp > best_imp:
                            best_imp = imp
                            best_move = ('swap11', i, t, j)

            # 3. Swap 2-1 (2 from Bottleneck)
            if len(src['items']) >= 2:
                n_src = len(src['items'])
                for i1 in range(n_src):
                    for i2 in range(i1+1, n_src):
                        m1, m2 = src['items'][i1], src['items'][i2]
                        pl, ps = m1['l']+m2['l'], m1['s']+m2['s']

                        for t in range(gpu_num):
                            if t == bottleneck or kvpr_map[t] >= highest_k: continue
                            tgt = curr_state[t]
                            for j, m3 in enumerate(tgt['items']):
                                ns_s = src['s'] - ps + m3['s']
                                nt_s = tgt['s'] - m3['s'] + ps
                                if ns_s >= GPU_MEM_SIZE or nt_s >= GPU_MEM_SIZE: continue

                                imp = eval_imp(
                                    src['l'] - pl + m3['l'], ns_s,
                                    tgt['l'] - m3['l'] + pl, nt_s
                                )
                                if imp > best_imp:
                                    best_imp = imp
                                    best_move = ('swap21', i1, i2, t, j)

            # 4. Swap 2-2 (2 from Bottleneck, 2 from Target)
            if len(src['items']) >= 2:
                n_src = len(src['items'])
                for i1 in range(n_src):
                    for i2 in range(i1+1, n_src):
                        m1, m2 = src['items'][i1], src['items'][i2]
                        pl_src, ps_src = m1['l']+m2['l'], m1['s']+m2['s']

                        for t in range(gpu_num):
                            if t == bottleneck or kvpr_map[t] >= highest_k: continue
                            tgt = curr_state[t]
                            if len(tgt['items']) < 2: continue

                            n_tgt = len(tgt['items'])
                            for j1 in range(n_tgt):
                                for j2 in range(j1+1, n_tgt):
                                    m3, m4 = tgt['items'][j1], tgt['items'][j2]
                                    pl_tgt, ps_tgt = m3['l']+m4['l'], m3['s']+m4['s']

                                    ns_s = src['s'] - ps_src + ps_tgt
                                    nt_s = tgt['s'] - ps_tgt + ps_src
                                    if ns_s >= GPU_MEM_SIZE or nt_s >= GPU_MEM_SIZE: continue

                                    imp = eval_imp(
                                        src['l'] - pl_src + pl_tgt, ns_s,
                                        tgt['l'] - pl_tgt + pl_src, nt_s
                                    )
                                    if imp > best_imp:
                                        best_imp = imp
                                        best_move = ('swap22', i1, i2, t, j1, j2)

            if best_move:
                mtype = best_move[0]
                if mtype == 'shift':
                    _, i, t = best_move
                    m = src['items'].pop(i)
                    tgt = curr_state[t]
                    tgt['items'].append(m)
                    src['l']-=m['l']; src['s']-=m['s']
                    tgt['l']+=m['l']; tgt['s']+=m['s']
                elif mtype == 'swap11':
                    _, i, t, j = best_move
                    tgt = curr_state[t]
                    m1, m2 = src['items'][i], tgt['items'][j]
                    src['items'][i] = m2
                    tgt['items'][j] = m1
                    diff_l = m2['l']-m1['l']; diff_s = m2['s']-m1['s']
                    src['l']+=diff_l; src['s']+=diff_s
                    tgt['l']-=diff_l; tgt['s']-=diff_s
                elif mtype == 'swap21':
                    _, i1, i2, t, j = best_move
                    tgt = curr_state[t]
                    # Pop largest index first
                    m2 = src['items'].pop(i2)
                    m1 = src['items'].pop(i1)
                    m3 = tgt['items'].pop(j)
                    src['items'].append(m3)
                    tgt['items'].extend([m1, m2])

                    pl, ps = m1['l']+m2['l'], m1['s']+m2['s']
                    src['l'] += m3['l'] - pl; src['s'] += m3['s'] - ps
                    tgt['l'] += pl - m3['l']; tgt['s'] += ps - m3['s']
                elif mtype == 'swap22':
                    _, i1, i2, t, j1, j2 = best_move
                    tgt = curr_state[t]
                    m_s2 = src['items'].pop(i2)
                    m_s1 = src['items'].pop(i1)
                    m_t2 = tgt['items'].pop(j2)
                    m_t1 = tgt['items'].pop(j1)

                    src['items'].extend([m_t1, m_t2])
                    tgt['items'].extend([m_s1, m_s2])

                    src['l'] = sum(x['l'] for x in src['items'])
                    src['s'] = sum(x['s'] for x in src['items'])
                    tgt['l'] = sum(x['l'] for x in tgt['items'])
                    tgt['s'] = sum(x['s'] for x in tgt['items'])

                # Update max_kvpr
                max_kvpr = 0.0
                for g in range(gpu_num):
                    k = get_kvpr(curr_state[g]['l'], curr_state[g]['s'])
                    if k > max_kvpr: max_kvpr = k
            else:
                break

        if max_kvpr < best_global_score:
            best_global_score = max_kvpr
            best_global_state = {g: [x['model'] for x in curr_state[g]['items']] for g in range(gpu_num)}

    if best_global_state is None:
        raise ValueError("No valid placement found.")

    return best_global_state
>>>>>>> REPLACE
</DIFF>