<NAME>
binary_search_bin_packing
</NAME>

<DESCRIPTION>
Replaces the greedy heuristic with a Binary Search approach that minimizes the maximum KVPR.
The KVPR constraint is transformed into a bin packing problem with dynamic weights w = load + K * size, which is checked using the First Fit Decreasing heuristic.
This approach effectively solves the minimax problem by searching for the smallest feasible max KVPR K.
An upper bound of 1e12 is used to handle cases with very tight memory constraints, and the binary search runs for 50 iterations to ensure high precision.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Greedy KVPR-minimizing placement based on Algorithm 1 (without Ï„ check)
    # 1) Sort models by r_j / s_j in descending order
    sorted_models = sorted(models, key=lambda m: (m.req_rate / m.slo), reverse=True)

    # 2) Initialize per-GPU states
    placement = {gpu_id: [] for gpu_id in range(gpu_num)}
    shared_kv = [GPU_MEM_SIZE for _ in range(gpu_num)]  # remaining memory per GPU
    weighted_req_rate = [0.0 for _ in range(gpu_num)]   # sum of r_j / s_j per GPU

    # 3) Assign each model to the GPU that minimizes current KVPR while fitting in memory
    for model in sorted_models:
        best_idx = None
        best_ratio = float('inf')

        for gpu_id in range(gpu_num):
            if model.model_size <= shared_kv[gpu_id] and shared_kv[gpu_id] > 0:
                current_ratio = weighted_req_rate[gpu_id] / shared_kv[gpu_id]
                if current_ratio < best_ratio:
                    best_ratio = current_ratio
                    best_idx = gpu_id

        # Failure: if no GPU can fit, raise an error instead of overcommitting
        if best_idx is None:
            raise ValueError(
                f"Unable to place model of size {model.model_size} GB on any GPU. "
                f"Remaining per-GPU memory: {shared_kv}"
            )

        placement[best_idx].append(model)
        weighted_req_rate[best_idx] += model.req_rate / model.slo
        shared_kv[best_idx] -= model.model_size

    return placement
=======
    # Precompute model data: l = req_rate/slo, s = model_size
    model_data = []
    for m in models:
        model_data.append({
            'model': m,
            'l': m.req_rate / m.slo,
            's': m.model_size
        })

    # Binary search for the optimal max KVPR (K)
    # Check if a KVPR 'K' is feasible by transforming to Bin Packing:
    # Constraint: sum(l) / (M - sum(s)) <= K
    #          => sum(l) + K * sum(s) <= K * M
    # Item weight: w = l + K * s
    # Bin capacity: C = K * M

    def can_pack(target_k):
        # Calculate weights for current K
        items = []
        for d in model_data:
            items.append((d['l'] + target_k * d['s'], d['model']))

        # Sort by weight descending (First Fit Decreasing)
        items.sort(key=lambda x: x[0], reverse=True)

        capacity = target_k * GPU_MEM_SIZE
        bins_load = [0.0] * gpu_num
        bins_models = [[] for _ in range(gpu_num)]

        for w, model in items:
            placed = False
            for i in range(gpu_num):
                if bins_load[i] + w <= capacity:
                    bins_load[i] += w
                    bins_models[i].append(model)
                    placed = True
                    break
            if not placed:
                return None
        return bins_models

    # Search range for K
    low = 0.0
    high = 1e12  # Large upper bound to cover tight memory cases
    best_placement = None

    # Check if feasible at all (at high K, memory constraints dominate)
    best_placement = can_pack(high)
    if best_placement is None:
        raise ValueError("Unable to fit models into GPU memory.")

    # Binary Search
    for _ in range(50):
        mid = (low + high) / 2
        res = can_pack(mid)
        if res is not None:
            best_placement = res
            high = mid
        else:
            low = mid

    # Format the result
    return {i: best_placement[i] for i in range(gpu_num)}
>>>>>>> REPLACE
</DIFF>