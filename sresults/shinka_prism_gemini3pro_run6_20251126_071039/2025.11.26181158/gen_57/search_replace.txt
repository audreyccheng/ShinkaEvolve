<NAME>
expand_swaps_and_add_lns_perturbation
</NAME>

<DESCRIPTION>
1. Expand the scope of Swap 1-2 and Swap 2-1 operators to consider the top 4 bottleneck GPUs (instead of top 3 or 2). This increases the chance of finding improvements in secondary bottlenecks that might be blocking progress.
2. Replace the single-model perturbation with a "Ruins and Recreate" (LNS) perturbation. This operator completely empties the worst bottleneck GPU and attempts to redistribute its models to other GPUs using a Best Fit heuristic (minimizing resulting KVPR). Models that cannot fit elsewhere are returned to the original GPU. This allows for more significant jumps in the search space to escape deep local optima.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        # --- Operator 3: Swap 1-2 (Source gives 1, Dest gives 2) ---
        # Expanded to top 3 bottlenecks
        for source in sources[:3]:
            for i, m_a in enumerate(source.models):
=======
        # --- Operator 3: Swap 1-2 (Source gives 1, Dest gives 2) ---
        # Expanded to top 4 bottlenecks
        for source in sources[:4]:
            for i, m_a in enumerate(source.models):
>>>>>>> REPLACE
<<<<<<< SEARCH
        # --- Operator 4: Swap 2-1 (Source gives 2, Dest gives 1) ---
        # Expanded to top 2 bottlenecks
        for source in sources[:2]:
            if len(source.models) < 2: continue
=======
        # --- Operator 4: Swap 2-1 (Source gives 2, Dest gives 1) ---
        # Expanded to top 4 bottlenecks
        for source in sources[:4]:
            if len(source.models) < 2: continue
>>>>>>> REPLACE
<<<<<<< SEARCH
        # --- Perturbation ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        # Guided Kick: Prefer moving high-load models
        sorted_models_idx = sorted(range(len(worst_gpu.models)),
                                   key=lambda k: worst_gpu.models[k].req_rate/worst_gpu.models[k].slo,
                                   reverse=True)
        # Pick from top 50%
        n_candidates = max(1, len(sorted_models_idx) // 2)
        m_idx_idx = random.randint(0, n_candidates - 1)
        m_idx = sorted_models_idx[m_idx_idx]

        model_to_move = worst_gpu.models[m_idx]

        feasible_dests = [g for g in current_gpus if g.id != worst_gpu.id and g.can_fit(model_to_move.model_size)]

        if feasible_dests:
            feasible_dests.sort(key=lambda g: g.kvpr())
            target_candidates = feasible_dests[:4]
            dest = random.choice(target_candidates)

            worst_gpu.remove(m_idx)
            dest.add(model_to_move)
            current_vector = get_vector(current_gpus)
        else:
            break
=======
        # --- Perturbation (Ruins and Recreate) ---
        iter_cnt += 1
        if iter_cnt > max_iter - 10: break

        worst_gpu = sorted_gpus[0]
        if not worst_gpu.models: break

        # Ruin: Remove all models from the bottleneck GPU
        displaced_models = list(worst_gpu.models)
        while worst_gpu.models:
            worst_gpu.remove(0)

        # Recreate: Try to distribute models to other GPUs
        # Sort by size (with noise) to pack large items first but allow variation
        displaced_models.sort(key=lambda m: m.model_size * random.uniform(0.95, 1.05), reverse=True)

        other_gpus = [g for g in current_gpus if g.id != worst_gpu.id]

        for m in displaced_models:
            best_dest = None
            best_dest_val = float('inf')

            for dest in other_gpus:
                if dest.can_fit(m.model_size):
                    rem = GPU_MEM_SIZE - (dest.used_mem + m.model_size)
                    if rem > 1e-7:
                        # Heuristic: minimize resulting KVPR
                        val = (dest.load + m.req_rate/m.slo) / rem
                        if val < best_dest_val:
                            best_dest_val = val
                            best_dest = dest

            if best_dest:
                best_dest.add(m)
            else:
                # If no other place, put back in source
                worst_gpu.add(m)

        current_vector = get_vector(current_gpus)
        # Always accept perturbation to escape local optima
        # Check if we accidentally found a better state
        if current_vector < best_vector:
            best_vector = current_vector
            for k in range(gpu_num): best_gpus[k].copy_from(current_gpus[k])
>>>>>>> REPLACE
</DIFF>