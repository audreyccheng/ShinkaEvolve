<NAME>
calibrate_confidence_with_prob_and_fit
</NAME>

<DESCRIPTION>
Improves confidence calibration by:
1. Tracking the Bayesian probability of the winning hypothesis (`decision_confidences`) during the iterative repair process. This captures the ambiguity of the solution (e.g., if multiple values fit equally well).
2. Refining the final confidence calculation to be the product of this decision confidence and the geometric mean of local and peer router fitness. This ensures confidence is high only when both the local decision was clear AND the global network state is consistent.
3. Applying router fitness penalties even to "Anchor" (symmetric) links, preventing overconfidence in scenarios where symmetric links are connected to unbalanced routers.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Estimates state: {if_id: {'rx': val, 'tx': val}}
    estimates = {}

    # Classification
=======
    # Estimates state: {if_id: {'rx': val, 'tx': val}}
    estimates = {}
    decision_confidences = {} # Map (if_id, metric) -> confidence of decision (0.0-1.0)

    # Classification
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
                # Pick winner
                best_idx = scores.index(max(scores))
                win_val = hyps[best_idx]
                updates.append((src, 'tx', win_val))
                updates.append((dst, 'rx', win_val))

            elif flow['type'] == 'external':
=======
                # Pick winner
                total_score = sum(scores)
                if total_score > 0:
                    win_prob = max(scores) / total_score
                else:
                    win_prob = 0.0

                decision_confidences[(src, 'tx')] = win_prob
                decision_confidences[(dst, 'rx')] = win_prob

                best_idx = scores.index(max(scores))
                win_val = hyps[best_idx]
                updates.append((src, 'tx', win_val))
                updates.append((dst, 'rx', win_val))

            elif flow['type'] == 'external':
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
                best_idx = scores.index(max(scores))
                win_val = hyps[best_idx]
                updates.append((if_id, metric, win_val))

        # Apply Updates with Momentum
=======
                total_score = sum(scores)
                if total_score > 0:
                    win_prob = max(scores) / total_score
                else:
                    win_prob = 0.0

                decision_confidences[(if_id, metric)] = win_prob

                best_idx = scores.index(max(scores))
                win_val = hyps[best_idx]
                updates.append((if_id, metric, win_val))

        # Apply Updates with Momentum
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
        # Confidence Calculation
        # 1. Base Confidence: How much did we deviate?
        #    If Anchor -> High.
        #    If Deviation small -> High.
        #    If Deviation large -> Low (unless R_Fit is very high, meaning we fixed it)

        def calc_conf(rep_val, orig_val, is_anchor):
            if is_anchor: return 0.95

            # If not anchor, we relied on conservation
            # If router conservation is bad, confidence is bad
            base = r_fit

            # If we changed the value significantly, check if it makes sense
            if abs(rep_val - orig_val) > max(orig_val * 0.1, 1.0):
                # Big change.
                # If fit is perfect (1.0), we are confident we fixed it (Conf ~ 0.9)
                # If fit is poor (0.5), we are unsure (Conf ~ 0.4)
                return 0.9 * base
            else:
                # Small change. We trust the measurement roughly, adjusted by fit.
                # If fit is bad, maybe measurement was wrong but we couldn't find better?
                return 0.8 * base + 0.15

        conf_rx = calc_conf(rep_rx, orig_rx, (if_id, 'rx') in anchors)
        conf_tx = calc_conf(rep_tx, orig_tx, (if_id, 'tx') in anchors)

        # Clamp
=======
        # Confidence Calculation
        def calc_conf(if_id, metric, rep_val, orig_val):
            is_anchor = (if_id, metric) in anchors

            # 1. Router Fit (Global Consistency)
            # Combine local and peer router fits for robust scoring
            rid = if_to_router.get(if_id)
            local_fit = router_fits.get(rid, 0.8)

            peer_id = telemetry[if_id].get('connected_to')
            peer_fit = 1.0
            if peer_id and peer_id in if_to_router:
                prid = if_to_router[peer_id]
                peer_fit = router_fits.get(prid, 0.8)

            # Geometric mean of fits (penalize if either side is broken)
            fit_quality = math.sqrt(local_fit * peer_fit)

            # 2. Decision Confidence (Local Ambiguity)
            # For anchors, we implicitly trust them (high decision confidence)
            # For suspects, we use the Bayesian probability margin
            dec_conf = decision_confidences.get((if_id, metric), 0.95 if is_anchor else 0.5)

            # 3. Combined Confidence
            conf = dec_conf * fit_quality

            # 4. Bonuses/Penalties
            # If validated (value didn't change), boost confidence towards fit_quality
            # This handles cases where measurement was correct and we confirmed it
            if abs(rep_val - orig_val) < max(orig_val * 0.05, 0.5):
                conf = max(conf, 0.8 * fit_quality + 0.1)

            # Anchors are strong evidence, don't drop too low unless catastrophic fit
            if is_anchor:
                conf = max(conf, 0.6 * fit_quality)

            return conf

        conf_rx = calc_conf(if_id, 'rx', rep_rx, orig_rx)
        conf_tx = calc_conf(if_id, 'tx', rep_tx, orig_tx)

        # Clamp
>>>>>>> REPLACE
</DIFF>