<NAME>
adaptive_consensus_calibration
</NAME>

<DESCRIPTION>
Updates the consensus and calibration logic to incorporate "Variance-Adaptive Flow Sigma" and "Magnitude-Dependent Zero Priors".

1.  **Adaptive Flow Sigma**: In both the iterative solver (`solve_direction`) and the final calibration (`calc_conf`), the standard deviation used for the flow target is scaled by `1.0 + 3.0 * (1.0 - balance)`. This allows the algorithm to trust flow targets more loosely when the router is known to be imbalanced (noisy), preventing valid flow conservation signals from being rejected due to minor discrepancies.
2.  **Magnitude-Dependent Zero Prior**: Replaces the fixed 0.1 penalty for the zero-value hypothesis. The new penalty is `0.1 / (1.0 + max_mag / 10.0)`, which makes the zero hypothesis (phantom traffic) significantly less likely when other hypotheses suggest high-magnitude traffic (e.g., 100 Mbps), while remaining plausible for low-magnitude signals.
3.  **Cluster Mass Calibration**: Refines the confidence calculation to use a "Cluster Mass" approach (`winner_mass / total_mass`) rather than "Winner vs Runner-Up". This better accounts for the density of supporting hypotheses and aligns with the adaptive sigma logic.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
            def solve_direction(current, s_val, p_val, is_rx):
                f_val = None
                f_weight = 0.0

                if r_info:
                    if is_rx:
                        others = r_info['sin'] - current
                        f_val = max(0.0, r_info['sout'] - others)
                    else:
                        others = r_info['sout'] - current
                        f_val = max(0.0, r_info['sin'] - others)

                    # Weight: High if router is an Anchor (Neighbors agree) AND Balanced
                    # Max weight 2.5 allows overriding local signals if context is perfect
                    f_weight = 2.5 * r_info['anchor'] * (0.2 + 0.8 * r_info['balance'])

                # Hypothesis Generation
                candidates = [s_val, 0.0]
                if d['has_peer']: candidates.append(p_val)
                if f_val is not None: candidates.append(f_val)

                # Conditional Means for noise reduction (Cluster merging)
                if d['has_peer']:
                    diff = abs(s_val - p_val)
                    if diff > ABS_TOL and (diff / max(s_val, p_val, 1.0)) < 0.15:
                        candidates.append((s_val + p_val) / 2.0)

                if f_val is not None and d['has_peer']:
                    diff_p_f = abs(p_val - f_val)
                    if diff_p_f > ABS_TOL and (diff_p_f / max(p_val, f_val, 1.0)) < 0.15:
                        candidates.append((p_val + f_val) / 2.0)

                best_val = current
                best_score = -1.0

                unique_candidates = sorted(list(set(candidates)))

                for cand in unique_candidates:
                    sigma = get_sigma(cand)

                    # Likelihoods
                    # Peer (1.0) trusted more than Self (0.8)
                    l_self = 0.8 * gaussian_score(cand, s_val, sigma)
                    l_peer = 1.0 * gaussian_score(cand, p_val, sigma) if d['has_peer'] else 0.0
                    l_flow = f_weight * gaussian_score(cand, f_val, sigma) if f_val is not None else 0.0

                    score = l_self + l_peer + l_flow

                    # Status-Aware Prior: Penalize Zero if UP
                    # (Unless supported by strong evidence, score will drop significantly)
                    if is_zero(cand):
                        score *= 0.1

                    if score > best_score:
                        best_score = score
                        best_val = cand

                return best_val
=======
            def solve_direction(current, s_val, p_val, is_rx):
                f_val = None
                f_weight = 0.0
                f_sigma_scale = 1.0

                if r_info:
                    if is_rx:
                        others = r_info['sin'] - current
                        f_val = max(0.0, r_info['sout'] - others)
                    else:
                        others = r_info['sout'] - current
                        f_val = max(0.0, r_info['sin'] - others)

                    f_weight = 2.5 * r_info['anchor'] * (0.2 + 0.8 * r_info['balance'])
                    # Adaptive Variance: allow wider flow match if router is imbalanced
                    f_sigma_scale = 1.0 + 3.0 * (1.0 - r_info['balance'])

                # Hypothesis Generation
                candidates = [s_val, 0.0]
                if d['has_peer']: candidates.append(p_val)
                if f_val is not None: candidates.append(f_val)

                if d['has_peer']:
                    diff = abs(s_val - p_val)
                    if diff > ABS_TOL and (diff / max(s_val, p_val, 1.0)) < 0.15:
                        candidates.append((s_val + p_val) / 2.0)

                if f_val is not None and d['has_peer']:
                    diff_p_f = abs(p_val - f_val)
                    if diff_p_f > ABS_TOL and (diff_p_f / max(p_val, f_val, 1.0)) < 0.15:
                        candidates.append((p_val + f_val) / 2.0)

                best_val = current
                best_score = -1.0

                unique_candidates = sorted(list(set(candidates)))
                max_mag = max(unique_candidates) if unique_candidates else 0.0

                for cand in unique_candidates:
                    sigma = get_sigma(cand)

                    l_self = 0.8 * gaussian_score(cand, s_val, sigma)
                    l_peer = 1.0 * gaussian_score(cand, p_val, sigma) if d['has_peer'] else 0.0
                    l_flow = 0.0
                    if f_val is not None:
                        l_flow = f_weight * gaussian_score(cand, f_val, sigma * f_sigma_scale)

                    score = l_self + l_peer + l_flow

                    # Magnitude-Dependent Zero Penalty
                    if is_zero(cand):
                        penalty = 0.1 / (1.0 + max_mag / 10.0)
                        score *= penalty

                    if score > best_score:
                        best_score = score
                        best_val = cand

                return best_val
>>>>>>> REPLACE
<<<<<<< SEARCH
            def calc_conf(val, s_val, p_val, is_rx):
                f_val = None
                f_w = 0.0
                if r_info:
                    if is_rx: target = r_info['sout'] - (r_info['sin'] - d['est_rx'])
                    else:     target = r_info['sin'] - (r_info['sout'] - d['est_tx'])
                    f_val = max(0.0, target)
                    f_w = 2.5 * r_info['anchor'] * (0.2 + 0.8 * r_info['bal'])

                # Hypothesis Set for Probability Mass
                hyps = {val, s_val, 0.0}
                if d['has_peer']: hyps.add(p_val)
                if f_val is not None: hyps.add(f_val)
                if d['has_peer']: hyps.add((s_val + p_val)/2.0)

                sigma_win = get_sigma(val)

                def get_score(h, sigma):
                    sc_s = 0.8 * gaussian_score(h, s_val, sigma)
                    sc_p = 1.0 * gaussian_score(h, p_val, sigma) if d['has_peer'] else 0.0
                    sc_f = f_w * gaussian_score(h, f_val, sigma) if f_val is not None else 0.0
                    tot = sc_s + sc_p + sc_f
                    if is_zero(h): tot *= 0.1
                    return tot

                winner_score = get_score(val, sigma_win)

                # Calculate "Runner Up" Score (Best alternative hypothesis)
                runner_up_score = 0.0
                sorted_hyps = sorted(list(hyps))
                for h in sorted_hyps:
                    # Ignore hypotheses clustered with the winner
                    if abs(h - val) <= sigma_win: continue

                    sigma_h = get_sigma(h)
                    s = get_score(h, sigma_h)
                    if s > runner_up_score:
                        runner_up_score = s

                # Probability Dominance: Winner / (Winner + RunnerUp)
                total_mass = winner_score + runner_up_score + 1e-9
                prob = winner_score / total_mass

                # Absolute Fit Check
                # If even the winner is a bad fit (low absolute score), reduce confidence.
                max_possible = 0.8 + (1.0 if d['has_peer'] else 0.0) + (f_w if f_val is not None else 0.0)
                fit_ratio = winner_score / max(1.0, max_possible)

                final_conf = prob
                if fit_ratio < 0.5:
                    final_conf *= fit_ratio * 2.0

                # Support Verification
                peer_agrees = d['has_peer'] and abs(val - p_val) <= sigma_win
                flow_agrees = f_val is not None and abs(val - f_val) <= sigma_win

                # Cap confidence if unsupported by external validators (Peer/Flow)
                if not peer_agrees and not flow_agrees:
                    final_conf = min(final_conf, 0.75)

                return max(0.5, min(1.0, final_conf))
=======
            def calc_conf(val, s_val, p_val, is_rx):
                f_val = None
                f_w = 0.0
                f_sigma_scale = 1.0

                if r_info:
                    if is_rx: target = r_info['sout'] - (r_info['sin'] - d['est_rx'])
                    else:     target = r_info['sin'] - (r_info['sout'] - d['est_tx'])
                    f_val = max(0.0, target)
                    f_w = 2.5 * r_info['anchor'] * (0.2 + 0.8 * r_info['bal'])
                    f_sigma_scale = 1.0 + 3.0 * (1.0 - r_info['bal'])

                hyps = {val, s_val, 0.0}
                if d['has_peer']: hyps.add(p_val)
                if f_val is not None: hyps.add(f_val)
                if d['has_peer']: hyps.add((s_val + p_val)/2.0)

                max_mag = max(hyps)
                sigma_win = get_sigma(val)

                total_mass = 0.0
                winner_mass = 0.0

                for h in hyps:
                    sigma = get_sigma(h)
                    sc_s = 0.8 * gaussian_score(h, s_val, sigma)
                    sc_p = 1.0 * gaussian_score(h, p_val, sigma) if d['has_peer'] else 0.0
                    sc_f = 0.0
                    if f_val is not None:
                         sc_f = f_w * gaussian_score(h, f_val, sigma * f_sigma_scale)

                    score = sc_s + sc_p + sc_f

                    if is_zero(h):
                        penalty = 0.1 / (1.0 + max_mag / 10.0)
                        score *= penalty

                    total_mass += score
                    if abs(h - val) <= sigma_win:
                        winner_mass += score

                if total_mass < 1e-9: return 0.5

                prob = winner_mass / total_mass

                # Absolute Fit Check
                w_sigma = get_sigma(val)
                ws_s = 0.8 * gaussian_score(val, s_val, w_sigma)
                ws_p = 1.0 * gaussian_score(val, p_val, w_sigma) if d['has_peer'] else 0.0
                ws_f = 0.0
                if f_val is not None:
                    ws_f = f_w * gaussian_score(val, f_val, w_sigma * f_sigma_scale)

                winner_raw_score = ws_s + ws_p + ws_f
                max_possible = 0.8 + (1.0 if d['has_peer'] else 0.0) + (f_w if f_val is not None else 0.0)

                fit_ratio = winner_raw_score / max(1.0, max_possible)

                final_conf = prob
                if fit_ratio < 0.5:
                    final_conf *= fit_ratio * 2.0

                # Support Verification
                peer_agrees = d['has_peer'] and abs(val - p_val) <= sigma_win
                flow_agrees = f_val is not None and abs(val - f_val) <= (sigma_win * f_sigma_scale)

                if not peer_agrees and not flow_agrees:
                    final_conf = min(final_conf, 0.75)

                return max(0.5, min(1.0, final_conf))
>>>>>>> REPLACE
</DIFF>