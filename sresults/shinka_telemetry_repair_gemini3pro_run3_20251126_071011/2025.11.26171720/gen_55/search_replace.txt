<NAME>
relaxed_constraints_and_boosted_flow
</NAME>

<DESCRIPTION>
Refines the hypothesis selection algorithm to improve confidence calibration and repair accuracy.
1.  **Widened Sigma**: Increases the Gaussian kernel width (3x tolerance) in `solve_hypothesis`. This prevents confidence from dropping too sharply for minor deviations, addressing underconfidence.
2.  **Adaptive Flow Weights**: Significantly boosts flow conservation weight (3x) for interfaces without peers (external links), allowing the algorithm to correct isolated sensors using router-wide consensus.
3.  **Relaxed Reliability**: Increases the router imbalance tolerance knee from 5% to 8%, allowing flow conservation logic to persist through noisier intermediate states during iteration.
4.  **Increased Iterations**: Bumps refinement iterations from 3 to 4 to ensure convergence with the softer constraints.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # --- Phase 2: Iterative Refinement ---
    ITERATIONS = 3

    for _ in range(ITERATIONS):
        # 1. Calculate Router Context
        router_stats = {}
        for r_id, if_list in topology.items():
            valid_ifs = [i for i in if_list if i in working_state]
            t_in = sum(working_state[i]['est_rx'] for i in valid_ifs)
            t_out = sum(working_state[i]['est_tx'] for i in valid_ifs)

            # Reliability score using sigmoid-like decay
            imb = abs(t_in - t_out)
            mx = max(t_in, t_out, 1.0)
            # Knee at 5% imbalance
            reliability = 1.0 / (1.0 + (imb / (mx * 0.05))**2)

            router_stats[r_id] = {'in': t_in, 'out': t_out, 'rel': reliability}
=======
    # --- Phase 2: Iterative Refinement ---
    ITERATIONS = 4

    for _ in range(ITERATIONS):
        # 1. Calculate Router Context
        router_stats = {}
        for r_id, if_list in topology.items():
            valid_ifs = [i for i in if_list if i in working_state]
            t_in = sum(working_state[i]['est_rx'] for i in valid_ifs)
            t_out = sum(working_state[i]['est_tx'] for i in valid_ifs)

            # Reliability score using sigmoid-like decay
            imb = abs(t_in - t_out)
            mx = max(t_in, t_out, 1.0)
            # Knee at 8% imbalance (relaxed to allow flow logic to work through noise)
            reliability = 1.0 / (1.0 + (imb / (mx * 0.08))**2)

            router_stats[r_id] = {'in': t_in, 'out': t_out, 'rel': reliability}
>>>>>>> REPLACE
<<<<<<< SEARCH
    # --- Phase 3: Final Output Generation ---
    # Re-calc router stats for final flow context
    final_router_stats = {}
    for r_id, if_list in topology.items():
        valid_ifs = [i for i in if_list if i in working_state]
        t_in = sum(working_state[i]['est_rx'] for i in valid_ifs)
        t_out = sum(working_state[i]['est_tx'] for i in valid_ifs)
        imb = abs(t_in - t_out)
        mx = max(t_in, t_out, 1.0)
        final_router_stats[r_id] = {'in': t_in, 'out': t_out, 'rel': 1.0 / (1.0 + (imb/(mx*0.05))**2)}
=======
    # --- Phase 3: Final Output Generation ---
    # Re-calc router stats for final flow context
    final_router_stats = {}
    for r_id, if_list in topology.items():
        valid_ifs = [i for i in if_list if i in working_state]
        t_in = sum(working_state[i]['est_rx'] for i in valid_ifs)
        t_out = sum(working_state[i]['est_tx'] for i in valid_ifs)
        imb = abs(t_in - t_out)
        mx = max(t_in, t_out, 1.0)
        final_router_stats[r_id] = {'in': t_in, 'out': t_out, 'rel': 1.0 / (1.0 + (imb/(mx*0.08))**2)}
>>>>>>> REPLACE
<<<<<<< SEARCH
def solve_hypothesis(v_self, v_peer, v_flow, has_peer, flow_qual, rel_tol, abs_tol):
    """
    Selects the best estimate by scoring candidates against available signals.
    """
    # 1. Define Candidates
    candidates = {v_self, 0.0}
    if has_peer:
        candidates.add(v_peer)
        candidates.add((v_self + v_peer) / 2.0)
    if flow_qual > 0.1:
        candidates.add(v_flow)
        if has_peer:
            candidates.add((v_peer + v_flow) / 2.0)
        candidates.add((v_self + v_flow) / 2.0)

    # 2. Weights
    W_SELF = 1.0
    W_PEER = 1.2 # Symmetry is strong
    W_FLOW = 1.5 * flow_qual # Flow can override self if high quality

    # 3. Score Candidates
    best_score = -1.0
    best_val = v_self

    for cand in candidates:
        score = 0.0

        # Self Score
        sigma_s = max(abs(cand) * rel_tol, abs_tol)
        z_s = abs(cand - v_self) / sigma_s
        score += W_SELF * math.exp(-0.5 * min(z_s*z_s, 20.0))

        # Peer Score
        if has_peer:
            sigma_p = max(abs(cand) * rel_tol, abs_tol)
            z_p = abs(cand - v_peer) / sigma_p
            score += W_PEER * math.exp(-0.5 * min(z_p*z_p, 20.0))

        # Flow Score
        if flow_qual > 0.0:
            sigma_f = max(abs(cand) * rel_tol, abs_tol)
            z_f = abs(cand - v_flow) / sigma_f
            score += W_FLOW * math.exp(-0.5 * min(z_f*z_f, 20.0))

        if score > best_score:
            best_score = score
            best_val = cand

    # 4. Confidence
    max_weight = W_SELF + (W_PEER if has_peer else 0) + (W_FLOW if flow_qual > 0 else 0)
    confidence = best_score / max(max_weight, 1.0)

    return best_val, min(1.0, max(0.0, confidence))
=======
def solve_hypothesis(v_self, v_peer, v_flow, has_peer, flow_qual, rel_tol, abs_tol):
    """
    Selects the best estimate by scoring candidates against available signals.
    """
    # 1. Define Candidates
    candidates = {v_self, 0.0}
    if has_peer:
        candidates.add(v_peer)
        candidates.add((v_self + v_peer) / 2.0)

    # Allow flow if it has even minimal quality
    if flow_qual > 0.05:
        candidates.add(v_flow)
        if has_peer:
            candidates.add((v_peer + v_flow) / 2.0)
        candidates.add((v_self + v_flow) / 2.0)

    # 2. Weights
    # Widen sigma: 3x tolerance window to improve confidence calibration and recall
    # This reflects that "within 3 sigma" (approx 6%) is still strongly supportive
    SIGMA_SCALE = 3.0

    W_SELF = 1.0
    W_PEER = 1.5 # Boosted symmetry importance

    # Adaptive Flow Weight
    # If no peer, flow is the only external validator. Trust it more if quality is decent.
    if not has_peer:
        W_FLOW = 3.0 * flow_qual
    else:
        W_FLOW = 1.5 * flow_qual

    # 3. Score Candidates
    best_score = -1.0
    best_val = v_self

    for cand in candidates:
        score = 0.0

        # Helper for Gaussian Kernel
        def kernel(val, ref, w):
            sigma = SIGMA_SCALE * max(abs(val) * rel_tol, abs_tol)
            z = abs(val - ref) / sigma
            return w * math.exp(-0.5 * min(z*z, 20.0))

        # Self Score
        score += kernel(cand, v_self, W_SELF)

        # Peer Score
        if has_peer:
            score += kernel(cand, v_peer, W_PEER)

        # Flow Score
        if flow_qual > 0.05:
            score += kernel(cand, v_flow, W_FLOW)

        if score > best_score:
            best_score = score
            best_val = cand

    # 4. Confidence
    # Normalize by potential max score to get [0,1] confidence
    max_weight = W_SELF + (W_PEER if has_peer else 0) + (W_FLOW if flow_qual > 0.05 else 0)
    confidence = best_score / max(max_weight, 1.0)

    return best_val, min(1.0, max(0.0, confidence))
>>>>>>> REPLACE
</DIFF>