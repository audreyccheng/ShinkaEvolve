<NAME>
robust_init_and_plateau_traversal
</NAME>

<DESCRIPTION>
1. Enhance `_check_feasibility_robust` by adding randomized shuffling attempts. If deterministic sorting heuristics fail to find a feasible packing for a given K, the algorithm retries with randomly shuffled items. This increases the probability of finding a tighter lower bound (lower K) during the binary search initialization phase, providing a better starting point for local search.
2. Improve `_steepest_descent_ils` by relaxing the pruning condition for moves and swaps. The algorithm now considers moves where the new local peak pressure is equal to the current bottleneck (within tolerance), provided that the variance (sum of squared pressures) is reduced. This allows the local search to traverse plateaus in the fitness landscape and better balance the load on non-bottleneck GPUs.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    for key_func, rev in heuristics:
        pack_items.sort(key=key_func, reverse=rev)
        if res := _pack_bfd(gpu_num, pack_items, virtual_cap):
            return True, res

    return False, None
=======
    for key_func, rev in heuristics:
        pack_items.sort(key=key_func, reverse=rev)
        if res := _pack_bfd(gpu_num, pack_items, virtual_cap):
            return True, res

    # Random shuffle retries for robustness
    # This helps when structured sorts get stuck in local optima
    rng_state = random.getstate()
    for _ in range(5):
        random.shuffle(pack_items)
        if res := _pack_bfd(gpu_num, pack_items, virtual_cap):
            random.setstate(rng_state)
            return True, res
    random.setstate(rng_state)

    return False, None
>>>>>>> REPLACE
<<<<<<< SEARCH
                nk_src = (gpu_w[src] - w) / (GPU_MEM_SIZE - (gpu_s[src] - s) + 1e-9)
                nk_dst = (gpu_w[dst] + w) / (GPU_MEM_SIZE - (gpu_s[dst] + s) + 1e-9)

                local_max = max(nk_src, nk_dst)
                if local_max >= max_k: continue

                delta_sq = (nk_src**2 + nk_dst**2) - (current_ks[src]**2 + current_ks[dst]**2)
=======
                nk_src = (gpu_w[src] - w) / (GPU_MEM_SIZE - (gpu_s[src] - s) + 1e-9)
                nk_dst = (gpu_w[dst] + w) / (GPU_MEM_SIZE - (gpu_s[dst] + s) + 1e-9)

                local_max = max(nk_src, nk_dst)
                # Allow equal peak moves if they improve variance (tie-breaking)
                if local_max > max_k + 1e-7: continue

                delta_sq = (nk_src**2 + nk_dst**2) - (current_ks[src]**2 + current_ks[dst]**2)
>>>>>>> REPLACE
<<<<<<< SEARCH
                    nk_src = (gpu_w[src] - w1 + w2) / (GPU_MEM_SIZE - ns_src + 1e-9)
                    nk_dst = (gpu_w[dst] - w2 + w1) / (GPU_MEM_SIZE - ns_dst + 1e-9)

                    local_max = max(nk_src, nk_dst)
                    if local_max >= max_k: continue

                    delta_sq = (nk_src**2 + nk_dst**2) - (current_ks[src]**2 + current_ks[dst]**2)
=======
                    nk_src = (gpu_w[src] - w1 + w2) / (GPU_MEM_SIZE - ns_src + 1e-9)
                    nk_dst = (gpu_w[dst] - w2 + w1) / (GPU_MEM_SIZE - ns_dst + 1e-9)

                    local_max = max(nk_src, nk_dst)
                    # Allow equal peak moves if they improve variance (tie-breaking)
                    if local_max > max_k + 1e-7: continue

                    delta_sq = (nk_src**2 + nk_dst**2) - (current_ks[src]**2 + current_ks[dst]**2)
>>>>>>> REPLACE
</DIFF>