<NAME>
targeted_redistribution_kick
</NAME>

<DESCRIPTION>
Improves the perturbation phase of the Iterated Local Search (ILS) by introducing a "Targeted Redistribution" strategy.
Instead of relying solely on random moves when stuck in a local optimum, the algorithm now attempts to pair the bottleneck GPU (highest KVPR) with the least loaded GPU and redistributes their combined models using a greedy heuristic.
This "Pairwise Balancing" directly addresses load imbalance, a common cause of suboptimal peaks, by effectively performing multiple coordinated swaps.
A random burst kick serves as a fallback if redistribution fails or is impossible.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        # 2. Burst Kick
        if no_improve > patience:
            # Execute Burst Kick: 3-5 random valid moves
            kick_len = random.randint(3, 5)
            for _ in range(kick_len):
                # Try to find a valid move
                for _ in range(10):
                    s = random.randint(0, gpu_num - 1)
                    if not placement[s]: continue
                    d = random.randint(0, gpu_num - 1)
                    if s == d: continue

                    m_idx = random.randint(0, len(placement[s]) - 1)
                    m = placement[s][m_idx]

                    if gpu_s[d] + m.model_size <= GPU_MEM_SIZE:
                        placement[d].append(m)
                        placement[s].pop(m_idx)
                        gpu_s[s] -= m.model_size; gpu_w[s] -= m.req_rate/m.slo
                        gpu_s[d] += m.model_size; gpu_w[d] += m.req_rate/m.slo
                        current_ks[s] = get_k(s)
                        current_ks[d] = get_k(d)
                        break
            no_improve = 0
            continue
=======
        # 2. Perturbation (Kick)
        if no_improve > patience:
            # Strategy A: Balance Pair (Targeted Redistribution)
            # Try to rebalance the bottleneck with the least loaded GPU
            # This allows escaping local optima where single moves fail

            # Find current bottleneck and least loaded
            sorted_gpus = sorted(range(gpu_num), key=lambda i: current_ks[i])
            src = sorted_gpus[-1] # Max K
            dst = sorted_gpus[0]  # Min K

            success = False
            if src != dst and placement[src]:
                # Attempt Redistribution
                pool = placement[src] + placement[dst]
                # Sort by weight desc (biggest pressure contributors first)
                pool.sort(key=lambda m: m.req_rate/m.slo, reverse=True)

                # Backup
                backup_src = (list(placement[src]), gpu_s[src], gpu_w[src])
                backup_dst = (list(placement[dst]), gpu_s[dst], gpu_w[dst])

                # Clear
                placement[src] = []
                placement[dst] = []
                gpu_s[src] = 0.0; gpu_w[src] = 0.0
                gpu_s[dst] = 0.0; gpu_w[dst] = 0.0

                valid_redist = True
                for m in pool:
                    # Try to place in src or dst to minimize peak K
                    s, w = m.model_size, m.req_rate/m.slo

                    can_src = (gpu_s[src] + s <= GPU_MEM_SIZE)
                    can_dst = (gpu_s[dst] + s <= GPU_MEM_SIZE)

                    if not can_src and not can_dst:
                        valid_redist = False; break

                    choice = None
                    if can_src and not can_dst:
                        choice = 'src'
                    elif can_dst and not can_src:
                        choice = 'dst'
                    else:
                        # Both valid, choose best fit
                        rem_src = GPU_MEM_SIZE - (gpu_s[src] + s)
                        k_src = (gpu_w[src] + w) / rem_src if rem_src > 1e-7 else 1e9

                        rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + s)
                        k_dst = (gpu_w[dst] + w) / rem_dst if rem_dst > 1e-7 else 1e9

                        if k_src < k_dst:
                            choice = 'src'
                        else:
                            choice = 'dst'

                    if choice == 'src':
                        placement[src].append(m)
                        gpu_s[src] += s; gpu_w[src] += w
                    else:
                        placement[dst].append(m)
                        gpu_s[dst] += s; gpu_w[dst] += w

                # Update Ks
                current_ks[src] = get_k(src)
                current_ks[dst] = get_k(dst)

                if not valid_redist:
                    # Revert
                    placement[src], gpu_s[src], gpu_w[src] = backup_src
                    placement[dst], gpu_s[dst], gpu_w[dst] = backup_dst
                    current_ks[src] = get_k(src)
                    current_ks[dst] = get_k(dst)
                else:
                    success = True
                    no_improve = 0

            # Strategy B: Random Burst Kick (Fallback)
            if not success:
                kick_len = random.randint(3, 5)
                for _ in range(kick_len):
                    for _ in range(10):
                        s = random.randint(0, gpu_num - 1)
                        if not placement[s]: continue
                        d = random.randint(0, gpu_num - 1)
                        if s == d: continue

                        m_idx = random.randint(0, len(placement[s]) - 1)
                        m = placement[s][m_idx]

                        if gpu_s[d] + m.model_size <= GPU_MEM_SIZE:
                            placement[d].append(m)
                            placement[s].pop(m_idx)
                            gpu_s[s] -= m.model_size; gpu_w[s] -= m.req_rate/m.slo
                            gpu_s[d] += m.model_size; gpu_w[d] += m.req_rate/m.slo
                            current_ks[s] = get_k(s)
                            current_ks[d] = get_k(d)
                            break
                no_improve = 0
            continue
>>>>>>> REPLACE
</DIFF>