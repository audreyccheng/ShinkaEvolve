system
The user wants to improve a model placement algorithm to minimize the maximum KV cache pressure (KVPR) across GPUs.
I proposed an improvement that:
1.  Adds a 'Density' strategy to the Binary Search phase.
2.  Replaces the existing Hill Climbing (Move/Swap) local search with a "Pairwise Rebalancing" approach.

The SEARCH block contains the strategies definition in the binary search loop and the entire local search block.
The REPLACE block contains the updated strategies list and the new Pairwise Rebalancing implementation.

The SEARCH block must match the original code exactly.

Original code segment to replace:
```python
        # Strategies: Virtual Size, Physical Size, Load
        strategies = [
            lambda x: x['w'] + mid * x['s'],
            lambda x: x['s'],
            lambda x: x['w']
        ]

        for key_func in strategies:
            # Best Fit Decreasing
            items_sorted = sorted(m_data, key=key_func, reverse=True)
            gpu_models = [[] for _ in range(gpu_num)]
            gpu_l = [0.0] * gpu_num
            gpu_u = [0.0] * gpu_num
            ok = True

            for item in items_sorted:
                best_g = -1
                min_residual = float('inf')

                # Check all bins
                for g in range(gpu_num):
                    # Hard mem check
                    if gpu_u[g] + item['s'] >= GPU_MEM_SIZE - 1e-6: continue

                    # Constraint check: (L + w) <= K * (C - (U + s))
                    # Transformed: L + w + K(U + s) <= KC
                    lhs = (gpu_l[g] + item['w']) + mid * (gpu_u[g] + item['s'])
                    rhs = mid * GPU_MEM_SIZE

                    if lhs <= rhs + 1e-7:
                        # Best Fit: minimize residual capacity of transformed bin
                        res = rhs - lhs
                        if res < min_residual:
                            min_residual = res
                            best_g = g

                if best_g != -1:
                    gpu_models[best_g].append(item['obj'])
                    gpu_l[best_g] += item['w']
                    gpu_u[best_g] += item['s']
                else:
                    ok = False
                    break

            if ok:
                feasible = True
                res_placement = {i: gpu_models[i] for i in range(gpu_num)}
                break

        if feasible:
            final_placement = res_placement
            high = mid
        else:
            low = mid

    if final_placement is None:
        raise ValueError("Could not find feasible placement")

    # --- 3. Local Search Refinement ---
    current_placement = final_placement

    # Reconstruct load/used stats
    loads = [0.0] * gpu_num
    used = [0.0] * gpu_num
    for g in range(gpu_num):
        for m in current_placement[g]:
            loads[g] += m.req_rate / m.slo
            used[g] += m.model_size

    # Hill Climbing: Minimize Max Pressure
    iter_limit = 100
    for _ in range(iter_limit):
        max_p = -1.0
        bottleneck = -1

        # Identify bottleneck
        for g in range(gpu_num):
            rem = GPU_MEM_SIZE - used[g]
            p = loads[g] / rem if rem > 1e-6 else float('inf')
            if p > max_p:
                max_p = p
                bottleneck = g

        if bottleneck == -1: break

        improved = False
        bn_items = list(current_placement[bottleneck])

        # Try Moving an item out of bottleneck
        for m in bn_items:
            w = m.req_rate / m.slo
            s = m.model_size

            src_l = loads[bottleneck] - w
            src_u = used[bottleneck] - s

            for dst in range(gpu_num):
                if dst == bottleneck: continue
                if used[dst] + s >= GPU_MEM_SIZE - 1e-6: continue

                dst_l = loads[dst] + w
                dst_u = used[dst] + s
                dst_p = dst_l / (GPU_MEM_SIZE - dst_u) if (GPU_MEM_SIZE - dst_u) > 1e-6 else float('inf')

                # Accept if target GPU stays below current max pressure
                if dst_p < max_p - 1e-6:
                    current_placement[bottleneck].remove(m)
                    current_placement[dst].append(m)

                    loads[bottleneck] = src_l
                    used[bottleneck] = src_u
                    loads[dst] = dst_l
                    used[dst] = dst_u

                    improved = True
                    break
            if improved: break

        if improved: continue

        # Try Swapping an item from bottleneck with another
        for dst in range(gpu_num):
            if dst == bottleneck: continue

            dst_items = list(current_placement[dst])
            for m_src in bn_items:
                ws = m_src.req_rate / m_src.slo
                ss = m_src.model_size

                for m_dst in dst_items:
                    wd = m_dst.req_rate / m_dst.slo
                    sd = m_dst.model_size

                    # New bottleneck state
                    n_src_u = used[bottleneck] - ss + sd
                    if n_src_u >= GPU_MEM_SIZE - 1e-6: continue
                    n_src_l = loads[bottleneck] - ws + wd
                    n_src_p = n_src_l / (GPU_MEM_SIZE - n_src_u)

                    # New dest state
                    n_dst_u = used[dst] - sd + ss
                    if n_dst_u >= GPU_MEM_SIZE - 1e-6: continue
                    n_dst_l = loads[dst] - wd + ws
                    n_dst_p = n_dst_l / (GPU_MEM_SIZE - n_dst_u)

                    # Accept if both result in lower pressure than current max
                    if n_src_p < max_p - 1e-6 and n_dst_p < max_p - 1e-6:
                        current_placement[bottleneck].remove(m_src)
                        current_placement[bottleneck].append(m_dst)
                        current_placement[dst].remove(m_dst)
                        current_placement[dst].append(m_src)

                        loads[bottleneck] = n_src_l
                        used[bottleneck] = n_src_u
                        loads[dst] = n_dst_l
                        used[dst] = n_dst_u

                        improved = True
                        break
                if improved: break
            if improved: break

        if not improved: break
```

Looks correct.

<NAME>
pairwise_rebalance_search
</NAME>

<DESCRIPTION>
1.  Add 'Density' (req_rate/slo / size) to the set of packing strategies in the Binary Search phase. This heuristic effectively balances high-load/small-size items with low-load/large-size items.
2.  Replace the single-item Move/Swap Hill Climbing with a "Pairwise Rebalancing" Local Search. This new refinement iteratively selects the bottleneck GPU and a partner GPU (prioritizing low-load ones), pools their items, and optimally repacks them into the two GPUs using greedy heuristics. This generalizes moves and swaps, allowing for more complex rearrangements (e.g., moving 2 small items to make room for 1 large item) to escape local optima.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        # Strategies: Virtual Size, Physical Size, Load
        strategies = [
            lambda x: x['w'] + mid * x['s'],
            lambda x: x['s'],
            lambda x: x['w']
        ]

        for key_func in strategies:
            # Best Fit Decreasing
            items_sorted = sorted(m_data, key=key_func, reverse=True)
            gpu_models = [[] for _ in range(gpu_num)]
            gpu_l = [0.0] * gpu_num
            gpu_u = [0.0] * gpu_num
            ok = True

            for item in items_sorted:
                best_g = -1
                min_residual = float('inf')

                # Check all bins
                for g in range(gpu_num):
                    # Hard mem check
                    if gpu_u[g] + item['s'] >= GPU_MEM_SIZE - 1e-6: continue

                    # Constraint check: (L + w) <= K * (C - (U + s))
                    # Transformed: L + w + K(U + s) <= KC
                    lhs = (gpu_l[g] + item['w']) + mid * (gpu_u[g] + item['s'])
                    rhs = mid * GPU_MEM_SIZE

                    if lhs <= rhs + 1e-7:
                        # Best Fit: minimize residual capacity of transformed bin
                        res = rhs - lhs
                        if res < min_residual:
                            min_residual = res
                            best_g = g

                if best_g != -1:
                    gpu_models[best_g].append(item['obj'])
                    gpu_l[best_g] += item['w']
                    gpu_u[best_g] += item['s']
                else:
                    ok = False
                    break

            if ok:
                feasible = True
                res_placement = {i: gpu_models[i] for i in range(gpu_num)}
                break

        if feasible:
            final_placement = res_placement
            high = mid
        else:
            low = mid

    if final_placement is None:
        raise ValueError("Could not find feasible placement")

    # --- 3. Local Search Refinement ---
    current_placement = final_placement

    # Reconstruct load/used stats
    loads = [0.0] * gpu_num
    used = [0.0] * gpu_num
    for g in range(gpu_num):
        for m in current_placement[g]:
            loads[g] += m.req_rate / m.slo
            used[g] += m.model_size

    # Hill Climbing: Minimize Max Pressure
    iter_limit = 100
    for _ in range(iter_limit):
        max_p = -1.0
        bottleneck = -1

        # Identify bottleneck
        for g in range(gpu_num):
            rem = GPU_MEM_SIZE - used[g]
            p = loads[g] / rem if rem > 1e-6 else float('inf')
            if p > max_p:
                max_p = p
                bottleneck = g

        if bottleneck == -1: break

        improved = False
        bn_items = list(current_placement[bottleneck])

        # Try Moving an item out of bottleneck
        for m in bn_items:
            w = m.req_rate / m.slo
            s = m.model_size

            src_l = loads[bottleneck] - w
            src_u = used[bottleneck] - s

            for dst in range(gpu_num):
                if dst == bottleneck: continue
                if used[dst] + s >= GPU_MEM_SIZE - 1e-6: continue

                dst_l = loads[dst] + w
                dst_u = used[dst] + s
                dst_p = dst_l / (GPU_MEM_SIZE - dst_u) if (GPU_MEM_SIZE - dst_u) > 1e-6 else float('inf')

                # Accept if target GPU stays below current max pressure
                if dst_p < max_p - 1e-6:
                    current_placement[bottleneck].remove(m)
                    current_placement[dst].append(m)

                    loads[bottleneck] = src_l
                    used[bottleneck] = src_u
                    loads[dst] = dst_l
                    used[dst] = dst_u

                    improved = True
                    break
            if improved: break

        if improved: continue

        # Try Swapping an item from bottleneck with another
        for dst in range(gpu_num):
            if dst == bottleneck: continue

            dst_items = list(current_placement[dst])
            for m_src in bn_items:
                ws = m_src.req_rate / m_src.slo
                ss = m_src.model_size

                for m_dst in dst_items:
                    wd = m_dst.req_rate / m_dst.slo
                    sd = m_dst.model_size

                    # New bottleneck state
                    n_src_u = used[bottleneck] - ss + sd
                    if n_src_u >= GPU_MEM_SIZE - 1e-6: continue
                    n_src_l = loads[bottleneck] - ws + wd
                    n_src_p = n_src_l / (GPU_MEM_SIZE - n_src_u)

                    # New dest state
                    n_dst_u = used[dst] - sd + ss
                    if n_dst_u >= GPU_MEM_SIZE - 1e-6: continue
                    n_dst_l = loads[dst] - wd + ws
                    n_dst_p = n_dst_l / (GPU_MEM_SIZE - n_dst_u)

                    # Accept if both result in lower pressure than current max
                    if n_src_p < max_p - 1e-6 and n_dst_p < max_p - 1e-6:
                        current_placement[bottleneck].remove(m_src)
                        current_placement[bottleneck].append(m_dst)
                        current_placement[dst].remove(m_dst)
                        current_placement[dst].append(m_src)

                        loads[bottleneck] = n_src_l
                        used[bottleneck] = n_src_u
                        loads[dst] = n_dst_l
                        used[dst] = n_dst_u

                        improved = True
                        break
                if improved: break
            if improved: break

        if not improved: break
=======
        # Strategies: Virtual Size, Physical Size, Load, Density
        strategies = [
            lambda x: x['w'] + mid * x['s'],
            lambda x: x['s'],
            lambda x: x['w'],
            lambda x: x['w'] / x['s'] if x['s'] > 1e-6 else 0
        ]

        for key_func in strategies:
            # Best Fit Decreasing
            items_sorted = sorted(m_data, key=key_func, reverse=True)
            gpu_models = [[] for _ in range(gpu_num)]
            gpu_l = [0.0] * gpu_num
            gpu_u = [0.0] * gpu_num
            ok = True

            for item in items_sorted:
                best_g = -1
                min_residual = float('inf')

                # Check all bins
                for g in range(gpu_num):
                    # Hard mem check
                    if gpu_u[g] + item['s'] >= GPU_MEM_SIZE - 1e-6: continue

                    # Constraint check: (L + w) <= K * (C - (U + s))
                    # Transformed: L + w + K(U + s) <= KC
                    lhs = (gpu_l[g] + item['w']) + mid * (gpu_u[g] + item['s'])
                    rhs = mid * GPU_MEM_SIZE

                    if lhs <= rhs + 1e-7:
                        # Best Fit: minimize residual capacity of transformed bin
                        res = rhs - lhs
                        if res < min_residual:
                            min_residual = res
                            best_g = g

                if best_g != -1:
                    gpu_models[best_g].append(item['obj'])
                    gpu_l[best_g] += item['w']
                    gpu_u[best_g] += item['s']
                else:
                    ok = False
                    break

            if ok:
                feasible = True
                res_placement = {i: gpu_models[i] for i in range(gpu_num)}
                break

        if feasible:
            final_placement = res_placement
            high = mid
        else:
            low = mid

    if final_placement is None:
        raise ValueError("Could not find feasible placement")

    # --- 3. Local Search Refinement (Pairwise Rebalancing) ---
    current_placement = final_placement

    # Track gpu state: loads (l) and used memory (u)
    gpu_state = []
    for g in range(gpu_num):
        l = sum(m.req_rate / m.slo for m in current_placement[g])
        u = sum(m.model_size for m in current_placement[g])
        gpu_state.append({'l': l, 'u': u, 'items': list(current_placement[g])})

    def calc_pressure(l, u):
        rem = GPU_MEM_SIZE - u
        if rem <= 1e-6: return float('inf') if l > 1e-6 else 0.0
        return l / rem

    iter_limit = 200 # Increased iteration limit for faster but more frequent updates
    for _ in range(iter_limit):
        # Identify bottleneck
        max_p = -1.0
        bottleneck = -1
        for g in range(gpu_num):
            p = calc_pressure(gpu_state[g]['l'], gpu_state[g]['u'])
            if p > max_p:
                max_p = p
                bottleneck = g

        if bottleneck == -1: break

        improved = False

        # Try to rebalance bottleneck with another GPU
        # Sort partners by pressure (try emptiest first)
        partners = list(range(gpu_num))
        partners.sort(key=lambda g: calc_pressure(gpu_state[g]['l'], gpu_state[g]['u']))

        for partner in partners:
            if partner == bottleneck: continue

            # Combine items
            items_pool = gpu_state[bottleneck]['items'] + gpu_state[partner]['items']

            # Heuristics to repack 2 bins: Load, Size, Density
            sort_keys = [
                lambda m: m.req_rate / m.slo,
                lambda m: m.model_size,
                lambda m: (m.req_rate / m.slo) / m.model_size if m.model_size > 0 else 0
            ]

            best_local_sol = None
            # We strictly want to reduce the max pressure of this pair below global max_p
            best_pair_max = max_p

            for key in sort_keys:
                sorted_items = sorted(items_pool, key=key, reverse=True)

                # Greedy Best-Fit on 2 bins
                sub_l = [0.0, 0.0]
                sub_u = [0.0, 0.0]
                sub_bins = [[], []]
                possible = True

                for item in sorted_items:
                    w = item.req_rate / item.slo
                    s = item.model_size

                    best_b = -1
                    min_res_p = float('inf')

                    for b in [0, 1]:
                        rem = GPU_MEM_SIZE - sub_u[b] - s
                        if rem > 1e-6:
                            p = (sub_l[b] + w) / rem
                            if p < min_res_p:
                                min_res_p = p
                                best_b = b

                    if best_b == -1:
                        possible = False
                        break

                    sub_l[best_b] += w
                    sub_u[best_b] += s
                    sub_bins[best_b].append(item)

                if possible:
                    p0 = calc_pressure(sub_l[0], sub_u[0])
                    p1 = calc_pressure(sub_l[1], sub_u[1])
                    pair_max = max(p0, p1)

                    if pair_max < best_pair_max - 1e-6:
                        best_pair_max = pair_max
                        best_local_sol = (sub_l, sub_u, sub_bins)

            if best_local_sol:
                # Apply update
                sl, su, sbins = best_local_sol

                gpu_state[bottleneck]['l'] = sl[0]
                gpu_state[bottleneck]['u'] = su[0]
                gpu_state[bottleneck]['items'] = sbins[0]

                gpu_state[partner]['l'] = sl[1]
                gpu_state[partner]['u'] = su[1]
                gpu_state[partner]['items'] = sbins[1]

                current_placement[bottleneck] = sbins[0]
                current_placement[partner] = sbins[1]

                improved = True
                break

        if not improved: break
>>>>>>> REPLACE
</DIFF>