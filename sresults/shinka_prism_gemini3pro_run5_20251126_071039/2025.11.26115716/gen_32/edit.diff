--- a/original.py
+++ b/original.py
@@ -1,386 +1,347 @@
 # EVOLVE-BLOCK-START
-"""Model placement algorithm for minimizing maximum KV cache pressure using Multi-Strategy Packing and Iterated Local Search"""
+"""Model placement algorithm for minimizing maximum KV cache pressure using Robust BFD Binary Search and Best-Improvement ILS"""
 
 import copy
 import random
-
-GPU_MEM_SIZE = 80  # GB
+import math
+
+GPU_MEM_SIZE = 80.0  # GB
 
 def compute_model_placement(gpu_num, models):
     """
-    Minimizes max KVPR using Binary Search with 4 packing heuristics
-    followed by Iterated Local Search for refinement.
-    """
+    Minimizes max KVPR using Robust Binary Search with Best-Fit Decreasing (BFD) 
+    and Best-Improvement Iterated Local Search with Variance Tie-Breaking.
+    """
+    # 1. Pre-process and Validation
     total_size = sum(m.model_size for m in models)
     if total_size > gpu_num * GPU_MEM_SIZE:
         raise ValueError("Total model size exceeds total GPU memory capacity.")
 
     # Prepare items: (w, s, m)
-    # w = req_rate / slo
     items = [{'w': m.req_rate / m.slo, 's': m.model_size, 'm': m} for m in models]
 
-    # Binary Search for optimal KVPR
+    # 2. Binary Search for Initial Solution
     total_w = sum(x['w'] for x in items)
     slack = gpu_num * GPU_MEM_SIZE - total_size
-
-    # Robust High Bound Initialization
+    
+    # Heuristic bound initialization
     low = 0.0
-    if slack > 1e-4:
-        # Initial guess: Average Load / Average Slack per GPU
-        # Multiplier to account for fragmentation
-        high = (total_w / slack) * 4.0
+    if slack < 1e-6:
+        high = 10000.0
     else:
-        high = 1000.0
-    high = max(high, 10.0)
-
-    best_packing_placement = None
+        avg_pressure = total_w / slack
+        high = max(10.0, avg_pressure * 8.0)
+        
+    best_placement = None
     feasible_high = False
-
-    # 1. Find valid upper bound
+    
+    # Search for valid upper bound
     for _ in range(20):
-        feasible, placement = _check_feasibility_multi(gpu_num, items, high)
+        feasible, placement = _check_feasibility_bfd(gpu_num, items, high)
         if feasible:
-            best_packing_placement = placement
+            best_placement = placement
             feasible_high = True
             break
         low = high
         high *= 2.0
-
+        
     if not feasible_high:
-        # Fallback to a very loose check or raise error
-        feasible, placement = _check_feasibility_multi(gpu_num, items, 1e5)
+        raise ValueError("Unable to place models. Constraints likely too tight.")
+        
+    # Precise Binary Search
+    for _ in range(25):
+        mid = (low + high) / 2.0
+        feasible, placement = _check_feasibility_bfd(gpu_num, items, mid)
         if feasible:
-            best_packing_placement = placement
-        else:
-            raise ValueError("Unable to place models. Constraints likely too tight.")
-
-    # 2. Binary Search Refinement
-    for _ in range(30):
-        mid = (low + high) / 2.0
-        feasible, placement = _check_feasibility_multi(gpu_num, items, mid)
-        if feasible:
-            best_packing_placement = placement
+            best_placement = placement
             high = mid
         else:
             low = mid
-
-    # Convert to placement map {gpu_id: [models]}
-    placement_map = {i: best_packing_placement[i] for i in range(gpu_num)}
-
-    # 3. Iterated Local Search
-    # Refine the placement to reduce max KVPR further
-    final_placement = _iterated_local_search(gpu_num, placement_map)
-
-    return final_placement
-
-def _check_feasibility_multi(gpu_num, items, K):
-    """
-    Check if items can be packed with target KVPR 'K' using multiple heuristics.
-    virtual_size = w + K * s
-    Capacity_virtual = K * GPU_MEM_SIZE
+            
+    # Convert to map
+    placement_map = {i: best_placement[i] for i in range(gpu_num)}
+    
+    # 3. Refinement: Best-Improvement Local Search
+    return _best_improvement_ils(gpu_num, placement_map)
+
+def _check_feasibility_bfd(gpu_num, items, K):
+    """
+    Checks feasibility using Best-Fit Decreasing (BFD) with multiple sorting heuristics.
     """
     virtual_cap = K * GPU_MEM_SIZE
-
-    # Create pack items: (virtual_size, physical_size, weight, model)
+    
+    # Create augmented items for sorting
+    # We calculate keys once per check
     pack_items = []
     for x in items:
+        # Virtual Size: v = w + K * s
         v = x['w'] + K * x['s']
-        pack_items.append((v, x['s'], x['w'], x['m']))
-
-    # Strategy 1: FFD on Virtual Size
-    pack_items.sort(key=lambda x: x[0], reverse=True)
-    res = _pack_ffd(gpu_num, pack_items, virtual_cap)
-    if res: return True, res
-
-    # Strategy 2: BFD on Virtual Size
-    # Already sorted by virtual size
-    res = _pack_bfd(gpu_num, pack_items, virtual_cap)
-    if res: return True, res
-
-    # Strategy 3: FFD on Physical Size
-    pack_items.sort(key=lambda x: x[1], reverse=True)
-    res = _pack_ffd(gpu_num, pack_items, virtual_cap)
-    if res: return True, res
-
-    # Strategy 4: BFD on Physical Size
-    res = _pack_bfd(gpu_num, pack_items, virtual_cap)
-    if res: return True, res
-
-    # Strategy 5: FFD on Load (w)
-    pack_items.sort(key=lambda x: x[2], reverse=True)
-    res = _pack_ffd(gpu_num, pack_items, virtual_cap)
-    if res: return True, res
-
-    # Strategy 6: BFD on Load (w)
-    res = _pack_bfd(gpu_num, pack_items, virtual_cap)
-    if res: return True, res
-
+        # Density: w / s
+        d = x['w'] / (x['s'] + 1e-7)
+        pack_items.append({'v': v, 's': x['s'], 'w': x['w'], 'd': d, 'm': x['m']})
+        
+    # Heuristics: (key_lambda, reverse)
+    heuristics = [
+        (lambda x: x['v'], True),  # Virtual Size Desc
+        (lambda x: x['s'], True),  # Physical Size Desc
+        (lambda x: x['d'], True),  # Density Desc
+        (lambda x: x['w'], True),  # Load Desc (Intensity)
+    ]
+    
+    for key_func, rev in heuristics:
+        pack_items.sort(key=key_func, reverse=rev)
+        
+        # Run Best-Fit Decreasing
+        # BFD places item in the bin with minimum sufficient residual capacity
+        
+        bins_v = [0.0] * gpu_num
+        bins_p = [0.0] * gpu_num
+        placement = [[] for _ in range(gpu_num)]
+        possible = True
+        
+        for item in pack_items:
+            best_bin = -1
+            min_rem_v = float('inf')
+            
+            for i in range(gpu_num):
+                # Check constraints
+                if bins_p[i] + item['s'] <= GPU_MEM_SIZE and bins_v[i] + item['v'] <= virtual_cap + 1e-7:
+                    # Calculate residual virtual capacity
+                    rem = virtual_cap - (bins_v[i] + item['v'])
+                    if rem < min_rem_v:
+                        min_rem_v = rem
+                        best_bin = i
+            
+            if best_bin != -1:
+                bins_p[best_bin] += item['s']
+                bins_v[best_bin] += item['v']
+                placement[best_bin].append(item['m'])
+            else:
+                possible = False
+                break
+        
+        if possible:
+            return True, placement
+            
     return False, None
 
-def _pack_ffd(gpu_num, items, virtual_cap):
-    bins_v = [0.0] * gpu_num
-    bins_p = [0.0] * gpu_num
-    placement = [[] for _ in range(gpu_num)]
-
-    for v, p, w, m in items:
-        placed = False
-        for i in range(gpu_num):
-            if bins_p[i] + p <= GPU_MEM_SIZE and bins_v[i] + v <= virtual_cap + 1e-7:
-                bins_p[i] += p
-                bins_v[i] += v
-                placement[i].append(m)
-                placed = True
-                break
-        if not placed: return None
-    return placement
-
-def _pack_bfd(gpu_num, items, virtual_cap):
-    bins_v = [0.0] * gpu_num
-    bins_p = [0.0] * gpu_num
-    placement = [[] for _ in range(gpu_num)]
-
-    for v, p, w, m in items:
-        best_bin = -1
-        min_rem_v = float('inf')
-
-        for i in range(gpu_num):
-            if bins_p[i] + p <= GPU_MEM_SIZE and bins_v[i] + v <= virtual_cap + 1e-7:
-                rem = virtual_cap - (bins_v[i] + v)
-                if rem < min_rem_v:
-                    min_rem_v = rem
-                    best_bin = i
-
-        if best_bin != -1:
-            bins_p[best_bin] += p
-            bins_v[best_bin] += v
-            placement[best_bin].append(m)
-        else:
-            return None
-    return placement
-
-def _iterated_local_search(gpu_num, placement):
-    """
-    Hill Climbing with Random Kicks to minimize Max KVPR.
-    """
-    # Initialize state
+def _best_improvement_ils(gpu_num, placement):
+    """
+    Iterated Local Search with Best-Improvement Descent and Variance Tie-Breaking.
+    """
+    # Initialize State
     gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
     gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]
-
-    def get_kvpr(idx):
-        rem = GPU_MEM_SIZE - gpu_s[idx]
-        if rem <= 1e-7: return 1e9 # Penalty for full/overflow
-        return gpu_w[idx] / rem
-
-    best_sol = copy.deepcopy(placement)
-    best_max_k = max(get_kvpr(i) for i in range(gpu_num))
-
+    
+    def get_k(i):
+        rem = GPU_MEM_SIZE - gpu_s[i]
+        if rem <= 1e-7: return 1e9
+        return gpu_w[i] / rem
+        
+    current_ks = [get_k(i) for i in range(gpu_num)]
+    max_k = max(current_ks)
+    
+    best_placement = copy.deepcopy(placement)
+    best_max_k = max_k
+    
     no_improve = 0
-    max_steps = 400
-    patience = 50
-
-    for step in range(max_steps):
-        # 1. Identify bottleneck GPU
-        max_k = -1.0
-        src = -1
-        for i in range(gpu_num):
-            k = get_kvpr(i)
-            if k > max_k:
-                max_k = k
-                src = i
-
-        # 2. Check global improvement
-        if max_k < best_max_k - 1e-6:
-            best_max_k = max_k
-            best_sol = copy.deepcopy(placement)
-            no_improve = 0
-        else:
-            no_improve += 1
-
-        # 3. Kick / Perturbation if stuck
-        if no_improve > patience:
-            # Ruin and Recreate: Repack bottleneck + min load + random GPU
-            # Select GPUs involved
-            candidates = {src}
-
-            # Find min load GPU
-            min_k = float('inf')
-            min_idx = -1
-            for i in range(gpu_num):
-                if i == src: continue
-                k = get_kvpr(i)
-                if k < min_k:
-                    min_k = k
-                    min_idx = i
-            if min_idx != -1: candidates.add(min_idx)
-
-            # Add random GPU
-            for _ in range(5):
-                r = random.randint(0, gpu_num-1)
-                if r not in candidates:
-                    candidates.add(r)
-                    if len(candidates) >= 3: break
-
-            target_gpus = list(candidates)
-
-            # Save state before ruin
-            backup_placement = {g: list(placement[g]) for g in target_gpus}
-            backup_s = {g: gpu_s[g] for g in target_gpus}
-            backup_w = {g: gpu_w[g] for g in target_gpus}
-
-            # Collect models
-            repack_models = []
-            for g in target_gpus:
-                repack_models.extend(placement[g])
-                placement[g] = []
-                gpu_s[g] = 0.0
-                gpu_w[g] = 0.0
-
-            # Sort models descending by Physical Size (hardest to pack first)
-            repack_models.sort(key=lambda m: m.model_size, reverse=True)
-
-            # Recreate: Greedy Min-Max placement
-            success = True
-            for m in repack_models:
-                w, s = m.req_rate/m.slo, m.model_size
-                best_g = -1
-                best_local_k = float('inf')
-
-                # Find best GPU among targets
-                for g in target_gpus:
-                    if gpu_s[g] + s <= GPU_MEM_SIZE:
-                        rem = GPU_MEM_SIZE - (gpu_s[g] + s)
-                        k = (gpu_w[g] + w) / rem if rem > 1e-7 else 1e9
-                        if k < best_local_k:
-                            best_local_k = k
-                            best_g = g
-
-                if best_g != -1:
-                    placement[best_g].append(m)
-                    gpu_s[best_g] += s
-                    gpu_w[best_g] += w
-                else:
-                    success = False
-                    break
-
-            if success:
-                no_improve = 0 # Reset patience
-            else:
-                # Revert if failed to repack
-                for g in target_gpus:
-                    placement[g] = backup_placement[g]
-                    gpu_s[g] = backup_s[g]
-                    gpu_w[g] = backup_w[g]
-
-            continue
-
-        # 4. Greedy Descent: Reduce bottleneck 'src'
-        improved = False
-        models = placement[src]
-
-        # Phase A: Try Move
-        for i, m in enumerate(models):
-            w, s = m.req_rate/m.slo, m.model_size
-
+    max_steps = 300
+    patience = 20
+    
+    for _ in range(max_steps):
+        # 1. Identify Bottleneck
+        # Find all GPUs within tolerance of max_k to target
+        bottlenecks = [i for i, k in enumerate(current_ks) if k > max_k - 1e-5]
+        # Target the one with highest K (should be all similar)
+        src = bottlenecks[0]
+        
+        # 2. Greedy Descent: Find Best Move/Swap
+        best_move = None # (type, idx1, dst, idx2)
+        best_gain_k = -1.0
+        best_gain_var = -1.0
+        
+        src_models = placement[src]
+        
+        # Phase A: Moves
+        for i, m in enumerate(src_models):
+            s, w = m.model_size, m.req_rate/m.slo
             for dst in range(gpu_num):
                 if dst == src: continue
                 if gpu_s[dst] + s > GPU_MEM_SIZE: continue
-
-                # Predict new KVPRs
+                
+                # New Ks
                 rem_src = GPU_MEM_SIZE - (gpu_s[src] - s)
                 nk_src = (gpu_w[src] - w) / rem_src if rem_src > 1e-7 else 1e9
-
+                
                 rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + s)
                 nk_dst = (gpu_w[dst] + w) / rem_dst if rem_dst > 1e-7 else 1e9
-
-                if max(nk_src, nk_dst) < max_k - 1e-6:
-                    placement[dst].append(m)
-                    placement[src].pop(i)
-                    gpu_s[src] -= s; gpu_w[src] -= w
-                    gpu_s[dst] += s; gpu_w[dst] += w
-                    improved = True
-                    break
-            if improved: break
-
-        if improved: continue
-
-        # Phase B: Try Swap
-        for i, m1 in enumerate(models):
-            w1, s1 = m1.req_rate/m1.slo, m1.model_size
-
+                
+                # Check impact on local max
+                # We want to reduce max(src, dst) compared to old max(src, dst)
+                # Ideally, we want new_max < global_max
+                
+                new_local_max = max(nk_src, nk_dst)
+                
+                if new_local_max < current_ks[src] - 1e-7:
+                    gain_k = current_ks[src] - new_local_max
+                    # Variance reduction (positive is good)
+                    gain_var = (current_ks[src]**2 + current_ks[dst]**2) - (nk_src**2 + nk_dst**2)
+                    
+                    if gain_k > best_gain_k + 1e-7:
+                        best_gain_k = gain_k
+                        best_gain_var = gain_var
+                        best_move = ('move', i, dst, None)
+                    elif abs(gain_k - best_gain_k) < 1e-7:
+                        if gain_var > best_gain_var:
+                            best_gain_var = gain_var
+                            best_move = ('move', i, dst, None)
+
+        # Phase B: Swaps
+        # Only if we haven't found a massive improvement, or to check for better variance
+        # Check swaps with non-bottlenecks to avoid complexity
+        for i, m1 in enumerate(src_models):
+            s1, w1 = m1.model_size, m1.req_rate/m1.slo
             for dst in range(gpu_num):
                 if dst == src: continue
-
+                # Skip swapping with other high-pressure GPUs unless desperate
+                if current_ks[dst] > max_k * 0.95: continue
+                
                 for j, m2 in enumerate(placement[dst]):
-                    w2, s2 = m2.req_rate/m2.slo, m2.model_size
-
+                    s2, w2 = m2.model_size, m2.req_rate/m2.slo
+                    
                     ns_src = gpu_s[src] - s1 + s2
                     ns_dst = gpu_s[dst] - s2 + s1
                     if ns_src > GPU_MEM_SIZE or ns_dst > GPU_MEM_SIZE: continue
-
+                    
                     rem_src = GPU_MEM_SIZE - ns_src
+                    rem_dst = GPU_MEM_SIZE - ns_dst
                     nk_src = (gpu_w[src] - w1 + w2) / rem_src if rem_src > 1e-7 else 1e9
-
-                    rem_dst = GPU_MEM_SIZE - ns_dst
                     nk_dst = (gpu_w[dst] - w2 + w1) / rem_dst if rem_dst > 1e-7 else 1e9
-
-                    if max(nk_src, nk_dst) < max_k - 1e-6:
-                        placement[src][i] = m2
-                        placement[dst][j] = m1
-                        gpu_s[src] = ns_src; gpu_w[src] += (w2 - w1)
-                        gpu_s[dst] = ns_dst; gpu_w[dst] += (w1 - w2)
-                        improved = True
-                        break
-                if improved: break
-            if improved: break
-
-    return best_sol
+                    
+                    new_local_max = max(nk_src, nk_dst)
+                    
+                    if new_local_max < current_ks[src] - 1e-7:
+                        gain_k = current_ks[src] - new_local_max
+                        gain_var = (current_ks[src]**2 + current_ks[dst]**2) - (nk_src**2 + nk_dst**2)
+                        
+                        if gain_k > best_gain_k + 1e-7:
+                            best_gain_k = gain_k
+                            best_gain_var = gain_var
+                            best_move = ('swap', i, dst, j)
+                        elif abs(gain_k - best_gain_k) < 1e-7:
+                            if gain_var > best_gain_var:
+                                best_gain_var = gain_var
+                                best_move = ('swap', i, dst, j)
+
+        # 3. Apply Best Move
+        if best_move:
+            mtype, idx1, dst, idx2 = best_move
+            if mtype == 'move':
+                m = placement[src].pop(idx1)
+                placement[dst].append(m)
+                gpu_s[src] -= m.model_size; gpu_w[src] -= m.req_rate/m.slo
+                gpu_s[dst] += m.model_size; gpu_w[dst] += m.req_rate/m.slo
+            else:
+                m1 = placement[src][idx1]
+                m2 = placement[dst][idx2]
+                placement[src][idx1] = m2
+                placement[dst][idx2] = m1
+                gpu_s[src] = gpu_s[src] - m1.model_size + m2.model_size
+                gpu_w[src] = gpu_w[src] - (m1.req_rate/m1.slo) + (m2.req_rate/m2.slo)
+                gpu_s[dst] = gpu_s[dst] - m2.model_size + m1.model_size
+                gpu_w[dst] = gpu_w[dst] - (m2.req_rate/m2.slo) + (m1.req_rate/m1.slo)
+            
+            current_ks[src] = get_k(src)
+            current_ks[dst] = get_k(dst)
+            new_global_max = max(current_ks)
+            
+            if new_global_max < best_max_k - 1e-6:
+                best_max_k = new_global_max
+                best_placement = copy.deepcopy(placement)
+                no_improve = 0
+            else:
+                if new_global_max < max_k - 1e-6:
+                    no_improve = 0 # Progress made on local structure
+                else:
+                    no_improve += 1
+            max_k = new_global_max
+        else:
+            no_improve += 1
+            
+        # 4. Burst Kick (Perturbation)
+        if no_improve > patience:
+            # Burst: 3-4 random moves to escape basin
+            for _ in range(4):
+                # Pick random src (weighted towards utilized GPUs slightly by probability?)
+                # Just random is fine for speed and diversity
+                r_src = random.randint(0, gpu_num-1)
+                if not placement[r_src]: continue
+                
+                m_idx = random.randint(0, len(placement[r_src])-1)
+                m = placement[r_src][m_idx]
+                
+                r_dst = random.randint(0, gpu_num-1)
+                if r_src == r_dst: continue
+                
+                if gpu_s[r_dst] + m.model_size <= GPU_MEM_SIZE:
+                    placement[r_dst].append(m)
+                    placement[r_src].pop(m_idx)
+                    gpu_s[r_src] -= m.model_size; gpu_w[r_src] -= m.req_rate/m.slo
+                    gpu_s[r_dst] += m.model_size; gpu_w[r_dst] += m.req_rate/m.slo
+            
+            # Recalculate State
+            current_ks = [get_k(i) for i in range(gpu_num)]
+            max_k = max(current_ks)
+            no_improve = 0
+            
+    return best_placement
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")
