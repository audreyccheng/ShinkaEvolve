<NAME>
ils_ruin_recreate
</NAME>

<DESCRIPTION>
Implement Iterated Local Search (ILS) with Ruin and Recreate strategy to improve model placement.
Instead of a simple Hill Climbing that stops at the first local optimum, the new algorithm uses a perturbation mechanism. When the greedy descent (Hill Climbing) gets stuck, the algorithm "ruins" a part of the solution by removing models from the bottleneck GPU and a few random other GPUs, then "recreates" the partial solution using a density-based Best-Fit heuristic. This allows the search to escape local optima and explore the solution space more effectively, tracking the global best solution found.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # --- 3. Local Search Refinement ---
    current_placement = final_placement

    # Reconstruct load/used stats
    loads = [0.0] * gpu_num
    used = [0.0] * gpu_num
    for g in range(gpu_num):
        for m in current_placement[g]:
            loads[g] += m.req_rate / m.slo
            used[g] += m.model_size

    # Hill Climbing: Minimize Max Pressure
    iter_limit = 100
    for _ in range(iter_limit):
        max_p = -1.0
        bottleneck = -1

        # Identify bottleneck
        for g in range(gpu_num):
            rem = GPU_MEM_SIZE - used[g]
            p = loads[g] / rem if rem > 1e-6 else float('inf')
            if p > max_p:
                max_p = p
                bottleneck = g

        if bottleneck == -1: break

        improved = False
        bn_items = list(current_placement[bottleneck])

        # Try Moving an item out of bottleneck
        for m in bn_items:
            w = m.req_rate / m.slo
            s = m.model_size

            src_l = loads[bottleneck] - w
            src_u = used[bottleneck] - s

            for dst in range(gpu_num):
                if dst == bottleneck: continue
                if used[dst] + s >= GPU_MEM_SIZE - 1e-6: continue

                dst_l = loads[dst] + w
                dst_u = used[dst] + s
                dst_p = dst_l / (GPU_MEM_SIZE - dst_u) if (GPU_MEM_SIZE - dst_u) > 1e-6 else float('inf')

                # Accept if target GPU stays below current max pressure
                if dst_p < max_p - 1e-6:
                    current_placement[bottleneck].remove(m)
                    current_placement[dst].append(m)

                    loads[bottleneck] = src_l
                    used[bottleneck] = src_u
                    loads[dst] = dst_l
                    used[dst] = dst_u

                    improved = True
                    break
            if improved: break

        if improved: continue

        # Try Swapping an item from bottleneck with another
        for dst in range(gpu_num):
            if dst == bottleneck: continue

            dst_items = list(current_placement[dst])
            for m_src in bn_items:
                ws = m_src.req_rate / m_src.slo
                ss = m_src.model_size

                for m_dst in dst_items:
                    wd = m_dst.req_rate / m_dst.slo
                    sd = m_dst.model_size

                    # New bottleneck state
                    n_src_u = used[bottleneck] - ss + sd
                    if n_src_u >= GPU_MEM_SIZE - 1e-6: continue
                    n_src_l = loads[bottleneck] - ws + wd
                    n_src_p = n_src_l / (GPU_MEM_SIZE - n_src_u)

                    # New dest state
                    n_dst_u = used[dst] - sd + ss
                    if n_dst_u >= GPU_MEM_SIZE - 1e-6: continue
                    n_dst_l = loads[dst] - wd + ws
                    n_dst_p = n_dst_l / (GPU_MEM_SIZE - n_dst_u)

                    # Accept if both result in lower pressure than current max
                    if n_src_p < max_p - 1e-6 and n_dst_p < max_p - 1e-6:
                        current_placement[bottleneck].remove(m_src)
                        current_placement[bottleneck].append(m_dst)
                        current_placement[dst].remove(m_dst)
                        current_placement[dst].append(m_src)

                        loads[bottleneck] = n_src_l
                        used[bottleneck] = n_src_u
                        loads[dst] = n_dst_l
                        used[dst] = n_dst_u

                        improved = True
                        break
                if improved: break
            if improved: break

        if not improved: break

    return current_placement
=======
    # --- 3. Iterated Local Search (Ruin & Recreate) ---
    import random

    current_placement = final_placement

    # Helper to calculate state
    def get_state(plc):
        l = [0.0] * gpu_num
        u = [0.0] * gpu_num
        mx_p = 0.0
        bn = -1
        for g in range(gpu_num):
            for m in plc[g]:
                l[g] += m.req_rate / m.slo
                u[g] += m.model_size
            rem = GPU_MEM_SIZE - u[g]
            p = l[g] / rem if rem > 1e-6 else (float('inf') if l[g] > 0 else 0.0)
            if p > mx_p:
                mx_p = p
                bn = g
        return l, u, mx_p, bn

    loads, used, global_max_p, bottleneck = get_state(current_placement)
    best_placement = {k: list(v) for k, v in current_placement.items()}
    best_max_p = global_max_p

    # ILS Loop
    max_iters = 1000
    for i in range(max_iters):
        # --- Hill Climbing Step ---
        # Identify bottleneck
        max_p = -1.0
        bottleneck = -1
        for g in range(gpu_num):
            rem = GPU_MEM_SIZE - used[g]
            p = loads[g] / rem if rem > 1e-6 else float('inf')
            if p > max_p:
                max_p = p
                bottleneck = g

        # Update Global Best
        if max_p < best_max_p - 1e-6:
            best_max_p = max_p
            best_placement = {k: list(v) for k, v in current_placement.items()}

        improved = False
        if bottleneck != -1:
            bn_items = list(current_placement[bottleneck])
            # Move
            for m in bn_items:
                w = m.req_rate / m.slo
                s = m.model_size
                src_l = loads[bottleneck] - w
                src_u = used[bottleneck] - s

                for dst in range(gpu_num):
                    if dst == bottleneck: continue
                    if used[dst] + s >= GPU_MEM_SIZE - 1e-6: continue
                    dst_l = loads[dst] + w
                    dst_u = used[dst] + s
                    dst_p = dst_l / (GPU_MEM_SIZE - dst_u)

                    if dst_p < max_p - 1e-6:
                        current_placement[bottleneck].remove(m)
                        current_placement[dst].append(m)
                        loads[bottleneck] = src_l
                        used[bottleneck] = src_u
                        loads[dst] = dst_l
                        used[dst] = dst_u
                        improved = True
                        break
                if improved: break

            # Swap
            if not improved:
                for dst in range(gpu_num):
                    if dst == bottleneck: continue
                    dst_items = list(current_placement[dst])
                    for m_src in bn_items:
                        ws = m_src.req_rate / m_src.slo
                        ss = m_src.model_size
                        for m_dst in dst_items:
                            wd = m_dst.req_rate / m_dst.slo
                            sd = m_dst.model_size

                            n_src_u = used[bottleneck] - ss + sd
                            if n_src_u >= GPU_MEM_SIZE - 1e-6: continue
                            n_src_l = loads[bottleneck] - ws + wd
                            n_src_p = n_src_l / (GPU_MEM_SIZE - n_src_u)

                            n_dst_u = used[dst] - sd + ss
                            if n_dst_u >= GPU_MEM_SIZE - 1e-6: continue
                            n_dst_l = loads[dst] - wd + ws
                            n_dst_p = n_dst_l / (GPU_MEM_SIZE - n_dst_u)

                            if n_src_p < max_p - 1e-6 and n_dst_p < max_p - 1e-6:
                                current_placement[bottleneck].remove(m_src)
                                current_placement[bottleneck].append(m_dst)
                                current_placement[dst].remove(m_dst)
                                current_placement[dst].append(m_src)
                                loads[bottleneck] = n_src_l
                                used[bottleneck] = n_src_u
                                loads[dst] = n_dst_l
                                used[dst] = n_dst_u
                                improved = True
                                break
                        if improved: break
                    if improved: break

        if improved: continue

        # --- Ruin and Recreate (Perturbation) ---
        # If no improvement locally, kick the solution
        if bottleneck == -1: break

        # Select victims: bottleneck + 2 random
        victims = {bottleneck}
        others = [g for g in range(gpu_num) if g != bottleneck]
        if others:
            victims.update(random.sample(others, min(2, len(others))))
        victim_list = list(victims)

        # Ruin
        removed_models = []
        for v in victim_list:
            removed_models.extend(current_placement[v])
            current_placement[v] = []
            loads[v] = 0.0
            used[v] = 0.0

        # Recreate: Best-Fit Decreasing by Density
        removed_models.sort(key=lambda x: (x.req_rate/x.slo)/x.model_size if x.model_size>0 else 0, reverse=True)

        feasible_repack = True

        for m in removed_models:
            w = m.req_rate / m.slo
            s = m.model_size
            best_v = -1
            min_p = float('inf')

            for v in victim_list:
                rem = GPU_MEM_SIZE - used[v] - s
                if rem > 1e-6:
                    p = (loads[v] + w) / rem
                    if p < min_p:
                        min_p = p
                        best_v = v

            if best_v != -1:
                current_placement[best_v].append(m)
                loads[best_v] += w
                used[best_v] += s
            else:
                # Fallback: try to fit anywhere in victims (ignoring pressure min)
                for v in victim_list:
                    if used[v] + s <= GPU_MEM_SIZE - 1e-6:
                        best_v = v
                        current_placement[best_v].append(m)
                        loads[best_v] += w
                        used[best_v] += s
                        break
                if best_v == -1:
                    feasible_repack = False
                    break

        if not feasible_repack:
            # Revert to best known to recover from bad perturbation
            current_placement = {k: list(v) for k, v in best_placement.items()}
            loads, used, _, _ = get_state(current_placement)
            # Stop if we are just failing to repack repeatedly
            if i > max_iters * 0.8: break

    return best_placement
>>>>>>> REPLACE
</DIFF>