--- a/original.py
+++ b/original.py
@@ -1,294 +1,382 @@
 # EVOLVE-BLOCK-START
-"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs using Binary Search with Multi-Strategy Packing and Local Search Refinement"""
+"""Model placement algorithm for minimizing maximum KV cache pressure using Binary Search with Best-Fit Decreasing and Variance-Smoothing Local Search"""
 
 import copy
 import random
+import math
 
 GPU_MEM_SIZE = 80.0  # GB
 
 def compute_model_placement(gpu_num, models):
     """
-    Minimizes max KVPR using Robust Binary Search with multiple packing heuristics
-    followed by Iterated Local Search (Ruin & Recreate).
-    """
-    # 1. Validation
+    Minimizes max KVPR using Robust Binary Search (BFD Packing)
+    followed by Variance-Smoothing Iterated Local Search.
+    """
+    # 1. Validation and Pre-computation
     total_size = sum(m.model_size for m in models)
     if total_size > gpu_num * GPU_MEM_SIZE:
         raise ValueError("Total model size exceeds total GPU memory capacity.")
 
-    # Prepare items: (w, s, m)
-    items = [{'w': m.req_rate / m.slo, 's': m.model_size, 'm': m} for m in models]
-
-    # 2. Binary Search
+    # Wrap models with cached properties for speed
+    # Item: (w, s, density, model)
+    items = []
+    for m in models:
+        w = m.req_rate / m.slo
+        s = m.model_size
+        d = w / (s + 1e-7)
+        items.append({'w': w, 's': s, 'd': d, 'm': m})
+
+    # 2. Binary Search for Initial Feasible Solution
     total_w = sum(x['w'] for x in items)
     slack = gpu_num * GPU_MEM_SIZE - total_size
-
+    
     low = 0.0
+    # Heuristic upper bound
     if slack < 1e-6:
         high = 10000.0
     else:
-        avg_pressure = total_w / slack
-        high = max(10.0, avg_pressure * 8.0)
+        avg_k = total_w / slack
+        high = max(10.0, avg_k * 6.0)
 
     best_placement = None
     feasible_high = False
-
-    # Find valid upper bound
+    
+    # Exponential search for valid upper bound
     for _ in range(20):
         feasible, placement = _check_feasibility_robust(gpu_num, items, high)
         if feasible:
             best_placement = placement
             feasible_high = True
             break
         low = high
         high *= 2.0
-
+        
     if not feasible_high:
-        raise ValueError("Unable to place models. Constraints likely too tight.")
-
-    # Binary Search
+        raise ValueError("Unable to place models. Constraints too tight.")
+
+    # Binary Search Refinement (30 iters -> high precision)
     for _ in range(30):
         mid = (low + high) / 2.0
         feasible, placement = _check_feasibility_robust(gpu_num, items, mid)
         if feasible:
             best_placement = placement
             high = mid
         else:
             low = mid
-
+            
+    # Convert to standard format
     placement_map = {i: best_placement[i] for i in range(gpu_num)}
-
-    # 3. Iterated Local Search Refinement
-    return _iterated_local_search(gpu_num, placement_map)
+    
+    # 3. Variance-Smoothing Iterated Local Search
+    return _variance_smoothing_ils(gpu_num, placement_map)
 
 def _check_feasibility_robust(gpu_num, items, K):
     """
-    Check feasibility using multiple sorting strategies and packing algorithms.
+    Checks feasibility using Best-Fit Decreasing with multiple sorting keys.
     """
     virtual_cap = K * GPU_MEM_SIZE
-    # Augment items for sorting
+    
+    # Augment items with virtual size
     pack_items = []
     for x in items:
+        # Virtual Size: w + K*s
         v = x['w'] + K * x['s']
-        d = x['w'] / (x['s'] + 1e-7)
-        pack_items.append({'v': v, 's': x['s'], 'w': x['w'], 'd': d, 'm': x['m']})
-
-    # Heuristics: (key_lambda, reverse)
+        pack_items.append({'v': v, 's': x['s'], 'w': x['w'], 'd': x['d'], 'm': x['m']})
+        
+    # Heuristics: (Sort Key Lambda, Reverse)
     heuristics = [
-        (lambda x: x['v'], True),  # Virtual Size Desc
-        (lambda x: x['s'], True),  # Physical Size Desc
-        (lambda x: x['d'], True),  # Density Desc
-        (lambda x: x['w'], True),  # Load Desc
+        (lambda x: x['v'], True),  # Virtual Size Descending (Standard)
+        (lambda x: x['s'], True),  # Physical Size Descending
+        (lambda x: x['w'], True),  # Load Descending
+        (lambda x: x['d'], True),  # Density Descending
     ]
-
+    
     for key_func, rev in heuristics:
-        sorted_items = sorted(pack_items, key=key_func, reverse=rev)
-
-        # Try Best Fit Decreasing (usually better for tight bins)
-        if res := _pack_bfd(gpu_num, sorted_items, virtual_cap):
+        pack_items.sort(key=key_func, reverse=rev)
+        
+        # Best Fit Decreasing (Minimizes Residual Virtual Capacity)
+        if res := _pack_bfd(gpu_num, pack_items, virtual_cap):
             return True, res
-
-        # Try First Fit Decreasing
-        if res := _pack_ffd(gpu_num, sorted_items, virtual_cap):
-            return True, res
-
+            
     return False, None
 
-def _pack_ffd(gpu_num, items, virtual_cap):
+def _pack_bfd(gpu_num, items, virtual_cap):
+    """
+    Best Fit Decreasing: Places item in the bin with min sufficient virtual capacity.
+    """
     bins_v = [0.0] * gpu_num
     bins_p = [0.0] * gpu_num
     placement = [[] for _ in range(gpu_num)]
-
-    for item in items:
-        placed = False
-        for i in range(gpu_num):
-            if bins_p[i] + item['s'] <= GPU_MEM_SIZE and bins_v[i] + item['v'] <= virtual_cap + 1e-7:
-                bins_p[i] += item['s']
-                bins_v[i] += item['v']
-                placement[i].append(item['m'])
-                placed = True
-                break
-        if not placed: return None
-    return placement
-
-def _pack_bfd(gpu_num, items, virtual_cap):
-    bins_v = [0.0] * gpu_num
-    bins_p = [0.0] * gpu_num
-    placement = [[] for _ in range(gpu_num)]
-
+    
     for item in items:
         best_bin = -1
-        min_rem = float('inf')
-
+        min_rem_v = float('inf')
+        
         for i in range(gpu_num):
+            # Check constraints
             if bins_p[i] + item['s'] <= GPU_MEM_SIZE and bins_v[i] + item['v'] <= virtual_cap + 1e-7:
+                # Calculate residual
                 rem = virtual_cap - (bins_v[i] + item['v'])
-                if rem < min_rem:
-                    min_rem = rem
+                if rem < min_rem_v:
+                    min_rem_v = rem
                     best_bin = i
-
+        
         if best_bin != -1:
             bins_p[best_bin] += item['s']
             bins_v[best_bin] += item['v']
             placement[best_bin].append(item['m'])
         else:
-            return None
+            return None # Fail
+            
     return placement
 
-def _iterated_local_search(gpu_num, placement):
-    """
-    Refines placement using Ruin and Recreate.
-    """
-    # Initialize state
+def _variance_smoothing_ils(gpu_num, placement):
+    """
+    Iterated Local Search with Variance Tie-Breaking and Burst Kicks.
+    """
+    # State initialization
     gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
     gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]
-
-    def get_k(i):
-        rem = GPU_MEM_SIZE - gpu_s[i]
+    
+    def get_k(idx):
+        rem = GPU_MEM_SIZE - gpu_s[idx]
         if rem <= 1e-7: return 1e9
-        return gpu_w[i] / rem
-
+        return gpu_w[idx] / rem
+        
     best_max_k = max(get_k(i) for i in range(gpu_num))
-    best_placement = copy.deepcopy(placement)
-
-    max_steps = 300
-    patience = 30
+    best_sol = copy.deepcopy(placement)
+    
+    max_steps = 500
+    patience = 25
     no_improve = 0
-
+    
+    # Pre-calculate initial sum of squares
+    current_ks = [get_k(i) for i in range(gpu_num)]
+    
     for _ in range(max_steps):
-        # Identify bottleneck
-        current_ks = [get_k(i) for i in range(gpu_num)]
-        max_k = max(current_ks)
-
-        # Check global improvement
+        # 1. Status Check
+        max_k = -1.0
+        src = -1
+        sum_sq = 0.0
+        
+        for i in range(gpu_num):
+            k = get_k(i)
+            current_ks[i] = k
+            if k > max_k:
+                max_k = k
+                src = i
+            sum_sq += k*k
+            
+        # Update Global Best
         if max_k < best_max_k - 1e-6:
             best_max_k = max_k
-            best_placement = copy.deepcopy(placement)
+            best_sol = copy.deepcopy(placement)
             no_improve = 0
         else:
             no_improve += 1
-
-        # Ruin & Recreate
-        # Select bottleneck and random others
-        candidates = [i for i, k in enumerate(current_ks) if k > max_k * 0.99]
-        if not candidates: candidates = [random.randint(0, gpu_num-1)]
-        src = random.choice(candidates)
-
-        ruin_set = {src}
-        n_ruin = 2 + (1 if no_improve > patience else 0)
-
-        others = list(range(gpu_num))
-        random.shuffle(others)
-        for o in others:
-            if len(ruin_set) >= min(gpu_num, n_ruin): break
-            if o != src: ruin_set.add(o)
-
-        # Ruin
-        removed_models = []
-        backup = {}
-        for idx in ruin_set:
-            backup[idx] = (list(placement[idx]), gpu_s[idx], gpu_w[idx])
-            removed_models.extend(placement[idx])
-            placement[idx] = []
-            gpu_s[idx] = 0.0
-            gpu_w[idx] = 0.0
-
-        # Recreate (Greedy Best Fit)
-        # Sort by physical size desc to pack well
-        removed_models.sort(key=lambda m: m.model_size, reverse=True)
-
-        feasible = True
-        for m in removed_models:
-            best_idx = -1
-            best_score = float('inf')
-
-            m_s = m.model_size
-            m_w = m.req_rate / m.slo
-
-            for idx in ruin_set:
-                if gpu_s[idx] + m_s <= GPU_MEM_SIZE:
-                    # Score: minimizing resulting local K
-                    rem = GPU_MEM_SIZE - (gpu_s[idx] + m_s)
-                    k = (gpu_w[idx] + m_w) / rem if rem > 1e-7 else 1e9
-                    if k < best_score:
-                        best_score = k
-                        best_idx = idx
-
-            if best_idx != -1:
-                placement[best_idx].append(m)
-                gpu_s[best_idx] += m_s
-                gpu_w[best_idx] += m_w
+            
+        # 2. Burst Kick (Escape Local Optima)
+        if no_improve > patience:
+            moves_done = 0
+            # Perform a burst of 3-5 random moves
+            burst_size = random.randint(3, 5)
+            for _ in range(burst_size):
+                # Try to move from a high-load GPU
+                candidates = [i for i in range(gpu_num) if current_ks[i] > max_k * 0.7]
+                if not candidates: candidates = list(range(gpu_num))
+                
+                s_idx = random.choice(candidates)
+                if not placement[s_idx]: continue
+                
+                # To random destination
+                d_idx = random.randint(0, gpu_num - 1)
+                if s_idx == d_idx: continue
+                
+                m_idx = random.randint(0, len(placement[s_idx]) - 1)
+                m = placement[s_idx][m_idx]
+                
+                if gpu_s[d_idx] + m.model_size <= GPU_MEM_SIZE:
+                    placement[d_idx].append(m)
+                    placement[s_idx].pop(m_idx)
+                    
+                    gpu_s[d_idx] += m.model_size; gpu_w[d_idx] += m.req_rate/m.slo
+                    gpu_s[s_idx] -= m.model_size; gpu_w[s_idx] -= m.req_rate/m.slo
+                    # Update cache slightly for next kick step
+                    current_ks[s_idx] = get_k(s_idx)
+                    current_ks[d_idx] = get_k(d_idx)
+                    moves_done += 1
+            
+            if moves_done > 0:
+                # Reset patience partially to allow settling
+                no_improve = max(0, patience - 10)
+            continue
+
+        # 3. Steepest Descent with Variance Tie-Breaking
+        # We only look for moves from 'src' (bottleneck) to others
+        
+        best_move = None # (type, idx1, dst, idx2, ...)
+        best_imp_k = -1.0
+        best_imp_var = -1.0
+        
+        models = placement[src]
+        # Sort models by contribution (Weight) to find good moves earlier (optimization)
+        sorted_models_idx = sorted(range(len(models)), key=lambda i: models[i].req_rate/models[i].slo, reverse=True)
+        
+        # A. Try Moves
+        for i in sorted_models_idx:
+            m = models[i]
+            w, s = m.req_rate/m.slo, m.model_size
+            
+            for dst in range(gpu_num):
+                if dst == src: continue
+                if gpu_s[dst] + s > GPU_MEM_SIZE: continue
+                
+                # Check metrics
+                rem_src = GPU_MEM_SIZE - (gpu_s[src] - s)
+                nk_src = (gpu_w[src] - w) / rem_src if rem_src > 1e-7 else 1e9
+                
+                rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + s)
+                nk_dst = (gpu_w[dst] + w) / rem_dst if rem_dst > 1e-7 else 1e9
+                
+                new_peak = max(nk_src, nk_dst)
+                
+                # Delta calculations
+                diff_k = max_k - new_peak
+                
+                # Accept if Peak reduces significantly
+                if diff_k > 1e-6:
+                    if diff_k > best_imp_k:
+                        best_imp_k = diff_k
+                        best_move = ('move', i, dst, s, w)
+                        best_imp_var = 0 # Irrelevant if K improves
+                
+                # Accept if Peak is same (approx) but Variance reduces
+                elif abs(diff_k) < 1e-6 and best_imp_k < 1e-6:
+                    old_sq = current_ks[src]**2 + current_ks[dst]**2
+                    new_sq = nk_src**2 + nk_dst**2
+                    diff_var = old_sq - new_sq
+                    
+                    if diff_var > 1e-5 and diff_var > best_imp_var:
+                        best_imp_var = diff_var
+                        best_move = ('move', i, dst, s, w)
+
+        # B. Try Swaps (Only if no significant move found to save time)
+        if best_imp_k < 1e-6:
+            for i1 in sorted_models_idx:
+                m1 = models[i1]
+                w1, s1 = m1.req_rate/m1.slo, m1.model_size
+                
+                for dst in range(gpu_num):
+                    if dst == src: continue
+                    # Optimization: Don't swap with another bottleneck
+                    if current_ks[dst] > max_k * 0.95: continue
+                    
+                    for i2, m2 in enumerate(placement[dst]):
+                        w2, s2 = m2.req_rate/m2.slo, m2.model_size
+                        
+                        ns_src = gpu_s[src] - s1 + s2
+                        ns_dst = gpu_s[dst] - s2 + s1
+                        
+                        if ns_src > GPU_MEM_SIZE or ns_dst > GPU_MEM_SIZE: continue
+                        
+                        rem_src = GPU_MEM_SIZE - ns_src
+                        nk_src = (gpu_w[src] - w1 + w2) / rem_src if rem_src > 1e-7 else 1e9
+                        
+                        rem_dst = GPU_MEM_SIZE - ns_dst
+                        nk_dst = (gpu_w[dst] - w2 + w1) / rem_dst if rem_dst > 1e-7 else 1e9
+                        
+                        new_peak = max(nk_src, nk_dst)
+                        diff_k = max_k - new_peak
+                        
+                        if diff_k > 1e-6:
+                            if diff_k > best_imp_k:
+                                best_imp_k = diff_k
+                                best_move = ('swap', i1, dst, i2, s1, w1, s2, w2)
+                                best_imp_var = 0
+                        elif abs(diff_k) < 1e-6 and best_imp_k < 1e-6:
+                            old_sq = current_ks[src]**2 + current_ks[dst]**2
+                            new_sq = nk_src**2 + nk_dst**2
+                            diff_var = old_sq - new_sq
+                            
+                            if diff_var > 1e-5 and diff_var > best_imp_var:
+                                best_imp_var = diff_var
+                                best_move = ('swap', i1, dst, i2, s1, w1, s2, w2)
+
+        # Execute Best Move
+        if best_move:
+            if best_move[0] == 'move':
+                _, i, dst, s, w = best_move
+                m = placement[src].pop(i)
+                placement[dst].append(m)
+                gpu_s[src] -= s; gpu_w[src] -= w
+                gpu_s[dst] += s; gpu_w[dst] += w
+            elif best_move[0] == 'swap':
+                _, i1, dst, i2, s1, w1, s2, w2 = best_move
+                m1 = placement[src][i1]
+                m2 = placement[dst][i2]
+                placement[src][i1] = m2
+                placement[dst][i2] = m1
+                gpu_s[src] = gpu_s[src] - s1 + s2
+                gpu_w[src] = gpu_w[src] - w1 + w2
+                gpu_s[dst] = gpu_s[dst] - s2 + s1
+                gpu_w[dst] = gpu_w[dst] - w2 + w1
+            
+            # Reset patience slightly on successful variance reduction to allow climbing
+            if best_imp_k < 1e-6:
+                no_improve = max(0, patience - 5)
             else:
-                feasible = False
-                break
-
-        # Acceptance
-        accept = False
-        if feasible:
-            new_ks = [get_k(i) for i in range(gpu_num)]
-            new_max = max(new_ks)
-            if new_max < max_k - 1e-6:
-                accept = True
-            elif new_max < max_k + 1e-6:
-                # Accept equal moves to traverse plateau
-                accept = True
-            elif no_improve > patience and random.random() < 0.2:
-                # Random walk
-                accept = True
-
-        if not accept:
-            # Revert
-            for idx in ruin_set:
-                placement[idx], gpu_s[idx], gpu_w[idx] = backup[idx]
-
-    return best_placement
+                no_improve = 0
+        else:
+            no_improve += 1
+            
+    return best_sol
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")
