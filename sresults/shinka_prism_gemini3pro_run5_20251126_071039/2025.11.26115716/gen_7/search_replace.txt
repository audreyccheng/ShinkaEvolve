<NAME>
greedy_resultant_kvpr_size_sort
</NAME>

<DESCRIPTION>
Improves the model placement algorithm by:
1. Sorting models by size descending (instead of load density). This follows the First Fit Decreasing heuristic for bin packing, ensuring large models are placed when more space is available, preventing scenarios where they are forced into GPUs with little remaining memory (which causes KVPR spikes).
2. Changing the greedy selection criterion from minimizing "current KVPR" to minimizing "resultant KVPR". The previous approach selected the GPU with the lowest existing load ratio, which doesn't account for the impact of the new model's size reducing the denominator (available memory). The new approach calculates the KVPR *after* the hypothetical placement, selecting the GPU that minimizes the new maximum pressure.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Greedy KVPR-minimizing placement based on Algorithm 1 (without Ï„ check)
    # 1) Sort models by r_j / s_j in descending order
    sorted_models = sorted(models, key=lambda m: (m.req_rate / m.slo), reverse=True)

    # 2) Initialize per-GPU states
    placement = {gpu_id: [] for gpu_id in range(gpu_num)}
    shared_kv = [GPU_MEM_SIZE for _ in range(gpu_num)]  # remaining memory per GPU
    weighted_req_rate = [0.0 for _ in range(gpu_num)]   # sum of r_j / s_j per GPU

    # 3) Assign each model to the GPU that minimizes current KVPR while fitting in memory
    for model in sorted_models:
        best_idx = None
        best_ratio = float('inf')

        for gpu_id in range(gpu_num):
            if model.model_size <= shared_kv[gpu_id] and shared_kv[gpu_id] > 0:
                current_ratio = weighted_req_rate[gpu_id] / shared_kv[gpu_id]
                if current_ratio < best_ratio:
                    best_ratio = current_ratio
                    best_idx = gpu_id

        # Failure: if no GPU can fit, raise an error instead of overcommitting
        if best_idx is None:
            raise ValueError(
                f"Unable to place model of size {model.model_size} GB on any GPU. "
                f"Remaining per-GPU memory: {shared_kv}"
            )

        placement[best_idx].append(model)
        weighted_req_rate[best_idx] += model.req_rate / model.slo
        shared_kv[best_idx] -= model.model_size
=======
    # Greedy KVPR-minimizing placement
    # 1) Sort models by model_size in descending order to prioritize large items
    #    (reduces fragmentation and "cornering" into full GPUs)
    sorted_models = sorted(models, key=lambda m: (m.model_size, m.req_rate / m.slo), reverse=True)

    # 2) Initialize per-GPU states
    placement = {gpu_id: [] for gpu_id in range(gpu_num)}
    shared_kv = [GPU_MEM_SIZE for _ in range(gpu_num)]  # remaining memory per GPU
    weighted_req_rate = [0.0 for _ in range(gpu_num)]   # sum of r_j / s_j per GPU

    # 3) Assign each model to the GPU that minimizes *resultant* KVPR
    for model in sorted_models:
        best_idx = None
        best_kvpr = float('inf')

        # First pass: try to find a GPU where the model fits with remaining memory > 0
        for gpu_id in range(gpu_num):
            remaining_mem = shared_kv[gpu_id] - model.model_size
            if remaining_mem > 0:
                new_load = weighted_req_rate[gpu_id] + (model.req_rate / model.slo)
                new_kvpr = new_load / remaining_mem
                if new_kvpr < best_kvpr:
                    best_kvpr = new_kvpr
                    best_idx = gpu_id

        # Fallback: if no GPU preserves memory, check if any GPU can just fit the model (exact fit)
        if best_idx is None:
            for gpu_id in range(gpu_num):
                if shared_kv[gpu_id] >= model.model_size:
                    best_idx = gpu_id
                    break

        # Failure: if no GPU can fit, raise an error instead of overcommitting
        if best_idx is None:
            raise ValueError(
                f"Unable to place model of size {model.model_size} GB on any GPU. "
                f"Remaining per-GPU memory: {shared_kv}"
            )

        placement[best_idx].append(model)
        weighted_req_rate[best_idx] += model.req_rate / model.slo
        shared_kv[best_idx] -= model.model_size
>>>>>>> REPLACE
</DIFF>