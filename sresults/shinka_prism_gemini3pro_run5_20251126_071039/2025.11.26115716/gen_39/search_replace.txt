<NAME>
ils_vnd_refinement
</NAME>

<DESCRIPTION>
I propose replacing the existing `_lns_refinement` with `_ils_vnd_refinement`, a more structured Iterated Local Search (ILS) that incorporates Variable Neighborhood Descent (VND).

The previous local search relied on random moves or simple greedy descent. The new algorithm improves upon this by:
1.  **Steepest Descent with VND**: Instead of taking the first improving move, it exhaustively evaluates **all** possible Moves and Swaps involving the bottleneck GPU to find the *best possible improvement* at each step. This "Best Improvement" strategy is more effective for convergence.
2.  **Variance-based Tie Breaking**: When a move doesn't reduce the maximum pressure (e.g., on a plateau), the algorithm selects the move that maximizes **variance reduction** (reducing the sum of squared pressures). This helps smooth the load distribution, creating opportunities for future peak reductions.
3.  **Targeted Perturbation**: When trapped in a local optimum, the algorithm applies a targeted "Ruin & Recreate" perturbation to the bottleneck and random other GPUs, using a size-weighted heuristic to repack models.

This creates a balance between intensification (Steepest Descent) and diversification (Ruin-Recreate Kicks).
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # 3. LNS / Ruin & Recreate Refinement
    final_placement = _lns_refinement(gpu_num, placement_map)

    return final_placement
=======
    # 3. ILS with Variable Neighborhood Descent Refinement
    final_placement = _ils_vnd_refinement(gpu_num, placement_map)

    return final_placement
>>>>>>> REPLACE
<<<<<<< SEARCH
def _lns_refinement(gpu_num, placement):
    """
    Refines placement using Large Neighborhood Search (Ruin and Recreate).
    Target: Minimize Max KVPR.
    """
    # Initialize State
    gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
    gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]

    def get_k(idx):
        rem = GPU_MEM_SIZE - gpu_s[idx]
        if rem <= 1e-7: return 1e9
        return gpu_w[idx] / rem

    current_ks = [get_k(i) for i in range(gpu_num)]
    max_k = max(current_ks)

    best_max_k = max_k
    best_placement = copy.deepcopy(placement)

    # Parameters
    max_steps = 1000
    patience = 50
    no_improve = 0

    for step in range(max_steps):
        # Identify bottleneck
        # Get all GPUs close to max (within 1%) to diversify target
        candidates = [i for i in range(gpu_num) if current_ks[i] > max_k * 0.99]
        if not candidates: candidates = [random.randint(0, gpu_num-1)]
        src = random.choice(candidates)

        # Heuristic Decision: Small Move vs Ruin
        # If stuck (no improve), force Ruin. Else mix.
        do_ruin = (no_improve > patience) or (random.random() < 0.2)

        if do_ruin:
            # --- RUIN & RECREATE ---
            # Select subset of GPUs: Bottleneck + Randoms
            subset_indices = {src}
            # Add 2-3 random GPUs
            n_random = min(gpu_num - 1, random.randint(2, 4))
            others = list(range(gpu_num))
            random.shuffle(others)
            for o in others:
                if len(subset_indices) >= n_random + 1: break
                if o != src: subset_indices.add(o)

            subset_indices = list(subset_indices)

            # Extract models and Backup
            repack_models = []
            backup_state = {}
            old_subset_sq = 0

            for idx in subset_indices:
                old_subset_sq += current_ks[idx]**2
                backup_state[idx] = (list(placement[idx]), gpu_s[idx], gpu_w[idx], current_ks[idx])
                repack_models.extend(placement[idx])
                # Clear GPU
                placement[idx] = []
                gpu_s[idx] = 0.0
                gpu_w[idx] = 0.0
                current_ks[idx] = 0.0

            # Recreate: Sort models
            # Sort by Size Descending (packing efficiency) and Load Descending (pressure)
            repack_models.sort(key=lambda m: (m.model_size, m.req_rate/m.slo), reverse=True)

            feasible_repack = True

            # Greedy insertion
            for m in repack_models:
                best_idx = -1
                best_local_k = float('inf')

                m_s = m.model_size
                m_w = m.req_rate / m.slo

                for idx in subset_indices:
                    if gpu_s[idx] + m_s <= GPU_MEM_SIZE:
                        # Hypothetical K
                        rem = GPU_MEM_SIZE - (gpu_s[idx] + m_s)
                        k = (gpu_w[idx] + m_w) / rem if rem > 1e-7 else 1e9

                        # We want to minimize the PEAK K generated
                        if k < best_local_k:
                            best_local_k = k
                            best_idx = idx

                if best_idx != -1:
                    placement[best_idx].append(m)
                    gpu_s[best_idx] += m_s
                    gpu_w[best_idx] += m_w
                else:
                    feasible_repack = False
                    break

            if feasible_repack:
                # Evaluate Move
                new_subset_sq = 0
                for idx in subset_indices:
                    k = get_k(idx)
                    current_ks[idx] = k
                    new_subset_sq += k*k

                new_global_max = max(current_ks)

                improved = False
                if new_global_max < max_k - 1e-7:
                    improved = True
                elif new_global_max < max_k + 1e-7:
                    # Tie-breaking: Variance Reduction
                    if new_subset_sq < old_subset_sq:
                        improved = True

                if improved:
                    max_k = new_global_max
                    no_improve = 0
                else:
                    # Revert
                    for idx in subset_indices:
                        placement[idx], gpu_s[idx], gpu_w[idx], current_ks[idx] = backup_state[idx]
            else:
                # Revert
                for idx in subset_indices:
                    placement[idx], gpu_s[idx], gpu_w[idx], current_ks[idx] = backup_state[idx]

        else:
            # --- SIMPLE MOVE (Greedy Descent) ---
            if not placement[src]:
                no_improve += 1
                continue

            m_idx = random.randint(0, len(placement[src])-1)
            m = placement[src][m_idx]
            m = placement[src][m_idx]
            m_s = m.model_size
            m_w = m.req_rate / m.slo

            best_dst = -1
            best_impact = 0

            for dst in range(gpu_num):
                if dst == src: continue
                if gpu_s[dst] + m_s > GPU_MEM_SIZE: continue

                # New values
                rem_src = GPU_MEM_SIZE - (gpu_s[src] - m_s)
                new_k_src = (gpu_w[src] - m_w) / rem_src if rem_src > 1e-7 else 1e9

                rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + m_s)
                new_k_dst = (gpu_w[dst] + m_w) / rem_dst if rem_dst > 1e-7 else 1e9

                old_local_max = max(current_ks[src], current_ks[dst])
                new_local_max = max(new_k_src, new_k_dst)

                if new_local_max < old_local_max - 1e-7:
                    # Don't create new global bottleneck
                    if new_local_max < max_k + 1e-7:
                        improvement = old_local_max - new_local_max
                        if improvement > best_impact:
                            best_impact = improvement
                            best_dst = dst

            if best_dst != -1:
                dst = best_dst
                placement[dst].append(m)
                placement[src].pop(m_idx)

                gpu_s[src] -= m_s; gpu_w[src] -= m_w
                gpu_s[dst] += m_s; gpu_w[dst] += m_w

                current_ks[src] = get_k(src)
                current_ks[dst] = get_k(dst)

                max_k = max(current_ks)
                no_improve = 0
            else:
                no_improve += 1

        # Check Best
        if max_k < best_max_k - 1e-7:
            best_max_k = max_k
            best_placement = copy.deepcopy(placement)

    return best_placement
=======
def _ils_vnd_refinement(gpu_num, placement):
    """
    Refines placement using Iterated Local Search with Variable Neighborhood Descent.
    Phase 1: Steepest Descent (Best Improvement) using Move and Swap operators.
    Phase 2: Perturbation using Ruin & Recreate.
    """
    # Initialize State
    gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
    gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]

    def get_k(idx):
        rem = GPU_MEM_SIZE - gpu_s[idx]
        if rem <= 1e-7: return 1e9
        return gpu_w[idx] / rem

    best_max_k = max(get_k(i) for i in range(gpu_num))
    best_placement = copy.deepcopy(placement)

    no_improve = 0
    max_steps = 400
    patience = 5

    for _ in range(max_steps):
        # 1. Status Check
        current_ks = [get_k(i) for i in range(gpu_num)]
        max_k = max(current_ks)
        src = current_ks.index(max_k)

        # Update Global Best
        if max_k < best_max_k - 1e-7:
            best_max_k = max_k
            best_placement = copy.deepcopy(placement)
            no_improve = 0
        else:
            no_improve += 1

        # 2. Local Search (Steepest Descent)
        # Find best move or swap for the bottleneck GPU
        best_move = None # (type, data)
        best_peak_red = -1.0 # Peak reduction
        best_var_red = -1e9  # Variance reduction (tie-breaker)

        current_k_src = current_ks[src]
        src_models = placement[src]

        # A. Try Moves FROM bottleneck
        for i, m in enumerate(src_models):
            w, s = m.req_rate/m.slo, m.model_size
            for dst in range(gpu_num):
                if dst == src: continue
                if gpu_s[dst] + s > GPU_MEM_SIZE: continue

                rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + s)
                nk_dst = (gpu_w[dst] + w) / rem_dst if rem_dst > 1e-7 else 1e9

                rem_src = GPU_MEM_SIZE - (gpu_s[src] - s)
                nk_src = (gpu_w[src] - w) / rem_src if rem_src > 1e-7 else 1e9

                new_peak = max(nk_src, nk_dst)
                peak_red = max_k - new_peak

                if peak_red > 1e-7:
                    if peak_red > best_peak_red + 1e-7:
                        best_peak_red = peak_red
                        best_move = ('move', src, i, dst, s, w)
                        best_var_red = 0
                elif abs(peak_red) <= 1e-7:
                    # Tie: Check variance reduction
                    old_sq = current_k_src**2 + current_ks[dst]**2
                    new_sq = nk_src**2 + nk_dst**2
                    var_red = old_sq - new_sq

                    if abs(best_peak_red) < 1e-7 and var_red > best_var_red:
                        best_var_red = var_red
                        best_move = ('move', src, i, dst, s, w)
                        best_peak_red = 0

        # B. Try Swaps involving bottleneck
        for i1, m1 in enumerate(src_models):
            w1, s1 = m1.req_rate/m1.slo, m1.model_size
            for dst in range(gpu_num):
                if dst == src: continue
                if current_ks[dst] > max_k * 0.99: continue

                for i2, m2 in enumerate(placement[dst]):
                    w2, s2 = m2.req_rate/m2.slo, m2.model_size

                    ns_src = gpu_s[src] - s1 + s2
                    ns_dst = gpu_s[dst] - s2 + s1
                    if ns_src > GPU_MEM_SIZE or ns_dst > GPU_MEM_SIZE: continue

                    rem_src = GPU_MEM_SIZE - ns_src
                    nk_src = (gpu_w[src] - w1 + w2) / rem_src if rem_src > 1e-7 else 1e9

                    rem_dst = GPU_MEM_SIZE - ns_dst
                    nk_dst = (gpu_w[dst] - w2 + w1) / rem_dst if rem_dst > 1e-7 else 1e9

                    new_peak = max(nk_src, nk_dst)
                    peak_red = max_k - new_peak

                    if peak_red > 1e-7:
                        if peak_red > best_peak_red + 1e-7:
                            best_peak_red = peak_red
                            best_move = ('swap', src, i1, dst, i2, s1, w1, s2, w2)
                            best_var_red = 0
                    elif abs(peak_red) <= 1e-7:
                        old_sq = current_k_src**2 + current_ks[dst]**2
                        new_sq = nk_src**2 + nk_dst**2
                        var_red = old_sq - new_sq

                        if abs(best_peak_red) < 1e-7 and var_red > best_var_red:
                            best_var_red = var_red
                            best_move = ('swap', src, i1, dst, i2, s1, w1, s2, w2)
                            best_peak_red = 0

        if best_move:
            if best_move[0] == 'move':
                _, s_idx, m_idx, d_idx, s, w = best_move
                m = placement[s_idx].pop(m_idx)
                placement[d_idx].append(m)
                gpu_s[s_idx] -= s; gpu_w[s_idx] -= w
                gpu_s[d_idx] += s; gpu_w[d_idx] += w
            elif best_move[0] == 'swap':
                _, s_idx, m1_idx, d_idx, m2_idx, s1, w1, s2, w2 = best_move
                m1 = placement[s_idx][m1_idx]
                m2 = placement[d_idx][m2_idx]
                placement[s_idx][m1_idx] = m2
                placement[d_idx][m2_idx] = m1
                gpu_s[s_idx] += (s2 - s1); gpu_w[s_idx] += (w2 - w1)
                gpu_s[d_idx] += (s1 - s2); gpu_w[d_idx] += (w1 - w2)

            no_improve = 0
            continue

        # 3. Perturbation (Kick) if Local Optimum reached
        if no_improve > patience:
            # Ruin & Recreate on Bottleneck + 2 Random GPUs
            targets = {src}
            candidates = [x for x in range(gpu_num) if x != src]
            if candidates:
                targets.update(random.sample(candidates, min(len(candidates), 2)))

            removed = []
            for t in targets:
                removed.extend(placement[t])
                placement[t] = []
                gpu_s[t] = 0.0; gpu_w[t] = 0.0

            # Sort heuristic: Weight density + size penalty relative to current bottleneck
            alpha = max_k if max_k > 0 else 1.0
            removed.sort(key=lambda m: (m.req_rate/m.slo + alpha * m.model_size) * random.uniform(0.9, 1.1), reverse=True)

            for m in removed:
                best_t = -1
                best_v = float('inf')

                for t in targets:
                    if gpu_s[t] + m.model_size <= GPU_MEM_SIZE:
                        rem = GPU_MEM_SIZE - (gpu_s[t] + m.model_size)
                        k = (gpu_w[t] + m.req_rate/m.slo) / rem if rem > 1e-7 else 1e9
                        if k < best_v:
                            best_v = k
                            best_t = t

                if best_t != -1:
                    placement[best_t].append(m)
                    gpu_s[best_t] += m.model_size
                    gpu_w[best_t] += m.req_rate/m.slo
                else:
                    # Fallback
                    for t in targets:
                         if gpu_s[t] + m.model_size <= GPU_MEM_SIZE:
                            placement[t].append(m)
                            gpu_s[t] += m.model_size
                            gpu_w[t] += m.req_rate/m.slo
                            break
            no_improve = 0

    return best_placement
>>>>>>> REPLACE
</DIFF>