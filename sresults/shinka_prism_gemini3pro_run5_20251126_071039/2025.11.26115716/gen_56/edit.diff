--- a/original.py
+++ b/original.py
@@ -1,294 +1,432 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs using Binary Search with Multi-Strategy Packing and Local Search Refinement"""
 
 import copy
 import random
 
 GPU_MEM_SIZE = 80.0  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Minimizes max KVPR using Robust Binary Search with multiple packing heuristics
     followed by Steepest Descent Iterated Local Search (Descent + Ruin & Recreate).
     """
     # 1. Validation
     total_size = sum(m.model_size for m in models)
     if total_size > gpu_num * GPU_MEM_SIZE:
         raise ValueError("Total model size exceeds total GPU memory capacity.")
 
     # Prepare items: (w, s, m)
     items = [{'w': m.req_rate / m.slo, 's': m.model_size, 'm': m} for m in models]
 
     # 2. Binary Search
     total_w = sum(x['w'] for x in items)
     slack = gpu_num * GPU_MEM_SIZE - total_size
 
     low = 0.0
     if slack < 1e-6:
         high = 10000.0
     else:
         avg_pressure = total_w / slack
         high = max(10.0, avg_pressure * 8.0)
 
     best_placement = None
     feasible_high = False
 
     # Find valid upper bound
     for _ in range(20):
         feasible, placement = _check_feasibility_robust(gpu_num, items, high)
         if feasible:
             best_placement = placement
             feasible_high = True
             break
         low = high
         high *= 2.0
 
     if not feasible_high:
         raise ValueError("Unable to place models. Constraints likely too tight.")
 
     # Binary Search
     for _ in range(30):
         mid = (low + high) / 2.0
         feasible, placement = _check_feasibility_robust(gpu_num, items, mid)
         if feasible:
             best_placement = placement
             high = mid
         else:
             low = mid
 
     placement_map = {i: best_placement[i] for i in range(gpu_num)}
 
     # 3. Iterated Local Search Refinement
     return _iterated_local_search(gpu_num, placement_map)
 
 def _check_feasibility_robust(gpu_num, items, K):
     """
     Check feasibility using multiple sorting strategies and packing algorithms.
     """
     virtual_cap = K * GPU_MEM_SIZE
     # Augment items for sorting
     pack_items = []
     for x in items:
         v = x['w'] + K * x['s']
         d = x['w'] / (x['s'] + 1e-7)
         pack_items.append({'v': v, 's': x['s'], 'w': x['w'], 'd': d, 'm': x['m']})
 
     # Heuristics: (key_lambda, reverse)
     heuristics = [
         (lambda x: x['v'], True),  # Virtual Size Desc
         (lambda x: x['s'], True),  # Physical Size Desc
         (lambda x: x['d'], True),  # Density Desc
         (lambda x: x['w'], True),  # Load Desc
     ]
 
     for key_func, rev in heuristics:
         sorted_items = sorted(pack_items, key=key_func, reverse=rev)
 
         # Try Best Fit Decreasing (usually better for tight bins)
         if res := _pack_bfd(gpu_num, sorted_items, virtual_cap):
             return True, res
 
         # Try First Fit Decreasing
         if res := _pack_ffd(gpu_num, sorted_items, virtual_cap):
             return True, res
 
     return False, None
 
 def _pack_ffd(gpu_num, items, virtual_cap):
     bins_v = [0.0] * gpu_num
     bins_p = [0.0] * gpu_num
     placement = [[] for _ in range(gpu_num)]
 
     for item in items:
         placed = False
         for i in range(gpu_num):
             if bins_p[i] + item['s'] <= GPU_MEM_SIZE and bins_v[i] + item['v'] <= virtual_cap + 1e-7:
                 bins_p[i] += item['s']
                 bins_v[i] += item['v']
                 placement[i].append(item['m'])
                 placed = True
                 break
         if not placed: return None
     return placement
 
 def _pack_bfd(gpu_num, items, virtual_cap):
     bins_v = [0.0] * gpu_num
     bins_p = [0.0] * gpu_num
     placement = [[] for _ in range(gpu_num)]
 
     for item in items:
         best_bin = -1
         min_rem = float('inf')
 
         for i in range(gpu_num):
             if bins_p[i] + item['s'] <= GPU_MEM_SIZE and bins_v[i] + item['v'] <= virtual_cap + 1e-7:
                 rem = virtual_cap - (bins_v[i] + item['v'])
                 if rem < min_rem:
                     min_rem = rem
                     best_bin = i
 
         if best_bin != -1:
             bins_p[best_bin] += item['s']
             bins_v[best_bin] += item['v']
             placement[best_bin].append(item['m'])
         else:
             return None
     return placement
 
 def _iterated_local_search(gpu_num, placement):
     """
-    Refines placement using Ruin and Recreate.
+    Refines placement using Hybrid ILS: Steepest Descent (Move/Swap) + Ruin & Recreate.
+    Optimizes for Min(Max K) and breaks ties with Variance(K).
     """
     # Initialize state
     gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
     gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]
 
     def get_k(i):
         rem = GPU_MEM_SIZE - gpu_s[i]
         if rem <= 1e-7: return 1e9
         return gpu_w[i] / rem
 
-    best_max_k = max(get_k(i) for i in range(gpu_num))
+    current_ks = [get_k(i) for i in range(gpu_num)]
+    max_k = max(current_ks)
+    sum_sq = sum(k*k for k in current_ks)
+
+    best_max_k = max_k
     best_placement = copy.deepcopy(placement)
 
-    max_steps = 300
-    patience = 30
+    max_steps = 1000
+    patience = 50
     no_improve = 0
 
-    for _ in range(max_steps):
-        # Identify bottleneck
-        current_ks = [get_k(i) for i in range(gpu_num)]
-        max_k = max(current_ks)
-
-        # Check global improvement
-        if max_k < best_max_k - 1e-6:
+    for step in range(max_steps):
+        # 0. Check Global Improvement
+        if max_k < best_max_k - 1e-7:
             best_max_k = max_k
             best_placement = copy.deepcopy(placement)
             no_improve = 0
         else:
             no_improve += 1
 
-        # Ruin & Recreate
-        # Select bottleneck and random others
-        candidates = [i for i, k in enumerate(current_ks) if k > max_k * 0.99]
-        if not candidates: candidates = [random.randint(0, gpu_num-1)]
-        src = random.choice(candidates)
-
+        # 1. Identify Bottleneck
+        # Pick one of the GPUs with max_k
+        bottlenecks = [i for i, k in enumerate(current_ks) if k >= max_k - 1e-7]
+        src = random.choice(bottlenecks)
+
+        # 2. Descent Phase (Moves & Swaps from src)
+        best_move = None  # (type, args...)
+        # We want to improve the situation at 'src' without creating a worse bottleneck elsewhere.
+        # Score = (Reduction in Max K, Reduction in Sum Sq)
+
+        # 2a. Try Moving a model from src to dst
+        if len(placement[src]) > 0:
+            for m_idx, m in enumerate(placement[src]):
+                m_s, m_w = m.model_size, m.req_rate / m.slo
+
+                for dst in range(gpu_num):
+                    if dst == src: continue
+                    if gpu_s[dst] + m_s > GPU_MEM_SIZE: continue
+
+                    # New Ks
+                    rem_src = GPU_MEM_SIZE - (gpu_s[src] - m_s)
+                    nk_src = (gpu_w[src] - m_w) / rem_src if rem_src > 1e-7 else 1e9
+
+                    rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + m_s)
+                    nk_dst = (gpu_w[dst] + m_w) / rem_dst if rem_dst > 1e-7 else 1e9
+
+                    new_local_max = max(nk_src, nk_dst)
+
+                    # Valid move?
+                    if new_local_max < max_k + 1e-7:
+                        # Check gain
+                        # Gain 1: Reduction in peak
+                        peak_red = max_k - new_local_max
+
+                        # Gain 2: Reduction in var
+                        # old contribution: current_ks[src]**2 + current_ks[dst]**2
+                        # new contribution: nk_src**2 + nk_dst**2
+                        var_red = (current_ks[src]**2 + current_ks[dst]**2) - (nk_src**2 + nk_dst**2)
+
+                        if peak_red > 1e-7:
+                            # Immediate improvement in bottleneck
+                            best_move = ('move', m_idx, dst, m_s, m_w, nk_src, nk_dst)
+                            break # Greedy first improvement for speed
+                        elif peak_red > -1e-7 and var_red > 1e-5:
+                             # Equal peak, better variance
+                             best_move = ('move', m_idx, dst, m_s, m_w, nk_src, nk_dst)
+                if best_move: break
+
+        # 2b. Try Swapping (if no move found)
+        if not best_move and len(placement[src]) > 0:
+            # Limit swap search for speed: only swap with non-bottleneck or significantly better
+            for i1, m1 in enumerate(placement[src]):
+                s1, w1 = m1.model_size, m1.req_rate / m1.slo
+                for dst in range(gpu_num):
+                    if dst == src: continue
+                    # Optimization: Don't swap with another bottleneck unless it helps
+                    if current_ks[dst] > max_k * 0.99: continue
+
+                    for i2, m2 in enumerate(placement[dst]):
+                        s2, w2 = m2.model_size, m2.req_rate / m2.slo
+
+                        ns_src = gpu_s[src] - s1 + s2
+                        ns_dst = gpu_s[dst] - s2 + s1
+                        if ns_src > GPU_MEM_SIZE or ns_dst > GPU_MEM_SIZE: continue
+
+                        rem_src = GPU_MEM_SIZE - ns_src
+                        nk_src = (gpu_w[src] - w1 + w2) / rem_src if rem_src > 1e-7 else 1e9
+
+                        rem_dst = GPU_MEM_SIZE - ns_dst
+                        nk_dst = (gpu_w[dst] - w2 + w1) / rem_dst if rem_dst > 1e-7 else 1e9
+
+                        new_local_max = max(nk_src, nk_dst)
+
+                        if new_local_max < max_k + 1e-7:
+                            peak_red = max_k - new_local_max
+                            var_red = (current_ks[src]**2 + current_ks[dst]**2) - (nk_src**2 + nk_dst**2)
+
+                            if peak_red > 1e-7:
+                                best_move = ('swap', i1, dst, i2, s1, w1, s2, w2, ns_src, ns_dst, nk_src, nk_dst)
+                                break
+                            elif peak_red > -1e-7 and var_red > 1e-5:
+                                best_move = ('swap', i1, dst, i2, s1, w1, s2, w2, ns_src, ns_dst, nk_src, nk_dst)
+                    if best_move: break
+                if best_move: break
+
+        # Execute Descent
+        if best_move:
+            if best_move[0] == 'move':
+                _, m_idx, dst, m_s, m_w, nk_src, nk_dst = best_move
+                m = placement[src].pop(m_idx)
+                placement[dst].append(m)
+                gpu_s[src] -= m_s; gpu_w[src] -= m_w
+                gpu_s[dst] += m_s; gpu_w[dst] += m_w
+
+                # Update stats
+                sum_sq = sum_sq - current_ks[src]**2 - current_ks[dst]**2 + nk_src**2 + nk_dst**2
+                current_ks[src] = nk_src
+                current_ks[dst] = nk_dst
+                max_k = max(current_ks)
+                no_improve = 0
+                continue # Iterate
+            elif best_move[0] == 'swap':
+                _, i1, dst, i2, s1, w1, s2, w2, ns_src, ns_dst, nk_src, nk_dst = best_move
+                m1 = placement[src][i1]
+                m2 = placement[dst][i2]
+                placement[src][i1] = m2
+                placement[dst][i2] = m1
+                gpu_s[src] = ns_src; gpu_w[src] = gpu_w[src] - w1 + w2
+                gpu_s[dst] = ns_dst; gpu_w[dst] = gpu_w[dst] - w2 + w1
+
+                sum_sq = sum_sq - current_ks[src]**2 - current_ks[dst]**2 + nk_src**2 + nk_dst**2
+                current_ks[src] = nk_src
+                current_ks[dst] = nk_dst
+                max_k = max(current_ks)
+                no_improve = 0
+                continue
+
+        # 3. Perturbation (Ruin & Recreate) if Descent stuck
+        # Ruin src + randoms
         ruin_set = {src}
-        n_ruin = 2 + (1 if no_improve > patience else 0)
+        # Increase perturbation size if stuck for long
+        n_ruin = 2 + (2 if no_improve > patience else 0)
 
         others = list(range(gpu_num))
         random.shuffle(others)
         for o in others:
             if len(ruin_set) >= min(gpu_num, n_ruin): break
             if o != src: ruin_set.add(o)
 
-        # Ruin
+        # Backup
+        backup = {}
         removed_models = []
-        backup = {}
+        old_subset_sq = 0.0
+
         for idx in ruin_set:
-            backup[idx] = (list(placement[idx]), gpu_s[idx], gpu_w[idx])
+            backup[idx] = (list(placement[idx]), gpu_s[idx], gpu_w[idx], current_ks[idx])
             removed_models.extend(placement[idx])
+            old_subset_sq += current_ks[idx]**2
+            # Clear
             placement[idx] = []
             gpu_s[idx] = 0.0
             gpu_w[idx] = 0.0
 
-        # Recreate (Greedy Best Fit)
-        # Sort by physical size desc to pack well
-        removed_models.sort(key=lambda m: m.model_size, reverse=True)
+        # Recreate - Heuristic Sort
+        # Sort by Pressure Contribution (w) first to place heavy items, then Size
+        removed_models.sort(key=lambda m: (m.req_rate/m.slo, m.model_size), reverse=True)
 
         feasible = True
         for m in removed_models:
             best_idx = -1
             best_score = float('inf')
-
-            m_s = m.model_size
-            m_w = m.req_rate / m.slo
+            m_s, m_w = m.model_size, m.req_rate / m.slo
 
             for idx in ruin_set:
                 if gpu_s[idx] + m_s <= GPU_MEM_SIZE:
-                    # Score: minimizing resulting local K
                     rem = GPU_MEM_SIZE - (gpu_s[idx] + m_s)
                     k = (gpu_w[idx] + m_w) / rem if rem > 1e-7 else 1e9
                     if k < best_score:
                         best_score = k
                         best_idx = idx
 
             if best_idx != -1:
                 placement[best_idx].append(m)
                 gpu_s[best_idx] += m_s
                 gpu_w[best_idx] += m_w
             else:
                 feasible = False
                 break
 
-        # Acceptance
+        # Evaluate Recreate
         accept = False
         if feasible:
-            new_ks = [get_k(i) for i in range(gpu_num)]
-            new_max = max(new_ks)
-            if new_max < max_k - 1e-6:
+            # Calculate new subset stats
+            new_subset_sq = 0.0
+            new_subset_max = 0.0
+            for idx in ruin_set:
+                k = get_k(idx)
+                current_ks[idx] = k # Update temp
+                new_subset_sq += k*k
+                if k > new_subset_max: new_subset_max = k
+
+            # Check global max (approximate check: if new_subset_max < max_k, we likely improved or stayed same)
+            # We need strictly correct max_k
+            new_max_k = max(current_ks)
+
+            # Acceptance Criteria
+            if new_max_k < max_k - 1e-7:
                 accept = True
-            elif new_max < max_k + 1e-6:
-                # Accept equal moves to traverse plateau
-                accept = True
-            elif no_improve > patience and random.random() < 0.2:
-                # Random walk
-                accept = True
-
-        if not accept:
+            elif new_max_k < max_k + 1e-7:
+                # Tie-break with variance
+                # Check global variance change
+                delta_sq = new_subset_sq - old_subset_sq
+                if delta_sq < -1e-5:
+                    accept = True
+                elif no_improve > patience and random.random() < 0.2:
+                    accept = True # Random walk
+
+            if accept:
+                max_k = new_max_k
+                sum_sq = sum_sq - old_subset_sq + new_subset_sq
+                if new_max_k < max_k - 1e-7: no_improve = 0
+            else:
+                # Revert
+                for idx in ruin_set:
+                    placement[idx], gpu_s[idx], gpu_w[idx], current_ks[idx] = backup[idx]
+        else:
             # Revert
             for idx in ruin_set:
-                placement[idx], gpu_s[idx], gpu_w[idx] = backup[idx]
+                placement[idx], gpu_s[idx], gpu_w[idx], current_ks[idx] = backup[idx]
 
     return best_placement
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")