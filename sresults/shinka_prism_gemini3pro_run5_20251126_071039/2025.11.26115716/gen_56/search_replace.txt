The following changes are proposed to improve the `compute_model_placement` algorithm:

<NAME>
hybrid_ils_lns_refinement
</NAME>

<DESCRIPTION>
Replaces the random-walk based ILS with a Hybrid ILS strategy.
1. The search uses a "Steepest Descent" phase that explicitly attempts to move or swap models from the bottleneck GPU to improve the maximum KVPR or reduce variance (Sum of Squares of KVPR) as a tie-breaker.
2. If the descent is stuck (local optimum), it triggers a "Ruin & Recreate" perturbation on the bottleneck GPU and a few random neighbors.
3. The Recreate phase uses a heuristic that sorts models by 'pressure' (req_rate/slo) descending to better balance high-load models.
4. Increases iteration count to 1000 to leverage the fast execution time.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def _iterated_local_search(gpu_num, placement):
    """
    Refines placement using Ruin and Recreate.
    """
    # Initialize state
    gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
    gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]

    def get_k(i):
        rem = GPU_MEM_SIZE - gpu_s[i]
        if rem <= 1e-7: return 1e9
        return gpu_w[i] / rem

    best_max_k = max(get_k(i) for i in range(gpu_num))
    best_placement = copy.deepcopy(placement)

    max_steps = 300
    patience = 30
    no_improve = 0

    for _ in range(max_steps):
        # Identify bottleneck
        current_ks = [get_k(i) for i in range(gpu_num)]
        max_k = max(current_ks)

        # Check global improvement
        if max_k < best_max_k - 1e-6:
            best_max_k = max_k
            best_placement = copy.deepcopy(placement)
            no_improve = 0
        else:
            no_improve += 1

        # Ruin & Recreate
        # Select bottleneck and random others
        candidates = [i for i, k in enumerate(current_ks) if k > max_k * 0.99]
        if not candidates: candidates = [random.randint(0, gpu_num-1)]
        src = random.choice(candidates)

        ruin_set = {src}
        n_ruin = 2 + (1 if no_improve > patience else 0)

        others = list(range(gpu_num))
        random.shuffle(others)
        for o in others:
            if len(ruin_set) >= min(gpu_num, n_ruin): break
            if o != src: ruin_set.add(o)

        # Ruin
        removed_models = []
        backup = {}
        for idx in ruin_set:
            backup[idx] = (list(placement[idx]), gpu_s[idx], gpu_w[idx])
            removed_models.extend(placement[idx])
            placement[idx] = []
            gpu_s[idx] = 0.0
            gpu_w[idx] = 0.0

        # Recreate (Greedy Best Fit)
        # Sort by physical size desc to pack well
        removed_models.sort(key=lambda m: m.model_size, reverse=True)

        feasible = True
        for m in removed_models:
            best_idx = -1
            best_score = float('inf')

            m_s = m.model_size
            m_w = m.req_rate / m.slo

            for idx in ruin_set:
                if gpu_s[idx] + m_s <= GPU_MEM_SIZE:
                    # Score: minimizing resulting local K
                    rem = GPU_MEM_SIZE - (gpu_s[idx] + m_s)
                    k = (gpu_w[idx] + m_w) / rem if rem > 1e-7 else 1e9
                    if k < best_score:
                        best_score = k
                        best_idx = idx

            if best_idx != -1:
                placement[best_idx].append(m)
                gpu_s[best_idx] += m_s
                gpu_w[best_idx] += m_w
            else:
                feasible = False
                break

        # Acceptance
        accept = False
        if feasible:
            new_ks = [get_k(i) for i in range(gpu_num)]
            new_max = max(new_ks)
            if new_max < max_k - 1e-6:
                accept = True
            elif new_max < max_k + 1e-6:
                # Accept equal moves to traverse plateau
                accept = True
            elif no_improve > patience and random.random() < 0.2:
                # Random walk
                accept = True

        if not accept:
            # Revert
            for idx in ruin_set:
                placement[idx], gpu_s[idx], gpu_w[idx] = backup[idx]

    return best_placement
=======
def _iterated_local_search(gpu_num, placement):
    """
    Refines placement using Hybrid ILS: Steepest Descent (Move/Swap) + Ruin & Recreate.
    Optimizes for Min(Max K) and breaks ties with Variance(K).
    """
    # Initialize state
    gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
    gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]

    def get_k(i):
        rem = GPU_MEM_SIZE - gpu_s[i]
        if rem <= 1e-7: return 1e9
        return gpu_w[i] / rem

    current_ks = [get_k(i) for i in range(gpu_num)]
    max_k = max(current_ks)
    sum_sq = sum(k*k for k in current_ks)

    best_max_k = max_k
    best_placement = copy.deepcopy(placement)

    max_steps = 1000
    patience = 50
    no_improve = 0

    for step in range(max_steps):
        # 0. Check Global Improvement
        if max_k < best_max_k - 1e-7:
            best_max_k = max_k
            best_placement = copy.deepcopy(placement)
            no_improve = 0
        else:
            no_improve += 1

        # 1. Identify Bottleneck
        # Pick one of the GPUs with max_k
        bottlenecks = [i for i, k in enumerate(current_ks) if k >= max_k - 1e-7]
        src = random.choice(bottlenecks)

        # 2. Descent Phase (Moves & Swaps from src)
        best_move = None  # (type, args...)
        # We want to improve the situation at 'src' without creating a worse bottleneck elsewhere.
        # Score = (Reduction in Max K, Reduction in Sum Sq)

        # 2a. Try Moving a model from src to dst
        if len(placement[src]) > 0:
            for m_idx, m in enumerate(placement[src]):
                m_s, m_w = m.model_size, m.req_rate / m.slo

                for dst in range(gpu_num):
                    if dst == src: continue
                    if gpu_s[dst] + m_s > GPU_MEM_SIZE: continue

                    # New Ks
                    rem_src = GPU_MEM_SIZE - (gpu_s[src] - m_s)
                    nk_src = (gpu_w[src] - m_w) / rem_src if rem_src > 1e-7 else 1e9

                    rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + m_s)
                    nk_dst = (gpu_w[dst] + m_w) / rem_dst if rem_dst > 1e-7 else 1e9

                    new_local_max = max(nk_src, nk_dst)

                    # Valid move?
                    if new_local_max < max_k + 1e-7:
                        # Check gain
                        # Gain 1: Reduction in peak
                        peak_red = max_k - new_local_max

                        # Gain 2: Reduction in var
                        # old contribution: current_ks[src]**2 + current_ks[dst]**2
                        # new contribution: nk_src**2 + nk_dst**2
                        var_red = (current_ks[src]**2 + current_ks[dst]**2) - (nk_src**2 + nk_dst**2)

                        if peak_red > 1e-7:
                            # Immediate improvement in bottleneck
                            best_move = ('move', m_idx, dst, m_s, m_w, nk_src, nk_dst)
                            break # Greedy first improvement for speed
                        elif peak_red > -1e-7 and var_red > 1e-5:
                             # Equal peak, better variance
                             best_move = ('move', m_idx, dst, m_s, m_w, nk_src, nk_dst)
                if best_move: break

        # 2b. Try Swapping (if no move found)
        if not best_move and len(placement[src]) > 0:
            # Limit swap search for speed: only swap with non-bottleneck or significantly better
            for i1, m1 in enumerate(placement[src]):
                s1, w1 = m1.model_size, m1.req_rate / m1.slo
                for dst in range(gpu_num):
                    if dst == src: continue
                    # Optimization: Don't swap with another bottleneck unless it helps
                    if current_ks[dst] > max_k * 0.99: continue

                    for i2, m2 in enumerate(placement[dst]):
                        s2, w2 = m2.model_size, m2.req_rate / m2.slo

                        ns_src = gpu_s[src] - s1 + s2
                        ns_dst = gpu_s[dst] - s2 + s1
                        if ns_src > GPU_MEM_SIZE or ns_dst > GPU_MEM_SIZE: continue

                        rem_src = GPU_MEM_SIZE - ns_src
                        nk_src = (gpu_w[src] - w1 + w2) / rem_src if rem_src > 1e-7 else 1e9

                        rem_dst = GPU_MEM_SIZE - ns_dst
                        nk_dst = (gpu_w[dst] - w2 + w1) / rem_dst if rem_dst > 1e-7 else 1e9

                        new_local_max = max(nk_src, nk_dst)

                        if new_local_max < max_k + 1e-7:
                            peak_red = max_k - new_local_max
                            var_red = (current_ks[src]**2 + current_ks[dst]**2) - (nk_src**2 + nk_dst**2)

                            if peak_red > 1e-7:
                                best_move = ('swap', i1, dst, i2, s1, w1, s2, w2, ns_src, ns_dst, nk_src, nk_dst)
                                break
                            elif peak_red > -1e-7 and var_red > 1e-5:
                                best_move = ('swap', i1, dst, i2, s1, w1, s2, w2, ns_src, ns_dst, nk_src, nk_dst)
                    if best_move: break
                if best_move: break

        # Execute Descent
        if best_move:
            if best_move[0] == 'move':
                _, m_idx, dst, m_s, m_w, nk_src, nk_dst = best_move
                m = placement[src].pop(m_idx)
                placement[dst].append(m)
                gpu_s[src] -= m_s; gpu_w[src] -= m_w
                gpu_s[dst] += m_s; gpu_w[dst] += m_w

                # Update stats
                sum_sq = sum_sq - current_ks[src]**2 - current_ks[dst]**2 + nk_src**2 + nk_dst**2
                current_ks[src] = nk_src
                current_ks[dst] = nk_dst
                max_k = max(current_ks)
                no_improve = 0
                continue # Iterate
            elif best_move[0] == 'swap':
                _, i1, dst, i2, s1, w1, s2, w2, ns_src, ns_dst, nk_src, nk_dst = best_move
                m1 = placement[src][i1]
                m2 = placement[dst][i2]
                placement[src][i1] = m2
                placement[dst][i2] = m1
                gpu_s[src] = ns_src; gpu_w[src] = gpu_w[src] - w1 + w2
                gpu_s[dst] = ns_dst; gpu_w[dst] = gpu_w[dst] - w2 + w1

                sum_sq = sum_sq - current_ks[src]**2 - current_ks[dst]**2 + nk_src**2 + nk_dst**2
                current_ks[src] = nk_src
                current_ks[dst] = nk_dst
                max_k = max(current_ks)
                no_improve = 0
                continue

        # 3. Perturbation (Ruin & Recreate) if Descent stuck
        # Ruin src + randoms
        ruin_set = {src}
        # Increase perturbation size if stuck for long
        n_ruin = 2 + (2 if no_improve > patience else 0)

        others = list(range(gpu_num))
        random.shuffle(others)
        for o in others:
            if len(ruin_set) >= min(gpu_num, n_ruin): break
            if o != src: ruin_set.add(o)

        # Backup
        backup = {}
        removed_models = []
        old_subset_sq = 0.0

        for idx in ruin_set:
            backup[idx] = (list(placement[idx]), gpu_s[idx], gpu_w[idx], current_ks[idx])
            removed_models.extend(placement[idx])
            old_subset_sq += current_ks[idx]**2
            # Clear
            placement[idx] = []
            gpu_s[idx] = 0.0
            gpu_w[idx] = 0.0

        # Recreate - Heuristic Sort
        # Sort by Pressure Contribution (w) first to place heavy items, then Size
        removed_models.sort(key=lambda m: (m.req_rate/m.slo, m.model_size), reverse=True)

        feasible = True
        for m in removed_models:
            best_idx = -1
            best_score = float('inf')
            m_s, m_w = m.model_size, m.req_rate / m.slo

            for idx in ruin_set:
                if gpu_s[idx] + m_s <= GPU_MEM_SIZE:
                    rem = GPU_MEM_SIZE - (gpu_s[idx] + m_s)
                    k = (gpu_w[idx] + m_w) / rem if rem > 1e-7 else 1e9
                    if k < best_score:
                        best_score = k
                        best_idx = idx

            if best_idx != -1:
                placement[best_idx].append(m)
                gpu_s[best_idx] += m_s
                gpu_w[best_idx] += m_w
            else:
                feasible = False
                break

        # Evaluate Recreate
        accept = False
        if feasible:
            # Calculate new subset stats
            new_subset_sq = 0.0
            new_subset_max = 0.0
            for idx in ruin_set:
                k = get_k(idx)
                current_ks[idx] = k # Update temp
                new_subset_sq += k*k
                if k > new_subset_max: new_subset_max = k

            # Check global max (approximate check: if new_subset_max < max_k, we likely improved or stayed same)
            # We need strictly correct max_k
            new_max_k = max(current_ks)

            # Acceptance Criteria
            if new_max_k < max_k - 1e-7:
                accept = True
            elif new_max_k < max_k + 1e-7:
                # Tie-break with variance
                # Check global variance change
                delta_sq = new_subset_sq - old_subset_sq
                if delta_sq < -1e-5:
                    accept = True
                elif no_improve > patience and random.random() < 0.2:
                    accept = True # Random walk

            if accept:
                max_k = new_max_k
                sum_sq = sum_sq - old_subset_sq + new_subset_sq
                if new_max_k < max_k - 1e-7: no_improve = 0
            else:
                # Revert
                for idx in ruin_set:
                    placement[idx], gpu_s[idx], gpu_w[idx], current_ks[idx] = backup[idx]
        else:
            # Revert
            for idx in ruin_set:
                placement[idx], gpu_s[idx], gpu_w[idx], current_ks[idx] = backup[idx]

    return best_placement
>>>>>>> REPLACE
</DIFF>