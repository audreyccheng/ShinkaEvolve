--- a/original.py
+++ b/original.py
@@ -1,500 +1,435 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 import random
 import math
 
 GPU_MEM_SIZE = 80  # GB
 
 def compute_model_placement(gpu_num, models):
     """
-    Compute a model placement that minimizes the maximum KVPR across all GPUs.
-    Architecture:
-    1. Lower Bound Calculation
-    2. Binary Search with Multi-Strategy Best-Fit Decreasing
-    3. Iterated Local Search (Descent + Ruin & Recreate)
+    Minimizes maximum KV cache pressure using Binary Search on effective capacity
+    followed by Lexicographical Local Search optimization.
     """
-    rng = random.Random(42)
-
-    # 1. Data Preparation
+    
+    # 1. Setup and Preprocessing
     m_data = []
-    total_w = 0.0
-    total_s = 0.0
     for i, m in enumerate(models):
         w = m.req_rate / m.slo
         s = m.model_size
-        d = w / s if s > 1e-6 else 0.0
+        # Primary sort key for heuristics: Density
+        d = w / s if s > 1e-6 else 0
         m_data.append({'w': w, 's': s, 'd': d, 'obj': m, 'id': i})
-        total_w += w
-        total_s += s
-
-    # 2. Bound Estimation
-    # Ideal average pressure
-    rem_global = (gpu_num * GPU_MEM_SIZE) - total_s
-    if rem_global <= 1e-6:
-        low_bound = 0.0
-    else:
-        low_bound = total_w / rem_global
-
-    # Upper bound heuristic
-    def get_greedy_bound(key_func):
-        sorted_items = sorted(m_data, key=key_func, reverse=True)
-        # Simple Best Fit logic trying to minimize pressure increase
-        l = [0.0]*gpu_num
-        u = [0.0]*gpu_num
-
+
+    # Helper for pressure
+    def calc_pressure(l, u):
+        rem = GPU_MEM_SIZE - u
+        if rem <= 1e-6:
+            # If load > 0 and rem ~ 0, pressure is infinite. 
+            # If load = 0 and rem ~ 0, pressure is 0.
+            return float('inf') if l > 1e-7 else 0.0
+        return l / rem
+
+    # 2. Initial Solution & Bounds
+    # We generate an initial solution using heuristics to get a tight upper bound
+    best_placement = None
+    min_max_p = float('inf')
+
+    # Heuristic Packing strategies
+    # Strategy: Sort items, then place in bin that minimizes local pressure increase
+    # Keys: Density, Load, Size
+    sort_keys = [
+        lambda x: x['d'], 
+        lambda x: x['w'], 
+        lambda x: x['s']
+    ]
+
+    for key_fn in sort_keys:
+        sorted_items = sorted(m_data, key=key_fn, reverse=True)
+        current_alloc = [[] for _ in range(gpu_num)]
+        cur_l = [0.0] * gpu_num
+        cur_u = [0.0] * gpu_num
+        possible = True
+        
         for item in sorted_items:
             best_g = -1
-            best_score = float('inf')
-
+            best_p = float('inf')
+            
+            # Find best bin (Min-Pressure Fit)
             for g in range(gpu_num):
-                rem = GPU_MEM_SIZE - u[g] - item['s']
-                if rem > 1e-5:
-                    # Score: resulting pressure
-                    p = (l[g] + item['w']) / rem
-                    if p < best_score:
-                        best_score = p
+                if cur_u[g] + item['s'] <= GPU_MEM_SIZE - 1e-6:
+                    # Calculate potential pressure
+                    p = calc_pressure(cur_l[g] + item['w'], cur_u[g] + item['s'])
+                    if p < best_p:
+                        best_p = p
                         best_g = g
-
-            if best_g == -1: return float('inf')
-            l[best_g] += item['w']
-            u[best_g] += item['s']
-
-        return max((l[g]/(GPU_MEM_SIZE - u[g])) if (GPU_MEM_SIZE - u[g]) > 1e-6 else 0 for g in range(gpu_num))
-
-    ub_d = get_greedy_bound(lambda x: x['d'])
-    ub_s = get_greedy_bound(lambda x: x['s'])
-    ub_w = get_greedy_bound(lambda x: x['w'])
-
-    candidates = [b for b in [ub_d, ub_s, ub_w] if b != float('inf')]
-    high_bound = min(candidates) if candidates else 2000.0
-
-    # 3. Binary Search
-    final_placement_lists = None
-
-    # Check function
-    def can_pack(target_k):
-        # We need to pack all items such that (L+w)/(C-U-s) <= K
-        # Strategies: Density, Effective Size (w + K*s), Size, Load
-        strategies = [
-            lambda x: x['w'] + target_k * x['s'],
-            lambda x: x['d'],
-            lambda x: x['s'],
-            lambda x: x['w']
-        ]
-
-        # Deterministic passes
-        for key in strategies:
-            items_s = sorted(m_data, key=key, reverse=True)
-            bins_l = [0.0]*gpu_num
-            bins_u = [0.0]*gpu_num
-            bins_items = [[] for _ in range(gpu_num)]
-            possible = True
-
-            for item in items_s:
+            
+            if best_g != -1:
+                current_alloc[best_g].append(item['obj'])
+                cur_l[best_g] += item['w']
+                cur_u[best_g] += item['s']
+            else:
+                possible = False
+                break
+        
+        if possible:
+            # Evaluate global max pressure
+            current_max = max(calc_pressure(cur_l[g], cur_u[g]) for g in range(gpu_num))
+            if current_max < min_max_p:
+                min_max_p = current_max
+                best_placement = current_alloc
+
+    # If heuristics failed (unlikely), fallback to broad search
+    low_bound = 0.0
+    high_bound = min_max_p if best_placement else 2000.0
+    
+    # 3. Binary Search for Optimal K (Min-Max Pressure)
+    # Problem Transformation: Bin Packing with Item Size = w + K*s, Bin Cap = K*C
+    # Constraint: (L+w)/(C-U-s) <= K  <=>  L+w + K(U+s) <= K*C
+    
+    bs_placement = best_placement # Fallback
+    
+    # Check function for BS
+    def check_feasible(K):
+        eff_cap = K * GPU_MEM_SIZE
+        
+        # Helper for BFD packing
+        def try_pack(items_ordered):
+            bins_eff = [0.0] * gpu_num # Effective load accumulator
+            bins_u = [0.0] * gpu_num   # Physical usage check
+            alloc = [[] for _ in range(gpu_num)]
+            
+            for item in items_ordered:
+                eff_size = item['w'] + K * item['s']
+                
                 best_g = -1
-                min_slack = float('inf')
-
-                # Check all bins
+                min_rem = float('inf')
+                
                 for g in range(gpu_num):
-                    # Physical constraint
-                    if bins_u[g] + item['s'] >= GPU_MEM_SIZE - 1e-6: continue
-
-                    # Pressure constraint
-                    rem = GPU_MEM_SIZE - bins_u[g] - item['s']
-                    max_load = target_k * rem
-                    new_load = bins_l[g] + item['w']
-
-                    if new_load <= max_load + 1e-7:
-                        # Best Fit: minimize slack (unused capacity relative to K)
-                        slack = max_load - new_load
-                        if slack < min_slack:
-                            min_slack = slack
+                    # Check physical constraint
+                    if bins_u[g] + item['s'] > GPU_MEM_SIZE - 1e-6: continue
+                    
+                    # Check effective constraint: Load + Size <= Capacity
+                    if bins_eff[g] + eff_size <= eff_cap + 1e-7:
+                        # Best Fit: Minimize remaining capacity (tightest fit)
+                        rem = eff_cap - (bins_eff[g] + eff_size)
+                        if rem < min_rem:
+                            min_rem = rem
                             best_g = g
-
+                
                 if best_g != -1:
-                    bins_l[best_g] += item['w']
+                    bins_eff[best_g] += eff_size
                     bins_u[best_g] += item['s']
-                    bins_items[best_g].append(item['obj'])
+                    alloc[best_g].append(item['obj'])
                 else:
-                    possible = False
-                    break
-
-            if possible:
-                return bins_items
-
-        # Randomized passes to reduce false negatives
-        # This helps finding a feasible packing for tighter K bounds
-        indices = list(range(len(m_data)))
+                    return None
+            return alloc
+
+        # Strategy 1: Sort by Effective Size Descending (Standard BFD for transformed problem)
+        # As K changes, the effective size weights w and s differently.
+        items_by_eff = sorted(m_data, key=lambda x: x['w'] + K*x['s'], reverse=True)
+        res = try_pack(items_by_eff)
+        if res: return res
+        
+        # Strategy 2: Sort by Density (Heuristic backup)
+        items_by_dens = sorted(m_data, key=lambda x: x['d'], reverse=True)
+        res = try_pack(items_by_dens)
+        if res: return res
+        
+        # Strategy 3: Randomized Shuffles (Handle edge cases where greedy fails)
+        base_items = list(m_data)
+        # Seed logic to make it deterministic for a fixed K, but varying across K if needed
+        rng = random.Random(int(K * 100)) 
         for _ in range(5):
-            rng.shuffle(indices)
-            bins_l = [0.0]*gpu_num
-            bins_u = [0.0]*gpu_num
-            bins_items = [[] for _ in range(gpu_num)]
-            possible = True
-
-            for idx in indices:
-                item = m_data[idx]
-                best_g = -1
-                min_slack = float('inf')
-
-                for g in range(gpu_num):
-                    if bins_u[g] + item['s'] >= GPU_MEM_SIZE - 1e-6: continue
-                    rem = GPU_MEM_SIZE - bins_u[g] - item['s']
-                    max_load = target_k * rem
-                    new_load = bins_l[g] + item['w']
-                    if new_load <= max_load + 1e-7:
-                        slack = max_load - new_load
-                        if slack < min_slack:
-                            min_slack = slack
-                            best_g = g
-
-                if best_g != -1:
-                    bins_l[best_g] += item['w']
-                    bins_u[best_g] += item['s']
-                    bins_items[best_g].append(item['obj'])
-                else:
-                    possible = False
-                    break
-            if possible:
-                return bins_items
-
+            rng.shuffle(base_items)
+            res = try_pack(base_items)
+            if res: return res
+            
         return None
 
-    # BS Loop
-    best_bs_sol = None
-
-    # Try packing with high bound first to ensure feasibility
-    res = can_pack(high_bound)
-    if res:
-        best_bs_sol = res
-    else:
-        high_bound = 5000.0 # Fallback
-
-    for _ in range(20):
+    # BS execution
+    iterations = 20
+    for _ in range(iterations):
         if high_bound - low_bound < 1e-4: break
         mid = (low_bound + high_bound) / 2.0
-        res = can_pack(mid)
+        
+        res = check_feasible(mid)
         if res:
-            best_bs_sol = res
+            bs_placement = res
             high_bound = mid
         else:
             low_bound = mid
-
-    if best_bs_sol is None:
-        # Fallback naive distribution if optimization fails completely
-        best_bs_sol = [[] for _ in range(gpu_num)]
-        g = 0
-        u = [0.0]*gpu_num
-        for m in models:
-            if u[g] + m.model_size < GPU_MEM_SIZE:
-                best_bs_sol[g].append(m)
-                u[g] += m.model_size
-            else:
-                g = (g + 1) % gpu_num
-                best_bs_sol[g].append(m)
-                u[g] += m.model_size
-
-    # Convert to Dict
-    current_placement = {i: list(l) for i, l in enumerate(best_bs_sol)}
-
-    # 4. Iterated Local Search
-
-    # State tracking
-    loads = [0.0]*gpu_num
-    used = [0.0]*gpu_num
+            
+    if bs_placement is None:
+        raise ValueError("No feasible solution found.")
+        
+    # 4. Iterated Local Search (ILS) with Lexicographical Descent
+    
+    # State initialization
+    current_p = {i: list(bs_placement[i]) for i in range(gpu_num)}
+    
+    loads = [0.0] * gpu_num
+    used = [0.0] * gpu_num
     for g in range(gpu_num):
-        for m in current_placement[g]:
+        for m in current_p[g]:
             loads[g] += m.req_rate / m.slo
             used[g] += m.model_size
-
-    def get_p(l, u):
-        r = GPU_MEM_SIZE - u
-        return l/r if r > 1e-6 else (float('inf') if l > 0 else 0.0)
-
-    pressures = [get_p(loads[g], used[g]) for g in range(gpu_num)]
-    best_max_p = max(pressures)
-    best_placement = {k: list(v) for k, v in current_placement.items()}
-
-    max_iters = 200
-
-    for it in range(max_iters):
-        # Update Global
+            
+    # Track pressures
+    pressures = [calc_pressure(loads[g], used[g]) for g in range(gpu_num)]
+    
+    best_global_p = {k: list(v) for k,v in current_p.items()}
+    best_global_max = max(pressures)
+    
+    max_ils_iters = 250
+    
+    for it in range(max_ils_iters):
+        # Current metrics
         curr_max = max(pressures)
-        curr_sq = sum(p*p for p in pressures)
-
-        if curr_max < best_max_p - 1e-7:
-            best_max_p = curr_max
-            best_placement = {k: list(v) for k, v in current_placement.items()}
-
-        # Bottleneck Identification
-        bottleneck = -1
-        max_val = -1.0
-        for g in range(gpu_num):
-            if pressures[g] > max_val:
-                max_val = pressures[g]
-                bottleneck = g
-
-        if bottleneck == -1: break
-
-        # --- Descent Strategy ---
-        bn_items = current_placement[bottleneck]
-
-        # Sort items by density: move high intensity items first
-        bn_indices = sorted(range(len(bn_items)), key=lambda i: (bn_items[i].req_rate/bn_items[i].slo)/(bn_items[i].model_size+1e-6), reverse=True)
-
-        # Sort partners by pressure: move to least loaded first
-        partners = sorted([g for g in range(gpu_num) if g != bottleneck], key=lambda g: pressures[g])
-
-        # Fast "Max Others" excluding bottleneck and partner
-        sorted_p = sorted([(pressures[g], g) for g in range(gpu_num)], reverse=True)
-
-        best_move = None
-        # (type, pt, i_bn, i_pt, nbl, nbu, npl, npu, nm, nsq)
-
-        for partner in partners:
-            # Determine max_others
+        
+        # Update global
+        if curr_max < best_global_max - 1e-7:
+            best_global_max = curr_max
+            best_global_p = {k: list(v) for k,v in current_p.items()}
+            
+        # Identify bottleneck
+        g_indices = sorted(range(gpu_num), key=lambda g: pressures[g], reverse=True)
+        bottleneck = g_indices[0]
+        
+        # --- Descent Phase (Moves/Swaps) ---
+        improved = False
+        
+        # Top-K pressure values for fast comparison
+        # We need the max pressure excluding the GPUs involved in move
+        
+        candidates = [g for g in range(gpu_num) if g != bottleneck]
+        
+        # Try Moving Bottleneck Item -> Candidate
+        best_move = None 
+        # (type, cand, item_idx, swap_idx, n_bn_l, n_bn_u, n_cand_l, n_cand_u, new_max)
+        
+        bn_items = current_p[bottleneck]
+        
+        for cand in candidates:
+            # Max pressure among others
             max_others = 0.0
-            for p_val, p_g in sorted_p:
-                if p_g != bottleneck and p_g != partner:
-                    max_others = p_val
-                    break
-
-            sq_base = curr_sq - pressures[bottleneck]**2 - pressures[partner]**2
-
+            for g in g_indices:
+                if g != bottleneck and g != cand:
+                    max_others = pressures[g]
+                    break # Since g_indices is sorted
+            
             # 1. Move
-            for i in bn_indices:
-                m = bn_items[i]
-                w, s = m.req_rate/m.slo, m.model_size
-
-                if used[partner] + s >= GPU_MEM_SIZE - 1e-6: continue
-
+            for i, item in enumerate(bn_items):
+                w, s = item.req_rate/item.slo, item.model_size
+                
+                if used[cand] + s > GPU_MEM_SIZE - 1e-6: continue
+                
                 n_bl = loads[bottleneck] - w
                 n_bu = used[bottleneck] - s
-                n_pl = loads[partner] + w
-                n_pu = used[partner] + s
-
-                pb = get_p(n_bl, n_bu)
-                pp = get_p(n_pl, n_pu)
-
-                nm = max(max_others, pb, pp)
-
-                if nm > curr_max + 1e-9: continue
-                nsq = sq_base + pb**2 + pp**2
-
-                is_better = False
-                if nm < curr_max - 1e-9: is_better = True
-                elif nm < curr_max + 1e-9 and nsq < curr_sq - 1e-9: is_better = True
-
-                if is_better:
-                    if best_move is None or nm < best_move[8] - 1e-9 or (abs(nm - best_move[8]) < 1e-9 and nsq < best_move[9]):
-                        best_move = ('move', partner, i, -1, n_bl, n_bu, n_pl, n_pu, nm, nsq)
-
-            # 2. Swap
-            pt_items = current_placement[partner]
-            for i in bn_indices:
-                m1 = bn_items[i]
-                w1, s1 = m1.req_rate/m1.slo, m1.model_size
-                for j, m2 in enumerate(pt_items):
-                    w2, s2 = m2.req_rate/m2.slo, m2.model_size
-
+                n_cl = loads[cand] + w
+                n_cu = used[cand] + s
+                
+                pb = calc_pressure(n_bl, n_bu)
+                pc = calc_pressure(n_cl, n_cu)
+                
+                n_max = max(max_others, pb, pc)
+                
+                # Check for strict improvement in max pressure
+                if n_max < curr_max - 1e-8:
+                    if best_move is None or n_max < best_move[8]:
+                        best_move = ('move', cand, i, -1, n_bl, n_bu, n_cl, n_cu, n_max)
+            
+            # 2. Swap (Only if move didn't yield significant improvement or just to explore)
+            cand_items = current_p[cand]
+            for i, item1 in enumerate(bn_items):
+                w1, s1 = item1.req_rate/item1.slo, item1.model_size
+                for j, item2 in enumerate(cand_items):
+                    w2, s2 = item2.req_rate/item2.slo, item2.model_size
+                    
                     n_bu = used[bottleneck] - s1 + s2
-                    if n_bu >= GPU_MEM_SIZE - 1e-6: continue
-                    n_pu = used[partner] - s2 + s1
-                    if n_pu >= GPU_MEM_SIZE - 1e-6: continue
-
+                    if n_bu > GPU_MEM_SIZE - 1e-6: continue
+                    n_cu = used[cand] - s2 + s1
+                    if n_cu > GPU_MEM_SIZE - 1e-6: continue
+                    
                     n_bl = loads[bottleneck] - w1 + w2
-                    n_pl = loads[partner] - w2 + w1
-
-                    pb = get_p(n_bl, n_bu)
-                    pp = get_p(n_pl, n_pu)
-
-                    nm = max(max_others, pb, pp)
-                    if nm > curr_max + 1e-9: continue
-                    nsq = sq_base + pb**2 + pp**2
-
-                    is_better = False
-                    if nm < curr_max - 1e-9: is_better = True
-                    elif nm < curr_max + 1e-9 and nsq < curr_sq - 1e-9: is_better = True
-
-                    if is_better:
-                        if best_move is None or nm < best_move[8] - 1e-9 or (abs(nm - best_move[8]) < 1e-9 and nsq < best_move[9]):
-                            best_move = ('swap', partner, i, j, n_bl, n_bu, n_pl, n_pu, nm, nsq)
-
-        # Apply Best Move
+                    n_cl = loads[cand] - w2 + w1
+                    
+                    pb = calc_pressure(n_bl, n_bu)
+                    pc = calc_pressure(n_cl, n_cu)
+                    
+                    n_max = max(max_others, pb, pc)
+                    
+                    if n_max < curr_max - 1e-8:
+                        if best_move is None or n_max < best_move[8]:
+                             best_move = ('swap', cand, i, j, n_bl, n_bu, n_cl, n_cu, n_max)
+
         if best_move:
-            mtype, pt, i, j, nbl, nbu, npl, npu, _, _ = best_move
+            mtype, cand, i, j, nbl, nbu, ncl, ncu, _ = best_move
             if mtype == 'move':
-                item = current_placement[bottleneck].pop(i)
-                current_placement[pt].append(item)
+                it_obj = current_p[bottleneck].pop(i)
+                current_p[cand].append(it_obj)
             else:
-                item1 = current_placement[bottleneck][i]
-                item2 = current_placement[pt][j]
-                current_placement[bottleneck][i] = item2
-                current_placement[pt][j] = item1
-
+                it1 = current_p[bottleneck][i]
+                it2 = current_p[cand][j]
+                current_p[bottleneck][i] = it2
+                current_p[cand][j] = it1
+            
             loads[bottleneck], used[bottleneck] = nbl, nbu
-            loads[pt], used[pt] = npl, npu
-            pressures[bottleneck] = get_p(nbl, nbu)
-            pressures[pt] = get_p(npl, npu)
-            continue
-
-        # --- Ruin & Recreate (Perturbation) ---
-        # Victims: Bottleneck + Least Loaded + Random
-        victims = {bottleneck}
-
-        # Find least loaded
-        min_p = float('inf')
-        min_g = -1
-        for g in range(gpu_num):
-            if g != bottleneck and pressures[g] < min_p:
-                min_p = pressures[g]
-                min_g = g
-        if min_g != -1: victims.add(min_g)
-
-        # Add random to ensure coverage
-        cands = [g for g in range(gpu_num) if g not in victims]
-        if cands: victims.update(rng.sample(cands, min(len(cands), 1)))
-
-        victim_list = list(victims)
-
-        # Extract items
-        repack_items = []
-        for v in victim_list:
-            repack_items.extend(current_placement[v])
-            current_placement[v] = []
-            loads[v] = 0.0
-            used[v] = 0.0
-            pressures[v] = 0.0
-
-        # Recreate Loop (Try a few randomized packings)
-        best_local = None
-        best_local_max = float('inf')
-
-        # Strategies: Density, Size, Load (with noise)
-        strategies = [
-            lambda x: ((x.req_rate/x.slo)/(x.model_size+1e-6)) * rng.uniform(0.8, 1.2),
-            lambda x: x.model_size * rng.uniform(0.8, 1.2),
-            lambda x: (x.req_rate/x.slo) * rng.uniform(0.8, 1.2)
-        ]
-
-        for i in range(9):
-            iter_items = list(repack_items)
-            iter_items.sort(key=strategies[i%3], reverse=True)
-
-            l_loads = {v: 0.0 for v in victim_list}
-            l_used = {v: 0.0 for v in victim_list}
-            l_alloc = {v: [] for v in victim_list}
-            possible = True
-
-            for item in iter_items:
-                w, s = item.req_rate/item.slo, item.model_size
-                best_v = -1
-                best_sc = float('inf')
-
-                # Best Fit (Minimize pressure)
-                for v in victim_list:
-                    rem = GPU_MEM_SIZE - l_used[v] - s
-                    if rem > 1e-6:
-                        p = (l_loads[v] + w) / rem
-                        if p < best_sc:
-                            best_sc = p
-                            best_v = v
-
-                # Fallback
-                if best_v == -1:
-                    for v in victim_list:
-                        if l_used[v] + s <= GPU_MEM_SIZE - 1e-6:
-                            best_v = v
-                            break
-
-                if best_v != -1:
-                    l_alloc[best_v].append(item)
-                    l_loads[best_v] += w
-                    l_used[best_v] += s
+            loads[cand], used[cand] = ncl, ncu
+            pressures[bottleneck] = calc_pressure(nbl, nbu)
+            pressures[cand] = calc_pressure(ncl, ncu)
+            improved = True
+            
+        # --- Perturbation Phase (Ruin & Recreate) ---
+        # If descent failed to improve max pressure, we kick the system.
+        if not improved:
+            # Victims: Bottleneck + Least Loaded (to balance) + Random (to shuffle)
+            victims = {bottleneck}
+            if candidates:
+                # Least loaded
+                min_g = min(candidates, key=lambda g: pressures[g])
+                victims.add(min_g)
+                # Random
+                rem_cands = [g for g in candidates if g not in victims]
+                if rem_cands:
+                    victims.update(random.sample(rem_cands, 1))
+            
+            v_list = list(victims)
+            
+            # Extract items
+            repack_items = []
+            for v in v_list:
+                repack_items.extend(current_p[v])
+                current_p[v] = []
+                loads[v] = 0.0
+                used[v] = 0.0
+                pressures[v] = 0.0
+            
+            # Repack Strategies
+            # Try to redistribute items among v_list to minimize local max pressure.
+            # We use a greedy "Min-Pressure Fit" with randomized sorts.
+            
+            best_local_alloc = None
+            best_local_max = float('inf')
+            
+            sub_sorts = [
+                lambda x: x.req_rate/x.slo, # Load
+                lambda x: (x.req_rate/x.slo)/(x.model_size+1e-6), # Density
+                lambda x: x.model_size # Size
+            ]
+            
+            # Try multiple randomized passes
+            for k in range(12):
+                if k < 3:
+                    # Deterministic sorts with slight noise to break ties
+                    items_iter = sorted(repack_items, key=lambda x: sub_sorts[k](x) * random.uniform(0.99, 1.01), reverse=True)
                 else:
-                    possible = False
-                    break
-
-            if possible:
-                lm = 0.0
-                for v in victim_list:
-                     p = get_p(l_loads[v], l_used[v])
-                     if p > lm: lm = p
-
-                if lm < best_local_max:
-                    best_local_max = lm
-                    best_local = (l_alloc, l_loads, l_used)
-
-        # Update or Revert
-        if best_local:
-            l_alloc, l_loads, l_used = best_local
-            for v in victim_list:
-                current_placement[v] = l_alloc[v]
-                loads[v] = l_loads[v]
-                used[v] = l_used[v]
-                pressures[v] = get_p(loads[v], used[v])
-        else:
-            # Revert to global best
-            current_placement = {k: list(v) for k, v in best_placement.items()}
-            loads = [0.0]*gpu_num
-            used = [0.0]*gpu_num
-            for g in range(gpu_num):
-                for m in current_placement[g]:
-                    loads[g] += m.req_rate / m.slo
-                    used[g] += m.model_size
-            pressures = [get_p(loads[g], used[g]) for g in range(gpu_num)]
-            if it > max_iters * 0.9: break
-
-    return best_placement
-
+                    items_iter = list(repack_items)
+                    random.shuffle(items_iter)
+                
+                # Temp state
+                t_alloc = {v: [] for v in v_list}
+                t_l = {v: 0.0 for v in v_list}
+                t_u = {v: 0.0 for v in v_list}
+                ok = True
+                
+                for item in items_iter:
+                    w = item.req_rate/item.slo
+                    s = item.model_size
+                    
+                    best_v = -1
+                    min_p_incr = float('inf')
+                    
+                    # Try to put in bin with lowest resulting pressure (Greedy Balancing)
+                    for v in v_list:
+                        if t_u[v] + s <= GPU_MEM_SIZE - 1e-6:
+                            p = calc_pressure(t_l[v] + w, t_u[v] + s)
+                            if p < min_p_incr:
+                                min_p_incr = p
+                                best_v = v
+                    
+                    if best_v != -1:
+                        t_alloc[best_v].append(item)
+                        t_l[best_v] += w
+                        t_u[best_v] += s
+                    else:
+                        ok = False
+                        break
+                
+                if ok:
+                    # Check local max pressure of this configuration
+                    l_max = max(calc_pressure(t_l[v], t_u[v]) for v in v_list)
+                    if l_max < best_local_max:
+                        best_local_max = l_max
+                        best_local_alloc = (t_alloc, t_l, t_u)
+            
+            if best_local_alloc:
+                t_alloc, t_l, t_u = best_local_alloc
+                for v in v_list:
+                    current_p[v] = t_alloc[v]
+                    loads[v] = t_l[v]
+                    used[v] = t_u[v]
+                    pressures[v] = calc_pressure(loads[v], used[v])
+            else:
+                # Failed to repack (very rare, implies super tight packing), revert to global best
+                current_p = {k: list(v) for k,v in best_global_p.items()}
+                # Recompute state
+                loads = [0.0]*gpu_num
+                used = [0.0]*gpu_num
+                for g in range(gpu_num):
+                    for m in current_p[g]:
+                        loads[g] += m.req_rate / m.slo
+                        used[g] += m.model_size
+                pressures = [calc_pressure(loads[g], used[g]) for g in range(gpu_num)]
+                
+    return best_global_p
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")