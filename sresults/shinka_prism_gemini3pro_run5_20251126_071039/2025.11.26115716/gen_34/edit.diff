--- a/original.py
+++ b/original.py
@@ -1,403 +1,456 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 GPU_MEM_SIZE = 80  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
     Combines Binary Search with Multi-Strategy Best-Fit packing and Local Search refinement.
     """
+    import random
 
     # Precompute model data
     m_data = []
     for m in models:
         w = m.req_rate / m.slo
         s = m.model_size
         m_data.append({'w': w, 's': s, 'obj': m})
 
     # --- 1. Heuristic Initialization for Binary Search Bounds ---
     # We need a feasible upper bound.
 
     def try_pack_with_sort(sort_key):
         sorted_items = sorted(m_data, key=sort_key, reverse=True)
         placements = [[] for _ in range(gpu_num)]
         loads = [0.0] * gpu_num
         used = [0.0] * gpu_num
 
         for item in sorted_items:
             best_g = -1
             best_score = float('inf')
 
             for g in range(gpu_num):
                 rem = GPU_MEM_SIZE - used[g] - item['s']
                 if rem > 1e-6:
                     # Minimize resulting pressure
                     p = (loads[g] + item['w']) / rem
                     if p < best_score:
                         best_score = p
                         best_g = g
 
             if best_g == -1: return None, float('inf')
             placements[best_g].append(item['obj'])
             loads[best_g] += item['w']
             used[best_g] += item['s']
 
         max_p = 0.0
         for g in range(gpu_num):
             rem = GPU_MEM_SIZE - used[g]
             if rem <= 1e-6:
                 if loads[g] > 0: return None, float('inf')
             else:
                 max_p = max(max_p, loads[g]/rem)
         return placements, max_p
 
     # Try density sort (usually best)
     init_placement, upper_bound = try_pack_with_sort(lambda x: x['w'] / x['s'] if x['s'] > 0 else 0)
 
     # If fails, try size sort
     if init_placement is None:
         init_placement, upper_bound = try_pack_with_sort(lambda x: x['s'])
 
     # If still fails, use loose bound (assuming feasible solution exists)
     if init_placement is None:
         upper_bound = 1000.0
 
     # --- 2. Binary Search for Optimal K ---
     low = 0.0
     high = upper_bound
     final_placement = init_placement
 
     for _ in range(20):
         if high - low < 1e-4: break
         mid = (low + high) / 2.0
 
         feasible = False
         res_placement = None
 
         # Strategies: Virtual Size, Physical Size, Load, Density
         strategies = [
             lambda x: x['w'] + mid * x['s'],
             lambda x: x['s'],
             lambda x: x['w'],
             lambda x: x['w'] / x['s'] if x['s'] > 1e-6 else 0
         ]
 
         for key_func in strategies:
             # Best Fit Decreasing
             items_sorted = sorted(m_data, key=key_func, reverse=True)
             gpu_models = [[] for _ in range(gpu_num)]
             gpu_l = [0.0] * gpu_num
             gpu_u = [0.0] * gpu_num
             ok = True
 
             for item in items_sorted:
                 best_g = -1
                 min_residual = float('inf')
 
                 # Check all bins
                 for g in range(gpu_num):
                     # Hard mem check
                     if gpu_u[g] + item['s'] >= GPU_MEM_SIZE - 1e-6: continue
 
                     # Constraint check: (L + w) <= K * (C - (U + s))
                     # Transformed: L + w + K(U + s) <= KC
                     lhs = (gpu_l[g] + item['w']) + mid * (gpu_u[g] + item['s'])
                     rhs = mid * GPU_MEM_SIZE
 
                     if lhs <= rhs + 1e-7:
                         # Best Fit: minimize residual capacity of transformed bin
                         res = rhs - lhs
                         if res < min_residual:
                             min_residual = res
                             best_g = g
 
                 if best_g != -1:
                     gpu_models[best_g].append(item['obj'])
                     gpu_l[best_g] += item['w']
                     gpu_u[best_g] += item['s']
                 else:
                     ok = False
                     break
 
             if ok:
                 feasible = True
                 res_placement = {i: gpu_models[i] for i in range(gpu_num)}
                 break
+
+        # If deterministic strategies fail, try randomized packings to reduce false negatives
+        if not feasible:
+            base_items = list(m_data)
+            for _ in range(50):
+                random.shuffle(base_items)
+                gpu_models = [[] for _ in range(gpu_num)]
+                gpu_l = [0.0] * gpu_num
+                gpu_u = [0.0] * gpu_num
+                ok = True
+
+                for item in base_items:
+                    best_g = -1
+                    min_residual = float('inf')
+
+                    for g in range(gpu_num):
+                        if gpu_u[g] + item['s'] >= GPU_MEM_SIZE - 1e-6: continue
+
+                        lhs = (gpu_l[g] + item['w']) + mid * (gpu_u[g] + item['s'])
+                        rhs = mid * GPU_MEM_SIZE
+
+                        if lhs <= rhs + 1e-7:
+                            res = rhs - lhs
+                            if res < min_residual:
+                                min_residual = res
+                                best_g = g
+
+                    if best_g != -1:
+                        gpu_models[best_g].append(item['obj'])
+                        gpu_l[best_g] += item['w']
+                        gpu_u[best_g] += item['s']
+                    else:
+                        ok = False
+                        break
+
+                if ok:
+                    feasible = True
+                    res_placement = {i: gpu_models[i] for i in range(gpu_num)}
+                    break
 
         if feasible:
             final_placement = res_placement
             high = mid
         else:
             low = mid
 
     if final_placement is None:
         raise ValueError("Could not find feasible placement")
 
     # --- 3. Iterated Local Search (Descent with Variance Tie-Breaking) ---
     import random
 
     current_placement = final_placement
 
     # Track state
     loads = [0.0] * gpu_num
     used = [0.0] * gpu_num
     for g in range(gpu_num):
         for m in current_placement[g]:
             loads[g] += m.req_rate / m.slo
             used[g] += m.model_size
 
     best_placement = {k: list(v) for k, v in current_placement.items()}
 
     def get_pressure(l, u):
         rem = GPU_MEM_SIZE - u
         if rem <= 1e-6: return float('inf') if l > 1e-6 else 0.0
         return l / rem
 
     # Initial best pressure
     pressures = [get_pressure(loads[g], used[g]) for g in range(gpu_num)]
     best_max_p = max(pressures)
 
     # ILS Loop
     max_iters = 200 # Sufficient given best-improvement is expensive
 
     for iteration in range(max_iters):
         # Current state metrics
         current_max_p = max(pressures)
         current_sum_sq = sum(p*p for p in pressures)
 
         # Update global best
         if current_max_p < best_max_p - 1e-7:
             best_max_p = current_max_p
             best_placement = {k: list(v) for k, v in current_placement.items()}
 
         # Determine bottleneck(s)
         bottleneck = -1
         max_val = -1.0
         for g in range(gpu_num):
             if pressures[g] > max_val:
                 max_val = pressures[g]
                 bottleneck = g
 
         if bottleneck == -1: break
 
         # --- Best-Improvement Search ---
         best_move = None
         # Structure: (type, partner_idx, item_idx1, item_idx2, new_bn_l, new_bn_u, new_pt_l, new_pt_u, score_max, score_sq)
 
         pair_excl_max_cache = {} # Cache max pressure of other GPUs
 
         bn_items = current_placement[bottleneck]
 
         for partner in range(gpu_num):
             if partner == bottleneck: continue
 
             # Calculate max pressure excluding bn and partner
             if partner not in pair_excl_max_cache:
                 m_ex = 0.0
                 for g in range(gpu_num):
                     if g != bottleneck and g != partner:
                         if pressures[g] > m_ex: m_ex = pressures[g]
                 pair_excl_max_cache[partner] = m_ex
 
             pair_excl_max = pair_excl_max_cache[partner]
             pair_excl_sq = current_sum_sq - (pressures[bottleneck]**2 + pressures[partner]**2)
 
             # 1. Try Moving item from Bottleneck -> Partner
             for i, m in enumerate(bn_items):
                 w, s = m.req_rate / m.slo, m.model_size
 
                 if used[partner] + s >= GPU_MEM_SIZE - 1e-6: continue
 
                 n_bn_l = loads[bottleneck] - w
                 n_bn_u = used[bottleneck] - s
                 n_pt_l = loads[partner] + w
                 n_pt_u = used[partner] + s
 
                 p_bn = get_pressure(n_bn_l, n_bn_u)
                 p_pt = get_pressure(n_pt_l, n_pt_u)
 
                 new_max = max(pair_excl_max, p_bn, p_pt)
 
                 # Check feasibility and quality
                 if new_max > current_max_p + 1e-9: continue
 
                 new_sq = pair_excl_sq + p_bn**2 + p_pt**2
 
                 is_improvement = False
                 if new_max < current_max_p - 1e-9: is_improvement = True
                 elif new_max < current_max_p + 1e-9 and new_sq < current_sum_sq - 1e-9: is_improvement = True
 
                 if is_improvement:
                     if best_move is None:
                         best_move = ('move', partner, i, -1, n_bn_l, n_bn_u, n_pt_l, n_pt_u, new_max, new_sq)
                     else:
                         bm_max, bm_sq = best_move[8], best_move[9]
                         if new_max < bm_max - 1e-9:
                             best_move = ('move', partner, i, -1, n_bn_l, n_bn_u, n_pt_l, n_pt_u, new_max, new_sq)
                         elif abs(new_max - bm_max) < 1e-9 and new_sq < bm_sq - 1e-9:
                             best_move = ('move', partner, i, -1, n_bn_l, n_bn_u, n_pt_l, n_pt_u, new_max, new_sq)
 
             # 2. Try Swapping items
             pt_items = current_placement[partner]
             for i, m1 in enumerate(bn_items):
                 w1, s1 = m1.req_rate / m1.slo, m1.model_size
                 for j, m2 in enumerate(pt_items):
                     w2, s2 = m2.req_rate / m2.slo, m2.model_size
 
                     n_bn_u = used[bottleneck] - s1 + s2
                     if n_bn_u >= GPU_MEM_SIZE - 1e-6: continue
                     n_pt_u = used[partner] - s2 + s1
                     if n_pt_u >= GPU_MEM_SIZE - 1e-6: continue
 
                     n_bn_l = loads[bottleneck] - w1 + w2
                     n_pt_l = loads[partner] - w2 + w1
 
                     p_bn = get_pressure(n_bn_l, n_bn_u)
                     p_pt = get_pressure(n_pt_l, n_pt_u)
 
                     new_max = max(pair_excl_max, p_bn, p_pt)
                     if new_max > current_max_p + 1e-9: continue
 
                     new_sq = pair_excl_sq + p_bn**2 + p_pt**2
 
                     is_improvement = False
                     if new_max < current_max_p - 1e-9: is_improvement = True
                     elif new_max < current_max_p + 1e-9 and new_sq < current_sum_sq - 1e-9: is_improvement = True
 
                     if is_improvement:
                         if best_move is None:
                             best_move = ('swap', partner, i, j, n_bn_l, n_bn_u, n_pt_l, n_pt_u, new_max, new_sq)
                         else:
                             bm_max, bm_sq = best_move[8], best_move[9]
                             if new_max < bm_max - 1e-9:
                                 best_move = ('swap', partner, i, j, n_bn_l, n_bn_u, n_pt_l, n_pt_u, new_max, new_sq)
                             elif abs(new_max - bm_max) < 1e-9 and new_sq < bm_sq - 1e-9:
                                 best_move = ('swap', partner, i, j, n_bn_l, n_bn_u, n_pt_l, n_pt_u, new_max, new_sq)
 
         # Apply Move
         if best_move:
             mtype, pt, i, j, nbl, nbu, npl, npu, _, _ = best_move
             if mtype == 'move':
                 item = current_placement[bottleneck].pop(i)
                 current_placement[pt].append(item)
             else:
                 item1 = current_placement[bottleneck][i]
                 item2 = current_placement[pt][j]
                 current_placement[bottleneck][i] = item2
                 current_placement[pt][j] = item1
 
             loads[bottleneck] = nbl
             used[bottleneck] = nbu
             loads[pt] = npl
             used[pt] = npu
             pressures[bottleneck] = get_pressure(loads[bottleneck], used[bottleneck])
             pressures[pt] = get_pressure(loads[pt], used[pt])
 
         else:
-            # Perturbation if stuck
+            # Perturbation if stuck: Burst Kick (Multi-GPU Repack)
             candidates = [g for g in range(gpu_num) if g != bottleneck]
             if not candidates: break
-            partner = random.choice(candidates)
-            victims = [bottleneck, partner]
-
-            # Extract
+
+            # Select 2-3 random partners to form a larger repack group
+            k_partners = min(len(candidates), 3)
+            partners = random.sample(candidates, k_partners)
+            victims = [bottleneck] + partners
+
+            # Extract items
             repack_items = []
             for v in victims:
                 repack_items.extend(current_placement[v])
                 current_placement[v] = []
                 loads[v] = 0.0
                 used[v] = 0.0
                 pressures[v] = 0.0
 
-            random.shuffle(repack_items)
+            # Randomized BFD Repack
+            # Sort by density with noise to explore different packings
+            # Density = (req/slo)/size.
+            repack_items.sort(key=lambda x: (x.req_rate/x.slo)/(x.model_size+1e-6) * random.uniform(0.8, 1.2), reverse=True)
 
             # Greedy repack
             success = True
             for item in repack_items:
                 w, s = item.req_rate / item.slo, item.model_size
                 best_v = -1
                 min_p = float('inf')
 
                 for v in victims:
                     rem = GPU_MEM_SIZE - used[v] - s
                     if rem > 1e-6:
                         p = (loads[v] + w) / rem
                         if p < min_p:
                             min_p = p
                             best_v = v
+
+                # Fallback to any fit if min_p packing fails
+                if best_v == -1:
+                    for v in victims:
+                        if used[v] + s <= GPU_MEM_SIZE - 1e-6:
+                            best_v = v
+                            break
 
                 if best_v != -1:
                     current_placement[best_v].append(item)
                     loads[best_v] += w
                     used[best_v] += s
                 else:
                     success = False
                     break
 
             if success:
                 for v in victims:
                     pressures[v] = get_pressure(loads[v], used[v])
             else:
                 # Revert to best known
                 current_placement = {k: list(v) for k, v in best_placement.items()}
                 loads = [0.0]*gpu_num
                 used = [0.0]*gpu_num
                 for g in range(gpu_num):
                     for m in current_placement[g]:
                         loads[g] += m.req_rate / m.slo
                         used[g] += m.model_size
                     pressures[g] = get_pressure(loads[g], used[g])
 
     return best_placement
 
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")