<NAME>
steepest_descent_variance_ils
</NAME>

<DESCRIPTION>
Refactor the `_iterated_local_search` function to use a hybrid strategy:
1.  **Steepest Descent (Best Improvement)**: Instead of accepting the first improving move, it scans all valid moves and swaps involving the bottleneck GPU to find the one that yields the maximum reduction in peak KVPR.
2.  **Variance-Based Tie-Breaking**: If multiple moves yield the same peak KVPR (or no improvement in peak), it selects the move that minimizes the global Sum of Squared KVPRs (Variance). This smoothens the load distribution, allowing future moves to reduce the peak further.
3.  **Ruin & Recreate Perturbation**: If no improving move is found (Local Optimum), it triggers a Ruin & Recreate step on the bottleneck GPU and random neighbors to escape the local trap.

This approach is more computationally intensive per step but leverages the typically small number of GPUs and models to converge to higher quality solutions.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def compute_model_placement(gpu_num, models):
    """
    Minimizes max KVPR using Robust Binary Search with multiple packing heuristics
    followed by Iterated Local Search (Ruin & Recreate).
    """
=======
def compute_model_placement(gpu_num, models):
    """
    Minimizes max KVPR using Robust Binary Search with multiple packing heuristics
    followed by Steepest Descent Iterated Local Search (Descent + Ruin & Recreate).
    """
>>>>>>> REPLACE
<<<<<<< SEARCH
def _iterated_local_search(gpu_num, placement):
    """
    Refines placement using Ruin and Recreate.
    """
    # Initialize state
    gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
    gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]

    def get_k(i):
        rem = GPU_MEM_SIZE - gpu_s[i]
        if rem <= 1e-7: return 1e9
        return gpu_w[i] / rem

    best_max_k = max(get_k(i) for i in range(gpu_num))
    best_placement = copy.deepcopy(placement)

    max_steps = 300
    patience = 30
    no_improve = 0

    for _ in range(max_steps):
        # Identify bottleneck
        current_ks = [get_k(i) for i in range(gpu_num)]
        max_k = max(current_ks)

        # Check global improvement
        if max_k < best_max_k - 1e-6:
            best_max_k = max_k
            best_placement = copy.deepcopy(placement)
            no_improve = 0
        else:
            no_improve += 1

        # Ruin & Recreate
        # Select bottleneck and random others
        candidates = [i for i, k in enumerate(current_ks) if k > max_k * 0.99]
        if not candidates: candidates = [random.randint(0, gpu_num-1)]
        src = random.choice(candidates)

        ruin_set = {src}
        n_ruin = 2 + (1 if no_improve > patience else 0)

        others = list(range(gpu_num))
        random.shuffle(others)
        for o in others:
            if len(ruin_set) >= min(gpu_num, n_ruin): break
            if o != src: ruin_set.add(o)

        # Ruin
        removed_models = []
        backup = {}
        for idx in ruin_set:
            backup[idx] = (list(placement[idx]), gpu_s[idx], gpu_w[idx])
            removed_models.extend(placement[idx])
            placement[idx] = []
            gpu_s[idx] = 0.0
            gpu_w[idx] = 0.0

        # Recreate (Greedy Best Fit)
        # Sort by physical size desc to pack well
        removed_models.sort(key=lambda m: m.model_size, reverse=True)

        feasible = True
        for m in removed_models:
            best_idx = -1
            best_score = float('inf')

            m_s = m.model_size
            m_w = m.req_rate / m.slo

            for idx in ruin_set:
                if gpu_s[idx] + m_s <= GPU_MEM_SIZE:
                    # Score: minimizing resulting local K
                    rem = GPU_MEM_SIZE - (gpu_s[idx] + m_s)
                    k = (gpu_w[idx] + m_w) / rem if rem > 1e-7 else 1e9
                    if k < best_score:
                        best_score = k
                        best_idx = idx

            if best_idx != -1:
                placement[best_idx].append(m)
                gpu_s[best_idx] += m_s
                gpu_w[best_idx] += m_w
            else:
                feasible = False
                break

        # Acceptance
        accept = False
        if feasible:
            new_ks = [get_k(i) for i in range(gpu_num)]
            new_max = max(new_ks)
            if new_max < max_k - 1e-6:
                accept = True
            elif new_max < max_k + 1e-6:
                # Accept equal moves to traverse plateau
                accept = True
            elif no_improve > patience and random.random() < 0.2:
                # Random walk
                accept = True

        if not accept:
            # Revert
            for idx in ruin_set:
                placement[idx], gpu_s[idx], gpu_w[idx] = backup[idx]

    return best_placement
=======
def _iterated_local_search(gpu_num, placement):
    """
    Refines placement using Steepest Descent with Variance Tie-Breaking and Ruin & Recreate.
    """
    # Initialize state
    gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
    gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]

    def get_k(idx):
        rem = GPU_MEM_SIZE - gpu_s[idx]
        if rem <= 1e-7: return 1e9
        return gpu_w[idx] / rem

    current_ks = [get_k(i) for i in range(gpu_num)]
    max_k = max(current_ks)
    sum_sq = sum(k*k for k in current_ks)

    best_max_k = max_k
    best_placement = copy.deepcopy(placement)

    max_steps = 500
    patience = 20
    no_improve = 0
    epsilon = 1e-7

    for step in range(max_steps):
        # Update Global Best
        if max_k < best_max_k - epsilon:
            best_max_k = max_k
            best_placement = copy.deepcopy(placement)
            no_improve = 0
        else:
            no_improve += 1

        # Select Source (Bottleneck)
        candidates = [i for i, k in enumerate(current_ks) if k > max_k - epsilon]
        src = random.choice(candidates) if candidates else 0

        if no_improve <= patience:
            # --- STEEPEST DESCENT (Best Improvement) ---
            best_move = None # (type, data..., metrics)
            best_metric = None # (new_peak, delta_sq)

            src_models = placement[src]

            # 1. Evaluate Moves
            for i, m in enumerate(src_models):
                s, w = m.model_size, m.req_rate/m.slo

                for dst in range(gpu_num):
                    if dst == src: continue
                    if gpu_s[dst] + s > GPU_MEM_SIZE: continue

                    # New Ks
                    rem_src = GPU_MEM_SIZE - (gpu_s[src] - s)
                    nk_src = (gpu_w[src] - w) / rem_src if rem_src > 1e-7 else 1e9

                    rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + s)
                    nk_dst = (gpu_w[dst] + w) / rem_dst if rem_dst > 1e-7 else 1e9

                    local_peak = max(nk_src, nk_dst)
                    k_src, k_dst = current_ks[src], current_ks[dst]
                    delta_sq = (nk_src**2 + nk_dst**2) - (k_src**2 + k_dst**2)

                    # Criteria: Lower Global Peak OR (Same Peak AND Lower Variance)
                    # Note: We approximate global peak check by checking against current max_k
                    is_better = False
                    if local_peak < max_k - epsilon:
                        is_better = True
                    elif local_peak < max_k + epsilon and delta_sq < -epsilon:
                        is_better = True

                    if is_better:
                        metric = (local_peak, delta_sq)
                        if best_metric is None or \
                           metric[0] < best_metric[0] - epsilon or \
                           (abs(metric[0] - best_metric[0]) < epsilon and metric[1] < best_metric[1]):
                            best_metric = metric
                            best_move = ('move', i, dst, s, w, nk_src, nk_dst, delta_sq)

            # 2. Evaluate Swaps
            # Heuristic: limit swaps to non-bottleneck destinations to save time/complexity
            for i1, m1 in enumerate(src_models):
                s1, w1 = m1.model_size, m1.req_rate/m1.slo
                for dst in range(gpu_num):
                    if dst == src: continue
                    # Optimization: Only swap if destination is not already near peak
                    if current_ks[dst] > max_k - 1.0: continue

                    for i2, m2 in enumerate(placement[dst]):
                        s2, w2 = m2.model_size, m2.req_rate/m2.slo

                        ns_src = gpu_s[src] - s1 + s2
                        ns_dst = gpu_s[dst] - s2 + s1
                        if ns_src > GPU_MEM_SIZE or ns_dst > GPU_MEM_SIZE: continue

                        rem_src = GPU_MEM_SIZE - ns_src
                        nk_src = (gpu_w[src] - w1 + w2) / rem_src if rem_src > 1e-7 else 1e9

                        rem_dst = GPU_MEM_SIZE - ns_dst
                        nk_dst = (gpu_w[dst] - w2 + w1) / rem_dst if rem_dst > 1e-7 else 1e9

                        local_peak = max(nk_src, nk_dst)
                        k_src, k_dst = current_ks[src], current_ks[dst]
                        delta_sq = (nk_src**2 + nk_dst**2) - (k_src**2 + k_dst**2)

                        is_better = False
                        if local_peak < max_k - epsilon:
                            is_better = True
                        elif local_peak < max_k + epsilon and delta_sq < -epsilon:
                            is_better = True

                        if is_better:
                            metric = (local_peak, delta_sq)
                            if best_metric is None or \
                               metric[0] < best_metric[0] - epsilon or \
                               (abs(metric[0] - best_metric[0]) < epsilon and metric[1] < best_metric[1]):
                                best_metric = metric
                                best_move = ('swap', i1, dst, i2, s1, w1, s2, w2, nk_src, nk_dst, delta_sq)

            if best_move:
                if best_move[0] == 'move':
                    _, i, dst, s, w, nk_src, nk_dst, dsq = best_move
                    m = placement[src].pop(i)
                    placement[dst].append(m)
                    gpu_s[src] -= s; gpu_w[src] -= w
                    gpu_s[dst] += s; gpu_w[dst] += w
                    current_ks[src] = nk_src; current_ks[dst] = nk_dst
                    sum_sq += dsq
                else:
                    _, i1, dst, i2, s1, w1, s2, w2, nk_src, nk_dst, dsq = best_move
                    m1 = placement[src][i1]
                    m2 = placement[dst][i2]
                    placement[src][i1] = m2
                    placement[dst][i2] = m1
                    gpu_s[src] = gpu_s[src] - s1 + s2
                    gpu_w[src] = gpu_w[src] - w1 + w2
                    gpu_s[dst] = gpu_s[dst] - s2 + s1
                    gpu_w[dst] = gpu_w[dst] - w2 + w1
                    current_ks[src] = nk_src; current_ks[dst] = nk_dst
                    sum_sq += dsq

                max_k = max(current_ks)
                # If we reduced max_k significantly, no_improve reset is handled at top of loop
                # If we only reduced variance, we keep no_improve incrementing to eventually trigger kick
            else:
                no_improve = patience + 1 # Force perturbation

        else:
            # --- PERTURBATION (Ruin & Recreate) ---
            ruin_set = {src}
            # Add random others
            others = list(range(gpu_num))
            random.shuffle(others)
            for o in others:
                if len(ruin_set) >= 3: break
                if o != src: ruin_set.add(o)

            # Backup
            backup = {}
            popped = []
            for r in ruin_set:
                backup[r] = (list(placement[r]), gpu_s[r], gpu_w[r], current_ks[r])
                popped.extend(placement[r])
                placement[r] = []
                gpu_s[r] = 0.0; gpu_w[r] = 0.0; current_ks[r] = 0.0

            # Recreate: Sort by Physical Size Descending
            popped.sort(key=lambda x: x.model_size, reverse=True)

            feasible = True
            for m in popped:
                best_idx = -1
                best_score = float('inf')
                m_s, m_w = m.model_size, m.req_rate/m.slo

                for idx in ruin_set:
                    if gpu_s[idx] + m_s <= GPU_MEM_SIZE:
                        rem = GPU_MEM_SIZE - (gpu_s[idx] + m_s)
                        k = (gpu_w[idx] + m_w) / rem if rem > 1e-7 else 1e9
                        if k < best_score:
                            best_score = k
                            best_idx = idx

                if best_idx != -1:
                    placement[best_idx].append(m)
                    gpu_s[best_idx] += m_s
                    gpu_w[best_idx] += m_w
                else:
                    feasible = False
                    break

            if feasible:
                # Calc new stats
                new_sq_partial = 0
                old_sq_partial = 0
                for r in ruin_set:
                    old_sq_partial += backup[r][3]**2
                    current_ks[r] = get_k(r)
                    new_sq_partial += current_ks[r]**2

                new_max = max(current_ks)
                new_sum_sq = sum_sq - old_sq_partial + new_sq_partial

                accept = False
                if new_max < max_k - epsilon:
                    accept = True
                elif new_max < max_k + epsilon:
                    if new_sum_sq < sum_sq - epsilon:
                        accept = True
                    elif random.random() < 0.1: # Small chance to accept sideways/worse for exploration
                        accept = True

                if accept:
                    max_k = new_max
                    sum_sq = new_sum_sq
                    no_improve = 0
                else:
                     for