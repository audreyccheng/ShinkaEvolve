# EVOLVE-BLOCK-START
"""Model placement algorithm for minimizing maximum KV cache pressure using Robust Binary Search Packing and Steepest Descent with Burst Kicks"""

import copy
import random

GPU_MEM_SIZE = 80.0  # GB

def compute_model_placement(gpu_num, models):
    """
    Minimizes max KVPR using Robust Binary Search Packing (BFD with multiple sorts)
    followed by Steepest Descent Iterated Local Search with Burst Kicks.
    """
    # 1. Validation and Setup
    total_size = sum(m.model_size for m in models)
    if total_size > gpu_num * GPU_MEM_SIZE:
        raise ValueError("Total model size exceeds total GPU memory capacity.")

    # Prepare items: (req_rate/slo, model_size, model_obj)
    items = [{'w': m.req_rate / m.slo, 's': m.model_size, 'm': m} for m in models]

    # 2. Binary Search for Initial Feasible Solution
    total_w = sum(x['w'] for x in items)
    slack = gpu_num * GPU_MEM_SIZE - total_size

    # Heuristic upper bound
    low = 0.0
    if slack < 1e-6:
        high = 1e5 # Tight constraints
    else:
        avg_k = total_w / slack
        high = max(10.0, avg_k * 10.0)

    best_placement = None
    feasible_high = False

    # Exponential search for valid high
    for _ in range(20):
        feasible, placement = _check_feasibility_robust(gpu_num, items, high)
        if feasible:
            best_placement = placement
            feasible_high = True
            break
        low = high
        high *= 2.0

    if not feasible_high:
        raise ValueError("Unable to place models even with high KVPR limit.")

    # Binary Search Refinement
    for _ in range(32):
        mid = (low + high) / 2.0
        feasible, placement = _check_feasibility_robust(gpu_num, items, mid)
        if feasible:
            best_placement = placement
            high = mid
        else:
            low = mid

    placement_map = {i: best_placement[i] for i in range(gpu_num)}

    # 3. Refinement: Steepest Descent with Burst Kicks
    return _steepest_descent_ils(gpu_num, placement_map)

def _check_feasibility_robust(gpu_num, items, K):
    """
    Checks feasibility using multiple sorting heuristics and Best-Fit Decreasing.
    """
    virtual_cap = K * GPU_MEM_SIZE
    
    # Precompute sort keys
    # (virtual_size, physical_size, load, density, model)
    pack_items = []
    for x in items:
        v = x['w'] + K * x['s']
        d = x['w'] / (x['s'] + 1e-9)
        pack_items.append({'v': v, 's': x['s'], 'w': x['w'], 'd': d, 'm': x['m']})

    # Heuristics: (key_lambda, reverse_bool)
    heuristics = [
        (lambda x: x['v'], True),  # Virtual Size Desc (Standard)
        (lambda x: x['s'], True),  # Physical Size Desc (Big items first)
        (lambda x: x['d'], True),  # Density Desc (High intensity small items first)
        (lambda x: x['w'], True),  # Load Desc
    ]

    for key_func, rev in heuristics:
        pack_items.sort(key=key_func, reverse=rev)
        if res := _pack_bfd(gpu_num, pack_items, virtual_cap):
            return True, res

    return False, None

def _pack_bfd(gpu_num, items, virtual_cap):
    """
    Best Fit Decreasing packing: Minimize residual virtual capacity.
    """
    bins_v = [0.0] * gpu_num
    bins_p = [0.0] * gpu_num
    placement = [[] for _ in range(gpu_num)]

    for item in items:
        best_bin = -1
        min_rem_v = float('inf')

        for i in range(gpu_num):
            if bins_p[i] + item['s'] <= GPU_MEM_SIZE and bins_v[i] + item['v'] <= virtual_cap + 1e-7:
                rem = virtual_cap - (bins_v[i] + item['v'])
                if rem < min_rem_v:
                    min_rem_v = rem
                    best_bin = i

        if best_bin != -1:
            bins_p[best_bin] += item['s']
            bins_v[best_bin] += item['v']
            placement[best_bin].append(item['m'])
        else:
            return None
    return placement

def _steepest_descent_ils(gpu_num, placement):
    """
    Refines placement using Steepest Descent (Best Improvement) and Burst Kicks.
    Optimizes for Min-Max KVPR, breaking ties with Variance.
    """
    # State tracking
    gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
    gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]

    def get_k(i):
        rem = GPU_MEM_SIZE - gpu_s[i]
        if rem <= 1e-7: return 1e9
        return gpu_w[i] / rem

    # Initial State
    current_ks = [get_k(i) for i in range(gpu_num)]
    best_max_k = max(current_ks)
    best_sol = copy.deepcopy(placement)

    max_steps = 1000
    patience = 50
    no_improve = 0
    
    for _ in range(max_steps):
        # 1. Identify Bottleneck
        max_k = -1.0
        src = -1
        
        for i in range(gpu_num):
            k = current_ks[i]
            if k > max_k:
                max_k = k
                src = i
        
        # 2. Update Best
        if max_k < best_max_k - 1e-7:
            best_max_k = max_k
            best_sol = copy.deepcopy(placement)
            no_improve = 0
        else:
            no_improve += 1

        # 3. Burst Kick (if stuck)
        if no_improve > patience:
            # Perform sequence of random moves (3-5)
            n_kicks = random.randint(3, 5)
            for _ in range(n_kicks):
                # Try 10 times to find a valid move
                for _ in range(10):
                    s_idx = random.randint(0, gpu_num - 1)
                    if not placement[s_idx]: continue
                    d_idx = random.randint(0, gpu_num - 1)
                    if s_idx == d_idx: continue
                    
                    m_idx = random.randint(0, len(placement[s_idx]) - 1)
                    m = placement[s_idx][m_idx]
                    
                    if gpu_s[d_idx] + m.model_size <= GPU_MEM_SIZE:
                        placement[d_idx].append(m)
                        placement[s_idx].pop(m_idx)
                        gpu_s[d_idx] += m.model_size
                        gpu_w[d_idx] += m.req_rate/m.slo
                        gpu_s[s_idx] -= m.model_size
                        gpu_w[s_idx] -= m.req_rate/m.slo
                        current_ks[d_idx] = get_k(d_idx)
                        current_ks[s_idx] = get_k(s_idx)
                        break
            no_improve = 0 # Reset patience
            continue

        # 4. Steepest Descent Evaluation
        # Find best move or swap involving the bottleneck 'src'
        best_move = None # (type, params...)
        # Metric: (local_max_k, delta_variance)
        
        src_models = placement[src]
        
        # A. Evaluate Moves (src -> dst)
        for i, m in enumerate(src_models):
            s, w = m.model_size, m.req_rate/m.slo
            for dst in range(gpu_num):
                if dst == src: continue
                if gpu_s[dst] + s > GPU_MEM_SIZE: continue
                
                # New State Prediction
                nk_src = (gpu_w[src] - w) / (GPU_MEM_SIZE - (gpu_s[src] - s) + 1e-9)
                nk_dst = (gpu_w[dst] + w) / (GPU_MEM_SIZE - (gpu_s[dst] + s) + 1e-9)
                
                local_max = max(nk_src, nk_dst)
                
                # Pruning: If local max is worse than global max, ignore
                if local_max > max_k + 1e-7: continue
                
                delta_sq = (nk_src**2 + nk_dst**2) - (current_ks[src]**2 + current_ks[dst]**2)
                metric = (local_max, delta_sq)
                
                if best_move is None or metric < best_move[1]:
                    best_move = (('move', i, dst, s, w), metric)

        # B. Evaluate Swaps (src <-> dst)
        for i1, m1 in enumerate(src_models):
            s1, w1 = m1.model_size, m1.req_rate/m1.slo
            for dst in range(gpu_num):
                if dst == src: continue
                # Heuristic: Don't swap with highly stressed GPUs unless they are close to bottleneck
                if current_ks[dst] > max_k * 0.98: continue 
                
                for i2, m2 in enumerate(placement[dst]):
                    s2, w2 = m2.model_size, m2.req_rate/m2.slo
                    
                    ns_src = gpu_s[src] - s1 + s2
                    ns_dst = gpu_s[dst] - s2 + s1
                    if ns_src > GPU_MEM_SIZE or ns_dst > GPU_MEM_SIZE: continue
                    
                    nk_src = (gpu_w[src] - w1 + w2) / (GPU_MEM_SIZE - ns_src + 1e-9)
                    nk_dst = (gpu_w[dst] - w2 + w1) / (GPU_MEM_SIZE - ns_dst + 1e-9)
                    
                    local_max = max(nk_src, nk_dst)
                    if local_max > max_k + 1e-7: continue
                    
                    delta_sq = (nk_src**2 + nk_dst**2) - (current_ks[src]**2 + current_ks[dst]**2)
                    metric = (local_max, delta_sq)
                    
                    if best_move is None or metric < best_move[1]:
                        best_move = (('swap', i1, dst, i2, s1, w1, s2, w2), metric)
                        
        # 5. Execute Best Move
        if best_move:
            action, metric = best_move
            # Acceptance: strict improvement on max OR equal max with variance reduction
            
            is_better = False
            if metric[0] < max_k - 1e-7:
                is_better = True
            elif metric[0] < max_k + 1e-7 and metric[1] < -1e-5:
                is_better = True
                
            if is_better:
                if action[0] == 'move':
                    _, i, dst, s, w = action
                    m = placement[src].pop(i)
                    placement[dst].append(m)
                    gpu_s[src] -= s; gpu_w[src] -= w
                    gpu_s[dst] += s; gpu_w[dst] += w
                else:
                    _, i1, dst, i2, s1, w1, s2, w2 = action
                    m1 = placement[src][i1]
                    m2 = placement[dst][i2]
                    placement[src][i1] = m2
                    placement[dst][i2] = m1
                    gpu_s[src] = gpu_s[src] - s1 + s2
                    gpu_w[src] = gpu_w[src] - w1 + w2
                    gpu_s[dst] = gpu_s[dst] - s2 + s1
                    gpu_w[dst] = gpu_w[dst] - w2 + w1
                
                # Update cached Ks
                current_ks[src] = get_k(src)
                current_ks[dst] = get_k(dst)
                no_improve = 0
            else:
                no_improve += 1
        else:
            no_improve += 1
            
    return best_sol
# EVOLVE-BLOCK-END