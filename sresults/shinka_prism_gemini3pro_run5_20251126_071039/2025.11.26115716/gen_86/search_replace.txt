The following edit proposes improving the Iterated Local Search (ILS) algorithm by replacing the random "Burst Kicks" with a more structured "Ruin and Recreate" (Large Neighborhood Search) strategy. This strategy intelligently selects a subset of GPUs including the bottleneck and the least loaded ones, unassigns their models, and greedily repacks them to minimize local peak KVPR. This allows for more effective escape from local optima compared to random moves.

<NAME>
ils_with_lns_kick
</NAME>

<DESCRIPTION>
Replaces the random "Burst Kicks" in the Iterated Local Search with a more structured "Ruin and Recreate" (Large Neighborhood Search) strategy.
This strategy selects the bottleneck GPU and a subset of other GPUs (specifically targeting those with lower load to absorb pressure), removes their models, and aggressively repacks them to minimize the local peak KVPR.
This allows the algorithm to escape local optima by making coordinated moves of multiple models that would be impossible with single moves/swaps, effectively rebalancing the most critical part of the system.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def _iterated_local_search(gpu_num, placement):
    """
    Refines placement using Steepest Descent with Variance Tie-Breaking and Burst Kicks.
    """
    # Initialize state
    gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
    gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]

    def get_k(i):
        rem = GPU_MEM_SIZE - gpu_s[i]
        if rem <= 1e-7: return 1e9
        return gpu_w[i] / rem

    current_ks = [get_k(i) for i in range(gpu_num)]
    best_max_k = max(current_ks)
    best_placement = copy.deepcopy(placement)

    # Calculate initial sum of squared pressures (variance proxy)
    current_sum_sq = sum(k*k for k in current_ks)

    max_steps = 500
    patience = 20
    no_improve = 0

    for step in range(max_steps):
        # 1. Identify bottleneck
        max_k = -1.0
        src = -1
        for i in range(gpu_num):
            if current_ks[i] > max_k:
                max_k = current_ks[i]
                src = i

        # Check global improvement
        if max_k < best_max_k - 1e-7:
            best_max_k = max_k
            best_placement = copy.deepcopy(placement)
            no_improve = 0
        else:
            no_improve += 1

        # 2. Burst Kicks (if stuck)
        if no_improve > patience:
            # Perform a burst of random moves to escape local optimum
            n_kicks = random.randint(3, 5)
            for _ in range(n_kicks):
                for _ in range(10): # retry limit for valid move
                    s = random.randint(0, gpu_num - 1)
                    if not placement[s]: continue
                    d = random.randint(0, gpu_num - 1)
                    if s == d: continue

                    m_idx = random.randint(0, len(placement[s]) - 1)
                    m = placement[s][m_idx]

                    if gpu_s[d] + m.model_size <= GPU_MEM_SIZE:
                        placement[d].append(m)
                        placement[s].pop(m_idx)
                        gpu_s[d] += m.model_size; gpu_w[d] += m.req_rate/m.slo
                        gpu_s[s] -= m.model_size; gpu_w[s] -= m.req_rate/m.slo

                        current_ks[s] = get_k(s)
                        current_ks[d] = get_k(d)
                        current_sum_sq = sum(k*k for k in current_ks)
                        break
            no_improve = 0 # Reset patience after kick
            continue

        # 3. Steepest Descent (Greedy)
        # Try to improve the bottleneck 'src' by Moving or Swapping
        best_move = None # ((improvement_type, peak_diff, var_diff), action_tuple)

        # Sort items in bottleneck by workload descending (heuristic: move heavy items first)
        src_items = sorted(enumerate(placement[src]), key=lambda x: x[1].req_rate/x[1].slo, reverse=True)

        # A. Check Moves
        for idx, m in src_items:
            w, s = m.req_rate/m.slo, m.model_size
            for dst in range(gpu_num):
                if dst == src: continue
                if gpu_s[dst] + s > GPU_MEM_SIZE: continue

                # Predict new state
                rem_src = GPU_MEM_SIZE - (gpu_s[src] - s)
                nk_src = (gpu_w[src] - w) / rem_src if rem_src > 1e-7 else 1e9

                rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + s)
                nk_dst = (gpu_w[dst] + w) / rem_dst if rem_dst > 1e-7 else 1e9

                new_peak = max(nk_src, nk_dst)

                # Pruning: If local peak creates new global violation, skip
                if new_peak > max_k + 1e-7: continue

                peak_diff = max_k - new_peak
                var_diff = (current_ks[src]**2 + current_ks[dst]**2) - (nk_src**2 + nk_dst**2)

                score = None
                if peak_diff > 1e-7: # Strict improvement
                    score = (1, peak_diff, var_diff)
                elif peak_diff > -1e-7: # Plateau (Equal peak)
                    if var_diff > 1e-5: # Variance reduction
                        score = (0, 0, var_diff)

                if score and (best_move is None or score > best_move[0]):
                    best_move = (score, ('move', idx, dst, s, w))

        # B. Check Swaps (if no Move found or to see if Swap is better)
        # Only check if we haven't found a Peak Reduction move yet to save time
        if best_move is None or best_move[0][0] == 0:
            for i1, m1 in src_items:
                w1, s1 = m1.req_rate/m1.slo, m1.model_size
                for dst in range(gpu_num):
                    if dst == src: continue
                    # Optimization: Don't swap with highly stressed GPUs
                    if current_ks[dst] > max_k * 0.95: continue

                    for i2, m2 in enumerate(placement[dst]):
                        w2, s2 = m2.req_rate/m2.slo, m2.model_size

                        ns_src = gpu_s[src] - s1 + s2
                        ns_dst = gpu_s[dst] - s2 + s1
                        if ns_src > GPU_MEM_SIZE or ns_dst > GPU_MEM_SIZE: continue

                        rem_src = GPU_MEM_SIZE - ns_src
                        nk_src = (gpu_w[src] - w1 + w2) / rem_src if rem_src > 1e-7 else 1e9
                        rem_dst = GPU_MEM_SIZE - ns_dst
                        nk_dst = (gpu_w[dst] - w2 + w1) / rem_dst if rem_dst > 1e-7 else 1e9

                        new_peak = max(nk_src, nk_dst)
                        if new_peak > max_k + 1e-7: continue

                        peak_diff = max_k - new_peak
                        var_diff = (current_ks[src]**2 + current_ks[dst]**2) - (nk_src**2 + nk_dst**2)

                        score = None
                        if peak_diff > 1e-7:
                            score = (1, peak_diff, var_diff)
                        elif peak_diff > -1e-7:
                            if var_diff > 1e-5:
                                score = (0, 0, var_diff)

                        if score and (best_move is None or score > best_move[0]):
                            best_move = (score, ('swap', i1, dst, i2, s1, w1, s2, w2))

        # Apply Best Move
        if best_move:
            _, action = best_move
            if action[0] == 'move':
                _, idx, dst, s, w = action
                m = placement[src].pop(idx)
                placement[dst].append(m)
                gpu_s[src] -= s; gpu_w[src] -= w
                gpu_s[dst] += s; gpu_w[dst] += w
            elif action[0] == 'swap':
                _, i1, dst, i2, s1, w1, s2, w2 = action
                m1 = placement[src][i1]
                m2 = placement[dst][i2]
                placement[src][i1] = m2
                placement[dst][i2] = m1
                gpu_s[src] = gpu_s[src] - s1 + s2; gpu_w[src] = gpu_w[src] - w1 + w2
                gpu_s[dst] = gpu_s[dst] - s2 + s1; gpu_w[dst] = gpu_w[dst] - w2 + w1

            # Update State
            current_ks[src] = get_k(src)
            current_ks[dst] = get_k(dst)
            current_sum_sq = sum(k*k for k in current_ks)

            # If peak didn't drop (plateau move), increase no_improve counter slightly
            # but less than a full fail to delay kick
            if max(current_ks) >= max_k - 1e-7:
                no_improve += 1
            else:
                no_improve = 0
        else:
            no_improve += 1

    return best_placement
=======
def _iterated_local_search(gpu_num, placement):
    """
    Refines placement using Steepest Descent with Ruin-and-Recreate (LNS) Kicks.
    """
    # Initialize state
    gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
    gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]

    def get_k(i):
        rem = GPU_MEM_SIZE - gpu_s[i]
        if rem <= 1e-7: return 1e9
        return gpu_w[i] / rem

    current_ks = [get_k(i) for i in range(gpu_num)]
    best_max_k = max(current_ks)
    best_placement = copy.deepcopy(placement)

    max_steps = 600
    patience = 20
    no_improve = 0

    for step in range(max_steps):
        # 1. Identify bottleneck
        max_k = -1.0
        src = -1
        sorted_gpus = sorted(range(gpu_num), key=lambda x: current_ks[x])

        max_k = current_ks[sorted_gpus[-1]]
        src = sorted_gpus[-1]

        # Check global improvement
        if max_k < best_max_k - 1e-7:
            best_max_k = max_k
            best_placement = copy.deepcopy(placement)
            no_improve = 0
        else:
            no_improve += 1

        # 2. Ruin and Recreate Kick (if stuck)
        if no_improve > patience:
            # Select subset: Bottleneck + Random/Low-load
            subset = {src}
            # Pick 2 lowest loaded to help absorb load
            if gpu_num > 1: subset.add(sorted_gpus[0])
            if gpu_num > 2: subset.add(sorted_gpus[1])

            # Fill remaining slots with random
            target_size = min(gpu_num, random.randint(3, 5))
            while len(subset) < target_size:
                subset.add(random.randint(0, gpu_num - 1))

            subset = list(subset)

            # Ruin: Extract all models
            pool = []
            backup_state = {} # To revert if recreate fails badly (optional, but good for safety)

            for idx in subset:
                backup_state[idx] = (list(placement[idx]), gpu_s[idx], gpu_w[idx])
                pool.extend(placement[idx])
                placement[idx] = []
                gpu_s[idx] = 0.0
                gpu_w[idx] = 0.0
                current_ks[idx] = 0.0

            # Recreate: Sort and Pack
            # Sort by physical size descending (difficult items first), break ties with load
            pool.sort(key=lambda m: (m.model_size, m.req_rate/m.slo), reverse=True)

            valid_repack = True
            for m in pool:
                best_target = -1
                best_impact = float('inf') # Minimize resulting K

                for idx in subset:
                    if gpu_s[idx] + m.model_size <= GPU_MEM_SIZE:
                        # What would K be?
                        rem = GPU_MEM_SIZE - (gpu_s[idx] + m.model_size)
                        k = (gpu_w[idx] + m.req_rate/m.slo) / rem if rem > 1e-7 else 1e9
                        if k < best_impact:
                            best_impact = k
                            best_target = idx

                if best_target != -1:
                    placement[best_target].append(m)
                    gpu_s[best_target] += m.model_size
                    gpu_w[best_target] += m.req_rate/m.slo
                else:
                    valid_repack = False
                    break

            if valid_repack:
                # Update Ks
                for idx in subset:
                    current_ks[idx] = get_k(idx)
                # We always accept the kick to escape local optimum,
                # unless it causes a massive degradation (sanity check)
                if max(current_ks) > max_k * 1.5 and max(current_ks) > 1000:
                    # Revert if disastrous
                    for idx in subset:
                        placement[idx], gpu_s[idx], gpu_w[idx] = backup_state[idx]
                        current_ks[idx] = get_k(idx)
                else:
                    no_improve = 0 # Reset patience
            else:
                # Revert
                for idx in subset:
                    placement[idx], gpu_s[idx], gpu_w[idx] = backup_state[idx]
                    current_ks[idx] = get_k(idx)

            continue

        # 3. Steepest Descent (Greedy)
        best_move = None
        # Sort items in bottleneck by impact potential
        src_items = sorted(enumerate(placement[src]), key=lambda x: x[1].req_rate/x[1].slo, reverse=True)

        # A. Check Moves
        for idx, m in src_items:
            w, s = m.req_rate/m.slo, m.model_size
            for dst in range(gpu_num):
                if dst == src: continue
                if gpu_s[dst] + s > GPU_MEM_SIZE: continue

                rem_src = GPU_MEM_SIZE - (gpu_s[src] - s)
                nk_src = (gpu_w[src] - w) / rem_src if rem_src > 1e-7 else 1e9

                rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + s)
                nk_dst = (gpu_w[dst] + w) / rem_dst if rem_dst > 1e-7 else 1e9

                new_peak = max(nk_src, nk_dst)
                if new_peak > max_k + 1e-7: continue

                peak_diff = max_k - new_peak
                # Variance reduction
                var_diff = (current_ks[src]**2 + current_ks[dst]**2) - (nk_src**2 + nk_dst**2)

                score = None
                if peak_diff > 1e-7: score = (1, peak_diff, var_diff)
                elif peak_diff > -1e-7 and var_diff > 1e-5: score = (0, 0, var_diff)

                if score and (best_move is None or score > best_move[0]):
                    best_move = (score, ('move', idx, dst, s, w))

        # B. Check Swaps (if Move not decisive)
        if best_move is None or best_move[0][0] == 0:
            for i1, m1 in src_items:
                w1, s1 = m1.req_rate/m1.slo, m1.model_size
                for dst in range(gpu_num):
                    if dst == src: continue
                    # Optimization: Skip high stress dst
                    if current_ks[dst] > max_k * 0.95: continue

                    for i2, m2 in enumerate(placement[dst]):
                        w2, s2 = m2.req_rate/m2.slo, m2.model_size

                        ns_src = gpu_s[src] - s1 + s2
                        ns_dst = gpu_s[dst] - s2 + s1
                        if ns_src > GPU_MEM_SIZE or ns_dst > GPU_MEM_SIZE: continue

                        rem_src = GPU_MEM_SIZE - ns_src
                        nk_src = (gpu_w[src] - w1 + w2) / rem_src if rem_src > 1e-7 else 1e9
                        rem_dst = GPU_MEM_SIZE - ns_dst
                        nk_dst = (gpu_w[dst] - w2 + w1) / rem_dst if rem_dst > 1e-7 else 1e9

                        new_peak = max(nk_src, nk_dst)
                        if new_peak > max_k + 1e-7: continue

                        peak_diff = max_k - new_peak
                        var_diff = (current_ks[src]**2 + current_ks[dst]**2) - (nk_src**2 + nk_dst**2)

                        score = None
                        if peak_diff > 1e-7: score = (1, peak_diff, var_diff)
                        elif peak_diff > -1e-7 and var_diff > 1e-5: score = (0, 0, var_diff)

                        if score and (best_move is None or score > best_move[0]):
                            best_move = (score, ('swap', i1, dst, i2, s1, w1, s2, w2))

        # Apply Best Move
        if best_move:
            _, action = best_move
            if action[0] == 'move':
                _, idx, dst, s, w = action
                m = placement[src].pop(idx)
                placement[dst].append(m)
                gpu_s[src] -= s; gpu_w[src] -= w
                gpu_s[dst] += s; gpu_w[dst] += w
            elif action[0] == 'swap':
                _, i1, dst, i2, s1, w1, s2, w2 = action
                m1 = placement[src][i1]
                m2 = placement[dst][i2]
                placement[src][i1] = m2
                placement[dst][i2] = m1
                gpu_s[src] = gpu_s[src] - s1 + s2; gpu_w[src] = gpu_w[src] - w1 + w2
                gpu_s[dst] = gpu_s[dst] - s2 + s1; gpu_w[dst] = gpu_w[dst] - w2 + w1

            current_ks[src] = get_k(src)
            current_ks[dst] = get_k(dst)

            if max(current_ks) >= max_k - 1e-7:
                no_improve += 1
            else:
                no_improve = 0
        else:
            no_improve += 1

    return best_placement
>>>>>>> REPLACE
</DIFF>