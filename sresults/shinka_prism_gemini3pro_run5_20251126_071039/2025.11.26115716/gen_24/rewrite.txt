# EVOLVE-BLOCK-START
"""Model placement algorithm for minimizing maximum KV cache pressure using Multi-Strategy Binary Search and Guided ILS"""

import copy
import random
import math

GPU_MEM_SIZE = 80.0  # GB

def compute_model_placement(gpu_num, models):
    """
    Minimizes max KVPR using Multi-Strategy Binary Search Packing followed by 
    Guided Iterated Local Search with Multi-GPU Ruin-Recreate.
    """
    # 1. Validation and Pre-processing
    total_size = sum(m.model_size for m in models)
    if total_size > gpu_num * GPU_MEM_SIZE:
        raise ValueError("Total model size exceeds total GPU memory capacity.")

    # Prepare items: (w, s, m)
    items = [{'w': m.req_rate / m.slo, 's': m.model_size, 'm': m} for m in models]

    # 2. Binary Search for Initial Solution
    total_w = sum(x['w'] for x in items)
    slack = gpu_num * GPU_MEM_SIZE - total_size
    
    low = 0.0
    # Heuristic upper bound
    if slack < 1e-5:
        high = 1000.0
    else:
        avg_pressure = total_w / slack
        high = max(10.0, avg_pressure * 6.0)

    best_placement = None
    feasible_high = False
    
    # Find valid upper bound
    for _ in range(20):
        feasible, placement = _check_feasibility(gpu_num, items, high)
        if feasible:
            best_placement = placement
            feasible_high = True
            break
        high *= 2.0
        
    if not feasible_high:
        # Fallback for extremely tight cases
        high = 1e9
        feasible, placement = _check_feasibility(gpu_num, items, high)
        if feasible:
            best_placement = placement
        else:
            raise ValueError("Unable to place models. Constraints likely too tight.")

    # Binary Search
    for _ in range(30):
        mid = (low + high) / 2.0
        feasible, placement = _check_feasibility(gpu_num, items, mid)
        if feasible:
            best_placement = placement
            high = mid
        else:
            low = mid
            
    # Convert to dict format
    placement_map = {i: best_placement[i] for i in range(gpu_num)}
    
    # 3. Refinement
    return _guided_ils(gpu_num, placement_map)

def _check_feasibility(gpu_num, items, K):
    """
    Checks feasibility using FFD and BFD on various sort orders.
    Virtual Size: v = w + K * s
    Constraint: v <= K * Capacity
    """
    virtual_cap = K * GPU_MEM_SIZE
    
    # Precompute sort keys
    # pack_item: (v, s, w, density, m)
    pack_items = []
    for x in items:
        v = x['w'] + K * x['s']
        d = x['w'] / (x['s'] + 1e-6)
        pack_items.append({'v': v, 's': x['s'], 'w': x['w'], 'd': d, 'm': x['m']})
        
    # Heuristics: (Sort Key, Reverse)
    # 1. Virtual Size Descending (Balances w and s based on K)
    # 2. Physical Size Descending (Hard constraint focus)
    # 3. Load Descending (Objective focus)
    # 4. Density Descending (Efficiency focus)
    strategies = [
        lambda x: x['v'],
        lambda x: x['s'],
        lambda x: x['w'],
        lambda x: x['d']
    ]
    
    for key_func in strategies:
        sorted_items = sorted(pack_items, key=key_func, reverse=True)
        
        # Try FFD
        if res := _pack(gpu_num, sorted_items, virtual_cap, method='ffd'):
            return True, res
            
        # Try BFD
        if res := _pack(gpu_num, sorted_items, virtual_cap, method='bfd'):
            return True, res
            
    return False, None

def _pack(gpu_num, items, virtual_cap, method='ffd'):
    bins_v = [0.0] * gpu_num
    bins_p = [0.0] * gpu_num
    placement = [[] for _ in range(gpu_num)]
    
    for item in items:
        v, s, m = item['v'], item['s'], item['m']
        
        if method == 'ffd':
            placed = False
            for i in range(gpu_num):
                if bins_p[i] + s <= GPU_MEM_SIZE and bins_v[i] + v <= virtual_cap + 1e-7:
                    bins_p[i] += s
                    bins_v[i] += v
                    placement[i].append(m)
                    placed = True
                    break
            if not placed: return None
            
        elif method == 'bfd':
            best_bin = -1
            min_rem = float('inf')
            
            for i in range(gpu_num):
                if bins_p[i] + s <= GPU_MEM_SIZE and bins_v[i] + v <= virtual_cap + 1e-7:
                    # Minimize remaining virtual capacity
                    rem = virtual_cap - (bins_v[i] + v)
                    if rem < min_rem:
                        min_rem = rem
                        best_bin = i
            
            if best_bin != -1:
                bins_p[best_bin] += s
                bins_v[best_bin] += v
                placement[best_bin].append(m)
            else:
                return None
                
    return placement

def _guided_ils(gpu_num, placement):
    """
    Iterated Local Search with Bottleneck-Focused Moves and Multi-GPU Ruin-Recreate.
    """
    # State tracking
    gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
    gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]
    
    def get_k(i):
        rem = GPU_MEM_SIZE - gpu_s[i]
        if rem <= 1e-7: return 1e9
        return gpu_w[i] / rem

    best_sol = copy.deepcopy(placement)
    best_max_k = max(get_k(i) for i in range(gpu_num))
    
    max_steps = 300
    patience = 30
    no_improve = 0
    
    for step in range(max_steps):
        # Identify bottleneck
        ks = [get_k(i) for i in range(gpu_num)]
        max_k = max(ks)
        src = ks.index(max_k)
        
        if max_k < best_max_k - 1e-6:
            best_max_k = max_k
            best_sol = copy.deepcopy(placement)
            no_improve = 0
        else:
            no_improve += 1
            
        if no_improve <= patience:
            # 1. Local Search: Move/Swap from Bottleneck
            improved = False
            src_models = placement[src]
            
            # Sort models by size desc to try moving big chunks first
            src_models_sorted_idx = sorted(range(len(src_models)), 
                                         key=lambda i: src_models[i].req_rate/src_models[i].slo, 
                                         reverse=True)

            # Try Move
            for idx in src_models_sorted_idx:
                m = src_models[idx]
                s, w = m.model_size, m.req_rate/m.slo
                
                best_dst = -1
                best_improvement = max_k
                
                for dst in range(gpu_num):
                    if dst == src: continue
                    if gpu_s[dst] + s > GPU_MEM_SIZE: continue
                    
                    # Estimate new max
                    rem_src = GPU_MEM_SIZE - (gpu_s[src] - s)
                    nk_src = (gpu_w[src] - w) / rem_src if rem_src > 1e-7 else 1e9
                    
                    rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + s)
                    nk_dst = (gpu_w[dst] + w) / rem_dst if rem_dst > 1e-7 else 1e9
                    
                    local_max = max(nk_src, nk_dst)
                    
                    if local_max < best_improvement - 1e-6:
                        best_improvement = local_max
                        best_dst = dst
                
                if best_dst != -1 and best_improvement < max_k - 1e-6:
                    placement[best_dst].append(m)
                    placement[src].pop(idx)
                    gpu_s[src] -= s; gpu_w[src] -= w
                    gpu_s[best_dst] += s; gpu_w[best_dst] += w
                    improved = True
                    break
            
            if improved: continue
            
            # Try Swap
            if not improved:
                for idx1 in src_models_sorted_idx:
                    m1 = src_models[idx1]
                    s1, w1 = m1.model_size, m1.req_rate/m1.slo
                    
                    for dst in range(gpu_num):
                        if dst == src: continue
                        if ks[dst] > max_k * 0.95: continue # optimization
                        
                        for idx2, m2 in enumerate(placement[dst]):
                            s2, w2 = m2.model_size, m2.req_rate/m2.slo
                            
                            ns_src = gpu_s[src] - s1 + s2
                            ns_dst = gpu_s[dst] - s2 + s1
                            
                            if ns_src > GPU_MEM_SIZE or ns_dst > GPU_MEM_SIZE: continue
                            
                            rem_src = GPU_MEM_SIZE - ns_src
                            nk_src = (gpu_w[src] - w1 + w2) / rem_src if rem_src > 1e-7 else 1e9
                            
                            rem_dst = GPU_MEM_SIZE - ns_dst
                            nk_dst = (gpu_w[dst] - w2 + w1) / rem_dst if rem_dst > 1e-7 else 1e9
                            
                            if max(nk_src, nk_dst) < max_k - 1e-6:
                                placement[src][idx1] = m2
                                placement[dst][idx2] = m1
                                gpu_s[src] = ns_src; gpu_w[src] += (w2 - w1)
                                gpu_s[dst] = ns_dst; gpu_w[dst] += (w1 - w2)
                                improved = True
                                break
                        if improved: break
                    if improved: break
                    
        else:
            # 2. Ruin and Recreate (Multi-GPU)
            k_ruin = min(gpu_num, 4)
            # Get indices sorted by KVPR ascending (slackest first)
            sorted_gpus = sorted(range(gpu_num), key=lambda i: ks[i])
            
            # Select targets: Bottleneck + (K-1) lowest loaded
            targets = [src]
            for t_idx in sorted_gpus:
                if len(targets) >= k_ruin: break
                if t_idx != src:
                    targets.append(t_idx)
            
            # Collect models
            pool = []
            for t in targets:
                pool.extend(placement[t])
                placement[t] = []
                gpu_s[t] = 0.0
                gpu_w[t] = 0.0
            
            # Reconstruct: Min-Max Greedy
            pool.sort(key=lambda m: m.req_rate/m.slo, reverse=True)
            
            for m in pool:
                w, s = m.req_rate/m.slo, m.model_size
                best_t = -1
                best_obj = float('inf')
                
                # Try placing on each target to see which yields lowest local max
                for t in targets:
                    if gpu_s[t] + s <= GPU_MEM_SIZE:
                        rem = GPU_MEM_SIZE - (gpu_s[t] + s)
                        new_k_t = (gpu_w[t] + w) / rem if rem > 1e-7 else 1e9
                        
                        # Calculate max over the targets group with this change
                        current_max = 0
                        for other in targets:
                            if other == t:
                                k = new_k_t
                            else:
                                r = GPU_MEM_SIZE - gpu_s[other]
                                k = gpu_w[other]/r if r > 1e-7 else 1e9
                            if k > current_max: current_max = k
                            
                        if current_max < best_obj:
                            best_obj = current_max
                            best_t = t
                
                if best_t != -1:
                    placement[best_t].append(m)
                    gpu_s[best_t] += s
                    gpu_w[best_t] += w
                else:
                    # Fallback (First Fit)
                    for t in targets:
                        if gpu_s[t] + s <= GPU_MEM_SIZE:
                            placement[t].append(m)
                            gpu_s[t] += s
                            gpu_w[t] += w
                            break

            no_improve = max(0, patience - 8)
            
    return best_sol
# EVOLVE-BLOCK-END