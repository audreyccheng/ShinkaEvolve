To improve the performance of the model placement algorithm, I will replace the current `_iterated_local_search` function, which relies on a "Ruin and Recreate" strategy, with a **Steepest Descent with Variance Tie-Breaking and Burst Kicks** strategy.

The previous Ruin & Recreate approach attempts to rebuild a partial solution from scratch, which can be computationally expensive and may not effectively explore small, precise adjustments needed near the optimal solution. The new approach uses:
1.  **Steepest Descent**: Systematically evaluates all possible moves and swaps involving the bottleneck GPU to find the best immediate improvement.
2.  **Variance Tie-Breaking**: When no move directly reduces the maximum KVPR (peak), the algorithm accepts moves that reduce the *sum of squared pressures* (variance). This smooths the load distribution across GPUs, traversing plateaus in the fitness landscape and potentially enabling future peak reductions.
3.  **Burst Kicks**: Instead of a complex reconstruction when stuck in a local optimum, the algorithm executes a sequence of 3-5 random moves ("burst"). This is a faster and often more effective perturbation method for this problem size.

This aligns with the recommendations to "Implement Heuristic-Guided Greedy Descent", "Incorporate Secondary Objective for Plateau Traversal", and "Adopt Atomic Multi-Step 'Burst' Kicks".

<NAME>
steepest_descent_ils_burst
</NAME>

<DESCRIPTION>
Replaces the Ruin & Recreate local search with a Steepest Descent algorithm that uses variance reduction (sum of squared pressures) as a tie-breaker for plateau traversal. It also replaces the single random kick with 'Burst Kicks' (a sequence of random moves) to better escape local optima. This approach allows the algorithm to reorganize the placement on non-bottleneck GPUs to create slack, eventually reducing the global peak.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def _iterated_local_search(gpu_num, placement):
    """
    Refines placement using Ruin and Recreate.
    """
    # Initialize state
    gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
    gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]

    def get_k(i):
        rem = GPU_MEM_SIZE - gpu_s[i]
        if rem <= 1e-7: return 1e9
        return gpu_w[i] / rem

    best_max_k = max(get_k(i) for i in range(gpu_num))
    best_placement = copy.deepcopy(placement)

    max_steps = 300
    patience = 30
    no_improve = 0

    for _ in range(max_steps):
        # Identify bottleneck
        current_ks = [get_k(i) for i in range(gpu_num)]
        max_k = max(current_ks)

        # Check global improvement
        if max_k < best_max_k - 1e-6:
            best_max_k = max_k
            best_placement = copy.deepcopy(placement)
            no_improve = 0
        else:
            no_improve += 1

        # Ruin & Recreate
        # Select bottleneck and random others
        candidates = [i for i, k in enumerate(current_ks) if k > max_k * 0.99]
        if not candidates: candidates = [random.randint(0, gpu_num-1)]
        src = random.choice(candidates)

        ruin_set = {src}
        n_ruin = 2 + (1 if no_improve > patience else 0)

        others = list(range(gpu_num))
        random.shuffle(others)
        for o in others:
            if len(ruin_set) >= min(gpu_num, n_ruin): break
            if o != src: ruin_set.add(o)

        # Ruin
        removed_models = []
        backup = {}
        for idx in ruin_set:
            backup[idx] = (list(placement[idx]), gpu_s[idx], gpu_w[idx])
            removed_models.extend(placement[idx])
            placement[idx] = []
            gpu_s[idx] = 0.0
            gpu_w[idx] = 0.0

        # Recreate (Greedy Best Fit)
        # Sort by physical size desc to pack well
        removed_models.sort(key=lambda m: m.model_size, reverse=True)

        feasible = True
        for m in removed_models:
            best_idx = -1
            best_score = float('inf')

            m_s = m.model_size
            m_w = m.req_rate / m.slo

            for idx in ruin_set:
                if gpu_s[idx] + m_s <= GPU_MEM_SIZE:
                    # Score: minimizing resulting local K
                    rem = GPU_MEM_SIZE - (gpu_s[idx] + m_s)
                    k = (gpu_w[idx] + m_w) / rem if rem > 1e-7 else 1e9
                    if k < best_score:
                        best_score = k
                        best_idx = idx

            if best_idx != -1:
                placement[best_idx].append(m)
                gpu_s[best_idx] += m_s
                gpu_w[best_idx] += m_w
            else:
                feasible = False
                break

        # Acceptance
        accept = False
        if feasible:
            new_ks = [get_k(i) for i in range(gpu_num)]
            new_max = max(new_ks)
            if new_max < max_k - 1e-6:
                accept = True
            elif new_max < max_k + 1e-6:
                # Accept equal moves to traverse plateau
                accept = True
            elif no_improve > patience and random.random() < 0.2:
                # Random walk
                accept = True

        if not accept:
            # Revert
            for idx in ruin_set:
                placement[idx], gpu_s[idx], gpu_w[idx] = backup[idx]

    return best_placement
=======
def _iterated_local_search(gpu_num, placement):
    """
    Refines placement using Steepest Descent with Variance Tie-Breaking and Burst Kicks.
    """
    # Initialize state
    gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
    gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]

    def get_k(i):
        rem = GPU_MEM_SIZE - gpu_s[i]
        if rem <= 1e-7: return 1e9
        return gpu_w[i] / rem

    current_ks = [get_k(i) for i in range(gpu_num)]
    best_max_k = max(current_ks)
    best_placement = copy.deepcopy(placement)

    # Calculate initial sum of squared pressures (variance proxy)
    current_sum_sq = sum(k*k for k in current_ks)

    max_steps = 500
    patience = 20
    no_improve = 0

    for step in range(max_steps):
        # 1. Identify bottleneck
        max_k = -1.0
        src = -1
        for i in range(gpu_num):
            if current_ks[i] > max_k:
                max_k = current_ks[i]
                src = i

        # Check global improvement
        if max_k < best_max_k - 1e-7:
            best_max_k = max_k
            best_placement = copy.deepcopy(placement)
            no_improve = 0
        else:
            no_improve += 1

        # 2. Burst Kicks (if stuck)
        if no_improve > patience:
            # Perform a burst of random moves to escape local optimum
            n_kicks = random.randint(3, 5)
            for _ in range(n_kicks):
                for _ in range(10): # retry limit for valid move
                    s = random.randint(0, gpu_num - 1)
                    if not placement[s]: continue
                    d = random.randint(0, gpu_num - 1)
                    if s == d: continue

                    m_idx = random.randint(0, len(placement[s]) - 1)
                    m = placement[s][m_idx]

                    if gpu_s[d] + m.model_size <= GPU_MEM_SIZE:
                        placement[d].append(m)
                        placement[s].pop(m_idx)
                        gpu_s[d] += m.model_size; gpu_w[d] += m.req_rate/m.slo
                        gpu_s[s] -= m.model_size; gpu_w[s] -= m.req_rate/m.slo

                        current_ks[s] = get_k(s)
                        current_ks[d] = get_k(d)
                        current_sum_sq = sum(k*k for k in current_ks)
                        break
            no_improve = 0 # Reset patience after kick
            continue

        # 3. Steepest Descent (Greedy)
        # Try to improve the bottleneck 'src' by Moving or Swapping
        best_move = None # ((improvement_type, peak_diff, var_diff), action_tuple)

        # Sort items in bottleneck by workload descending (heuristic: move heavy items first)
        src_items = sorted(enumerate(placement[src]), key=lambda x: x[1].req_rate/x[1].slo, reverse=True)

        # A. Check Moves
        for idx, m in src_items:
            w, s = m.req_rate/m.slo, m.model_size
            for dst in range(gpu_num):
                if dst == src: continue
                if gpu_s[dst] + s > GPU_MEM_SIZE: continue

                # Predict new state
                rem_src = GPU_MEM_SIZE - (gpu_s[src] - s)
                nk_src = (gpu_w[src] - w) / rem_src if rem_src > 1e-7 else 1e9

                rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + s)
                nk_dst = (gpu_w[dst] + w) / rem_dst if rem_dst > 1e-7 else 1e9

                new_peak = max(nk_src, nk_dst)

                # Pruning: If local peak creates new global violation, skip
                if new_peak > max_k + 1e-7: continue

                peak_diff = max_k - new_peak
                var_diff = (current_ks[src]**2 + current_ks[dst]**2) - (nk_src**2 + nk_dst**2)

                score = None
                if peak_diff > 1e-7: # Strict improvement
                    score = (1, peak_diff, var_diff)
                elif peak_diff > -1e-7: # Plateau (Equal peak)
                    if var_diff > 1e-5: # Variance reduction
                        score = (0, 0, var_diff)

                if score and (best_move is None or score > best_move[0]):
                    best_move = (score, ('move', idx, dst, s, w))

        # B. Check Swaps (if no Move found or to see if Swap is better)
        # Only check if we haven't found a Peak Reduction move yet to save time
        if best_move is None or best_move[0][0] == 0:
            for i1, m1 in src_items:
                w1, s1 = m1.req_rate/m1.slo, m1.model_size
                for dst in range(gpu_num):
                    if dst == src: continue
                    # Optimization: Don't swap with highly stressed GPUs
                    if current_ks[dst] > max_k * 0.95: continue

                    for i2, m2 in enumerate(placement[dst]):
                        w2, s2 = m2.req_rate/m2.slo, m2.model_size

                        ns_src = gpu_s[src] - s1 + s2
                        ns_dst = gpu_s[dst] - s2 + s1
                        if ns_src > GPU_MEM_SIZE or ns_dst > GPU_MEM_SIZE: continue

                        rem_src = GPU_MEM_SIZE - ns_src
                        nk_src = (gpu_w[src] - w1 + w2) / rem_src if rem_src > 1e-7 else 1e9
                        rem_dst = GPU_MEM_SIZE - ns_dst
                        nk_dst = (gpu_w[dst] - w2 + w1) / rem_dst if rem_dst > 1e-7 else 1e9

                        new_peak = max(nk_src, nk_dst)
                        if new_peak > max_k + 1e-7: continue

                        peak_diff = max_k - new_peak
                        var_diff = (current_ks[src]**2 + current_ks[dst]**2) - (nk_src**2 + nk_dst**2)

                        score = None
                        if peak_diff > 1e-7:
                            score = (1, peak_diff, var_diff)
                        elif peak_diff > -1e-7:
                            if var_diff > 1e-5:
                                score = (0, 0, var_diff)

                        if score and (best_move is None or score > best_move[0]):
                            best_move = (score, ('swap', i1, dst, i2, s1, w1, s2, w2))

        # Apply Best Move
        if best_move:
            _, action = best_move
            if action[0] == 'move':
                _, idx, dst, s, w = action
                m = placement[src].pop(idx)
                placement[dst].append(m)
                gpu_s[src] -= s; gpu_w[src] -= w
                gpu_s[dst] += s; gpu_w[dst] += w
            elif action[0] == 'swap':
                _, i1, dst, i2, s1, w1, s2, w2 = action
                m1 = placement[src][i1]
                m2 = placement[dst][i2]
                placement[src][i1] = m2
                placement[dst][i2] = m1
                gpu_s[src] = gpu_s[src] - s1 + s2; gpu_w[src] = gpu_w[src] - w1 + w2
                gpu_s[dst] = gpu_s[dst] - s2 + s1; gpu_w[dst] = gpu_w[dst] - w2 + w1

            # Update State
            current_ks[src] = get_k(src)
            current_ks[dst] = get_k(dst)
            current_sum_sq = sum(k*k for k in current_ks)

            # If peak didn't drop (plateau move), increase no_improve counter slightly
            # but less than a full fail to delay kick
            if max(current_ks) >= max_k - 1e-7:
                no_improve += 1
            else:
                no_improve = 0
        else:
            no_improve += 1

    return best_placement
>>>>>>> REPLACE
</DIFF>