<NAME>
greedy_kvpr_projection
</NAME>

<DESCRIPTION>
The original algorithm employed a greedy strategy that placed models onto the GPU with the *current* lowest KVPR (KV cache pressure). This approach is suboptimal because it ignores the impact of the current model's load and size on the target GPU's KVPR. Specifically, placing a model increases the numerator (load) and decreases the denominator (available memory), which can drastically increase the KVPR, especially if memory becomes scarce.

This change improves the algorithm by:
1.  Modifying the sorting strategy to sort models primarily by `model_size` descending, and then by `req_rate / slo` descending. Sorting by size (similar to First Fit Decreasing) helps handle packing constraints and prevents scenarios where large models are forced into GPUs with little remaining memory, which would cause the denominator to shrink dangerously and spike the KVPR.
2.  Updating the greedy selection logic to calculate the *resulting* KVPR of a candidate GPU if the model were to be placed there. The algorithm then selects the GPU that minimizes this projected cost. This effectively minimizes the maximum KVPR in a greedy fashion by always choosing the path of least resistance for the objective function.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Greedy KVPR-minimizing placement based on Algorithm 1 (without Ï„ check)
    # 1) Sort models by r_j / s_j in descending order
    sorted_models = sorted(models, key=lambda m: (m.req_rate / m.slo), reverse=True)

    # 2) Initialize per-GPU states
    placement = {gpu_id: [] for gpu_id in range(gpu_num)}
    shared_kv = [GPU_MEM_SIZE for _ in range(gpu_num)]  # remaining memory per GPU
    weighted_req_rate = [0.0 for _ in range(gpu_num)]   # sum of r_j / s_j per GPU

    # 3) Assign each model to the GPU that minimizes current KVPR while fitting in memory
    for model in sorted_models:
        best_idx = None
        best_ratio = float('inf')

        for gpu_id in range(gpu_num):
            if model.model_size <= shared_kv[gpu_id] and shared_kv[gpu_id] > 0:
                current_ratio = weighted_req_rate[gpu_id] / shared_kv[gpu_id]
                if current_ratio < best_ratio:
                    best_ratio = current_ratio
                    best_idx = gpu_id

        # Failure: if no GPU can fit, raise an error instead of overcommitting
=======
    # Greedy KVPR-minimizing placement with projected cost
    # 1) Sort models by size descending (packing), then load descending
    sorted_models = sorted(models, key=lambda m: (m.model_size, m.req_rate / m.slo), reverse=True)

    # 2) Initialize per-GPU states
    placement = {gpu_id: [] for gpu_id in range(gpu_num)}
    shared_kv = [GPU_MEM_SIZE for _ in range(gpu_num)]  # remaining memory per GPU
    weighted_req_rate = [0.0 for _ in range(gpu_num)]   # sum of r_j / s_j per GPU

    # 3) Assign each model to the GPU that minimizes the resulting KVPR
    for model in sorted_models:
        best_idx = None
        best_kvpr = float('inf')
        model_load = model.req_rate / model.slo

        for gpu_id in range(gpu_num):
            if model.model_size <= shared_kv[gpu_id]:
                # Calculate resulting KVPR
                rem_mem = shared_kv[gpu_id] - model.model_size
                if rem_mem <= 1e-6:
                    new_kvpr = float('inf')
                else:
                    new_kvpr = (weighted_req_rate[gpu_id] + model_load) / rem_mem

                if best_idx is None or new_kvpr < best_kvpr:
                    best_kvpr = new_kvpr
                    best_idx = gpu_id

        # Failure: if no GPU can fit, raise an error instead of overcommitting
>>>>>>> REPLACE
</DIFF>