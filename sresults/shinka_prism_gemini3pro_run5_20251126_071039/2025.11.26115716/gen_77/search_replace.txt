<NAME>
ils_swap_always_and_multi_pack
</NAME>

<DESCRIPTION>
1.  **Multi-Strategy Packing in Feasibility Check**: Incorporate First Fit Decreasing (FFD) alongside Best Fit Decreasing (BFD) in the `_check_feasibility_robust` function. While BFD is generally superior for tight packing, FFD can sometimes find valid configurations that BFD misses due to different bin assignment logic. This robustness can allow the binary search to validate a lower initial $K$.
2.  **Comprehensive Steepest Descent**: Modify the Iterated Local Search (ILS) to always evaluate both "Move" and "Swap" operations from the bottleneck GPU. The previous implementation skipped Swap evaluation if a Move was found, potentially missing a Swap that yields a larger reduction in KVPR.
3.  **Unified Acceptance Criteria**: Standardize the logic for accepting moves/swaps. A candidate is accepted if it strictly improves the maximum KVPR compared to the best found so far in the iteration, OR if it matches the best KVPR improvement but offers a greater reduction in pressure variance (sum of squared KVPRs). This ensures the steepest possible descent and better plateau traversal.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def _check_feasibility_robust(gpu_num, items, K):
    """
    Checks feasibility using Best-Fit Decreasing with multiple sorting keys
    and randomized trials.
    """
    virtual_cap = K * GPU_MEM_SIZE

    # Augment items with virtual size
    pack_items = []
    for x in items:
        # Virtual Size: v = w + K*s
        v = x['w'] + K * x['s']
        pack_items.append({'v': v, 's': x['s'], 'w': x['w'], 'd': x['d'], 'm': x['m']})

    # 1. Deterministic Heuristics
    heuristics = [
        (lambda x: x['v'], True),  # Virtual Size Descending (Standard)
        (lambda x: x['s'], True),  # Physical Size Descending
        (lambda x: x['w'], True),  # Load Descending
        (lambda x: x['d'], True),  # Density Descending
    ]

    for key_func, rev in heuristics:
        pack_items.sort(key=key_func, reverse=rev)
        if res := _pack_bfd(gpu_num, pack_items, virtual_cap):
            return True, res

    # 2. Randomized Heuristics
    # Try random orderings to break specific pathological cases
    temp_items = pack_items[:]
    for _ in range(8):
        random.shuffle(temp_items)
        if res := _pack_bfd(gpu_num, temp_items, virtual_cap):
            return True, res

    return False, None

def _pack_bfd(gpu_num, items, virtual_cap):
    """
    Best Fit Decreasing: Places item in the bin with min sufficient virtual capacity.
    """
    bins_v = [0.0] * gpu_num
    bins_p = [0.0] * gpu_num
    placement = [[] for _ in range(gpu_num)]

    for item in items:
        best_bin = -1
        min_rem_v = float('inf')

        for i in range(gpu_num):
            # Check constraints
            if bins_p[i] + item['s'] <= GPU_MEM_SIZE and bins_v[i] + item['v'] <= virtual_cap + 1e-7:
                # Calculate residual
                rem = virtual_cap - (bins_v[i] + item['v'])
                if rem < min_rem_v:
                    min_rem_v = rem
                    best_bin = i

        if best_bin != -1:
            bins_p[best_bin] += item['s']
            bins_v[best_bin] += item['v']
            placement[best_bin].append(item['m'])
        else:
            return None # Fail

    return placement
=======
def _check_feasibility_robust(gpu_num, items, K):
    """
    Checks feasibility using Best-Fit and First-Fit Decreasing with multiple sorting keys.
    """
    virtual_cap = K * GPU_MEM_SIZE

    # Augment items with virtual size
    pack_items = []
    for x in items:
        v = x['w'] + K * x['s']
        pack_items.append({'v': v, 's': x['s'], 'w': x['w'], 'd': x['d'], 'm': x['m']})

    # 1. Deterministic Heuristics
    heuristics = [
        (lambda x: x['v'], True),  # Virtual Size Descending
        (lambda x: x['s'], True),  # Physical Size Descending
        (lambda x: x['w'], True),  # Load Descending
        (lambda x: x['d'], True),  # Density Descending
    ]

    for key_func, rev in heuristics:
        pack_items.sort(key=key_func, reverse=rev)
        # Try Best Fit Decreasing (usually superior for tight packing)
        if res := _pack_bfd(gpu_num, pack_items, virtual_cap):
            return True, res
        # Try First Fit Decreasing (adds diversity in bin assignment)
        if res := _pack_ffd(gpu_num, pack_items, virtual_cap):
            return True, res

    # 2. Randomized Heuristics (Fallback)
    temp_items = pack_items[:]
    for _ in range(4):
        random.shuffle(temp_items)
        if res := _pack_bfd(gpu_num, temp_items, virtual_cap):
            return True, res

    return False, None

def _pack_bfd(gpu_num, items, virtual_cap):
    bins_v = [0.0] * gpu_num
    bins_p = [0.0] * gpu_num
    placement = [[] for _ in range(gpu_num)]

    for item in items:
        best_bin = -1
        min_rem_v = float('inf')

        for i in range(gpu_num):
            if bins_p[i] + item['s'] <= GPU_MEM_SIZE and bins_v[i] + item['v'] <= virtual_cap + 1e-7:
                rem = virtual_cap - (bins_v[i] + item['v'])
                if rem < min_rem_v:
                    min_rem_v = rem
                    best_bin = i
        if best_bin != -1:
            bins_p[best_bin] += item['s']
            bins_v[best_bin] += item['v']
            placement[best_bin].append(item['m'])
        else:
            return None
    return placement

def _pack_ffd(gpu_num, items, virtual_cap):
    bins_v = [0.0] * gpu_num
    bins_p = [0.0] * gpu_num
    placement = [[] for _ in range(gpu_num)]

    for item in items:
        placed = False
        for i in range(gpu_num):
            if bins_p[i] + item['s'] <= GPU_MEM_SIZE and bins_v[i] + item['v'] <= virtual_cap + 1e-7:
                bins_p[i] += item['s']
                bins_v[i] += item['v']
                placement[i].append(item['m'])
                placed = True
                break
        if not placed: return None
    return placement
>>>>>>> REPLACE
<<<<<<< SEARCH
        # 3. Steepest Descent with Variance Tie-Breaking
        # We only look for moves from 'src' (bottleneck) to others

        best_move = None # (type, idx1, dst, idx2, ...)
        best_imp_k = -1.0
        best_imp_var = -1.0

        models = placement[src]
        # Sort models by contribution (Weight) to find good moves earlier (optimization)
        sorted_models_idx = sorted(range(len(models)), key=lambda i: models[i].req_rate/models[i].slo, reverse=True)

        # A. Try Moves
        for i in sorted_models_idx:
            m = models[i]
            w, s = m.req_rate/m.slo, m.model_size

            for dst in range(gpu_num):
                if dst == src: continue
                if gpu_s[dst] + s > GPU_MEM_SIZE: continue

                # Check metrics
                rem_src = GPU_MEM_SIZE - (gpu_s[src] - s)
                nk_src = (gpu_w[src] - w) / rem_src if rem_src > 1e-7 else 1e9

                rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + s)
                nk_dst = (gpu_w[dst] + w) / rem_dst if rem_dst > 1e-7 else 1e9

                new_peak = max(nk_src, nk_dst)

                # Delta calculations
                diff_k = max_k - new_peak

                # Accept if Peak reduces significantly
                if diff_k > 1e-6:
                    if diff_k > best_imp_k:
                        best_imp_k = diff_k
                        best_move = ('move', i, dst, s, w)
                        best_imp_var = 0 # Irrelevant if K improves

                # Accept if Peak is same (approx) but Variance reduces
                elif abs(diff_k) < 1e-6 and best_imp_k < 1e-6:
                    old_sq = current_ks[src]**2 + current_ks[dst]**2
                    new_sq = nk_src**2 + nk_dst**2
                    diff_var = old_sq - new_sq

                    if diff_var > 1e-5 and diff_var > best_imp_var:
                        best_imp_var = diff_var
                        best_move = ('move', i, dst, s, w)

        # B. Try Swaps (Only if no significant move found to save time)
        if best_imp_k < 1e-6:
            for i1 in sorted_models_idx:
                m1 = models[i1]
                w1, s1 = m1.req_rate/m1.slo, m1.model_size

                for dst in range(gpu_num):
                    if dst == src: continue
                    # Optimization: Don't swap with another bottleneck
                    if current_ks[dst] > max_k * 0.95: continue

                    for i2, m2 in enumerate(placement[dst]):
                        w2, s2 = m2.req_rate/m2.slo, m2.model_size

                        ns_src = gpu_s[src] - s1 + s2
                        ns_dst = gpu_s[dst] - s2 + s1

                        if ns_src > GPU_MEM_SIZE or ns_dst > GPU_MEM_SIZE: continue

                        rem_src = GPU_MEM_SIZE - ns_src
                        nk_src = (gpu_w[src] - w1 + w2) / rem_src if rem_src > 1e-7 else 1e9

                        rem_dst = GPU_MEM_SIZE - ns_dst
                        nk_dst = (gpu_w[dst] - w2 + w1) / rem_dst if rem_dst > 1e-7 else 1e9

                        new_peak = max(nk_src, nk_dst)
                        diff_k = max_k - new_peak

                        if diff_k > 1e-6:
                            if diff_k > best_imp_k:
                                best_imp_k = diff_k
                                best_move = ('swap', i1, dst, i2, s1, w1, s2, w2)
                                best_imp_var = 0
                        elif abs(diff_k) < 1e-6 and best_imp_k < 1e-6:
                            old_sq = current_ks[src]**2 + current_ks[dst]**2
                            new_sq = nk_src**2 + nk_dst**2
                            diff_var = old_sq - new_sq

                            if diff_var > 1e-5 and diff_var > best_imp_var:
                                best_imp_var = diff_var
                                best_move = ('swap', i1, dst, i2, s1, w1, s2, w2)

        # Execute Best Move
        if best_move:
=======
        # 3. Steepest Descent with Variance Tie-Breaking
        # Evaluate Moves AND Swaps to find the absolute best step

        best_move = None
        best_imp_k = -1.0
        best_imp_var = -1.0

        models = placement[src]
        sorted_models_idx = sorted(range(len(models)), key=lambda i: models[i].req_rate/models[i].slo, reverse=True)

        # A. Evaluate Moves
        for i in sorted_models_idx:
            m = models[i]
            w, s = m.req_rate/m.slo, m.model_size

            for dst in range(gpu_num):
                if dst == src: continue
                if gpu_s[dst] + s > GPU_MEM_SIZE: continue

                rem_src = GPU_MEM_SIZE - (gpu_s[src] - s)
                nk_src = (gpu_w[src] - w) / rem_src if rem_src > 1e-7 else 1e9

                rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + s)
                nk_dst = (gpu_w[dst] + w) / rem_dst if rem_dst > 1e-7 else 1e9

                new_peak = max(nk_src, nk_dst)
                diff_k = max_k - new_peak

                # Calculate variance change
                old_sq = current_ks[src]**2 + current_ks[dst]**2
                new_sq = nk_src**2 + nk_dst**2
                diff_var = old_sq - new_sq

                is_better = False

                if diff_k > 1e-6:
                    if diff_k > best_imp_k + 1e-7:
                        is_better = True
                    elif abs(diff_k - best_imp_k) < 1e-7:
                        # Tie on K improvement, check variance
                        if diff_var > best_imp_var + 1e-7:
                            is_better = True
                elif abs(diff_k) < 1e-6 and best_imp_k < 1e-6:
                    # Plateau move: only accept if variance improves and we haven't found a K-improving move
                    if diff_var > 1e-5 and diff_var > best_imp_var:
                        is_better = True

                if is_better:
                    best_imp_k = max(best_imp_k, diff_k)
                    best_imp_var = diff_var
                    best_move = ('move', i, dst, s, w)

        # B. Evaluate Swaps (Always check for better K improvement)
        for i1 in sorted_models_idx:
            m1 = models[i1]
            w1, s1 = m1.req_rate/m1.slo, m1.model_size

            for dst in range(gpu_num):
                if dst == src: continue

                for i2, m2 in enumerate(placement[dst]):
                    w2, s2 = m2.req_rate/m2.slo, m2.model_size

                    ns_src = gpu_s[src] - s1 + s2
                    ns_dst = gpu_s[dst] - s2 + s1

                    if ns_src > GPU_MEM_SIZE or ns_dst > GPU_MEM_SIZE: continue

                    rem_src = GPU_MEM_SIZE - ns_src
                    nk_src = (gpu_w[src] - w1 + w2) / rem_src if rem_src > 1e-7 else 1e9

                    rem_dst = GPU_MEM_SIZE - ns_dst
                    nk_dst = (gpu_w[dst] - w2 + w1) / rem_dst if rem_dst > 1e-7 else 1e9

                    new_peak = max(nk_src, nk_dst)
                    diff_k = max_k - new_peak

                    old_sq = current_ks[src]**2 + current_ks[dst]**2
                    new_sq = nk_src**2 + nk_dst**2
                    diff_var = old_sq - new_sq

                    is_better = False

                    if diff_k > 1e-6:
                        if diff_k > best_imp_k + 1e-7:
                            is_better = True
                        elif abs(diff_k - best_imp_k) < 1e-7:
                            if diff_var > best_imp_var + 1e-7:
                                is_better = True
                    elif abs(diff_k) < 1e-6 and best_imp_k < 1e-6:
                        if diff_var > 1e-5 and diff_var > best_imp_var:
                            is_better = True

                    if is_better:
                        best_imp_k = max(best_imp_k, diff_k)
                        best_imp_var = diff_var
                        best_move = ('swap', i1, dst, i2, s1, w1, s2, w2)

        # Execute Best Move
        if best_move:
>>>>>>> REPLACE
</DIFF>