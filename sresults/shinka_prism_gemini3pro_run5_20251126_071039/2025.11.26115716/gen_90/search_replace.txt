<NAME>
fix_density_calculation
</NAME>

<DESCRIPTION>
Modify the density calculation to prioritize items with zero or very small model size but high load. In the previous implementation, `d = w / s` resulted in `d=0` for `s=0` items (pure load), placing them last in the BFD packing. These items consume no memory but add pressure, so placing them late can spike the pressure of already filled GPUs. By using `w / (s + 1e-6)`, we assign them high density, ensuring they are placed early when pressure "headroom" is available, improving load balancing.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        # Density: pressure per unit size
        d = w / s if s > 1e-6 else 0
        m_data.append({'w': w, 's': s, 'd': d, 'obj': m})
=======
        # Density: pressure per unit size
        # Use small epsilon to handle zero-size items (high density)
        d = w / (s + 1e-6)
        m_data.append({'w': w, 's': s, 'd': d, 'obj': m})
>>>>>>> REPLACE
</DIFF>

<NAME>
improve_initialization_and_bounds
</NAME>

<DESCRIPTION>
Enhance the initialization phase by trying multiple sorting strategies (Density, Size, Load) instead of just Density to find a better initial upper bound. Also, refine the lower bound for Binary Search by considering the maximum individual pressure required by any single model (`w / (Cap - s)`). This tighter lower bound and better initial upper bound accelerate the convergence of the binary search and improve the quality of the starting solution for the ILS.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Try density sort to get a good upper bound
    init_res, init_max = pack_heuristic(lambda x: x['d'])
    if init_res:
        best_bs_placement = init_res
        high_bound = init_max
    else:
        # Fallback upper bound
        high_bound = 1000.0
        best_bs_placement = None

    # Binary Search Loop
    low = low_bound
    high = high_bound
    if high > 999 and init_res is None: high = 10.0 # Heuristic adjustment if no initial found
=======
    # Try multiple sort keys to get the best starting upper bound
    best_bs_placement = None
    high_bound = 1000.0
    found_init = False

    init_keys = [lambda x: x['d'], lambda x: x['s'], lambda x: x['w']]
    for key in init_keys:
        res, res_max = pack_heuristic(key)
        if res:
            if best_bs_placement is None or res_max < high_bound:
                best_bs_placement = res
                high_bound = res_max
                found_init = True

    # Binary Search Loop
    # Improve lower bound with individual item constraints
    max_indiv_p = 0.0
    for item in m_data:
        rem_s = GPU_MEM_SIZE - item['s']
        if rem_s > 1e-6:
             p = item['w'] / rem_s
             if p > max_indiv_p: max_indiv_p = p

    low = max(low_bound, max_indiv_p)
    high = high_bound
    if high > 999 and not found_init: high = 10.0 # Heuristic adjustment if no initial found
>>>>>>> REPLACE
</DIFF>

<NAME>
add_virtual_size_perturbation
</NAME>

<DESCRIPTION>
Add a "Virtual Size" strategy to the perturbation (kick) phase in the ILS. Virtual size (`req_rate/slo + K * model_size`) combines pressure and memory usage, which is effective for packing. We use the current global best max pressure `best_max_p` as the coefficient `K`, allowing the perturbation to adapt to the current solution quality. We also increase the number of trials to accommodate the new strategy.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
            strategies = [
                lambda x: (x.req_rate/x.slo)/(x.model_size+1e-6), # Density
                lambda x: x.req_rate/x.slo,                       # Load
                lambda x: x.model_size                            # Size
            ]

            for t in range(12): # 12 trials
                trial_items = list(repack_items)

                if t < 9:
                    # Deterministic strategies with multiplicative noise
                    base_key = strategies[t % 3]
                    trial_items.sort(key=lambda x: base_key(x) * random.uniform(0.8, 1.2), reverse=True)
                else:
=======
            strategies = [
                lambda x: (x.req_rate/x.slo)/(x.model_size+1e-6), # Density
                lambda x: x.req_rate/x.slo,                       # Load
                lambda x: x.model_size,                           # Size
                lambda x: (x.req_rate/x.slo) + best_max_p * x.model_size # Virtual Size
            ]

            for t in range(16): # Increased trials
                trial_items = list(repack_items)

                if t < 12:
                    # Deterministic strategies with multiplicative noise
                    base_key = strategies[t % 4]
                    trial_items.sort(key=lambda x: base_key(x) * random.uniform(0.85, 1.15), reverse=True)
                else:
>>>>>>> REPLACE
</DIFF>