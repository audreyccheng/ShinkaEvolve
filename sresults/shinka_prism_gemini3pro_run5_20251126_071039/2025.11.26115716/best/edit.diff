--- a/original.py
+++ b/original.py
@@ -1,171 +1,274 @@
 # EVOLVE-BLOCK-START
-"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs using Binary Search and FFD Packing"""
+"""Model placement algorithm for minimizing maximum KV cache pressure using Multi-Strategy Packing and Iterated Local Search"""
+
+import copy
+import random
 
 GPU_MEM_SIZE = 80  # GB
 
 def compute_model_placement(gpu_num, models):
     """
-    Compute a model placement that minimizes the maximum KVPR across all GPUs.
-    Uses binary search on the answer combined with First Fit Decreasing bin packing.
-
-    Args:
-        gpu_num: Number of GPUs
-        models: List of models to place
-
-    Returns:
-        A placement of models to GPUs
-    """
-    
-    # Check total physical size feasibility as a quick fail-safe
-    if sum(m.model_size for m in models) > gpu_num * GPU_MEM_SIZE:
+    Minimizes max KVPR using Binary Search with multiple packing heuristics 
+    followed by Iterated Local Search for refinement.
+    """
+    total_size = sum(m.model_size for m in models)
+    if total_size > gpu_num * GPU_MEM_SIZE:
         raise ValueError("Total model size exceeds total GPU memory capacity.")
 
-    # Prepare model data: (w, size, model_obj) where w = req_rate / slo
-    model_data = []
-    for m in models:
-        model_data.append(((m.req_rate / m.slo), m.model_size, m))
-
-    # Binary search for the minimum feasible max_kvpr (X).
-    # Range of X is [0, inf).
-    # We first find a feasible upper bound 'high'.
-    
-    high = 1.0
-    # Heuristic initialization for high: 
-    # Estimate based on average load or use a safe doubling strategy.
-    # Start with a heuristic guess to save iterations.
-    total_w = sum(x[0] for x in model_data)
-    total_slack = gpu_num * GPU_MEM_SIZE - sum(x[1] for x in model_data)
-    if total_slack > 1e-6:
-        high = max(high, (total_w / total_slack) * 2.0)
-    else:
-        high = 100.0
-
-    # Find a valid upper bound
-    best_placement = None
+    # Prepare items for packing: (w, s, m)
+    # w = req_rate / slo
+    items = [{'w': m.req_rate / m.slo, 's': m.model_size, 'm': m} for m in models]
+
+    # Binary Search for optimal KVPR
+    total_w = sum(x['w'] for x in items)
+    slack = gpu_num * GPU_MEM_SIZE - total_size
+    low = 0.0
+    # Heuristic upper bound
+    high = (total_w / slack) * 4.0 if slack > 1e-4 else 1000.0
+    high = max(high, 10.0)
+
+    best_packing_placement = None
     feasible_high = False
     
+    # Find valid upper bound
     for _ in range(20):
-        is_feasible, placement = _check_feasibility(gpu_num, model_data, high)
-        if is_feasible:
+        feasible, placement = _check_feasibility_multi(gpu_num, items, high)
+        if feasible:
+            best_packing_placement = placement
             feasible_high = True
-            best_placement = placement
             break
+        low = high
         high *= 2.0
-    
+        
     if not feasible_high:
-        # If we cannot find a placement even with very high KVPR, 
-        # it likely implies physical constraints are tight or fragmentation is high.
-        raise ValueError(f"Unable to place models. Physical constraints or fragmentation too high.")
+        raise ValueError("Unable to place models. Constraints likely too tight.")
 
     # Binary Search
-    low = 0.0
-    # 30 iterations is sufficient for high precision
     for _ in range(30):
         mid = (low + high) / 2.0
-        is_feasible, placement = _check_feasibility(gpu_num, model_data, mid)
-        if is_feasible:
-            best_placement = placement
+        feasible, placement = _check_feasibility_multi(gpu_num, items, mid)
+        if feasible:
+            best_packing_placement = placement
             high = mid
         else:
             low = mid
             
-    # Format the output as expected {gpu_id: [models]}
-    result = {}
-    for i in range(gpu_num):
-        result[i] = best_placement[i] if i < len(best_placement) else []
-        
-    return result
-
-def _check_feasibility(gpu_num, model_data, target_kvpr):
-    """
-    Check if models can be packed into gpu_num bins given a target KVPR.
-    Constraint: sum(w) / (Capacity - sum(size)) <= target_kvpr
-    Rearranged: sum(w + target_kvpr * size) <= target_kvpr * Capacity
-    """
-    # Virtual Capacity
-    cap = target_kvpr * GPU_MEM_SIZE
-    
-    # Calculate virtual sizes and sort for FFD
-    # Item tuple: (virtual_size, physical_size, model_obj)
-    # virtual_size = w + target_kvpr * size
-    items = []
-    for w, size, m in model_data:
-        v_size = w + target_kvpr * size
-        items.append((v_size, size, m))
-        
-    # Sort descending by virtual size (FFD heuristic)
-    items.sort(key=lambda x: x[0], reverse=True)
-    
-    # Bins state
-    bins_v_load = [0.0] * gpu_num
-    bins_p_load = [0.0] * gpu_num
-    placements = [[] for _ in range(gpu_num)]
-    
-    for v_size, p_size, m in items:
+    # Convert list-based placement to dict format
+    placement_map = {i: best_packing_placement[i] for i in range(gpu_num)}
+    
+    # Refinement using Iterated Local Search
+    final_placement = _iterated_local_search(gpu_num, placement_map)
+    
+    return final_placement
+
+def _check_feasibility_multi(gpu_num, items, K):
+    """
+    Check if items can be packed with target KVPR 'K' using multiple heuristics.
+    """
+    virtual_cap = K * GPU_MEM_SIZE
+    # Create pack items: (virtual_size, physical_size, model)
+    # virtual_size = w + K * s
+    pack_items = [(x['w'] + K * x['s'], x['s'], x['m']) for x in items]
+    
+    # Strategy 1: FFD on Virtual Size (Standard)
+    pack_items.sort(key=lambda x: x[0], reverse=True)
+    res = _pack(gpu_num, pack_items, virtual_cap)
+    if res: return True, res
+    
+    # Strategy 2: FFD on Physical Size (Good for size-constrained)
+    pack_items_p = sorted(pack_items, key=lambda x: x[1], reverse=True)
+    res = _pack(gpu_num, pack_items_p, virtual_cap)
+    if res: return True, res
+
+    return False, None
+
+def _pack(gpu_num, items, virtual_cap):
+    bins_v = [0.0] * gpu_num
+    bins_p = [0.0] * gpu_num
+    placement = [[] for _ in range(gpu_num)]
+    
+    for v, p, m in items:
         placed = False
         for i in range(gpu_num):
-            # Check virtual capacity constraint
-            # Using a small epsilon for float comparison stability
-            if bins_v_load[i] + v_size <= cap + 1e-7:
-                # Check physical capacity constraint (hard limit)
-                if bins_p_load[i] + p_size <= GPU_MEM_SIZE:
-                    bins_v_load[i] += v_size
-                    bins_p_load[i] += p_size
-                    placements[i].append(m)
-                    placed = True
+            if bins_p[i] + p <= GPU_MEM_SIZE and bins_v[i] + v <= virtual_cap + 1e-7:
+                bins_p[i] += p
+                bins_v[i] += v
+                placement[i].append(m)
+                placed = True
+                break
+        if not placed: return None
+    return placement
+
+def _iterated_local_search(gpu_num, placement):
+    """
+    Refines placement using Hill Climbing with Random Kicks.
+    """
+    # Initialize state
+    gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
+    gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]
+    
+    def get_kvpr(idx):
+        rem = GPU_MEM_SIZE - gpu_s[idx]
+        if rem <= 1e-7: return 1e9 # Penalty
+        return gpu_w[idx] / rem
+
+    best_max_k = max(get_kvpr(i) for i in range(gpu_num))
+    best_sol = copy.deepcopy(placement)
+    
+    no_improve = 0
+    max_steps = 300
+    patience = 30
+    
+    for _ in range(max_steps):
+        # Identify bottleneck GPU
+        max_k = -1.0
+        src = -1
+        for i in range(gpu_num):
+            k = get_kvpr(i)
+            if k > max_k:
+                max_k = k
+                src = i
+        
+        # Check global improvement
+        if max_k < best_max_k - 1e-6:
+            best_max_k = max_k
+            best_sol = copy.deepcopy(placement)
+            no_improve = 0
+        else:
+            no_improve += 1
+
+        # Kick / Perturbation
+        if no_improve > patience:
+            # Perform a random move to escape local optimum
+            for _ in range(20): # Try up to 20 times to find a valid random move
+                s_rnd = random.randint(0, gpu_num-1)
+                if not placement[s_rnd]: continue
+                d_rnd = random.randint(0, gpu_num-1)
+                if s_rnd == d_rnd: continue
+                
+                m_idx = random.randint(0, len(placement[s_rnd])-1)
+                m = placement[s_rnd][m_idx]
+                
+                if gpu_s[d_rnd] + m.model_size <= GPU_MEM_SIZE:
+                    # Execute Kick
+                    placement[d_rnd].append(m)
+                    placement[s_rnd].pop(m_idx)
+                    gpu_s[d_rnd] += m.model_size
+                    gpu_w[d_rnd] += m.req_rate/m.slo
+                    gpu_s[s_rnd] -= m.model_size
+                    gpu_w[s_rnd] -= m.req_rate/m.slo
+                    no_improve = 0 # Reset patience
                     break
-        if not placed:
-            return False, None
+            continue
+
+        # Greedy Descent: Try to reduce bottleneck 'src'
+        improved = False
+        models = placement[src]
+        
+        # 1. Try Move
+        for i, m in enumerate(models):
+            w, s = m.req_rate/m.slo, m.model_size
+            for dst in range(gpu_num):
+                if dst == src: continue
+                if gpu_s[dst] + s > GPU_MEM_SIZE: continue
+                
+                # Predict new KVPRs
+                rem_src = GPU_MEM_SIZE - (gpu_s[src] - s)
+                nk_src = (gpu_w[src] - w) / rem_src if rem_src > 1e-7 else 1e9
+                
+                rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + s)
+                nk_dst = (gpu_w[dst] + w) / rem_dst if rem_dst > 1e-7 else 1e9
+                
+                # Check if this move reduces the global peak
+                # We only care if we improve the bottleneck situation
+                if max(nk_src, nk_dst) < max_k - 1e-6:
+                    placement[dst].append(m)
+                    placement[src].pop(i)
+                    gpu_s[src] -= s; gpu_w[src] -= w
+                    gpu_s[dst] += s; gpu_w[dst] += w
+                    improved = True
+                    break
+            if improved: break
+        
+        if improved: continue
+
+        # 2. Try Swap
+        for i, m1 in enumerate(models):
+            w1, s1 = m1.req_rate/m1.slo, m1.model_size
+            for dst in range(gpu_num):
+                if dst == src: continue
+                for j, m2 in enumerate(placement[dst]):
+                    w2, s2 = m2.req_rate/m2.slo, m2.model_size
+                    
+                    ns_src = gpu_s[src] - s1 + s2
+                    ns_dst = gpu_s[dst] - s2 + s1
+                    if ns_src > GPU_MEM_SIZE or ns_dst > GPU_MEM_SIZE: continue
+                    
+                    rem_src = GPU_MEM_SIZE - ns_src
+                    nk_src = (gpu_w[src] - w1 + w2) / rem_src if rem_src > 1e-7 else 1e9
+                    
+                    rem_dst = GPU_MEM_SIZE - ns_dst
+                    nk_dst = (gpu_w[dst] - w2 + w1) / rem_dst if rem_dst > 1e-7 else 1e9
+                    
+                    if max(nk_src, nk_dst) < max_k - 1e-6:
+                        placement[src][i] = m2
+                        placement[dst][j] = m1
+                        gpu_s[src] = ns_src; gpu_w[src] += (w2 - w1)
+                        gpu_s[dst] = ns_dst; gpu_w[dst] += (w1 - w2)
+                        improved = True
+                        break
+                if improved: break
+            if improved: break
             
-    return True, placements
+    return best_sol
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
     
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
     
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
     
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
     
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
     
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")
 
