--- a/original.py
+++ b/original.py
@@ -1,389 +1,401 @@
 # EVOLVE-BLOCK-START
-"""Model placement algorithm for minimizing maximum KV cache pressure using Robust Binary Search and Simulated Annealing with Variance Penalty"""
+"""Model placement algorithm for minimizing maximum KV cache pressure using Robust Binary Search and Large Neighborhood Search (Ruin & Recreate)"""
 
 import copy
 import random
 import math
 
 GPU_MEM_SIZE = 80.0  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Minimizes max KVPR using:
-    1. Robust Binary Search with multiple packing heuristics (FFD/BFD on Virtual/Physical/Density).
-    2. Simulated Annealing refinement with an energy function that penalizes variance.
+    1. Robust Binary Search with multiple packing heuristics.
+    2. Large Neighborhood Search (LNS) using Ruin & Recreate strategy.
     
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         A placement of models to GPUs {gpu_id: [models]}
     """
     # 1. Validation and Pre-processing
     total_size = sum(m.model_size for m in models)
     if total_size > gpu_num * GPU_MEM_SIZE:
         raise ValueError("Total model size exceeds total GPU memory capacity.")
 
     # Prepare items for packing: (w, s, m)
     # w = req_rate / slo
     items = [{'w': m.req_rate / m.slo, 's': m.model_size, 'm': m} for m in models]
 
     # 2. Binary Search for Initial Feasible Solution
-    # Calculate range
     total_w = sum(x['w'] for x in items)
     slack = gpu_num * GPU_MEM_SIZE - total_size
     
     low = 0.0
     # Heuristic upper bound
     if slack < 1e-6:
-        high = 10000.0 # Fallback for tight packing
+        high = 10000.0 
     else:
         avg_pressure = total_w / slack
-        high = max(10.0, avg_pressure * 8.0)
+        high = max(10.0, avg_pressure * 6.0)
 
     best_placement = None
     feasible_high = False
     
     # Find valid upper bound
     for _ in range(20):
         feasible, placement = _check_feasibility_robust(gpu_num, items, high)
         if feasible:
             best_placement = placement
             feasible_high = True
             break
         low = high
         high *= 2.0
         
     if not feasible_high:
         raise ValueError("Unable to place models. Constraints likely too tight.")
 
     # Binary Search
-    for _ in range(30):
+    # 32 iterations for high precision
+    for _ in range(32):
         mid = (low + high) / 2.0
         feasible, placement = _check_feasibility_robust(gpu_num, items, mid)
         if feasible:
             best_placement = placement
             high = mid
         else:
             low = mid
             
     # Convert list placement to dictionary map
     placement_map = {i: best_placement[i] for i in range(gpu_num)}
     
-    # 3. Simulated Annealing Refinement
-    final_placement = _simulated_annealing_refinement(gpu_num, placement_map)
+    # 3. LNS / Ruin & Recreate Refinement
+    final_placement = _lns_refinement(gpu_num, placement_map)
     
     return final_placement
 
 def _check_feasibility_robust(gpu_num, items, K):
     """
     Checks feasibility using multiple sorting strategies and packing algorithms (FFD/BFD).
     Constraint: sum(w + K*s) <= K*Capacity
     """
     virtual_cap = K * GPU_MEM_SIZE
     
     # Create augmented items for sorting
     pack_items = []
     for x in items:
         # Virtual Size: v = w + K*s
         v = x['w'] + K * x['s']
         # Density: Load per unit size
-        density = x['w'] / (x['s'] + 1e-7)
+        d = x['w'] / (x['s'] + 1e-7)
         pack_items.append({
             'v': v, 
             's': x['s'], 
             'w': x['w'], 
-            'd': density,
+            'd': d,
             'm': x['m']
         })
         
     # Heuristics: List of (sort_key_lambda, reverse_bool)
-    # 1. Virtual Size Descending (Standard)
-    # 2. Physical Size Descending (Good for large models)
-    # 3. Density Descending (Good for mixing high/low pressure models)
-    # 4. Load Descending
     heuristics = [
-        (lambda x: x['v'], True),
-        (lambda x: x['s'], True),
-        (lambda x: x['d'], True),
-        (lambda x: x['w'], True),
+        (lambda x: x['v'], True),  # Virtual Desc
+        (lambda x: x['s'], True),  # Physical Desc
+        (lambda x: x['d'], True),  # Density Desc
+        (lambda x: x['w'], True),  # Load Desc
     ]
     
     for key_func, rev in heuristics:
         sorted_items = sorted(pack_items, key=key_func, reverse=rev)
         
+        # Try Best Fit Decreasing (BFD) - usually superior for tight packing
+        if res := _pack_bfd(gpu_num, sorted_items, virtual_cap):
+            return True, res
+            
         # Try First Fit Decreasing (FFD)
         if res := _pack_ffd(gpu_num, sorted_items, virtual_cap):
-            return True, res
-            
-        # Try Best Fit Decreasing (BFD)
-        if res := _pack_bfd(gpu_num, sorted_items, virtual_cap):
             return True, res
             
     return False, None
 
 def _pack_ffd(gpu_num, items, virtual_cap):
     """First Fit Decreasing Packing"""
     bins_v = [0.0] * gpu_num
     bins_p = [0.0] * gpu_num
     placement = [[] for _ in range(gpu_num)]
     
     for item in items:
         placed = False
         for i in range(gpu_num):
             if bins_p[i] + item['s'] <= GPU_MEM_SIZE and bins_v[i] + item['v'] <= virtual_cap + 1e-7:
                 bins_p[i] += item['s']
                 bins_v[i] += item['v']
                 placement[i].append(item['m'])
                 placed = True
                 break
         if not placed: return None
     return placement
 
 def _pack_bfd(gpu_num, items, virtual_cap):
-    """Best Fit Decreasing Packing"""
+    """Best Fit Decreasing Packing (Minimizing Residual Capacity)"""
     bins_v = [0.0] * gpu_num
     bins_p = [0.0] * gpu_num
     placement = [[] for _ in range(gpu_num)]
     
     for item in items:
         best_bin = -1
         min_rem_v = float('inf')
         
         for i in range(gpu_num):
             if bins_p[i] + item['s'] <= GPU_MEM_SIZE and bins_v[i] + item['v'] <= virtual_cap + 1e-7:
-                # Minimize remaining virtual capacity (tightest fit)
+                # Minimize remaining virtual capacity
                 rem = virtual_cap - (bins_v[i] + item['v'])
                 if rem < min_rem_v:
                     min_rem_v = rem
                     best_bin = i
         
         if best_bin != -1:
             bins_p[best_bin] += item['s']
             bins_v[best_bin] += item['v']
             placement[best_bin].append(item['m'])
         else:
             return None
     return placement
 
-def _simulated_annealing_refinement(gpu_num, placement):
-    """
-    Refines placement using Simulated Annealing.
-    Energy Function: E = Max(K) + Variance_Penalty.
-    The variance penalty acts as a tie-breaker and landscape smoother.
-    """
-    # Initialize State Cache
+def _lns_refinement(gpu_num, placement):
+    """
+    Refines placement using Large Neighborhood Search (Ruin and Recreate).
+    Target: Minimize Max KVPR.
+    """
+    # Initialize State
     gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
     gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]
     
-    def get_k(s, w):
-        rem = GPU_MEM_SIZE - s
-        if rem <= 1e-7: return 1e9 # Penalty for full/overflow
-        return w / rem
-        
-    current_ks = [get_k(gpu_s[i], gpu_w[i]) for i in range(gpu_num)]
-    
-    cur_max = max(current_ks)
-    cur_sum_sq = sum(k*k for k in current_ks)
-    
-    best_max = cur_max
-    # Use shallow copy for speed (models are objects, lists are structure)
-    best_placement = {i: list(placement[i]) for i in range(gpu_num)}
-    
-    # SA Parameters
-    T = max(1.0, cur_max * 0.1)
-    alpha = 0.98
-    steps = 1000
-    
-    for step in range(steps):
-        # 1. Source Selection: Bias towards bottleneck
-        # Identify bottleneck indices
-        sorted_indices = sorted(range(gpu_num), key=lambda i: current_ks[i], reverse=True)
-        
-        # 50% chance bottleneck, 30% second bottleneck, 20% random
-        r = random.random()
-        if r < 0.5: src = sorted_indices[0]
-        elif r < 0.8 and gpu_num > 1: src = sorted_indices[1]
-        else: src = random.randint(0, gpu_num - 1)
-        
-        if not placement[src]: continue
-        
-        # 2. Move Generation: Move (70%) or Swap (30%)
-        move_type = 'swap' if random.random() < 0.3 else 'move'
-        accepted = False
-        
-        if move_type == 'move':
+    def get_k(idx):
+        rem = GPU_MEM_SIZE - gpu_s[idx]
+        if rem <= 1e-7: return 1e9
+        return gpu_w[idx] / rem
+        
+    current_ks = [get_k(i) for i in range(gpu_num)]
+    max_k = max(current_ks)
+    
+    best_max_k = max_k
+    best_placement = copy.deepcopy(placement)
+    
+    # Parameters
+    max_steps = 1000
+    patience = 50
+    no_improve = 0
+    
+    for step in range(max_steps):
+        # Identify bottleneck
+        # Get all GPUs close to max (within 1%) to diversify target
+        candidates = [i for i in range(gpu_num) if current_ks[i] > max_k * 0.99]
+        if not candidates: candidates = [random.randint(0, gpu_num-1)]
+        src = random.choice(candidates)
+        
+        # Heuristic Decision: Small Move vs Ruin
+        # If stuck (no improve), force Ruin. Else mix.
+        do_ruin = (no_improve > patience) or (random.random() < 0.2)
+        
+        if do_ruin:
+            # --- RUIN & RECREATE ---
+            # Select subset of GPUs: Bottleneck + Randoms
+            subset_indices = {src}
+            # Add 2-3 random GPUs
+            n_random = min(gpu_num - 1, random.randint(2, 4))
+            others = list(range(gpu_num))
+            random.shuffle(others)
+            for o in others:
+                if len(subset_indices) >= n_random + 1: break
+                if o != src: subset_indices.add(o)
+                
+            subset_indices = list(subset_indices)
+            
+            # Extract models and Backup
+            repack_models = []
+            backup_state = {} 
+            old_subset_sq = 0
+            
+            for idx in subset_indices:
+                old_subset_sq += current_ks[idx]**2
+                backup_state[idx] = (list(placement[idx]), gpu_s[idx], gpu_w[idx], current_ks[idx])
+                repack_models.extend(placement[idx])
+                # Clear GPU
+                placement[idx] = []
+                gpu_s[idx] = 0.0
+                gpu_w[idx] = 0.0
+                current_ks[idx] = 0.0
+                
+            # Recreate: Sort models
+            # Sort by Size Descending (packing efficiency) and Load Descending (pressure)
+            repack_models.sort(key=lambda m: (m.model_size, m.req_rate/m.slo), reverse=True)
+            
+            feasible_repack = True
+            
+            # Greedy insertion
+            for m in repack_models:
+                best_idx = -1
+                best_local_k = float('inf')
+                
+                m_s = m.model_size
+                m_w = m.req_rate / m.slo
+                
+                for idx in subset_indices:
+                    if gpu_s[idx] + m_s <= GPU_MEM_SIZE:
+                        # Hypothetical K
+                        rem = GPU_MEM_SIZE - (gpu_s[idx] + m_s)
+                        k = (gpu_w[idx] + m_w) / rem if rem > 1e-7 else 1e9
+                        
+                        # We want to minimize the PEAK K generated
+                        if k < best_local_k:
+                            best_local_k = k
+                            best_idx = idx
+                            
+                if best_idx != -1:
+                    placement[best_idx].append(m)
+                    gpu_s[best_idx] += m_s
+                    gpu_w[best_idx] += m_w
+                else:
+                    feasible_repack = False
+                    break
+            
+            if feasible_repack:
+                # Evaluate Move
+                new_subset_sq = 0
+                for idx in subset_indices:
+                    k = get_k(idx)
+                    current_ks[idx] = k
+                    new_subset_sq += k*k
+                    
+                new_global_max = max(current_ks)
+                
+                improved = False
+                if new_global_max < max_k - 1e-7:
+                    improved = True
+                elif new_global_max < max_k + 1e-7:
+                    # Tie-breaking: Variance Reduction
+                    if new_subset_sq < old_subset_sq:
+                        improved = True
+                        
+                if improved:
+                    max_k = new_global_max
+                    no_improve = 0 
+                else:
+                    # Revert
+                    for idx in subset_indices:
+                        placement[idx], gpu_s[idx], gpu_w[idx], current_ks[idx] = backup_state[idx]
+            else:
+                # Revert
+                for idx in subset_indices:
+                    placement[idx], gpu_s[idx], gpu_w[idx], current_ks[idx] = backup_state[idx]
+                    
+        else:
+            # --- SIMPLE MOVE (Greedy Descent) ---
+            if not placement[src]: 
+                no_improve += 1
+                continue
+                
             m_idx = random.randint(0, len(placement[src])-1)
             m = placement[src][m_idx]
-            dst = random.randint(0, gpu_num - 1)
-            if src == dst: continue
-            
-            # Check Physical Feasibility
-            if gpu_s[dst] + m.model_size <= GPU_MEM_SIZE:
-                # Calculate new states
-                new_s_src = gpu_s[src] - m.model_size
-                new_w_src = gpu_w[src] - (m.req_rate/m.slo)
-                new_k_src = get_k(new_s_src, new_w_src)
-                
-                new_s_dst = gpu_s[dst] + m.model_size
-                new_w_dst = gpu_w[dst] + (m.req_rate/m.slo)
-                new_k_dst = get_k(new_s_dst, new_w_dst)
-                
-                # Delta evaluation
-                old_k_src = current_ks[src]
-                old_k_dst = current_ks[dst]
-                
-                # Update temp
-                current_ks[src] = new_k_src
-                current_ks[dst] = new_k_dst
-                new_max = max(current_ks)
-                
-                delta_max = new_max - cur_max
-                
-                # Sum of squares change
-                new_sum_sq = cur_sum_sq - old_k_src**2 - old_k_dst**2 + new_k_src**2 + new_k_dst**2
-                delta_sq = new_sum_sq - cur_sum_sq
-                
-                # Acceptance Logic
-                if delta_max < -1e-7:
-                    accepted = True
-                elif delta_max < 1e-7:
-                    # Tie-breaking with variance
-                    if delta_sq < 0:
-                        accepted = True
-                    else:
-                        # Allow slightly higher variance if temp is high
-                        prob = math.exp(-delta_sq / (T * cur_max * 10))
-                        if random.random() < prob: accepted = True
-                else:
-                    # Allow worsening max
-                    prob = math.exp(-delta_max / T)
-                    if random.random() < prob: accepted = True
-                
-                if accepted:
-                    placement[dst].append(m)
-                    placement[src].pop(m_idx)
-                    gpu_s[src] = new_s_src; gpu_w[src] = new_w_src
-                    gpu_s[dst] = new_s_dst; gpu_w[dst] = new_w_dst
-                    cur_max = new_max
-                    cur_sum_sq = new_sum_sq
-                else:
-                    # Revert temp
-                    current_ks[src] = old_k_src
-                    current_ks[dst] = old_k_dst
-
-        elif move_type == 'swap':
-            m1_idx = random.randint(0, len(placement[src])-1)
-            m1 = placement[src][m1_idx]
-            
-            dst = random.randint(0, gpu_num - 1)
-            if src == dst or not placement[dst]: continue
-            
-            m2_idx = random.randint(0, len(placement[dst])-1)
-            m2 = placement[dst][m2_idx]
-            
-            new_s_src = gpu_s[src] - m1.model_size + m2.model_size
-            new_s_dst = gpu_s[dst] - m2.model_size + m1.model_size
-            
-            if new_s_src <= GPU_MEM_SIZE and new_s_dst <= GPU_MEM_SIZE:
-                w_change = (m2.req_rate/m2.slo) - (m1.req_rate/m1.slo)
-                new_w_src = gpu_w[src] + w_change
-                new_w_dst = gpu_w[dst] - w_change
-                
-                new_k_src = get_k(new_s_src, new_w_src)
-                new_k_dst = get_k(new_s_dst, new_w_dst)
-                
-                old_k_src = current_ks[src]
-                old_k_dst = current_ks[dst]
-                
-                current_ks[src] = new_k_src
-                current_ks[dst] = new_k_dst
-                new_max = max(current_ks)
-                
-                delta_max = new_max - cur_max
-                new_sum_sq = cur_sum_sq - old_k_src**2 - old_k_dst**2 + new_k_src**2 + new_k_dst**2
-                delta_sq = new_sum_sq - cur_sum_sq
-                
-                if delta_max < -1e-7:
-                    accepted = True
-                elif delta_max < 1e-7:
-                    if delta_sq < 0:
-                        accepted = True
-                    else:
-                        prob = math.exp(-delta_sq / (T * cur_max * 10))
-                        if random.random() < prob: accepted = True
-                else:
-                    prob = math.exp(-delta_max / T)
-                    if random.random() < prob: accepted = True
-                    
-                if accepted:
-                    placement[src][m1_idx] = m2
-                    placement[dst][m2_idx] = m1
-                    gpu_s[src] = new_s_src; gpu_w[src] = new_w_src
-                    gpu_s[dst] = new_s_dst; gpu_w[dst] = new_w_dst
-                    cur_max = new_max
-                    cur_sum_sq = new_sum_sq
-                else:
-                    current_ks[src] = old_k_src
-                    current_ks[dst] = old_k_dst
-        
-        # Update Global Best
-        if cur_max < best_max - 1e-7:
-            best_max = cur_max
-            best_placement = {i: list(placement[i]) for i in range(gpu_num)}
-            
-        T *= alpha
-        if T < 1e-4: break
-        
+            m_s = m.model_size
+            m_w = m.req_rate / m.slo
+            
+            best_dst = -1
+            best_impact = 0 
+            
+            for dst in range(gpu_num):
+                if dst == src: continue
+                if gpu_s[dst] + m_s > GPU_MEM_SIZE: continue
+                
+                # New values
+                rem_src = GPU_MEM_SIZE - (gpu_s[src] - m_s)
+                new_k_src = (gpu_w[src] - m_w) / rem_src if rem_src > 1e-7 else 1e9
+                
+                rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + m_s)
+                new_k_dst = (gpu_w[dst] + m_w) / rem_dst if rem_dst > 1e-7 else 1e9
+                
+                old_local_max = max(current_ks[src], current_ks[dst])
+                new_local_max = max(new_k_src, new_k_dst)
+                
+                if new_local_max < old_local_max - 1e-7:
+                    # Don't create new global bottleneck
+                    if new_local_max < max_k + 1e-7:
+                        improvement = old_local_max - new_local_max
+                        if improvement > best_impact:
+                            best_impact = improvement
+                            best_dst = dst
+            
+            if best_dst != -1:
+                dst = best_dst
+                placement[dst].append(m)
+                placement[src].pop(m_idx)
+                
+                gpu_s[src] -= m_s; gpu_w[src] -= m_w
+                gpu_s[dst] += m_s; gpu_w[dst] += m_w
+                
+                current_ks[src] = get_k(src)
+                current_ks[dst] = get_k(dst)
+                
+                max_k = max(current_ks)
+                no_improve = 0
+            else:
+                no_improve += 1
+
+        # Check Best
+        if max_k < best_max_k - 1e-7:
+            best_max_k = max_k
+            best_placement = copy.deepcopy(placement)
+            
     return best_placement
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")
