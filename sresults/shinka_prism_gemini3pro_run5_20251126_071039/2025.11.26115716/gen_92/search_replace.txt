<NAME>
bfd_lns_refinement
</NAME>

<DESCRIPTION>
I propose two major improvements to the model placement algorithm to minimize the maximum KV cache pressure (KVPR):

1.  **Best-Fit Decreasing (BFD) Packing**: Replace the First-Fit strategy in the binary search feasibility check with Best-Fit Decreasing. BFD minimizes the residual virtual capacity in each bin, which reduces fragmentation and typically allows for tighter packing bounds ($K$) than First-Fit. I also added a 'Density' sorting heuristic (load per unit size) to better handle heterogeneous workloads.

2.  **Ruin & Recreate (LNS) Refinement**: Replace the simple hill-climbing local search with a Large Neighborhood Search (LNS) using a "Ruin and Recreate" strategy. This approach identifies the bottleneck GPU and a set of random peers, removes all their models ("ruin"), and repacks them ("recreate") using a greedy heuristic that minimizes the local peak KVPR. This mechanism effectively breaks deadlocks where single moves or swaps are insufficient to reduce pressure on the bottleneck GPU.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Refinement using Iterated Local Search
    final_placement = _iterated_local_search(gpu_num, placement_map)

    return final_placement

def _check_feasibility_multi(gpu_num, items, K):
    """
    Check if items can be packed with target KVPR 'K' using multiple heuristics.
    """
    virtual_cap = K * GPU_MEM_SIZE
    # Create pack items: (virtual_size, physical_size, model)
    # virtual_size = w + K * s
    pack_items = [(x['w'] + K * x['s'], x['s'], x['m']) for x in items]

    # Strategy 1: FFD on Virtual Size (Standard)
    pack_items.sort(key=lambda x: x[0], reverse=True)
    res = _pack(gpu_num, pack_items, virtual_cap)
    if res: return True, res

    # Strategy 2: FFD on Physical Size (Good for size-constrained)
    pack_items_p = sorted(pack_items, key=lambda x: x[1], reverse=True)
    res = _pack(gpu_num, pack_items_p, virtual_cap)
    if res: return True, res

    return False, None

def _pack(gpu_num, items, virtual_cap):
    bins_v = [0.0] * gpu_num
    bins_p = [0.0] * gpu_num
    placement = [[] for _ in range(gpu_num)]

    for v, p, m in items:
        placed = False
        for i in range(gpu_num):
            if bins_p[i] + p <= GPU_MEM_SIZE and bins_v[i] + v <= virtual_cap + 1e-7:
                bins_p[i] += p
                bins_v[i] += v
                placement[i].append(m)
                placed = True
                break
        if not placed: return None
    return placement

def _iterated_local_search(gpu_num, placement):
    """
    Refines placement using Hill Climbing with Random Kicks.
    """
    # Initialize state
    gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
    gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]

    def get_kvpr(idx):
        rem = GPU_MEM_SIZE - gpu_s[idx]
        if rem <= 1e-7: return 1e9 # Penalty
        return gpu_w[idx] / rem

    best_max_k = max(get_kvpr(i) for i in range(gpu_num))
    best_sol = copy.deepcopy(placement)

    no_improve = 0
    max_steps = 300
    patience = 30

    for _ in range(max_steps):
        # Identify bottleneck GPU
        max_k = -1.0
        src = -1
        for i in range(gpu_num):
            k = get_kvpr(i)
            if k > max_k:
                max_k = k
                src = i

        # Check global improvement
        if max_k < best_max_k - 1e-6:
            best_max_k = max_k
            best_sol = copy.deepcopy(placement)
            no_improve = 0
        else:
            no_improve += 1

        # Kick / Perturbation
        if no_improve > patience:
            # Perform a random move to escape local optimum
            for _ in range(20): # Try up to 20 times to find a valid random move
                s_rnd = random.randint(0, gpu_num-1)
                if not placement[s_rnd]: continue
                d_rnd = random.randint(0, gpu_num-1)
                if s_rnd == d_rnd: continue

                m_idx = random.randint(0, len(placement[s_rnd])-1)
                m = placement[s_rnd][m_idx]

                if gpu_s[d_rnd] + m.model_size <= GPU_MEM_SIZE:
                    # Execute Kick
                    placement[d_rnd].append(m)
                    placement[s_rnd].pop(m_idx)
                    gpu_s[d_rnd] += m.model_size
                    gpu_w[d_rnd] += m.req_rate/m.slo
                    gpu_s[s_rnd] -= m.model_size
                    gpu_w[s_rnd] -= m.req_rate/m.slo
                    no_improve = 0 # Reset patience
                    break
            continue

        # Greedy Descent: Try to reduce bottleneck 'src'
        improved = False
        models = placement[src]

        # 1. Try Move
        for i, m in enumerate(models):
            w, s = m.req_rate/m.slo, m.model_size
            for dst in range(gpu_num):
                if dst == src: continue
                if gpu_s[dst] + s > GPU_MEM_SIZE: continue

                # Predict new KVPRs
                rem_src = GPU_MEM_SIZE - (gpu_s[src] - s)
                nk_src = (gpu_w[src] - w) / rem_src if rem_src > 1e-7 else 1e9

                rem_dst = GPU_MEM_SIZE - (gpu_s[dst] + s)
                nk_dst = (gpu_w[dst] + w) / rem_dst if rem_dst > 1e-7 else 1e9

                # Check if this move reduces the global peak
                # We only care if we improve the bottleneck situation
                if max(nk_src, nk_dst) < max_k - 1e-6:
                    placement[dst].append(m)
                    placement[src].pop(i)
                    gpu_s[src] -= s; gpu_w[src] -= w
                    gpu_s[dst] += s; gpu_w[dst] += w
                    improved = True
                    break
            if improved: break

        if improved: continue

        # 2. Try Swap
        for i, m1 in enumerate(models):
            w1, s1 = m1.req_rate/m1.slo, m1.model_size
            for dst in range(gpu_num):
                if dst == src: continue
                for j, m2 in enumerate(placement[dst]):
                    w2, s2 = m2.req_rate/m2.slo, m2.model_size

                    ns_src = gpu_s[src] - s1 + s2
                    ns_dst = gpu_s[dst] - s2 + s1
                    if ns_src > GPU_MEM_SIZE or ns_dst > GPU_MEM_SIZE: continue

                    rem_src = GPU_MEM_SIZE - ns_src
                    nk_src = (gpu_w[src] - w1 + w2) / rem_src if rem_src > 1e-7 else 1e9

                    rem_dst = GPU_MEM_SIZE - ns_dst
                    nk_dst = (gpu_w[dst] - w2 + w1) / rem_dst if rem_dst > 1e-7 else 1e9

                    if max(nk_src, nk_dst) < max_k - 1e-6:
                        placement[src][i] = m2
                        placement[dst][j] = m1
                        gpu_s[src] = ns_src; gpu_w[src] += (w2 - w1)
                        gpu_s[dst] = ns_dst; gpu_w[dst] += (w1 - w2)
                        improved = True
                        break
                if improved: break
            if improved: break

    return best_sol
=======
    # Refinement using Large Neighborhood Search (Ruin and Recreate)
    final_placement = _lns_refinement(gpu_num, placement_map)

    return final_placement

def _check_feasibility_multi(gpu_num, items, K):
    """
    Check if items can be packed with target KVPR 'K' using multiple heuristics.
    """
    virtual_cap = K * GPU_MEM_SIZE

    # Prepare items with sorting keys
    pack_items = []
    for x in items:
        v = x['w'] + K * x['s']
        d = x['w'] / (x['s'] + 1e-7)
        pack_items.append({'v': v, 's': x['s'], 'w': x['w'], 'd': d, 'm': x['m']})

    # Heuristics: (Sort Key Lambda, Reverse)
    # 1. Virtual Size Desc (Standard)
    # 2. Physical Size Desc (Fit large items first)
    # 3. Density Desc (Fit high load/size items first)
    heuristics = [
        (lambda x: x['v'], True),
        (lambda x: x['s'], True),
        (lambda x: x['d'], True),
        (lambda x: x['w'], True),
    ]

    for key_func, rev in heuristics:
        sorted_items = sorted(pack_items, key=key_func, reverse=rev)
        # Use Best Fit Decreasing (BFD)
        if res := _pack_bfd(gpu_num, sorted_items, virtual_cap):
            return True, res

    return False, None

def _pack_bfd(gpu_num, items, virtual_cap):
    """Best Fit Decreasing Packing"""
    bins_v = [0.0] * gpu_num
    bins_p = [0.0] * gpu_num
    placement = [[] for _ in range(gpu_num)]

    for item in items:
        best_bin = -1
        min_rem_v = float('inf')

        for i in range(gpu_num):
            if bins_p[i] + item['s'] <= GPU_MEM_SIZE and bins_v[i] + item['v'] <= virtual_cap + 1e-7:
                # Minimize remaining virtual capacity
                rem = virtual_cap - (bins_v[i] + item['v'])
                if rem < min_rem_v:
                    min_rem_v = rem
                    best_bin = i

        if best_bin != -1:
            bins_p[best_bin] += item['s']
            bins_v[best_bin] += item['v']
            placement[best_bin].append(item['m'])
        else:
            return None
    return placement

def _lns_refinement(gpu_num, placement):
    """
    Refines placement using Ruin and Recreate (LNS).
    """
    gpu_s = [sum(m.model_size for m in placement[i]) for i in range(gpu_num)]
    gpu_w = [sum(m.req_rate / m.slo for m in placement[i]) for i in range(gpu_num)]

    def get_k(i):
        rem = GPU_MEM_SIZE - gpu_s[i]
        if rem <= 1e-7: return 1e9
        return gpu_w[i] / rem

    best_max_k = max(get_k(i) for i in range(gpu_num))
    best_sol = copy.deepcopy(placement)

    max_steps = 500
    patience = 40
    no_improve = 0

    for _ in range(max_steps):
        # Identify bottleneck
        current_ks = [get_k(i) for i in range(gpu_num)]
        max_k = max(current_ks)

        if max_k < best_max_k - 1e-6:
            best_max_k = max_k
            best_sol = copy.deepcopy(placement)
            no_improve = 0
        else:
            no_improve += 1

        # Select Ruin Targets
        # Bottleneck + Random
        candidates = [i for i, k in enumerate(current_ks) if k > max_k * 0.98]
        if not candidates: candidates = [random.randint(0, gpu_num-1)]
        src = random.choice(candidates)

        ruin_set = {src}
        # Dynamic ruin size
        n_ruin = 2
        if no_improve > patience // 2: n_ruin = 3
        if no_improve > patience: n_ruin = 4

        others = list(range(gpu_num))
        random.shuffle(others)
        for o in others:
            if len(ruin_set) >= min(gpu_num, n_ruin): break
            if o != src: ruin_set.add(o)

        # Ruin
        removed_models = []
        backup = {}
        for r in ruin_set:
            backup[r] = (list(placement[r]), gpu_s[r], gpu_w[r])
            removed_models.extend(placement[r])
            placement[r] = []
            gpu_s[r] = 0.0
            gpu_w[r] = 0.0

        # Recreate: Greedy Best Fit on Local K
        # Sort large models first to pack better
        removed_models.sort(key=lambda m: (m.model_size, m.req_rate/m.slo), reverse=True)

        feasible = True
        for m in removed_models:
            best_t = -1
            best_k = float('inf')

            for t in ruin_set:
                if gpu_s[t] + m.model_size <= GPU_MEM_SIZE:
                    rem = GPU_MEM_SIZE - (gpu_s[t] + m.model_size)
                    k = (gpu_w[t] + m.req_rate/m.slo) / rem if rem > 1e-7 else 1e9
                    if k < best_k:
                        best_k = k
                        best_t = t

            if best_t != -1:
                placement[best_t].append(m)
                gpu_s[best_t] += m.model_size
                gpu_w[best_t] += m.req_rate/m.slo
            else:
                feasible = False
                break

        if feasible:
            # Check acceptance
            new_max = max(get_k(i) for i in range(gpu_num))
            accept = False

            if new_max < max_k - 1e-6:
                accept = True
            elif new_max < max_k + 1e-6:
                # Accept neutral moves to traverse plateau
                accept = True
            elif no_improve > patience and random.random() < 0.1:
                # Escape local optima
                accept = True

            if not accept:
                # Revert
                for r in ruin_set:
                    placement[r], gpu_s[r], gpu_w[r] = backup[r]
        else:
            # Revert
            for r in ruin_set:
                placement[r], gpu_s[r], gpu_w[r] = backup[r]

    return best_sol
>>>>>>> REPLACE
</DIFF>