<NAME>
binary_search_packing
</NAME>

<DESCRIPTION>
Replaces the greedy heuristic with a Binary Search for the optimal maximum KVPR target ($K$).
The problem of minimizing max KVPR is transformed into a decision problem: "Can we pack all models such that for every GPU, pressure $\le K$?".
This inequality $W_i / (M - S_i) \le K$ rearranges to a linear constraint $W_i + K \cdot S_i \le K \cdot M$, which is a Bin Packing problem with item sizes $w_j + K \cdot s_j$ and bin capacity $K \cdot M$.
The algorithm binary searches for the minimal feasible $K$, using Best Fit Decreasing to check feasibility at each step. This approach better balances load ($w$) and size ($s$) globally than a simple greedy sort.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Args:
        gpu_num: Number of GPUs
        models: List of models to place

    Returns:
        A placement of models to GPUs
    """

    # Greedy KVPR-minimizing placement
    # 1) Sort models by r_j / s_j descending (high load first).
    #    Secondary sort by model_size descending (large items first).
    sorted_models = sorted(models, key=lambda m: (m.req_rate / m.slo, m.model_size), reverse=True)

    # 2) Initialize per-GPU states
    placement = {gpu_id: [] for gpu_id in range(gpu_num)}
    shared_kv = [GPU_MEM_SIZE for _ in range(gpu_num)]  # remaining memory per GPU
    weighted_req_rate = [0.0 for _ in range(gpu_num)]   # sum of r_j / s_j per GPU

    # 3) Assign each model to the GPU that minimizes *resultant* KVPR
    for model in sorted_models:
        best_idx = None
        best_kvpr = float('inf')

        w_m = model.req_rate / model.slo
        z_m = model.model_size

        # Try to find a GPU where the model fits and leaves some memory (>0)
        for gpu_id in range(gpu_num):
            remaining_mem = shared_kv[gpu_id] - z_m
            if remaining_mem > 0:
                # Calculate projected KVPR: (current_load + new_load) / remaining_mem
                new_kvpr = (weighted_req_rate[gpu_id] + w_m) / remaining_mem
                if new_kvpr < best_kvpr:
                    best_kvpr = new_kvpr
                    best_idx = gpu_id

        # Fallback: if no GPU has >0 remaining memory, try exact fit (remaining == 0)
        if best_idx is None:
            for gpu_id in range(gpu_num):
                if shared_kv[gpu_id] >= z_m:
                    best_idx = gpu_id
                    break

        # Failure: if no GPU can fit, raise an error
        if best_idx is None:
            raise ValueError(
                f"Unable to place model of size {model.model_size} GB on any GPU. "
                f"Remaining per-GPU memory: {shared_kv}"
            )

        placement[best_idx].append(model)
        weighted_req_rate[best_idx] += w_m
        shared_kv[best_idx] -= z_m

    return placement
=======
def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.
    Uses Binary Search on the maximum pressure K, transforming the problem
    into a Bin Packing problem with item sizes w_j + K * s_j.
    """

    # Precompute model data
    m_data = []
    for m in models:
        m_data.append({
            'w': m.req_rate / m.slo,
            's': m.model_size,
            'obj': m
        })

    # Helper for greedy placement (used for initialization)
    def run_greedy(sort_key_func):
        sorted_items = sorted(m_data, key=sort_key_func, reverse=True)
        placements = {i: [] for i in range(gpu_num)}
        gpu_load = [0.0] * gpu_num
        gpu_used = [0.0] * gpu_num

        for item in sorted_items:
            best_gpu = -1
            best_score = float('inf')

            # Try to find best fit minimizing KVPR increase
            for g in range(gpu_num):
                rem = GPU_MEM_SIZE - (gpu_used[g] + item['s'])
                if rem > 0:
                    score = (gpu_load[g] + item['w']) / rem
                    if score < best_score:
                        best_score = score
                        best_gpu = g

            # Fallback to any valid fit if precise fit fails
            if best_gpu == -1:
                for g in range(gpu_num):
                    if gpu_used[g] + item['s'] < GPU_MEM_SIZE:
                         best_gpu = g
                         break

            if best_gpu == -1:
                return None, float('inf')

            placements[best_gpu].append(item['obj'])
            gpu_load[best_gpu] += item['w']
            gpu_used[best_gpu] += item['s']

        # Calculate max pressure
        max_p = 0.0
        for g in range(gpu_num):
            rem = GPU_MEM_SIZE - gpu_used[g]
            if rem <= 0:
                if gpu_load[g] > 0: max_p = float('inf')
            else:
                p = gpu_load[g] / rem
                if p > max_p: max_p = p
        return placements, max_p

    # 1. Get initial upper bound from a smart greedy approach
    # Sort by density (load/size) which is a strong heuristic
    best_placement, min_max_kvpr = run_greedy(lambda x: x['w'] / x['s'] if x['s'] > 0 else 0)

    # If density sort fails (e.g. fragmentation), try size sort
    if best_placement is None:
        best_placement, min_max_kvpr = run_greedy(lambda x: x['s'])

    if best_placement is None:
         raise ValueError("Unable to place models on GPUs.")

    # 2. Binary Search for optimal max pressure K
    # Range [0, greedy_result]
    low = 0.0
    high = min_max_kvpr

    # Optimization: limit iterations for speed
    for _ in range(20):
        if high - low < 1e-4:
            break

        mid = (low + high) / 2.0
        target_k = mid

        # Check feasibility with Best Fit Decreasing
        # Effective size v_j = w_j + K * s_j. Bin capacity C = K * M.
        # Sort items by effective size
        check_items = sorted(m_data, key=lambda x: x['w'] + target_k * x['s'], reverse=True)

        gpu_load = [0.0] * gpu_num
        gpu_used = [0.0] * gpu_num
        current_placement = {i: [] for i in range(gpu_num)}
        feasible = True

        for item in check_items:
            best_bin = -1
            min_residual = float('inf')
            item_cost = item['w'] + target_k * item['s']

            for g in range(gpu_num):
                # Hard memory constraint check (strictly < GPU_MEM_SIZE for valid KVPR)
                if gpu_used[g] + item['s'] >= GPU_MEM_SIZE:
                    continue

                # Transformed capacity constraint check
                # (load + w) + K(used + s) <= K*M
                current_cost = gpu_load[g] + target_k * gpu_used[g]

                if current_cost + item_cost <= target_k * GPU_MEM_SIZE + 1e-7:
                    # Best Fit: minimize residual capacity
                    res = (target_k * GPU_MEM_SIZE) - (current_cost + item_cost)
                    if res < min_residual:
                        min_residual = res
                        best_bin = g

            if best_bin != -1:
                current_placement[best_bin].append(item['obj'])
                gpu_load[best_bin] += item['w']
                gpu_used[best_bin] += item['s']
            else:
                feasible = False
                break

        if feasible:
            best_placement = current_placement
            high = mid
        else:
            low = mid

    return best_placement
>>>>>>> REPLACE
</DIFF>