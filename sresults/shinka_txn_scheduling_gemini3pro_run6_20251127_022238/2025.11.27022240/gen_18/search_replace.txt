<NAME>
parallelism_heuristic_and_weighted_sampling
</NAME>

<DESCRIPTION>
1.  **Parallelism Efficiency Heuristic**: Change the priority function from minimizing `makespan` to minimizing `makespan - total_duration_scheduled`. This heuristic (`score`) rewards schedules that achieve high parallelism (low makespan relative to the work done) and naturally encourages scheduling "heavy" items early (LPT) if they can fit efficiently without extending the makespan excessively. This aligns with conflict-aware scheduling where maximizing overlap is key.
2.  **Weighted Sampling**: Replace uniform random sampling with weighted sampling based on transaction duration. This biases the search towards "harder" transactions, helping to resolve bottlenecks earlier, while still allowing exploration.
3.  **Beam Width**: Increase beam width multiplier to 2.0 to support the more diverse search space.
4.  **State Tracking**: Add `total_dur` to beam state to support the heuristic calculation.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # --- Parameters ---
    # Beam Width: Scale with effort, ensure minimum size for stability
    BEAM_WIDTH = int(max(10, num_seqs * 1.5))

    # Samples per node: Candidates to evaluate from each parent
    SAMPLES_PER_NODE = 20

    # Diversity: Max children to accept from a single parent in the next beam
    # This prevents one "lucky" parent from flooding the beam with minor variations
    MAX_CHILDREN = 3

    num_txns = workload.num_txns

    # --- Precompute Heuristics ---
    # Duration: Proxy for transaction complexity/conflict potential.
    txn_durations = {t: workload.get_opt_seq_cost([t]) for t in range(num_txns)}

    # Sorted list for easy LPT (Longest Processing Time) access
    sorted_by_duration = sorted(range(num_txns), key=lambda t: txn_durations[t], reverse=True)

    # --- Initialization ---
    # Seed beam with:
    # 1. Top LPT transactions (often best anchors)
    # 2. Random transactions (for diversity)
    start_candidates = set()
    start_candidates.update(sorted_by_duration[:BEAM_WIDTH])

    # Fill remaining slots with random starts
    while len(start_candidates) < BEAM_WIDTH * 2:
        start_candidates.add(random.randint(0, num_txns - 1))

    beam = []
    for t in start_candidates:
        rem = set(range(num_txns))
        rem.remove(t)
        beam.append({
            'cost': txn_durations[t],
            'seq': [t],
            'rem': rem
        })

    # Initial prune: Sort by cost, tie-break by duration (longer first is better usually)
    beam.sort(key=lambda x: (x['cost'], -txn_durations[x['seq'][0]]))
    beam = beam[:BEAM_WIDTH]

    # --- Beam Search Loop ---
    for _ in range(num_txns - 1):
        candidates = []

        for p_idx, parent in enumerate(beam):
            rem_list = list(parent['rem'])

            # --- Hybrid Sampling ---
            samples = set()

            # 1. Deterministic LPT: Always consider the longest remaining transactions
            # This ensures we don't delay "heavy" items that cause bottlenecks
            lpt_count = 0
            for t in sorted_by_duration:
                if t in parent['rem']:
                    samples.add(t)
                    lpt_count += 1
                    if lpt_count >= 2: # Top 2
                        break

            # 2. Random Sampling: Explore the rest of the space
            needed = SAMPLES_PER_NODE - len(samples)
            if needed > 0:
                pool = [x for x in rem_list if x not in samples]
                if len(pool) <= needed:
                    samples.update(pool)
                else:
                    samples.update(random.sample(pool, needed))

            # --- Evaluation ---
            for t in samples:
                new_seq = parent['seq'] + [t]
                new_cost = workload.get_opt_seq_cost(new_seq)

                # Priority:
                # 1. Minimize Makespan (Cost)
                # 2. Tie-breaker: Maximize duration of added item (schedule heavy items ASAP)
                priority = (new_cost, -txn_durations[t])

                candidates.append({
                    'priority': priority,
                    'cost': new_cost,
                    'seq': new_seq,
                    'rem': parent['rem'], # Ref only
                    'added': t,
                    'p_idx': p_idx
                })

        # Sort candidates by priority (Best first)
        candidates.sort(key=lambda x: x['priority'])

        # --- Selection with Deduplication & Diversity ---
        new_beam = []
        seen_states = set()
        parent_usage = {i: 0 for i in range(len(beam))}
        reserve = []

        # Identify unique state candidates first
        # Because candidates are sorted by cost, the first time we see a state,
        # it is via the optimal path found so far.
        unique_candidates = []
        for cand in candidates:
            # Construct new state key
            new_rem = cand['rem'].copy()
            new_rem.remove(cand['added'])
            # frozenset is O(N) hashing, faster than sorted tuple O(N log N)
            state_key = frozenset(new_rem)

            if state_key not in seen_states:
                seen_states.add(state_key)
                cand['state_rem'] = new_rem # Store for next iteration
                unique_candidates.append(cand)

        # Fill beam from unique candidates
        for cand in unique_candidates:
            if len(new_beam) >= BEAM_WIDTH:
                break

            p_idx = cand['p_idx']
            # Diversity Check
            if parent_usage[p_idx] < MAX_CHILDREN:
                new_beam.append({
                    'cost': cand['cost'],
                    'seq': cand['seq'],
                    'rem': cand['state_rem']
                })
                parent_usage[p_idx] += 1
            else:
                reserve.append(cand)

        # Fill from reserve (relax diversity if beam not full)
        if len(new_beam) < BEAM_WIDTH:
            for cand in reserve:
                if len(new_beam) >= BEAM_WIDTH:
                    break
                new_beam.append({
                    'cost': cand['cost'],
                    'seq': cand['seq'],
                    'rem': cand['state_rem']
                })
=======
    # --- Parameters ---
    # Beam Width: Scale with effort, ensure minimum size for stability
    BEAM_WIDTH = int(max(10, num_seqs * 2.0))

    # Samples per node: Candidates to evaluate from each parent
    SAMPLES_PER_NODE = 20

    # Diversity: Max children to accept from a single parent in the next beam
    # This prevents one "lucky" parent from flooding the beam with minor variations
    MAX_CHILDREN = 3

    num_txns = workload.num_txns

    # --- Precompute Heuristics ---
    # Duration: Proxy for transaction complexity/conflict potential.
    txn_durations = {t: workload.get_opt_seq_cost([t]) for t in range(num_txns)}

    # Sorted list for easy LPT (Longest Processing Time) access
    sorted_by_duration = sorted(range(num_txns), key=lambda t: txn_durations[t], reverse=True)

    # --- Initialization ---
    # Seed beam with:
    # 1. Top LPT transactions (often best anchors)
    # 2. Random transactions (for diversity)
    start_candidates = set()
    start_candidates.update(sorted_by_duration[:BEAM_WIDTH])

    # Fill remaining slots with random starts
    while len(start_candidates) < BEAM_WIDTH * 2:
        start_candidates.add(random.randint(0, num_txns - 1))

    beam = []
    for t in start_candidates:
        rem = set(range(num_txns))
        rem.remove(t)
        beam.append({
            'cost': txn_durations[t],
            'total_dur': txn_durations[t],
            'seq': [t],
            'rem': rem
        })

    # Initial prune: Sort by cost, tie-break by total duration
    beam.sort(key=lambda x: (x['cost'], -x['total_dur']))
    beam = beam[:BEAM_WIDTH]

    # --- Beam Search Loop ---
    for _ in range(num_txns - 1):
        candidates = []

        for p_idx, parent in enumerate(beam):
            rem_list = list(parent['rem'])

            # --- Hybrid Sampling ---
            samples = set()

            # 1. Deterministic LPT: Always consider the longest remaining transactions
            # This ensures we don't delay "heavy" items that cause bottlenecks
            lpt_count = 0
            for t in sorted_by_duration:
                if t in parent['rem']:
                    samples.add(t)
                    lpt_count += 1
                    if lpt_count >= 2: # Top 2
                        break

            # 2. Weighted Random Sampling: Favor longer transactions
            needed = SAMPLES_PER_NODE - len(samples)
            if needed > 0:
                pool = [x for x in rem_list if x not in samples]
                if len(pool) <= needed:
                    samples.update(pool)
                else:
                    # Weighted sampling without replacement (approximation)
                    # We pop to ensure uniqueness
                    pool_copy = pool[:]
                    weights = [txn_durations[x] for x in pool_copy]

                    while len(samples) < SAMPLES_PER_NODE and pool_copy:
                        try:
                            pick = random.choices(pool_copy, weights=weights, k=1)[0]
                            samples.add(pick)
                            idx = pool_copy.index(pick)
                            pool_copy.pop(idx)
                            weights.pop(idx)
                        except (ValueError, IndexError):
                            break

            # --- Evaluation ---
            for t in samples:
                new_seq = parent['seq'] + [t]
                new_cost = workload.get_opt_seq_cost(new_seq)
                new_total_dur = parent['total_dur'] + txn_durations[t]

                # Priority:
                # Score = Makespan - TotalDuration. Lower is better (implies high parallelism).
                # Tie-breaker: Maximize TotalDuration (prefer efficiently scheduling heavy items).
                score = new_cost - new_total_dur
                priority = (score, -new_total_dur)

                candidates.append({
                    'priority': priority,
                    'cost': new_cost,
                    'total_dur': new_total_dur,
                    'seq': new_seq,
                    'rem': parent['rem'], # Ref only
                    'added': t,
                    'p_idx': p_idx
                })

        # Sort candidates by priority (Best first)
        candidates.sort(key=lambda x: x['priority'])

        # --- Selection with Deduplication & Diversity ---
        new_beam = []
        seen_states = set()
        parent_usage = {i: 0 for i in range(len(beam))}
        reserve = []

        # Identify unique state candidates first
        # Because candidates are sorted by priority, the first time we see a state,
        # it is via the optimal path found so far.
        unique_candidates = []
        for cand in candidates:
            # Construct new state key
            new_rem = cand['rem'].copy()
            new_rem.remove(cand['added'])
            # frozenset is O(N) hashing, faster than sorted tuple O(N log N)
            state_key = frozenset(new_rem)

            if state_key not in seen_states:
                seen_states.add(state_key)
                cand['state_rem'] = new_rem # Store for next iteration
                unique_candidates.append(cand)

        # Fill beam from unique candidates
        for cand in unique_candidates:
            if len(new_beam) >= BEAM_WIDTH:
                break

            p_idx = cand['p_idx']
            # Diversity Check
            if parent_usage[p_idx] < MAX_CHILDREN:
                new_beam.append({
                    'cost': cand['cost'],
                    'total_dur': cand['total_dur'],
                    'seq': cand['seq'],
                    'rem': cand['state_rem']
                })
                parent_usage[p_idx] += 1
            else:
                reserve.append(cand)

        # Fill from reserve (relax diversity if beam not full)
        if len(new_beam) < BEAM_WIDTH:
            for cand in reserve:
                if len(new_beam) >= BEAM_WIDTH:
                    break
                new_beam.append({
                    'cost': cand['cost'],
                    'total_dur': cand['total_dur'],
                    'seq': cand['seq'],
                    'rem': cand['state_rem']
                })
>>>>>>> REPLACE
</DIFF>