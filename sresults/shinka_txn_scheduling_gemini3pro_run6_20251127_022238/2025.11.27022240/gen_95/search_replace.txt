<NAME>
quintic_beam_weighting
</NAME>

<DESCRIPTION>
1. Replaced Quartic (d^4) weighting with Quintic (d^5) weighting in the stochastic sampling phase of the beam search. This significantly intensifies the bias towards the most urgent transactions (longest duration/highest conflict), aligning with a "Big Rocks First" packing strategy while maintaining just enough stochasticity for escaping local traps.
2. Widened the sampling pool from top-20 to top-25 items to provide the steeper probability curve a slightly larger domain to operate on.
3. Adjusted the Hybrid Lookahead metric from (0.6 min / 0.4 avg) to (0.7 min / 0.3 avg). This places higher value on the "best-case scenario" (finding at least one highly compatible urgent follower), which is critical for minimizing makespan by filling gaps in the schedule.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def get_best_schedule(workload, num_seqs):
    """
    Get optimal schedule using Quartic-Anchored Beam Search and Threshold-Acceptance ILS.

    Innovations:
    1. Quartic-Anchored Beam Search: Combines deterministic "anchor" selection (Top-3) with
       Quartic (d^4) weighted sampling to balance stability and exploration.
    2. Hybrid Lookahead Metric: Evaluates candidates using a weighted mix (0.6 min / 0.4 avg)
       of compatibility with the Top-4 remaining urgent items.
    3. Threshold Acceptance ILS: Allows slight degradation (0.2%) in schedule quality during
       ILS to escape local optima, combined with aggressive Ruin-and-Recreate perturbations.

    Args:
        workload: Workload object
        num_seqs: Budget parameter

    Returns:
        (lowest_makespan, schedule)
    """

    num_txns = workload.num_txns
=======
def get_best_schedule(workload, num_seqs):
    """
    Get optimal schedule using Quintic-Anchored Beam Search and Threshold-Acceptance ILS.

    Innovations:
    1. Quintic-Anchored Beam Search: Combines deterministic "anchor" selection (Top-3) with
       Quintic (d^5) weighted sampling to strongly prioritize critical path items.
    2. Hybrid Lookahead Metric: Evaluates candidates using a weighted mix (0.7 min / 0.3 avg)
       of compatibility with the Top-4 remaining urgent items.
    3. Threshold Acceptance ILS: Allows slight degradation (0.2%) in schedule quality during
       ILS to escape local optima, combined with aggressive Ruin-and-Recreate perturbations.

    Args:
        workload: Workload object
        num_seqs: Budget parameter

    Returns:
        (lowest_makespan, schedule)
    """

    num_txns = workload.num_txns
>>>>>>> REPLACE
<<<<<<< SEARCH
            # 2. Quartic Weighted Sampling
            # Select from a broader pool but with steep probability curve
            if len(sorted_remain) > 3:
                pool = sorted_remain[3 : 20] # Look at next 17 items
                weights = [txn_urgency[x]**4 for x in pool]
                if pool:
                    k_samples = min(3, len(pool))
                    samples = random.choices(pool, weights=weights, k=k_samples)
                    candidates_to_eval.update(samples)

            # 3. Random Explorer (Top 50%)
=======
            # 2. Quintic Weighted Sampling
            # Select from a broader pool but with steeper probability curve to favor urgent items
            if len(sorted_remain) > 3:
                pool = sorted_remain[3 : 25] # Look at next 22 items
                weights = [txn_urgency[x]**5 for x in pool]
                if pool:
                    k_samples = min(3, len(pool))
                    samples = random.choices(pool, weights=weights, k=k_samples)
                    candidates_to_eval.update(samples)

            # 3. Random Explorer (Top 50%)
>>>>>>> REPLACE
<<<<<<< SEARCH
                    if probe_costs:
                        min_c = min(probe_costs)
                        avg_c = sum(probe_costs) / len(probe_costs)
                        # Hybrid Metric: 0.6 Best Case + 0.4 Average Case
                        # Balances "Unlocks Critical Path" vs "General Compatibility"
                        metric_score = 0.6 * min_c + 0.4 * avg_c

                # Score: (Metric, ImmediateCost, -Urgency)
=======
                    if probe_costs:
                        min_c = min(probe_costs)
                        avg_c = sum(probe_costs) / len(probe_costs)
                        # Hybrid Metric: 0.7 Best Case + 0.3 Average Case
                        # Emphasize ability to fit the most urgent next item tightly
                        metric_score = 0.7 * min_c + 0.3 * avg_c

                # Score: (Metric, ImmediateCost, -Urgency)
>>>>>>> REPLACE
</DIFF>