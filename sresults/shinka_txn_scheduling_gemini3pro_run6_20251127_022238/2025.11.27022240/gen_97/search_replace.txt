<NAME>
sextic_beam_annealing_ils
</NAME>

<DESCRIPTION>
Refines the scheduling algorithm by intensifying the "Big Rocks First" strategy and adding simulated annealing dynamics to the ILS.
1.  **Sextic-Anchored Beam Search**: Upgrades sampling weight to Sextic ($d^6$) and increases deterministic anchors to Top-5. This extreme bias ensures the algorithm is hyper-focused on the critical path, which is essential for minimizing makespan in conflict-heavy workloads.
2.  **Unlock-Focused Lookahead**: Expands lookahead to Top-5 and shifts weighting to 0.8/0.2 (Min/Avg). This prioritizes "unlocking" at least one critical transaction immediately, preventing pipeline stalls.
3.  **Annealing ILS**: Replaces fixed threshold acceptance with a linear cooling schedule (0.5% down to 0.0%). This promotes exploration in early iterations and strict optimization in later ones.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # --- 2. BEAM SEARCH WITH LOOKAHEAD ---

    BEAM_WIDTH = max(5, int(num_seqs))

    # Beam State: (immediate_cost, schedule, remaining_list)
    beam = [(0, [], list(range(num_txns)))]

    for _ in range(num_txns):
        candidates_pool = []

        for p_cost, p_sched, p_remain in beam:

            # Sort remaining by Urgency
            sorted_remain = sorted(p_remain, key=lambda x: txn_urgency[x], reverse=True)

            candidates_to_eval = set()

            # 1. Deterministic Anchors (Top 3)
            # Ensures we always check the theoretically "best" moves
            candidates_to_eval.update(sorted_remain[:3])

            # 2. Quintic Weighted Sampling
            # Select from a broader pool but with steeper probability curve to favor urgent items
            if len(sorted_remain) > 3:
                pool = sorted_remain[3 : 25] # Look at next 22 items
                weights = [txn_urgency[x]**5 for x in pool]
                if pool:
                    k_samples = min(3, len(pool))
                    samples = random.choices(pool, weights=weights, k=k_samples)
                    candidates_to_eval.update(samples)

            # 3. Random Explorer (Top 50%)
            if len(sorted_remain) > 10:
                mid = len(sorted_remain) // 2
                candidates_to_eval.update(random.sample(sorted_remain[:mid], 1))

            # Evaluate with Hybrid Lookahead
            for cand in candidates_to_eval:
                curr_sched = p_sched + [cand]
                curr_cost = workload.get_opt_seq_cost(curr_sched)

                # Lookahead: Check Top-4 remaining items
                metric_score = curr_cost

                probes = []
                count = 0
                for r in sorted_remain:
                    if r != cand:
                        probes.append(r)
                        count += 1
                    if count >= 4: break

                if probes:
                    probe_costs = []
                    for p in probes:
                        probe_costs.append(workload.get_opt_seq_cost(curr_sched + [p]))

                    if probe_costs:
                        min_c = min(probe_costs)
                        avg_c = sum(probe_costs) / len(probe_costs)
                        # Hybrid Metric: 0.7 Best Case + 0.3 Average Case
                        # Emphasize ability to fit the most urgent next item tightly
                        metric_score = 0.7 * min_c + 0.3 * avg_c

                # Score: (Metric, ImmediateCost, -Urgency)
                score = (metric_score, curr_cost, -txn_urgency[cand])

                rem_c = list(p_remain)
                rem_c.remove(cand)

                candidates_pool.append((score, curr_sched, rem_c))
=======
    # --- 2. BEAM SEARCH WITH LOOKAHEAD ---

    BEAM_WIDTH = max(5, int(num_seqs))

    # Beam State: (immediate_cost, schedule, remaining_list)
    beam = [(0, [], list(range(num_txns)))]

    for _ in range(num_txns):
        candidates_pool = []

        for p_cost, p_sched, p_remain in beam:

            # Sort remaining by Urgency
            sorted_remain = sorted(p_remain, key=lambda x: txn_urgency[x], reverse=True)

            candidates_to_eval = set()

            # 1. Deterministic Anchors (Top 5)
            # Increased anchors to secure critical path stability
            candidates_to_eval.update(sorted_remain[:5])

            # 2. Sextic (d^6) Weighted Sampling
            # Select from a broader pool with extreme probability curve to find best fits for urgent items
            if len(sorted_remain) > 5:
                pool = sorted_remain[5 : 30] # Look at next 25 items
                weights = [txn_urgency[x]**6 for x in pool]
                if pool:
                    k_samples = min(3, len(pool))
                    samples = random.choices(pool, weights=weights, k=k_samples)
                    candidates_to_eval.update(samples)

            # 3. Random Explorer (Top 50%)
            if len(sorted_remain) > 15:
                mid = len(sorted_remain) // 2
                candidates_to_eval.update(random.sample(sorted_remain[:mid], 1))

            # Evaluate with Hybrid Lookahead
            for cand in candidates_to_eval:
                curr_sched = p_sched + [cand]
                curr_cost = workload.get_opt_seq_cost(curr_sched)

                # Lookahead: Check Top-5 remaining items
                metric_score = curr_cost

                probes = []
                count = 0
                for r in sorted_remain:
                    if r != cand:
                        probes.append(r)
                        count += 1
                    if count >= 5: break # Increased lookahead depth

                if probes:
                    probe_costs = []
                    for p in probes:
                        probe_costs.append(workload.get_opt_seq_cost(curr_sched + [p]))

                    if probe_costs:
                        min_c = min(probe_costs)
                        avg_c = sum(probe_costs) / len(probe_costs)
                        # Hybrid Metric: 0.8 Best Case + 0.2 Average Case
                        # Strongly favor unlocking at least one critical follower
                        metric_score = 0.8 * min_c + 0.2 * avg_c

                # Score: (Metric, ImmediateCost, -Urgency)
                score = (metric_score, curr_cost, -txn_urgency[cand])

                rem_c = list(p_remain)
                rem_c.remove(cand)

                candidates_pool.append((score, curr_sched, rem_c))
>>>>>>> REPLACE
<<<<<<< SEARCH
    # ILS Loop with Threshold Acceptance
    max_kicks = 5
    threshold = 0.002 # Allow 0.2% degradation to escape local optima

    for k in range(max_kicks):
        neighbor = list(current_schedule)
=======
    # ILS Loop with Annealing Threshold
    max_kicks = 5

    for k in range(max_kicks):
        # Linear annealing: Start 0.5% -> End 0.0%
        # Allows early exploration and late polishing
        threshold = 0.005 * (1.0 - k/max_kicks)

        neighbor = list(current_schedule)
>>>>>>> REPLACE
</DIFF>