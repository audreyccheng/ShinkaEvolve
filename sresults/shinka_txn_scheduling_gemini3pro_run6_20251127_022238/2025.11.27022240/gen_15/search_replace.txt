<NAME>
deduplicated_beam_search_with_lpt_sampling
</NAME>

<DESCRIPTION>
Integrates state deduplication into the Beam Search to avoid redundant exploration of identical transaction states (sets of remaining transactions). Enhances the candidate sampling strategy to deterministically include the longest processing time (LPT) transaction to address scheduling bottlenecks early. Adds a post-construction Local Search refinement step using adjacent swaps to fix minor greedy ordering errors.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def get_best_schedule(workload, num_seqs):
    """
    Get optimal schedule using Beam Search with heuristic tie-breaking.

    Args:
        workload: Workload object containing transaction data
        num_seqs: Used to determine the beam width (effort level)

    Returns:
        Tuple of (lowest makespan, corresponding schedule)
    """
    # Beam Search Parameters
    # Beam width determines how many parallel paths we explore.
    # Using max(5, num_seqs) ensures reasonable exploration even if num_seqs is low.
    BEAM_WIDTH = max(5, num_seqs)
    SAMPLES_PER_NODE = 16          # Number of candidates to evaluate per beam node
    MAX_CHILDREN = 3               # Diversity constraint: max paths from one parent

    num_txns = workload.num_txns

    # Precompute transaction durations (proxy: cost of running txn alone)
    # Used for tie-breaking: if costs are equal, prefer longer transactions to fill gaps.
    txn_durations = {}
    for t in range(num_txns):
        txn_durations[t] = workload.get_opt_seq_cost([t])

    # Initialize Beam with distinct random starts
    start_candidates = list(range(num_txns))
    random.shuffle(start_candidates)

    # Beam Item: {'cost': float, 'seq': list, 'rem': set}
    beam = []

    # Initialize the beam with single-transaction sequences
    for t in start_candidates[:BEAM_WIDTH]:
        seq = [t]
        cost = txn_durations[t]
        rem = set(range(num_txns))
        rem.remove(t)
        beam.append({'cost': cost, 'seq': seq, 'rem': rem})

    # Beam Search Construction Loop
    # Grow sequences one transaction at a time
    for _ in range(num_txns - 1):
        candidates = []

        # Expand each node in the current beam
        for p_idx, parent in enumerate(beam):
            parent_rem_list = list(parent['rem'])

            # Determine candidates to sample
            if len(parent_rem_list) <= SAMPLES_PER_NODE:
                samples = parent_rem_list
            else:
                samples = random.sample(parent_rem_list, SAMPLES_PER_NODE)

            for t in samples:
                new_seq = parent['seq'] + [t]

                # Evaluate cost (makespan) - this is the expensive step
                new_cost = workload.get_opt_seq_cost(new_seq)

                # Priority Tuple: (Primary: Cost, Secondary: -Duration)
                # We want minimal cost. If costs equal, we want maximal duration (min -Duration)
                priority = (new_cost, -txn_durations[t])

                candidates.append({
                    'priority': priority,
                    'cost': new_cost,
                    'seq': new_seq,
                    'added': t,
                    'p_idx': p_idx
                })

        # Sort all candidates by priority (lowest cost, then longest txn)
        candidates.sort(key=lambda x: x['priority'])

        # Select next beam with diversity constraints
        new_beam = []
        parent_usage = {i: 0 for i in range(len(beam))}
        reserve = []

        # First pass: fill beam respecting MAX_CHILDREN to maintain path diversity
        for cand in candidates:
            if len(new_beam) >= BEAM_WIDTH:
                break

            p_idx = cand['p_idx']
            if parent_usage[p_idx] < MAX_CHILDREN:
                # Add to beam
                new_rem = beam[p_idx]['rem'].copy()
                new_rem.remove(cand['added'])
                new_beam.append({'cost': cand['cost'], 'seq': cand['seq'], 'rem': new_rem})
                parent_usage[p_idx] += 1
            else:
                reserve.append(cand)

        # Second pass: if beam not full, fill from reserve (relax constraint)
        if len(new_beam) < BEAM_WIDTH:
            for cand in reserve:
                if len(new_beam) >= BEAM_WIDTH:
                    break
                p_idx = cand['p_idx']
                new_rem = beam[p_idx]['rem'].copy()
                new_rem.remove(cand['added'])
                new_beam.append({'cost': cand['cost'], 'seq': cand['seq'], 'rem': new_rem})

        if not new_beam:
            break

        beam = new_beam

    # Find best schedule in final beam
    best_result = min(beam, key=lambda x: x['cost'])

    return best_result['cost'], best_result['seq']
=======
def get_best_schedule(workload, num_seqs):
    """
    Get optimal schedule using Deduplicated Beam Search with Smart Sampling and Local Search.

    Args:
        workload: Workload object containing transaction data
        num_seqs: Used to determine the beam width (effort level)

    Returns:
        Tuple of (lowest makespan, corresponding schedule)
    """
    # Beam Search Parameters
    # Increased beam width to allow for effective state deduplication
    BEAM_WIDTH = max(8, num_seqs)
    SAMPLES_PER_NODE = 16

    num_txns = workload.num_txns

    # Precompute transaction durations
    txn_durations = {}
    for t in range(num_txns):
        txn_durations[t] = workload.get_opt_seq_cost([t])

    # Initialization
    # Seed beam with a mix of longest transactions (LPT) and random ones.
    # LPT is often a good heuristic for the start of a schedule.
    sorted_by_duration = sorted(range(num_txns), key=lambda t: txn_durations[t], reverse=True)

    start_candidates = set()
    start_candidates.update(sorted_by_duration[:BEAM_WIDTH // 2])

    # Fill remaining slots with random starts
    pool = [t for t in range(num_txns) if t not in start_candidates]
    if len(start_candidates) < BEAM_WIDTH and pool:
        start_candidates.update(random.sample(pool, min(len(pool), BEAM_WIDTH - len(start_candidates))))

    beam = []
    for t in start_candidates:
        rem = set(range(num_txns))
        rem.remove(t)
        beam.append({
            'cost': txn_durations[t],
            'seq': [t],
            'rem': rem
        })

    # Beam Search Loop
    for _ in range(num_txns - 1):
        candidates = []

        for parent in beam:
            rem_list = list(parent['rem'])

            # Smart Sampling Strategy:
            # 1. Deterministic: Always try the longest remaining transaction (LPT).
            #    This handles bottlenecks early.
            # 2. Random: Sample other transactions to escape greedy traps.
            if len(rem_list) <= SAMPLES_PER_NODE:
                samples = rem_list
            else:
                samples = set()
                # Find max duration txn (LPT)
                max_txn = max(rem_list, key=lambda t: txn_durations[t])
                samples.add(max_txn)

                # Fill rest randomly
                needed = SAMPLES_PER_NODE - len(samples)
                pool = [t for t in rem_list if t not in samples]
                samples.update(random.sample(pool, min(len(pool), needed)))

            for t in samples:
                new_seq = parent['seq'] + [t]
                new_cost = workload.get_opt_seq_cost(new_seq)

                # Priority: (Cost, -Duration)
                # Minimize Cost, then Maximize Duration of added item
                priority = (new_cost, -txn_durations[t])

                candidates.append({
                    'priority': priority,
                    'cost': new_cost,
                    'seq': new_seq,
                    'rem': parent['rem'], # Reference, copy later
                    'added': t
                })

        # Sort candidates
        candidates.sort(key=lambda x: x['priority'])

        # Construct Next Beam with Deduplication
        new_beam = []
        seen_states = set()

        for cand in candidates:
            if len(new_beam) >= BEAM_WIDTH:
                break

            # Create new remaining set
            new_rem = cand['rem'].copy()
            new_rem.remove(cand['added'])

            # State Deduplication:
            # The state is defined by the set of remaining transactions.
            # If we reach the same 'remaining' set via multiple paths, we only keep the best one.
            state_key = frozenset(new_rem)

            if state_key not in seen_states:
                seen_states.add(state_key)
                new_beam.append({
                    'cost': cand['cost'],
                    'seq': cand['seq'],
                    'rem': new_rem
                })

        if not new_beam:
            break
        beam = new_beam

    # Select best from beam
    best_result = min(beam, key=lambda x: x['cost'])
    best_cost = best_result['cost']
    best_seq = best_result['seq']

    # Local Search Refinement
    # Perform adjacent swaps to fix minor local ordering inefficiencies.
    for _ in range(2):
        improved = False
        for i in range(num_txns - 1):
            # Swap
            best_seq[i], best_seq[i+1] = best_seq[i+1], best_seq[i]
            new_c = workload.get_opt_seq_cost(best_seq)

            if new_c < best_cost:
                best_cost = new_c
                improved = True
            else:
                # Revert
                best_seq[i], best_seq[i+1] = best_seq[i+1], best_seq[i]

        if not improved:
            break

    return best_cost, best_seq
>>>>>>> REPLACE
</DIFF>