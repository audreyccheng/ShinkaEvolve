<NAME>
stochastic_lookahead_ils
</NAME>

<DESCRIPTION>
Replaces the 'Work-Density Beam Search' with a 'Lookahead Beam Search' that integrates Stochastic Sampling and Iterated Local Search (ILS).

Improvements include:
1.  **Lookahead:** Instead of just using the immediate density heuristic, the algorithm performs a one-step lookahead by tentatively adding the next largest transaction (LPT). This helps identify paths that might look good locally (low makespan) but block critical heavy transactions.
2.  **Weighted Sampling:** Replaces uniform random sampling with weighted sampling based on transaction duration. This biases the search towards "heavier" items which are harder to schedule, ensuring they aren't left for the end where they cause fragmentation.
3.  **Iterated Local Search (ILS):** Adds a perturbation (random swap) phase after the initial local search descent. This "kick" allows the algorithm to escape local optima that simple hill-climbing cannot resolve.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def get_best_schedule(workload, num_seqs):
    """
    Get optimal schedule using Work-Density Beam Search with Enhanced Local Search.

    Key Improvements:
    1. 'Work Density' Heuristic: Minimizes (Makespan - TotalDuration). This prefers schedules
       that pack more work (duration) into the same makespan, acting as a proxy for parallelism.
    2. Windowed Swaps & Insertions: Local search refinement looks beyond adjacent neighbors.
    3. State Deduplication: Ensures beam diversity.

    Args:
        workload: Workload object containing transaction data
        num_seqs: Used to scale the beam width

    Returns:
        Tuple of (lowest makespan, corresponding schedule)
    """
    # --- Parameters ---
    BEAM_WIDTH = int(max(10, num_seqs * 2.0))
    SAMPLES_PER_NODE = 20

    num_txns = workload.num_txns

    # --- Precompute Heuristics ---
    txn_durations = {t: workload.get_opt_seq_cost([t]) for t in range(num_txns)}

    # Sorted list for LPT sampling
    sorted_by_duration = sorted(range(num_txns), key=lambda t: txn_durations[t], reverse=True)

    # --- Initialization ---
    start_candidates = set()
    start_candidates.update(sorted_by_duration[:BEAM_WIDTH])

    while len(start_candidates) < BEAM_WIDTH * 2:
        start_candidates.add(random.randint(0, num_txns - 1))

    beam = []
    for t in start_candidates:
        rem = set(range(num_txns))
        rem.remove(t)
        beam.append({
            'cost': txn_durations[t],
            'total_dur': txn_durations[t],
            'seq': [t],
            'rem': rem
        })

    # Prune initial beam
    # Priority: (Cost - TotalDuration, -TotalDuration)
    beam.sort(key=lambda x: (x['cost'] - x['total_dur'], -x['total_dur']))
    beam = beam[:BEAM_WIDTH]

    # --- Beam Search Loop ---
    for _ in range(num_txns - 1):
        candidates = []

        for parent in beam:
            rem_set = parent['rem']

            # --- Hybrid Sampling ---
            samples = set()

            # 1. Deterministic LPT (Top 3)
            lpt_count = 0
            for t in sorted_by_duration:
                if t in rem_set:
                    samples.add(t)
                    lpt_count += 1
                    if lpt_count >= 3:
                        break

            # 2. Random Sampling
            needed = SAMPLES_PER_NODE - len(samples)
            if needed > 0:
                pool = list(rem_set - samples)
                if len(pool) <= needed:
                    samples.update(pool)
                else:
                    samples.update(random.sample(pool, needed))

            # --- Evaluation ---
            for t in samples:
                new_seq = parent['seq'] + [t]
                new_cost = workload.get_opt_seq_cost(new_seq)
                new_total_dur = parent['total_dur'] + txn_durations[t]

                # Heuristic: Minimize (Makespan - TotalDuration)
                # Ideally, Makespan grows slower than TotalDuration due to parallelism.
                score = new_cost - new_total_dur

                candidates.append({
                    'priority': (score, -new_total_dur),
                    'cost': new_cost,
                    'total_dur': new_total_dur,
                    'seq': new_seq,
                    'rem': rem_set,
                    'added': t
                })

        # Sort candidates
        candidates.sort(key=lambda x: x['priority'])

        # --- Deduplication ---
        new_beam = []
        seen_states = set()

        for cand in candidates:
            if len(new_beam) >= BEAM_WIDTH:
                break

            new_rem = cand['rem'].copy()
            new_rem.remove(cand['added'])

            state_key = frozenset(new_rem)

            if state_key not in seen_states:
                seen_states.add(state_key)
                new_beam.append({
                    'cost': cand['cost'],
                    'total_dur': cand['total_dur'],
                    'seq': cand['seq'],
                    'rem': new_rem
                })

        if not new_beam:
            break
        beam = new_beam

    # --- Result Selection ---
    best_result = min(beam, key=lambda x: x['cost'])
    best_cost = best_result['cost']
    best_seq = best_result['seq']

    # --- Local Search Refinement ---

    # Phase 1: Windowed Swaps (Higher impact)
    # Checks swaps within a window of 4-5 items
    WINDOW = 5
    for _ in range(2):
        improved = False
        for i in range(num_txns - 1):
            for offset in range(1, WINDOW + 1):
                j = i + offset
                if j >= num_txns: break

                best_seq[i], best_seq[j] = best_seq[j], best_seq[i]
                new_c = workload.get_opt_seq_cost(best_seq)

                if new_c < best_cost:
                    best_cost = new_c
                    improved = True
                else:
                    best_seq[i], best_seq[j] = best_seq[j], best_seq[i]
        if not improved:
            break

    # Phase 2: Targeted Insertions (Fine tuning)
    # Take a txn and try moving it slightly forward or backward
    # We limit this to a small window to avoid O(N^2) overhead
    for i in range(num_txns):
        curr = best_seq[i]
        start = max(0, i - 4)
        end = min(num_txns, i + 4)

        for j in range(start, end):
            if i == j: continue

            # Construct candidate sequence by moving txn i to position j
            if i < j:
                cand_seq = best_seq[:i] + best_seq[i+1:j+1] + [curr] + best_seq[j+1:]
            else:
                cand_seq = best_seq[:j] + [curr] + best_seq[j:i] + best_seq[i+1:]

            c = workload.get_opt_seq_cost(cand_seq)
            if c < best_cost:
                best_cost = c
                best_seq = cand_seq
                # If we moved it, indices shift, so break to restart or continue carefully
                # For simplicity, we just keep going with the new seq, though 'i' might point to new val
                break

    return best_cost, best_seq
=======
def get_best_schedule(workload, num_seqs):
    """
    Get optimal schedule using Lookahead Beam Search with Stochastic Sampling and ILS.

    Key Features:
    1. Lookahead: Evaluates candidates not just on the immediate move, but on the
       feasibility of scheduling the next largest task (LPT).
    2. Weighted Sampling: Biases exploration towards longer transactions.
    3. Iterated Local Search: Uses perturbation to escape local optima after beam search.

    Args:
        workload: Workload object containing transaction data
        num_seqs: Used to scale the beam width

    Returns:
        Tuple of (lowest makespan, corresponding schedule)
    """
    # --- Parameters ---
    BEAM_WIDTH = int(max(10, num_seqs * 2.5))
    SAMPLES_PER_NODE = 16
    LOOKAHEAD_FACTOR = 2  # Evaluate lookahead for Top N * Factor candidates

    num_txns = workload.num_txns

    # --- Precompute Heuristics ---
    txn_durations = {t: workload.get_opt_seq_cost([t]) for t in range(num_txns)}
    sorted_lpt = sorted(range(num_txns), key=lambda t: txn_durations[t], reverse=True)

    # --- Initialization ---
    start_candidates = set()
    start_candidates.update(sorted_lpt[:BEAM_WIDTH])

    # Fill with random if needed
    attempts = 0
    while len(start_candidates) < BEAM_WIDTH * 2 and attempts < num_txns * 2:
        start_candidates.add(random.randint(0, num_txns - 1))
        attempts += 1

    beam = []
    for t in start_candidates:
        rem = set(range(num_txns))
        rem.remove(t)
        cost = txn_durations[t]
        # Score: Cost - TotalDuration (Work Density). Lower is better.
        beam.append({
            'cost': cost,
            'total_dur': cost,
            'seq': [t],
            'rem': rem,
            'base_score': cost - cost
        })

    beam.sort(key=lambda x: (x['base_score'], -x['total_dur']))
    beam = beam[:BEAM_WIDTH]

    # --- Beam Search Loop ---
    for _ in range(num_txns - 1):
        candidates = []

        for parent in beam:
            rem_list = list(parent['rem'])
            if not rem_list: continue

            # --- Hybrid Sampling ---
            samples = set()

            # 1. Deterministic LPT (Top 3)
            lpt_added = 0
            for t in sorted_lpt:
                if t in parent['rem']:
                    samples.add(t)
                    lpt_added += 1
                    if lpt_added >= 3:
                        break

            # 2. Weighted Random Sampling
            needed = SAMPLES_PER_NODE - len(samples)
            if needed > 0:
                pool = [t for t in rem_list if t not in samples]
                if pool:
                    if len(pool) <= needed:
                        samples.update(pool)
                    else:
                        weights = [txn_durations[t] for t in pool]
                        for _ in range(needed * 2): # Oversample to handle collisions
                            if len(samples) >= SAMPLES_PER_NODE: break
                            pick = random.choices(pool, weights=weights, k=1)[0]
                            samples.add(pick)

            # --- Base Evaluation ---
            for t in samples:
                new_seq = parent['seq'] + [t]
                new_cost = workload.get_opt_seq_cost(new_seq)
                new_total_dur = parent['total_dur'] + txn_durations[t]

                # Base Score: Density
                base_score = new_cost - new_total_dur

                candidates.append({
                    'cost': new_cost,
                    'total_dur': new_total_dur,
                    'seq': new_seq,
                    'rem': parent['rem'], # Ref only
                    'added': t,
                    'base_score': base_score
                })

        # --- Lookahead Stage ---
        # 1. Filter candidates by base score to keep manageable set for lookahead
        candidates.sort(key=lambda x: (x['base_score'], -x['total_dur']))

        # Deduplicate states before lookahead
        unique_cands = []
        seen_states = set()
        for cand in candidates:
            # New rem set
            new_rem = cand['rem'].copy()
            new_rem.remove(cand['added'])

            # State key
            state_key = frozenset(new_rem)
            if state_key not in seen_states:
                seen_states.add(state_key)
                cand['new_rem'] = new_rem
                unique_cands.append(cand)

        # Limit pool for expensive lookahead
        lookahead_pool = unique_cands[:BEAM_WIDTH * LOOKAHEAD_FACTOR]

        # 2. Perform Lookahead
        # Tentatively add the largest remaining transaction to see if it fits well
        for cand in lookahead_pool:
            if not cand['new_rem']:
                cand['final_score'] = cand['base_score']
                continue

            # Find max duration item in remaining
            next_heavy = None
            for t in sorted_lpt:
                if t in cand['new_rem']:
                    next_heavy = t
                    break

            if next_heavy is not None:
                la_seq = cand['seq'] + [next_heavy]
                la_cost = workload.get_opt_seq_cost(la_seq)
                la_total = cand['total_dur'] + txn_durations[next_heavy]
                cand['final_score'] = la_cost - la_total
            else:
                cand['final_score'] = cand['base_score']

        # 3. Final Selection
        lookahead_pool.sort(key=lambda x: (x['final_score'], -x['total_dur']))

        new_beam = []
        for cand in lookahead_pool[:BEAM_WIDTH]:
            new_beam.append({
                'cost': cand['cost'],
                'total_dur': cand['total_dur'],
                'seq': cand['seq'],
                'rem': cand['new_rem'],
                'base_score': cand['base_score']
            })

        if not new_beam:
            break
        beam = new_beam

    # Select best
    best_result = min(beam, key=lambda x: x['cost'])
    best_cost = best_result['cost']
    best_seq = best_result['seq']

    # --- Iterated Local Search (ILS) ---
    def refine(seq, current_cost):
        """Greedy insertion refinement"""
        w_size = 8
        improved = True
        while improved:
            improved = False
            for i in range(len(seq)):
                # Define window
                start = max(0, i - w_size)
                end = min(len(seq), i + w_size)

                # Try moving seq[i] to positions in window
                txn = seq[i]
                temp = seq[:i] + seq[i+1:]

                for k in range(start, end): # position in temp
                    cand = temp[:k] + [txn] + temp[k:]
                    c = workload.get_opt_seq_cost(cand)
                    if c < current_cost:
                        current_cost = c
                        seq = cand
                        improved = True
                        break
                if improved: break
        return seq, current_cost

    # Initial Descent
    best_seq, best_cost = refine(best_seq, best_cost)

    # Perturbation (Kick)
    for _ in range(2): # Try 2 random kicks
        p_seq = best_seq[:]
        if len(p_seq) > 2:
            idx1, idx2 = random.sample(range(len(p_seq)), 2)
            p_seq[idx1], p_seq[idx2] = p_seq[idx2], p_seq[idx1]

            # Repair
            p_seq, p_cost = refine(p_seq, workload.get_opt_seq_cost(p_seq))

            if p_cost < best_cost:
                best_cost = p_cost
                best_seq = p_seq

    return best_cost, best_seq
>>>>>>> REPLACE
</DIFF>