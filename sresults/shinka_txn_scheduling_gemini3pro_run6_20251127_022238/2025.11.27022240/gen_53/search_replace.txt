<NAME>
tune_params
</NAME>

<DESCRIPTION>
Update beam search parameters to increase search width and depth. Increase ILS cycles for better local refinement.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # --- Parameters ---
    BEAM_WIDTH = int(max(10, num_seqs * 2.5))
    SAMPLES_PER_NODE = 20
    MAX_CHILDREN = 3
    ILS_CYCLES = 6  # Number of perturbation-repair iterations

    # Lookahead pool size (subset of candidates to apply expensive lookahead to)
    LOOKAHEAD_POOL_SIZE = int(BEAM_WIDTH * 1.5)
=======
    # --- Parameters ---
    BEAM_WIDTH = int(max(10, num_seqs * 2.8))
    SAMPLES_PER_NODE = 24
    MAX_CHILDREN = 4
    ILS_CYCLES = 8  # Increased cycles for deeper local search

    # Lookahead pool size (subset of candidates to apply expensive lookahead to)
    LOOKAHEAD_POOL_SIZE = int(BEAM_WIDTH * 2.0)
>>>>>>> REPLACE
</DIFF>

<NAME>
cubic_sampling
</NAME>

<DESCRIPTION>
Switch from linear to cubic weighted sampling in the beam search expansion. This aggressively biases the search towards scheduling longer ("heavier") transactions earlier, which is critical for minimizing makespan as these transactions are the bottlenecks.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
            # 2. Weighted Random Sampling
            needed = SAMPLES_PER_NODE - len(samples)
            if needed > 0:
                pool = [x for x in rem_list if x not in samples]
                if pool:
                    if len(pool) <= needed:
                        samples.update(pool)
                    else:
                        # Weight by duration
                        weights = [txn_durations[x] for x in pool]

                        picks = set()
                        # Oversample to account for potential duplicates in choices
                        try:
                            chosen = random.choices(pool, weights=weights, k=needed * 2)
                            samples.update(chosen)
                        except ValueError:
                            # Fallback if weights sum to 0 (unlikely)
                            samples.update(random.sample(pool, min(len(pool), needed)))

                        # Fill if still needed
                        if len(samples) < SAMPLES_PER_NODE:
                             needed_now = SAMPLES_PER_NODE - len(samples)
                             rem_pool = [x for x in pool if x not in samples]
                             if rem_pool:
                                 samples.update(random.sample(rem_pool, min(len(rem_pool), needed_now)))
=======
            # 2. Cubic Weighted Random Sampling
            needed = SAMPLES_PER_NODE - len(samples)
            if needed > 0:
                pool = [x for x in rem_list if x not in samples]
                if pool:
                    if len(pool) <= needed:
                        samples.update(pool)
                    else:
                        # Cubic weights to aggressively favor heavy transactions
                        weights = [txn_durations[x]**3 for x in pool]

                        picks = set()
                        # Oversample to account for potential duplicates in choices
                        try:
                            chosen = random.choices(pool, weights=weights, k=needed * 2)
                            samples.update(chosen)
                        except ValueError:
                            samples.update(random.sample(pool, min(len(pool), needed)))

                        # Fill if still needed
                        if len(samples) < SAMPLES_PER_NODE:
                             needed_now = SAMPLES_PER_NODE - len(samples)
                             rem_pool = [x for x in pool if x not in samples]
                             if rem_pool:
                                 samples.update(random.sample(rem_pool, min(len(rem_pool), needed_now)))
>>>>>>> REPLACE
</DIFF>

<NAME>
multi_target_lookahead
</NAME>

<DESCRIPTION>
Enhance the lookahead mechanism to evaluate the top 3 largest remaining transactions instead of just the single largest. The score for a candidate is updated to the best (lowest) score found among these targets. This allows the beam search to recognize states that handle at least one critical transaction well, preventing the pruning of branches that clash with the absolute largest item but fit other heavy items perfectly.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        # 3. Apply One-Step Lookahead (LPT)
        for cand in lookahead_candidates:
            rem_set = cand['new_rem']
            if not rem_set:
                cand['final_score'] = cand['base_score']
                continue

            # Find next largest transaction available in this branch
            next_t = None
            for t in sorted_lpt:
                if t in rem_set:
                    next_t = t
                    break

            if next_t is not None:
                # Tentatively schedule it
                la_seq = cand['seq'] + [next_t]
                la_cost = workload.get_opt_seq_cost(la_seq)
                la_total = cand['total_dur'] + txn_durations[next_t]

                # Update score based on lookahead result
                cand['final_score'] = la_cost - la_total
            else:
                cand['final_score'] = cand['base_score']
=======
        # 3. Apply Multi-Target One-Step Lookahead
        # Check top 3 largest remaining transactions and take the best outcome
        for cand in lookahead_candidates:
            rem_set = cand['new_rem']
            if not rem_set:
                cand['final_score'] = cand['base_score']
                continue

            # Identify top 3 critical transactions
            targets = []
            count = 0
            for t in sorted_lpt:
                if t in rem_set:
                    targets.append(t)
                    count += 1
                    if count >= 3:
                        break

            if targets:
                best_la_score = float('inf')
                for next_t in targets:
                    la_seq = cand['seq'] + [next_t]
                    la_cost = workload.get_opt_seq_cost(la_seq)
                    la_total = cand['total_dur'] + txn_durations[next_t]
                    score = la_cost - la_total
                    if score < best_la_score:
                        best_la_score = score
                cand['final_score'] = best_la_score
            else:
                cand['final_score'] = cand['base_score']
>>>>>>> REPLACE
</DIFF>

<NAME>
ils_ruin_recreate
</NAME>

<DESCRIPTION>
Replace the simple random swap perturbation in ILS with a stronger "Ruin and Recreate" strategy. This removes a small block of transactions (3-5 items) and re-inserts them greedily into their optimal positions. This provides a more structural perturbation capable of escaping deeper local optima compared to simple swaps.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    for i in range(ILS_CYCLES):
        # Perturb (Kick)
        # Vary kick strength: every 3rd cycle do a double swap
        num_swaps = 2 if (i % 3 == 0) else 1

        p_seq = curr_seq[:]
        if len(p_seq) > 4:
            for _ in range(num_swaps):
                idx1, idx2 = random.sample(range(len(p_seq)), 2)
                p_seq[idx1], p_seq[idx2] = p_seq[idx2], p_seq[idx1]

        # Repair
        p_seq, p_cost = run_local_search(p_seq, workload.get_opt_seq_cost(p_seq))

        # Acceptance Criterion (Simple Greedy)
        # We accept if better than globally best, or better than current base
        # Here we track global best
        if p_cost < best_cost:
            best_cost = p_cost
            best_seq = p_seq
            curr_seq = p_seq # Move to new basin
            curr_cost = p_cost
        elif p_cost < curr_cost:
            # Accept improvement over current even if not global best (unlikely in greedy ILS but logical)
            curr_seq = p_seq
            curr_cost = p_cost
        # Else: Reject and loop again from curr_seq (implicitly, as we use curr_seq for next kick)
=======
    for i in range(ILS_CYCLES):
        # Perturbation: Ruin and Recreate
        p_seq = curr_seq[:]

        if len(p_seq) > 5:
            # Ruin: Remove random set of transactions
            ruin_size = random.choice([3, 4])
            removed = []
            for _ in range(ruin_size):
                if not p_seq: break
                idx = random.randint(0, len(p_seq) - 1)
                removed.append(p_seq.pop(idx))

            # Recreate: Insert back in best position (Greedy Best-Fit)
            # Sorting removed items by duration (LPT) usually helps packing
            removed.sort(key=lambda t: txn_durations[t], reverse=True)

            for t in removed:
                best_pos = -1
                best_c = float('inf')

                # Scan all valid insertion positions
                for pos in range(len(p_seq) + 1):
                    cand = p_seq[:pos] + [t] + p_seq[pos:]
                    c = workload.get_opt_seq_cost(cand)
                    if c < best_c:
                        best_c = c
                        best_pos = pos

                if best_pos != -1:
                    p_seq.insert(best_pos, t)
                else:
                    p_seq.append(t)

        # Repair: Local Search
        p_seq, p_cost = run_local_search(p_seq, workload.get_opt_seq_cost(p_seq))

        # Acceptance Criterion
        if p_cost < best_cost:
            best_cost = p_cost
            best_seq = p_seq
            curr_seq = p_seq
            curr_cost = p_cost
        elif p_cost < curr_cost:
            curr_seq = p_seq
            curr_cost = p_cost
        else:
            # Revert to current sequence for next iteration if rejected
            pass
>>>>>>> REPLACE
</DIFF>