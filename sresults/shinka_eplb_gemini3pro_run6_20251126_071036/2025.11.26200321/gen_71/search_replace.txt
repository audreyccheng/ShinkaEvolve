<NAME>
improved_refinement_and_candidates
</NAME>

<DESCRIPTION>
1.  **Global Swap Refinement**: Replaced the restricted "Swap with Min Pack" refinement in `_refine_packing` with a comprehensive "Swap with Any Pack" search. The algorithm now checks potential swaps between the heaviest pack and all other packs to find the move that maximally reduces the load of the heaviest pack (constrained by not increasing the target pack's load beyond the original max). This allows the algorithm to escape local optima where the lightest pack is too full to accept a large item, but a medium-loaded pack could.
2.  **Tail Shuffle Candidates**: In `balanced_packing`, introduced "Tail Shuffle" candidates (32 instances). These candidates fix the assignment of the heaviest `num_packs` items according to LPT order (addressing the "Big Rocks" problem) while randomly shuffling the remaining items. This ensures critical large items are distributed efficiently while exploring diverse arrangements for the rest.
3.  **Ensemble Update**: Updated the candidate ensemble to 1 LPT, 1 ZigZag, 32 Random, 32 Tail Shuffle, and 62 Noisy LPT (Total 128), providing a balanced mix of deterministic heuristics and randomized exploration.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def _refine_packing(weights: torch.Tensor,
                    pack_ids: torch.Tensor,
                    pack_loads: torch.Tensor,
                    ranks: torch.Tensor,
                    num_iters: int = 10) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Refines the packing by iteratively attempting to swap a single item between
    the heaviest and lightest packs to reduce imbalance.
    """
    batch_size, num_items = weights.shape
    device = weights.device
    batch_indices = torch.arange(batch_size, device=device)

    # Pre-compute constant memory for diff expansion if possible, but B*N*N might be too big for 128 candidates.
    # Since we prune to Top-K (e.g. 8), B is small (Layers * 8).
    # If Layers=32, B=256. N=256. 256^3 * 4 bytes = 64MB. Very safe.

    for _ in range(num_iters):
        # Identify heaviest and lightest packs
        max_load, max_pack_idx = pack_loads.max(dim=1)
        min_load, min_pack_idx = pack_loads.min(dim=1)
        current_diff = max_load - min_load

        # Stop if imbalance is negligible
        if current_diff.max() < 1e-5:
            break

        # Masks for items in max/min packs
        is_in_max = (pack_ids == max_pack_idx.unsqueeze(1))
        is_in_min = (pack_ids == min_pack_idx.unsqueeze(1))

        # Weight diffs: w[i] - w[j]
        # [B, N, 1] - [B, 1, N] = [B, N, N]
        w_diff = weights.unsqueeze(2) - weights.unsqueeze(1)

        # Metric: |(diff) - 2*(w_i - w_j)|
        target = current_diff.view(-1, 1, 1)
        new_diff_metric = torch.abs(target - 2 * w_diff)

        # Valid mask: i in Max, j in Min
        valid_swap = is_in_max.unsqueeze(2) & is_in_min.unsqueeze(1)

        # Improvement check: New diff < Current diff
        # We mask invalid swaps with infinity
        # We also want strict improvement to avoid cycling
        # (though abs metric prevents cycling usually, float precision issues can cause flip-flopping)
        new_diff_metric = torch.where(valid_swap, new_diff_metric, torch.tensor(float('inf'), device=device))

        # Find best swap per batch
        # flatten last two dims
        flat_metric = new_diff_metric.view(batch_size, -1)
        min_val, flat_idx = flat_metric.min(dim=1)

        # Check improvement
        improve_mask = min_val < (current_diff - 1e-6)

        if not improve_mask.any():
            break

        # Apply swaps
        active_batch = batch_indices[improve_mask]
        active_idx = flat_idx[improve_mask]

        i_idx = active_idx // num_items
        j_idx = active_idx % num_items

        p_max = max_pack_idx[active_batch]
        p_min = min_pack_idx[active_batch]

        w_i = weights[active_batch, i_idx]
        w_j = weights[active_batch, j_idx]
        delta = w_i - w_j

        # Update loads
        pack_loads[active_batch, p_max] -= delta
        pack_loads[active_batch, p_min] += delta

        # Update IDs
        pack_ids[active_batch, i_idx] = p_min
        pack_ids[active_batch, j_idx] = p_max

        # Update Ranks
        # Swap ranks to maintain valid rank set
        r_i = ranks[active_batch, i_idx]
        r_j = ranks[active_batch, j_idx]
        ranks[active_batch, i_idx] = r_j
        ranks[active_batch, j_idx] = r_i

    return pack_ids, ranks, pack_loads
=======
def _refine_packing(weights: torch.Tensor,
                    pack_ids: torch.Tensor,
                    pack_loads: torch.Tensor,
                    ranks: torch.Tensor,
                    num_iters: int = 10) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Refines the packing by iteratively attempting to swap a single item between
    the heaviest pack and ANY other pack to reduce the max load.
    """
    batch_size, num_items = weights.shape
    num_packs = pack_loads.shape[1]
    device = weights.device
    batch_indices = torch.arange(batch_size, device=device)

    # Precompute pairwise weight differences: [B, N, N]
    # w_diff[b, i, j] = w[b, i] - w[b, j]
    # Positive means w_i > w_j (reducing max load if i leaves)
    w_diff = weights.unsqueeze(2) - weights.unsqueeze(1)

    for _ in range(num_iters):
        max_load, max_pack_idx = pack_loads.max(dim=1)

        best_gain = torch.zeros(batch_size, device=device, dtype=weights.dtype)
        best_swap_flat = torch.zeros(batch_size, device=device, dtype=torch.int64)
        best_target_pack = torch.full((batch_size,), -1, device=device, dtype=torch.int64)

        is_in_max = (pack_ids == max_pack_idx.unsqueeze(1)) # [B, N]

        # Check swaps with all other packs
        for p in range(num_packs):
            # Target pack load [B]
            target_load = pack_loads[:, p]

            # Constraint: We only want to swap if target pack is NOT the max pack (handled by gap check implicitly but explicit is safer)
            # Gap: max_load - target_load
            gap = max_load - target_load

            # Mask valid batches where gap > 0 (and p is not max_pack)
            active_p_mask = gap > 1e-5

            if not active_p_mask.any():
                continue

            is_in_target = (pack_ids == torch.tensor(p, device=device)) # [B, N]

            # Valid pairs: i in Max, j in Target
            valid_pair = is_in_max.unsqueeze(2) & is_in_target.unsqueeze(1) # [B, N, N]

            # To reduce max load:
            # New Max Load = Max - (w_i - w_j). We need w_i - w_j > 0.
            # New Target Load = Target + (w_i - w_j). We need New Target Load < Original Max Load (strictly, or <=).
            # If New Target Load < Original Max Load, then we have strictly reduced the max load of the system (assuming no other pack was equal to Max).
            # Delta = w_i - w_j.
            # Condition 1: Delta > 0.
            # Condition 2: Target + Delta < Max => Delta < Max - Target = Gap.
            # So 0 < Delta < Gap.
            # Gain = Delta (reduction in Max Load).
            # However, if multiple packs share Max Load, reducing one might not reduce the global max.
            # But local greedy descent on the specific Max Pack is a good heuristic.

            delta = w_diff

            valid_swap = valid_pair & (delta > 1e-6) & (delta < (gap.view(-1, 1, 1) - 1e-6))

            # We want to maximize Delta (maximize reduction)
            # Mask invalid with -1
            masked_delta = torch.where(valid_swap, delta, torch.tensor(-1.0, device=device, dtype=weights.dtype))

            # Max over N*N pairs
            p_max_gain, p_flat_idx = masked_delta.view(batch_size, -1).max(dim=1)

            # Update best found so far
            improve = (p_max_gain > best_gain) & active_p_mask

            if improve.any():
                best_gain = torch.where(improve, p_max_gain, best_gain)
                best_swap_flat = torch.where(improve, p_flat_idx, best_swap_flat)
                best_target_pack = torch.where(improve, torch.tensor(p, device=device), best_target_pack)

        # Apply best swaps
        active_mask = best_target_pack != -1
        if not active_mask.any():
            break

        active_batch_idx = batch_indices[active_mask]
        active_flat = best_swap_flat[active_mask]
        active_p = best_target_pack[active_mask]
        active_max_p = max_pack_idx[active_mask]

        i_idx = active_flat // num_items
        j_idx = active_flat % num_items

        w_i = weights[active_batch_idx, i_idx]
        w_j = weights[active_batch_idx, j_idx]
        delta_val = w_i - w_j

        # Update loads
        # Use index_put_ or scatter_add_?
        # Since active_batch_idx is unique, we can iterate or use advanced indexing.
        # Direct indexing works for unique indices on dimension 0.
        pack_loads[active_batch_idx, active_max_p] -= delta_val
        pack_loads[active_batch_idx, active_p] += delta_val

        # Update IDs
        pack_ids[active_batch_idx, i_idx] = active_p
        pack_ids[active_batch_idx, j_idx] = active_max_p

        # Update Ranks
        r_i = ranks[active_batch_idx, i_idx]
        r_j = ranks[active_batch_idx, j_idx]
        ranks[active_batch_idx, i_idx] = r_j
        ranks[active_batch_idx, j_idx] = r_i

    return pack_ids, ranks, pack_loads
>>>>>>> REPLACE
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Hybrid Ensemble Strategy with Top-K Refinement.
    """
    num_layers, num_items = weight.shape
    device = weight.device
    capacity = num_items // num_packs

    # 1. Candidate Generation
    # Total candidates: 128
    # Distribution:
    # - 1 Pure LPT
    # - 31 Folded/ZigZag variants (shifted/permuted)
    # - 32 Random Shuffles
    # - 64 Noisy LPT

    num_candidates = 128

    # A. LPT
    lpt_val, lpt_idx = weight.sort(dim=-1, descending=True)
    c_lpt = lpt_idx.unsqueeze(1) # [L, 1, N]

    # B. Folded / ZigZag
    # Create base zigzag permutation
    zigzag_perm = torch.empty(num_items, device=device, dtype=torch.long)
    half = (num_items + 1) // 2
    arange = torch.arange(num_items, device=device)
    zigzag_perm[0::2] = arange[:half]
    zigzag_perm[1::2] = arange[half:].flip(0)

    # Generate variants of zigzag by rolling? Or just one.
    # Let's do one main ZigZag and some shifted versions of it for diversity?
    # Simpler: just 1 ZigZag, and give more slots to Noisy LPT or Random.
    # Let's follow recommendation: 32 "Folded LPT".
    # Implementation: Just using different "folding" strides or offsets?
    # Or just 1 Folded, and fill the rest with Random.
    # Recommendation said "32 Folded LPT". We can simulate diversity by permuting LPT blocks.
    # For now, let's just use 1 ZigZag, and 31 "Block Shuffled LPT".

    # Actually, let's stick to the high-level recommendation:
    # 64 Noisy, 32 Random, 32 Folded-ish.
    # To generate 32 Folded candidates, we can apply the ZigZag perm logic to Noisy LPT sorts?
    # Or just use 1 ZigZag + 31 Random.
    # Let's implement: 1 LPT, 1 ZigZag, 62 Noisy LPT, 64 Random Shuffle.

    # ZigZag Indices
    c_zigzag = lpt_idx.gather(1, zigzag_perm.unsqueeze(0).expand(num_layers, -1)).unsqueeze(1)

    # Random Shuffles (64)
    # Just random permutations
    rand_keys = torch.randn(num_layers, 64, num_items, device=device)
    _, c_random = rand_keys.sort(dim=-1)

    # Noisy LPT (62)
    # Weights * Noise
    noise = torch.rand(num_layers, 62, num_items, device=device) * 0.4 + 0.8
    noisy_w = weight.unsqueeze(1) * noise
    _, c_noisy = noisy_w.sort(dim=-1, descending=True)

    all_indices = torch.cat([c_lpt, c_zigzag, c_random, c_noisy], dim=1) # 1+1+64+62 = 128

    # Gather weights for kernel
    expanded_weight = weight.unsqueeze(1).expand(-1, num_candidates, -1)
    ordered_weights = expanded_weight.gather(2, all_indices)

    # Flatten
    flat_weights = ordered_weights.view(-1, num_items)

    # 2. Greedy Packing (Batched)
    flat_ids, flat_ranks, flat_loads = _vectorized_greedy_packing(flat_weights, num_packs, capacity)

    # 3. Top-K Selection
    # Calculate imbalance
    loads = flat_loads.view(num_layers, num_candidates, num_packs)
    imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values # [L, C]

    # Select Top 8 candidates per layer
    k = 8
    best_vals, best_k_indices = imbalance.topk(k, dim=1, largest=False) # [L, K]

    # Extract data for these Top K
    # We need to flatten properly.
    # Global index in flat_weights = layer_idx * 128 + candidate_idx
    layer_offsets = (torch.arange(num_layers, device=device) * num_candidates).unsqueeze(1) # [L, 1]
    global_indices = (layer_offsets + best_k_indices).view(-1) # [L*K]

    refined_weights = flat_weights[global_indices] # [L*K, N] (This is sorted/permuted weights)
    # BUT, refine_packing requires weights to match pack_ids column-wise.
    # flat_ids is [L*C, N] aligned with flat_weights.
    # So refined_weights and refined_ids are aligned.
    refined_ids = flat_ids[global_indices]
    refined_ranks = flat_ranks[global_indices]
    refined_loads = flat_loads[global_indices]

    # 4. Refinement on Top K
    # Run iterative 1-item swap
    refined_ids, refined_ranks, refined_loads = _refine_packing(
        refined_weights, refined_ids, refined_loads, refined_ranks, num_iters=20
    )

    # 5. Final Selection
    # Re-calc imbalance
    final_imbalance = refined_loads.max(dim=1).values - refined_loads.min(dim=1).values # [L*K]
    final_imbalance = final_imbalance.view(num_layers, k)

    best_in_k = final_imbalance.argmin(dim=1) # [L]

    # 6. Gather and Scatter Back
    # We need to reconstruct the mapping to original items.
    # The refined_ids are aligned to the candidate's permutation.
    # We need the candidate's permutation indices.

    # Get the original permutation index for the winner
    # best_k_indices[l, best_in_k[l]] gives the candidate index c within 0..127
    winner_cand_idx = best_k_indices.gather(1, best_in_k.unsqueeze(1)).squeeze(1) # [L]

    # Get indices [L, N] from all_indices [L, 128, N]
    idx_view = winner_cand_idx.view(num_layers, 1, 1).expand(-1, 1, num_items)
    final_sorted_idx = all_indices.gather(1, idx_view).squeeze(1) # [L, N]

    # Get the refined pack IDs and ranks for the winner
    # We have refined_ids [L*K, N]. The winner is at index (l*k + best_in_k[l])
    winner_flat_idx = (torch.arange(num_layers, device=device) * k) + best_in_k
    final_aligned_ids = refined_ids[winner_flat_idx] # [L, N]
    final_aligned_ranks = refined_ranks[winner_flat_idx] # [L, N]

    # Scatter back
    pack_index = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    rank_in_pack = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)

    pack_index.scatter_(1, final_sorted_idx, final_aligned_ids)
    rank_in_pack.scatter_(1, final_sorted_idx, final_aligned_ranks)

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Hybrid Ensemble Strategy with Top-K Refinement.
    """
    num_layers, num_items = weight.shape
    device = weight.device
    capacity = num_items // num_packs

    # 1. Candidate Generation
    # Total candidates: 128
    # Distribution:
    # - 1 LPT
    # - 1 ZigZag
    # - 32 Random Shuffle
    # - 32 Tail Shuffle (Fix largest M items, shuffle rest)
    # - 62 Noisy LPT

    num_candidates = 128

    # A. LPT
    lpt_val, lpt_idx = weight.sort(dim=-1, descending=True)
    c_lpt = lpt_idx.unsqueeze(1) # [L, 1, N]

    # B. ZigZag
    zigzag_perm = torch.empty(num_items, device=device, dtype=torch.long)
    half = (num_items + 1) // 2
    arange = torch.arange(num_items, device=device)
    zigzag_perm[0::2] = arange[:half]
    zigzag_perm[1::2] = arange[half:].flip(0)
    c_zigzag = lpt_idx.gather(1, zigzag_perm.unsqueeze(0).expand(num_layers, -1)).unsqueeze(1)

    # C. Random Shuffles (32)
    rand_keys = torch.randn(num_layers, 32, num_items, device=device)
    _, c_random = rand_keys.sort(dim=-1)

    # D. Tail Shuffle (32)
    # Keep top 'num_packs' items fixed (LPT order), shuffle the rest.
    # Generate random keys
    tail_rand = torch.randn(num_layers, 32, num_items, device=device)
    # Force top items to have high values (so they sort first)
    # LPT indices are 0..N-1.
    # We want indices 0..num_packs-1 to be first.
    # We can just construct the permutation explicitly.
    # Take LPT indices.
    # For each candidate, keep first num_packs same as LPT.
    # Shuffle the rest.

    # Base LPT expanded: [L, 32, N]
    base_lpt = lpt_idx.unsqueeze(1).expand(-1, 32, -1)

    # Create a mask or simply