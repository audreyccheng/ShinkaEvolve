--- a/original.py
+++ b/original.py
@@ -1,360 +1,383 @@
 # EVOLVE-BLOCK-START
 """
 Expert parallelism load balancer (EPLB) for vLLM.
 
 This module implements the core rearrangement algorithm.
 
 The rearrangement algorithm is adapted from
 [DeepSeek EPLB](https://github.com/deepseek-ai/eplb).
 
 Please find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example
 on how the EPLB algorithm works.
 """
 
 import torch
-import math
 
 def balanced_packing(weight: torch.Tensor,
                      num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
     """
     Pack n weighted objects to m packs, such that each bin contains exactly
     n/m objects and the weights of all packs are as balanced as possible.
-
-    Uses a Vectorized ZigZag initialization followed by a GPU-accelerated
-    Swap-based local search refinement.
+    
+    Uses ZigZag initialization followed by a vectorized Max-Min Swap local search.
 
     Parameters:
-        weight: [X, n], the weight of each item
+        weight: [layers, n], the weight of each item
         num_packs: number of packs
 
     Returns:
-        pack_index: [X, n], the pack index of each item
-        rank_in_pack: [X, n], the rank of the item in the pack
+        pack_index: [layers, n], the pack index of each item
+        rank_in_pack: [layers, n], the rank of the item in the pack
     """
     num_layers, num_groups = weight.shape
     device = weight.device
     assert num_groups % num_packs == 0
     groups_per_pack = num_groups // num_packs
 
+    # Trivial case
     if groups_per_pack == 1:
-        pack_index = torch.arange(num_groups, dtype=torch.int64, device=device).expand(num_layers, num_groups)
+        pack_index = torch.arange(num_packs, dtype=torch.int64, device=device).expand(num_layers, -1)
         rank_in_pack = torch.zeros_like(pack_index)
         return pack_index, rank_in_pack
 
-    # 1. Sort weights descending [L, N]
+    # 1. Sort weights descending: [L, N]
     sorted_weight, sorted_indices = weight.sort(dim=-1, descending=True)
 
-    # 2. Zigzag initialization
+    # 2. ZigZag Initialization
     # Pattern: 0, 1, ..., m-1, m-1, ..., 0
+    # This distributes large items across packs evenly.
     pattern = torch.cat([
-        torch.arange(num_packs, dtype=torch.int64, device=device),
-        torch.arange(num_packs - 1, -1, -1, dtype=torch.int64, device=device)
+        torch.arange(num_packs, device=device),
+        torch.arange(num_packs - 1, -1, -1, device=device)
     ])
     num_patterns = (num_groups + len(pattern) - 1) // len(pattern)
-    pack_assignments = pattern.repeat(num_patterns)[:num_groups]
-
-    # sorted_pack_index: [L, N]
-    sorted_pack_index = pack_assignments.view(1, -1).expand(num_layers, -1).clone()
-
-    # 3. Vectorized Swap Optimization
+    assignments = pattern.repeat(num_patterns)[:num_groups] # [N]
+
+    # Convert assignments to pack_contents: [M, K]
+    # This tells us which sorted-index is in which pack.
+    # We sort the assignments to group by pack id.
+    # Since ZigZag is deterministic, we can compute the indices once.
+    # We want indices such that sorted_weight[indices] gives the items in each pack.
+    # To get that, we just need the indices 0..N grouped by their assignment.
+    # Stable sort of assignments gives us the permutation of 0..N that groups by pack.
+    _, perm = assignments.sort(stable=True)
+    
+    # pack_contents_indices: [M, K] contains indices into sorted_weight dim 1
+    pack_contents_indices = perm.view(num_packs, groups_per_pack)
+    
+    # Expand to all layers: [L, M, K]
+    pack_contents = pack_contents_indices.unsqueeze(0).expand(num_layers, -1, -1).clone()
+
+    # 3. Vectorized Local Search
+    # Iteratively swap items between the heaviest (Max) and lightest (Min) packs.
+    # Maximize K (groups_per_pack) is usually small (e.g., 4-32). M (num_packs) is small (e.g., 8).
+    # Operations are mostly on small matrices [L, K, K].
+    
     num_iters = 20
+    layer_arange = torch.arange(num_layers, device=device)
 
     for _ in range(num_iters):
-        # Compute pack sums [L, M]
-        pack_sums = torch.zeros(num_layers, num_packs, device=device, dtype=weight.dtype)
-        pack_sums.scatter_add_(1, sorted_pack_index, sorted_weight)
-
-        # Find max and min packs
-        max_val, max_pack = pack_sums.max(dim=1) # [L]
-        min_val, min_pack = pack_sums.min(dim=1) # [L]
-
-        # Identify items in max/min packs
-        # sorted_pack_index is [L, N]. max_pack is [L].
-        is_max = (sorted_pack_index == max_pack.unsqueeze(1))
-        is_min = (sorted_pack_index == min_pack.unsqueeze(1))
-
-        # Extract indices. Each row has exactly groups_per_pack items.
-        # indices_* shape: [L, K]
-        indices_max = torch.nonzero(is_max, as_tuple=True)[1].view(num_layers, groups_per_pack)
-        indices_min = torch.nonzero(is_min, as_tuple=True)[1].view(num_layers, groups_per_pack)
-
-        # Gather weights: [L, K]
+        # Gather weights: [L, M, K]
+        # Flatten for gather: [L, M*K]
+        flat_contents = pack_contents.view(num_layers, -1)
+        current_weights = torch.gather(sorted_weight, 1, flat_contents).view(num_layers, num_packs, groups_per_pack)
+        
+        # Compute sums: [L, M]
+        pack_sums = current_weights.sum(dim=2)
+        
+        # Find Max and Min packs: [L]
+        val_max, idx_max_pack = pack_sums.max(dim=1)
+        val_min, idx_min_pack = pack_sums.min(dim=1)
+        
+        gap = val_max - val_min # [L]
+        
+        # If gap is small, we might break early? (Optional, but vectorization makes it hard to break per-layer)
+        
+        # Get weights in Max and Min packs: [L, K]
+        # We need to gather from current_weights using the pack indices
+        # idx_max_pack: [L] -> expand to [L, 1, K] for gather is complex on [L, M, K]
+        # Easier to gather from the sorted_weight using the indices in pack_contents
+        
+        # Get indices: [L, K]
+        # pack_contents: [L, M, K]
+        indices_max = pack_contents[layer_arange, idx_max_pack, :] # [L, K]
+        indices_min = pack_contents[layer_arange, idx_min_pack, :] # [L, K]
+        
+        # Get weights: [L, K]
         w_max = torch.gather(sorted_weight, 1, indices_max)
         w_min = torch.gather(sorted_weight, 1, indices_min)
-
-        # Compute pairwise diffs: [L, K, K] (max_item - min_item)
+        
+        # Compute diff matrix: [L, K, K]
+        # D[l, i, j] = w_max[l, i] - w_min[l, j]
         diffs = w_max.unsqueeze(2) - w_min.unsqueeze(1)
-
-        # Target diff to equalize sums: (Max - Min) / 2
-        current_diff = (max_val - min_val).view(num_layers, 1, 1)
-        target = current_diff / 2.0
-
-        # Validity mask:
-        # 1. diff > 0 (must reduce max)
-        # 2. diff < current_diff (must not overshoot and make min > old max)
-        valid = (diffs > 0) & (diffs < current_diff)
-
-        # Error from target
-        error = (diffs - target).abs()
-        # Set invalid to inf
-        error = torch.where(valid, error, torch.tensor(float('inf'), device=device))
-
-        # Find best swap
-        # Flatten [L, K*K]
-        min_err, flat_idx = error.view(num_layers, -1).min(dim=1)
-
-        # Check if valid swap exists
-        found = min_err < float('inf')
-        if not found.any():
+        
+        # We want to swap if it reduces the max load and doesn't make min load > old max load.
+        # Ideally, we want to minimize abs((SumMax - diff) - (SumMin + diff)) = abs(Gap - 2*diff)
+        # Constraint: diff > 0 (to reduce Max)
+        # Constraint: SumMax - diff >= SumMin + diff => 2*diff <= Gap (technically we can go slightly beyond, but safely 2*diff < Gap is good)
+        # Actually, best swap minimizes max(SumMax - diff, SumMin + diff).
+        # This is equivalent to minimizing abs(Gap - 2*diff) if diff < Gap.
+        
+        cost = (gap.view(num_layers, 1, 1) - 2 * diffs).abs()
+        
+        # Mask invalid swaps:
+        # We only want swaps where diff > 0 (reduce Max).
+        # Also, strictly speaking, we shouldn't overshoot too much, but minimizing cost usually handles it.
+        # Just ensure diff > 0.
+        mask = diffs > 0
+        cost = torch.where(mask, cost, torch.tensor(float('inf'), device=device))
+        
+        # Find best swap per layer
+        # flatten last 2 dims: [L, K*K]
+        flat_cost = cost.view(num_layers, -1)
+        min_cost, flat_idx = flat_cost.min(dim=1)
+        
+        # Determine which layers have a valid swap
+        valid_swap = min_cost < float('inf')
+        # If no layer has a valid swap, we could break, but let's just continue (no-op for invalid)
+        
+        if not valid_swap.any():
             break
-
+            
         # Decode indices
-        row_idx = flat_idx // groups_per_pack
-        col_idx = flat_idx % groups_per_pack
-
+        # idx_in_max_pack (0..K-1), idx_in_min_pack (0..K-1)
+        idx_i = flat_idx // groups_per_pack
+        idx_j = flat_idx % groups_per_pack
+        
         # Apply swaps for valid layers
-        l_idx = torch.nonzero(found).squeeze(1)
-
-        # Get item indices in sorted_pack_index
-        idx_max = indices_max[l_idx, row_idx[l_idx]]
-        idx_min = indices_min[l_idx, col_idx[l_idx]]
-
-        # Swap packs
-        p_max = max_pack[l_idx]
-        p_min = min_pack[l_idx]
-
-        sorted_pack_index[l_idx, idx_max] = p_min
-        sorted_pack_index[l_idx, idx_min] = p_max
+        # We need to swap elements in pack_contents
+        # pack_contents[l, max_p, i] <-> pack_contents[l, min_p, j]
+        
+        l_indices_valid = torch.nonzero(valid_swap).squeeze(1)
+        
+        if len(l_indices_valid) > 0:
+            p_max = idx_max_pack[l_indices_valid]
+            p_min = idx_min_pack[l_indices_valid]
+            i_idx = idx_i[l_indices_valid]
+            j_idx = idx_j[l_indices_valid]
+            
+            # Values to swap (indices into sorted_weight)
+            val_i = pack_contents[l_indices_valid, p_max, i_idx]
+            val_j = pack_contents[l_indices_valid, p_min, j_idx]
+            
+            # Perform swap
+            pack_contents[l_indices_valid, p_max, i_idx] = val_j
+            pack_contents[l_indices_valid, p_min, j_idx] = val_i
 
     # 4. Construct outputs
-    # pack_index: map back to original indices
-    pack_index = torch.empty_like(sorted_pack_index)
-    pack_index.scatter_(1, sorted_indices, sorted_pack_index)
-
-    # rank_in_pack: ensure 0..k-1 per pack
-    # Sort sorted_pack_index to group items by pack
-    # Stable sort preserves relative order (weight order)
-    _, sort_idx = sorted_pack_index.sort(dim=1, stable=True)
-
-    # Assign ranks 0..k-1 repeated
-    ranks = torch.arange(groups_per_pack, device=device).repeat(num_packs)
-    ranks = ranks.view(1, -1).expand(num_layers, -1)
-
-    # Map ranks back to sorted items
-    temp_ranks = torch.empty_like(ranks)
-    temp_ranks.scatter_(1, sort_idx, ranks)
-
-    # Map back to original items
-    rank_in_pack = torch.empty_like(pack_index)
-    rank_in_pack.scatter_(1, sorted_indices, temp_ranks)
+    # pack_contents [L, M, K] contains indices into sorted_weight.
+    # We need to map back to original indices.
+    
+    # Flatten pack_contents to [L, N] (but strictly speaking order is M blocks of K)
+    # The value at pack_contents[l, m, k] is the index 's' in sorted_weight.
+    # The original item index is sorted_indices[l, s].
+    # That item belongs to pack 'm'.
+    
+    # Create pack_ids tensor corresponding to pack_contents structure
+    # [L, M, K] filled with m
+    pack_ids = torch.arange(num_packs, device=device).view(1, num_packs, 1).expand(num_layers, -1, groups_per_pack)
+    
+    # Create rank_ids tensor
+    # [L, M, K] filled with k
+    rank_ids = torch.arange(groups_per_pack, device=device).view(1, 1, groups_per_pack).expand(num_layers, num_packs, -1)
+    
+    # Flatten everything
+    flat_sorted_idx_ptr = pack_contents.view(num_layers, -1) # [L, N] pointers to sorted array columns
+    flat_pack_ids = pack_ids.reshape(num_layers, -1) # [L, N]
+    flat_rank_ids = rank_ids.reshape(num_layers, -1) # [L, N]
+    
+    # Resolve to original item indices
+    # original_idx[l, i] = sorted_indices[l, flat_sorted_idx_ptr[l, i]]
+    original_idx = torch.gather(sorted_indices, 1, flat_sorted_idx_ptr)
+    
+    # Now scatter pack_ids and rank_ids to their original positions
+    pack_index = torch.empty_like(flat_pack_ids)
+    rank_in_pack = torch.empty_like(flat_rank_ids)
+    
+    pack_index.scatter_(1, original_idx, flat_pack_ids)
+    rank_in_pack.scatter_(1, original_idx, flat_rank_ids)
 
     return pack_index, rank_in_pack
 
 
 def replicate_experts(
         weight: torch.Tensor,
         num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Replicate `num_log` experts to `num_phy` replicas, such that the maximum
     load of all replicas is minimized.
 
     Parameters:
         weight: [X, num_log]
         num_phy: total number of experts after replication
 
     Returns:
         phy2log: [X, num_phy], logical expert id of each physical expert
         rank: [X, num_phy], the replica rank
         logcnt: [X, num_log], number of replicas for each logical expert
     """
     n, num_log = weight.shape
     num_redundant = num_phy - num_log
     assert num_redundant >= 0
     device = weight.device
-
+    
     # Initialize with 1 replica per expert
     phy2log = torch.arange(num_phy, dtype=torch.int64, device=device).repeat(n, 1)
     rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)
     logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)
     arangen = torch.arange(n, dtype=torch.int64, device=device)
-
+    
     # Greedily add replicas to the expert with highest current load per replica
-    # This loop runs K times where K is number of redundant slots.
-    # For common MoE configs, K is comparable to num_log.
+    # We execute this sequentially because the greedy choice depends on the previous step.
+    # However, for GPU, the operations inside the loop are vectorized over the batch dimension 'n'.
+    
     for i in range(num_log, num_phy):
-        # Find which expert has the max load per replica
-        # metric = weight / count
-        redundant_indices = (weight / logcnt).max(dim=-1).indices
-
+        # Metric: current load per replica = weight / count
+        # Find expert with max metric
+        metrics = weight / logcnt
+        redundant_indices = metrics.max(dim=-1).indices # [N]
+        
         phy2log[:, i] = redundant_indices
         rank[:, i] = logcnt[arangen, redundant_indices]
         logcnt[arangen, redundant_indices] += 1
-
+        
     return phy2log, rank, logcnt
 
 
 def rebalance_experts_hierarchical(
     weight: torch.Tensor,
     num_physical_experts: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ):
     """
     Parameters:
         weight: [num_moe_layers, num_logical_experts]
         num_physical_experts: number of physical experts after replication
         num_groups: number of expert groups
-        num_nodes: number of server nodes, where the intra-node network
-        (e.g, NVLink) is faster
-        num_gpus: number of GPUs, must be a multiple of `num_nodes`
+        num_nodes: number of server nodes
+        num_gpus: number of GPUs
 
     Returns:
-        physical_to_logical_map: [num_moe_layers, num_physical_experts]
-        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]
-        logical_count: [num_moe_layers, num_logical_experts]
+        physical_to_logical_map, logical_to_physical_map, logical_count
     """
     num_layers, num_logical_experts = weight.shape
     assert num_logical_experts % num_groups == 0
     group_size = num_logical_experts // num_groups
     assert num_groups % num_nodes == 0
     groups_per_node = num_groups // num_nodes
     assert num_gpus % num_nodes == 0
     assert num_physical_experts % num_gpus == 0
     phy_experts_per_gpu = num_physical_experts // num_gpus
 
     def inverse(perm: torch.Tensor) -> torch.Tensor:
         inv = torch.empty_like(perm)
         inv.scatter_(
             1,
             perm,
             torch.arange(perm.size(1), dtype=torch.int64,
                          device=perm.device).expand(perm.shape),
         )
         return inv
 
     # Step 1: pack groups to nodes
-    # Sum weights within each group
     tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)
-
-    # Use improved packing
+    
     group_pack_index, group_rank_in_pack = balanced_packing(
         tokens_per_group, num_nodes)
-
+        
     log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *
                  group_size).unsqueeze(-1) +
                 torch.arange(group_size,
                              dtype=torch.int64,
                              device=group_pack_index.device)).flatten(-2)
     mlog2log = inverse(log2mlog)
 
     # Step 2: construct redundant experts within nodes
     # [num_layers * num_nodes, num_logical_experts // num_nodes]
     tokens_per_mlog = weight.gather(-1, mlog2log).view(
         -1, num_logical_experts // num_nodes)
-
+        
     phy2mlog, phyrank, mlogcnt = replicate_experts(
         tokens_per_mlog, num_physical_experts // num_nodes)
 
     # Step 3: pack physical_experts to GPUs
     # [num_layers * num_nodes, num_physical_experts // num_nodes]
-    # Each physical expert has weight approx (total_weight / num_replicas)
     tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
-
-    # Use improved packing
+    
     pack_index, rank_in_pack = balanced_packing(tokens_per_phy,
                                                 num_gpus // num_nodes)
-
+                                                
     phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
     pphy2phy = inverse(phy2pphy)
 
     pphy2mlog = phy2mlog.gather(
         -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]
     pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(
         0,
         num_logical_experts,
         num_logical_experts // num_nodes,
         device=group_pack_index.device,
     ).view(1, -1, 1)).flatten(-2)
     pphy2log = mlog2log.gather(-1, pphy2mlog)
     pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)
     logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)
     return pphy2log, pphyrank, logcnt
 
 
 def rebalance_experts(
     weight: torch.Tensor,
     num_replicas: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Entry point for expert-parallelism load balancer.
-
-    Parameters:
-        weight: [layers, num_logical_experts], the load statistics for all
-            logical experts
-        num_replicas: number of physical experts, must be a multiple of
-            `num_gpus`
-        num_groups: number of expert groups
-        num_nodes: number of server nodes, where the intra-node network
-            (e.g, NVLink) is faster
-        num_gpus: number of GPUs, must be a multiple of `num_nodes`
-
-    Returns:
-        physical_to_logical_map: [layers, num_replicas], the expert index of
-            each replica
-        logical_to_physical_map: [layers, num_logical_experts, X], the replica
-            indices for each expert
-        expert_count: [layers, num_logical_experts], number of physical
-            replicas for each logical expert
     """
     num_layers, num_logical_experts = weight.shape
-    weight = weight.float().cpu()
-
+    # NOTE: We keep weight on its original device to allow GPU acceleration.
+    weight = weight.float()
+    
     if num_groups % num_nodes == 0:
         # use hierarchical load-balance policy
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, num_groups, num_nodes, num_gpus)
     else:
         # use global load-balance policy
-        # Treating as if 1 huge group per layer, so packing step 1 is trivial
-        # But here logic passes num_groups=1, so group_size=all experts.
-        # Step 1 packs 1 item to 1 node? No, step 1 uses num_nodes=1.
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, 1, 1, num_gpus)
-
+            
     num_redundant_experts = num_replicas - num_logical_experts
     maxlogcnt = num_redundant_experts + 1
+    
     log2phy: torch.Tensor = torch.full(
         (num_layers, num_logical_experts, maxlogcnt),
         -1,
         dtype=torch.int64,
         device=logcnt.device,
     )
-
-    # Create the reverse map
-    # phy2log * maxlogcnt + phyrank gives a unique index for (expert, replica_id)
-    # We scatter the physical index (0..num_replicas) into this location
+    
+    # Scatter to create the reverse map
     log2phy.view(num_layers, -1).scatter_(
         -1,
         phy2log * maxlogcnt + phyrank,
         torch.arange(num_replicas, dtype=torch.int64,
                      device=log2phy.device).expand(num_layers, -1),
     )
     return phy2log, log2phy, logcnt
 # EVOLVE-BLOCK-END
 
 
 # This part remains fixed (not evolved)
 def run_eplb(weight: torch.Tensor, num_replicas: int, num_groups: int,
              num_nodes: int, num_gpus: int):
     """Run the expert parallelism load balancer"""
     phy2log, log2phy, logcnt = rebalance_experts(
         weight, num_replicas, num_groups, num_nodes, num_gpus
     )
     return phy2log, log2phy, logcnt
 
 
 __all__ = ["rebalance_experts", "run_eplb"]
