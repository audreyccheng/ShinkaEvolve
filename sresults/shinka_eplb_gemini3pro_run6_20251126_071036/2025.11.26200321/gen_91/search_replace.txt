<NAME>
improved_refinement_and_selection
</NAME>

<DESCRIPTION>
1. Replaces the Max-Min swap refinement with a more powerful Max-Any swap strategy (`_refine_packing`). This allows swapping items from the heaviest pack to *any* other pack to reduce the maximum load, breaking out of local optima where the lightest pack cannot accept large items.
2. Updates `balanced_packing` to use a larger ensemble (256 candidates) with more Noisy LPT candidates for diversity.
3. Improves the candidate selection logic in `balanced_packing` to use a hierarchical strategy: first selecting the top candidates based on Max Load (primary objective), and then breaking ties using L2 Norm (load variance) to prefer smoother distributions.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def _refine_packing(weights: torch.Tensor,
                    pack_ids: torch.Tensor,
                    pack_loads: torch.Tensor,
                    ranks: torch.Tensor,
                    num_iters: int = 10) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Refines the packing by iteratively attempting to swap a single item between
    the heaviest and lightest packs to reduce imbalance.
    """
    batch_size, num_items = weights.shape
    device = weights.device
    batch_indices = torch.arange(batch_size, device=device)

    # Pre-compute constant memory for diff expansion if possible, but B*N*N might be too big for 128 candidates.
    # Since we prune to Top-K (e.g. 8), B is small (Layers * 8).
    # If Layers=32, B=256. N=256. 256^3 * 4 bytes = 64MB. Very safe.

    for _ in range(num_iters):
        # Identify heaviest and lightest packs
        max_load, max_pack_idx = pack_loads.max(dim=1)
        min_load, min_pack_idx = pack_loads.min(dim=1)
        current_diff = max_load - min_load

        # Stop if imbalance is negligible
        if current_diff.max() < 1e-5:
            break

        # Masks for items in max/min packs
        is_in_max = (pack_ids == max_pack_idx.unsqueeze(1))
        is_in_min = (pack_ids == min_pack_idx.unsqueeze(1))

        # Weight diffs: w[i] - w[j]
        # [B, N, 1] - [B, 1, N] = [B, N, N]
        w_diff = weights.unsqueeze(2) - weights.unsqueeze(1)

        # Metric: |(diff) - 2*(w_i - w_j)|
        target = current_diff.view(-1, 1, 1)
        new_diff_metric = torch.abs(target - 2 * w_diff)

        # Valid mask: i in Max, j in Min
        valid_swap = is_in_max.unsqueeze(2) & is_in_min.unsqueeze(1)

        # Improvement check: New diff < Current diff
        # We mask invalid swaps with infinity
        # We also want strict improvement to avoid cycling
        # (though abs metric prevents cycling usually, float precision issues can cause flip-flopping)
        new_diff_metric = torch.where(valid_swap, new_diff_metric, torch.tensor(float('inf'), device=device))

        # Find best swap per batch
        # flatten last two dims
        flat_metric = new_diff_metric.view(batch_size, -1)
        min_val, flat_idx = flat_metric.min(dim=1)

        # Check improvement
        improve_mask = min_val < (current_diff - 1e-6)

        if not improve_mask.any():
            break

        # Apply swaps
        active_batch = batch_indices[improve_mask]
        active_idx = flat_idx[improve_mask]

        i_idx = active_idx // num_items
        j_idx = active_idx % num_items

        p_max = max_pack_idx[active_batch]
        p_min = min_pack_idx[active_batch]

        w_i = weights[active_batch, i_idx]
        w_j = weights[active_batch, j_idx]
        delta = w_i - w_j

        # Update loads
        pack_loads[active_batch, p_max] -= delta
        pack_loads[active_batch, p_min] += delta

        # Update IDs
        pack_ids[active_batch, i_idx] = p_min
        pack_ids[active_batch, j_idx] = p_max

        # Update Ranks
        # Swap ranks to maintain valid rank set
        r_i = ranks[active_batch, i_idx]
        r_j = ranks[active_batch, j_idx]
        ranks[active_batch, i_idx] = r_j
        ranks[active_batch, j_idx] = r_i

    return pack_ids, ranks, pack_loads


def _vectorized_greedy_packing(weights: torch.Tensor,
                               num_packs: int,
                               capacity: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
=======
def _refine_packing(weights: torch.Tensor,
                    pack_ids: torch.Tensor,
                    pack_loads: torch.Tensor,
                    ranks: torch.Tensor,
                    num_iters: int = 20) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Refines packing by attempting pairwise swaps between the heaviest pack and
    ANY other pack to strictly reduce the maximum load.
    """
    batch_size, num_items = weights.shape
    device = weights.device
    num_packs = pack_loads.shape[1]

    # Precompute pairwise weight differences: [B, N, N]
    # w_diff[b, i, j] = w[b, i] - w[b, j]
    w_diff = weights.unsqueeze(2) - weights.unsqueeze(1)

    batch_range = torch.arange(batch_size, device=device)

    for _ in range(num_iters):
        # Identify Max Pack
        max_vals, max_indices = pack_loads.max(dim=1) # [B]

        # Track best swap per batch
        best_gain = torch.full((batch_size,), -1.0, device=device, dtype=weights.dtype)
        best_flat_idx = torch.zeros(batch_size, dtype=torch.long, device=device)
        best_target_p = torch.full((batch_size,), -1, dtype=torch.long, device=device)

        # Mask for items in Max Pack [B, N]
        mask_max = (pack_ids == max_indices.unsqueeze(1))

        # Iterate over all possible target packs
        for p in range(num_packs):
            # Optimization: Skip if no batch has max_pack != p
            # (Basically we iterate p from 0..M-1. If for some batch Max==p, we skip for that batch)
            valid_batch = (max_indices != p)
            if not valid_batch.any():
                continue

            target_vals = pack_loads[:, p]
            load_diff = max_vals - target_vals

            # Gain = min(delta, gap - delta)
            # gap = Max - Target
            # delta = w_i - w_j

            delta = w_diff

            # Mask Target Pack [B, N]
            mask_target = (pack_ids == p)

            # Valid Swap: i in Max, j in Target
            valid_swap = mask_max.unsqueeze(2) & mask_target.unsqueeze(1)
            valid_swap &= valid_batch.view(-1, 1, 1)

            gap = load_diff.view(-1, 1, 1)
            gain = torch.min(delta, gap - delta)

            # Filter for positive gain
            valid_swap &= (gain > 1e-5)

            gain = torch.where(valid_swap, gain, torch.tensor(-1.0, device=device, dtype=weights.dtype))

            # Max gain for this target pack p
            flat_gain = gain.view(batch_size, -1)
            p_max_gain, p_max_idx = flat_gain.max(dim=1)

            # Update best across packs
            improve = p_max_gain > best_gain
            if improve.any():
                best_gain = torch.where(improve, p_max_gain, best_gain)
                best_flat_idx = torch.where(improve, p_max_idx, best_flat_idx)
                best_target_p = torch.where(improve, torch.tensor(p, device=device), best_target_p)

        # Apply best swap found
        active = best_gain > 1e-5
        if not active.any():
            break

        active_batch = batch_range[active]
        active_flat = best_flat_idx[active]
        p_target = best_target_p[active]
        p_max = max_indices[active]

        i_idx = active_flat // num_items
        j_idx = active_flat % num_items

        w_i = weights[active_batch, i_idx]
        w_j = weights[active_batch, j_idx]
        delta_val = w_i - w_j

        pack_loads[active_batch, p_max] -= delta_val
        pack_loads[active_batch, p_target] += delta_val

        pack_ids[active_batch, i_idx] = p_target
        pack_ids[active_batch, j_idx] = p_max

        r_i = ranks[active_batch, i_idx]
        r_j = ranks[active_batch, j_idx]
        ranks[active_batch, i_idx] = r_j
        ranks[active_batch, j_idx] = r_i

    return pack_ids, ranks, pack_loads


def _vectorized_greedy_packing(weights: torch.Tensor,
                               num_packs: int,
                               capacity: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
>>>>>>> REPLACE
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Hybrid Ensemble Strategy with Top-K Refinement.
    """
    num_layers, num_items = weight.shape
    device = weight.device
    capacity = num_items // num_packs

    # 1. Candidate Generation
    # Total candidates: 128
    # Distribution:
    # - 1 Pure LPT
    # - 31 Folded/ZigZag variants (shifted/permuted)
    # - 32 Random Shuffles
    # - 64 Noisy LPT

    num_candidates = 128

    # A. LPT
    lpt_val, lpt_idx = weight.sort(dim=-1, descending=True)
    c_lpt = lpt_idx.unsqueeze(1) # [L, 1, N]

    # B. Folded / ZigZag
    # Create base zigzag permutation
    zigzag_perm = torch.empty(num_items, device=device, dtype=torch.long)
    half = (num_items + 1) // 2
    arange = torch.arange(num_items, device=device)
    zigzag_perm[0::2] = arange[:half]
    zigzag_perm[1::2] = arange[half:].flip(0)

    # Generate variants of zigzag by rolling? Or just one.
    # Let's do one main ZigZag and some shifted versions of it for diversity?
    # Simpler: just 1 ZigZag, and give more slots to Noisy LPT or Random.
    # Let's follow recommendation: 32 "Folded LPT".
    # Implementation: Just using different "folding" strides or offsets?
    # Or just 1 Folded, and fill the rest with Random.
    # Recommendation said "32 Folded LPT". We can simulate diversity by permuting LPT blocks.
    # For now, let's just use 1 ZigZag, and 31 "Block Shuffled LPT".

    # Actually, let's stick to the high-level recommendation:
    # 64 Noisy, 32 Random, 32 Folded-ish.
    # To generate 32 Folded candidates, we can apply the ZigZag perm logic to Noisy LPT sorts?
    # Or just use 1 ZigZag + 31 Random.
    # Let's implement: 1 LPT, 1 ZigZag, 62 Noisy LPT, 64 Random Shuffle.

    # ZigZag Indices
    c_zigzag = lpt_idx.gather(1, zigzag_perm.unsqueeze(0).expand(num_layers, -1)).unsqueeze(1)

    # Random Shuffles (64)
    # Just random permutations
    rand_keys = torch.randn(num_layers, 64, num_items, device=device)
    _, c_random = rand_keys.sort(dim=-1)

    # Noisy LPT (62)
    # Weights * Noise
    noise = torch.rand(num_layers, 62, num_items, device=device) * 0.4 + 0.8
    noisy_w = weight.unsqueeze(1) * noise
    _, c_noisy = noisy_w.sort(dim=-1, descending=True)

    all_indices = torch.cat([c_lpt, c_zigzag, c_random, c_noisy], dim=1) # 1+1+64+62 = 128

    # Gather weights for kernel
    expanded_weight = weight.unsqueeze(1).expand(-1, num_candidates, -1)
    ordered_weights = expanded_weight.gather(2, all_indices)

    # Flatten
    flat_weights = ordered_weights.view(-1, num_items)

    # 2. Greedy Packing (Batched)
    flat_ids, flat_ranks, flat_loads = _vectorized_greedy_packing(flat_weights, num_packs, capacity)

    # 3. Top-K Selection
    # Calculate imbalance
    loads = flat_loads.view(num_layers, num_candidates, num_packs)
    imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values # [L, C]

    # Select Top 8 candidates per layer
    k = 8
    best_vals, best_k_indices = imbalance.topk(k, dim=1, largest=False) # [L, K]

    # Extract data for these Top K
    # We need to flatten properly.
    # Global index in flat_weights = layer_idx * 128 + candidate_idx
    layer_offsets = (torch.arange(num_layers, device=device) * num_candidates).unsqueeze(1) # [L, 1]
    global_indices = (layer_offsets + best_k_indices).view(-1) # [L*K]

    refined_weights = flat_weights[global_indices] # [L*K, N] (This is sorted/permuted weights)
    # BUT, refine_packing requires weights to match pack_ids column-wise.
    # flat_ids is [L*C, N] aligned with flat_weights.
    # So refined_weights and refined_ids are aligned.
    refined_ids = flat_ids[global_indices]
    refined_ranks = flat_ranks[global_indices]
    refined_loads = flat_loads[global_indices]

    # 4. Refinement on Top K
    # Run iterative 1-item swap
    refined_ids, refined_ranks, refined_loads = _refine_packing(
        refined_weights, refined_ids, refined_loads, refined_ranks, num_iters=20
    )

    # 5. Final Selection
    # Re-calc imbalance
    final_imbalance = refined_loads.max(dim=1).values - refined_loads.min(dim=1).values # [L*K]
    final_imbalance = final_imbalance.view(num_layers, k)

    best_in_k = final_imbalance.argmin(dim=1) # [L]

    # 6. Gather and Scatter Back
    # We need to reconstruct the mapping to original items.
    # The refined_ids are aligned to the candidate's permutation.
    # We need the candidate's permutation indices.

    # Get the original permutation index for the winner
    # best_k_indices[l, best_in_k[l]] gives the candidate index c within 0..127
    winner_cand_idx = best_k_indices.gather(1, best_in_k.unsqueeze(1)).squeeze(1) # [L]

    # Get indices [L, N] from all_indices [L, 128, N]
    idx_view = winner_cand_idx.view(num_layers, 1, 1).expand(-1, 1, num_items)
    final_sorted_idx = all_indices.gather(1, idx_view).squeeze(1) # [L, N]

    # Get the refined pack IDs and ranks for the winner
    # We have refined_ids [L*K, N]. The winner is at index (l*k + best_in_k[l])
    winner_flat_idx = (torch.arange(num_layers, device=device) * k) + best_in_k
    final_aligned_ids = refined_ids[winner_flat_idx] # [L, N]
    final_aligned_ranks = refined_ranks[winner_flat_idx] # [L, N]

    # Scatter back
    pack_index = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    rank_in_pack = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)

    pack_index.scatter_(1, final_sorted_idx, final_aligned_ids)
    rank_in_pack.scatter_(1, final_sorted_idx, final_aligned_ranks)

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Hybrid Ensemble Strategy with Top-K Refinement.
    """
    num_layers, num_items = weight.shape
    device = weight.device
    capacity = num_items // num_packs

    num_candidates = 256

    # 1. Candidate Generation
    # A. LPT
    lpt_val, lpt_idx = weight.sort(dim=-1, descending=True)
    c_lpt = lpt_idx.unsqueeze(1) # [L, 1, N]

    # B. ZigZag
    zigzag_perm = torch.empty(num_items, device=device, dtype=torch.long)
    half = (num_items + 1) // 2
    arange = torch.arange(num_items, device=device)
    zigzag_perm[0::2] = arange[:half]
    zigzag_perm[1::2] = arange[half:].flip(0)
    c_zigzag = lpt_idx.gather(1, zigzag_perm.unsqueeze(0).expand(num_layers, -1)).unsqueeze(1)

    # C. Random Shuffles
    num_random = 64
    rand_keys = torch.randn(num_layers, num_random, num_items, device=device)
    _, c_random = rand_keys.sort(dim=-1)

    # D. Noisy LPT
    num_noisy = num_candidates - 2 - num_random # 190
    noise = torch.rand(num_layers, num_noisy, num_items, device=device) * 0.5 + 0.75
    noisy_w = weight.unsqueeze(1) * noise
    _, c_noisy = noisy_w.sort(dim=-1, descending=True)

    all_indices = torch.cat([c_lpt, c_zigzag, c_random, c_noisy], dim=1)

    # Gather weights for kernel
    expanded_weight = weight.unsqueeze(1).expand(-1, num_candidates, -1)
    ordered_weights = expanded_weight.gather(2, all_indices)

    # Flatten
    flat_weights = ordered_weights.view(-1, num_items)

    # 2. Greedy Packing (Batched)
    flat_ids, flat_ranks, flat_loads = _vectorized_greedy_packing(flat_weights, num_packs, capacity)

    # 3. Top-K Selection with Hierarchical Scoring
    loads = flat_loads.view(num_layers, num_candidates, num_packs)

    # Metric 1: Max Load (Minimize)
    max_loads = loads.max(dim=-1).values
    # Metric 2: L2 Norm (Minimize, tie-breaker)
    l2_norms = loads.pow(2).sum(dim=-1)

    num_refine = 16

    # Step 3a: Pre-select top 4*K candidates by Max Load
    k_pre = num_refine * 4
    _, top_pre_indices = max_loads.topk(k_pre, dim=1, largest=False) # [L, 4K]

    # Step 3b: Select top K candidates by L2 Norm from the pre-selected set
    l2_subset = l2_norms.gather(1, top_pre_indices) # [L, 4K]
    _, top_k_local = l2_subset.topk(num_refine, dim=1, largest=False) # [L, K]

    best_k_indices = top_pre_indices.gather(1, top_k_local) # [L, K]

    # Extract data for these Top K
    layer_offsets = (torch.arange(num_layers, device=device) * num_candidates).unsqueeze(1)
    global_indices = (layer_offsets + best_k_indices).view(-1)

    refined_weights = flat_weights[global_indices]
    refined_ids = flat_ids[global_indices]
    refined_ranks = flat_ranks[global_indices]
    refined_loads = flat_loads[global_indices]

    # 4. Refinement on Top K (Max-Any Swap)
    refined_ids, refined_ranks, refined_loads = _refine_packing(
        refined_weights, refined_ids, refined_loads, refined_ranks, num_iters=20
    )

    # 5. Final Selection
    # Again using Max Load then L2
    refined_loads_view = refined_loads.view(num_layers, num_refine, num_packs)
    rmax = refined_loads_view.max(dim=-1).values
    rl2 = refined_loads_view.pow(2).sum(dim=-1)

    # Lexicographical min: rmax first, then rl2
    # score = rmax + rl2 * 1e-9 (assuming rmax dominate)
    score = rmax + rl2 * 1e-9
    best_in_k = score.argmin(dim=1) # [L]

    # 6. Gather and Scatter Back
    winner_cand_idx = best_k_indices.gather(1, best_in_k.unsqueeze(1)).squeeze(1) # [L]

    idx_view = winner_cand_idx.view(num_layers, 1, 1).expand(-1, 1, num_items)
    final_sorted_idx = all_indices.gather(1, idx_view).squeeze(1) # [L, N]

    winner_flat_idx = (torch.arange(num_layers, device=device) * num_refine) + best_in_k
    final_aligned_ids = refined_ids[winner_flat_idx]
    final_aligned_ranks = refined_ranks[winner_flat_idx]

    pack_index = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    rank_in_pack = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)

    pack_index.scatter_(1, final_sorted_idx, final_aligned_ids)
    rank_in_pack.scatter_(1, final_sorted_idx, final_aligned_ranks)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>