<NAME>
mixed_offsets_greedy
</NAME>

<DESCRIPTION>
Increase the number of candidates to 128 and introduce "Randomized Bin Capacity Offsets" to the initialization phase. Half of the candidates (64) will use standard randomized LPT, while the other half (64) will initialize pack weights with random offsets. This forces the greedy algorithm to explore diverse packing configurations by avoiding certain bins early on, helping to escape local optima that rely solely on sorting order.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # --- Parallel Candidate Expansion ---
    num_candidates = 64
    num_problems = num_layers * num_candidates

    # [L*C, N]
    w_expanded = weight.repeat_interleave(num_candidates, dim=0)

    # Noise Spectrum: 0.0 to 0.2
    # Candidates 0 in each layer group will have 0 noise (Pure LPT)
    scales = torch.linspace(0, 0.2, num_candidates, device=device)
    scales_expanded = scales.repeat(num_layers).view(-1, 1)

    # Apply noise
    noise = torch.rand_like(w_expanded) * w_expanded * scales_expanded
    sort_keys = w_expanded + noise

    # Sort descending [L*C, N]
    _, sorted_indices = sort_keys.sort(dim=-1, descending=True)
    sorted_weight = torch.gather(w_expanded, 1, sorted_indices)

    # --- Phase 1: Vectorized Greedy LPT ---
    pack_weights = torch.zeros(num_problems, num_packs, device=device, dtype=weight.dtype)
    pack_counts = torch.zeros(num_problems, num_packs, device=device, dtype=torch.int64)
    sorted_pack_index = torch.zeros_like(sorted_indices)

    # We fill items one by one into the emptiest available pack
    # This loop is N iterations, where N is usually small (e.g. 64-256)
    for i in range(num_groups):
        w_item = sorted_weight[:, i:i+1] # [L*C, 1]

        # Mask full packs
        is_full = (pack_counts >= groups_per_pack)
        masked_w = pack_weights.clone()
        masked_w[is_full] = float('inf')

        # Choose pack with min weight
        chosen_pack = masked_w.argmin(dim=1, keepdim=True)

        sorted_pack_index[:, i:i+1] = chosen_pack
        pack_weights.scatter_add_(1, chosen_pack, w_item)
        pack_counts.scatter_add_(1, chosen_pack, torch.ones_like(chosen_pack))
=======
    # --- Parallel Candidate Expansion ---
    num_candidates = 128
    num_problems = num_layers * num_candidates

    # [L*C, N]
    w_expanded = weight.repeat_interleave(num_candidates, dim=0)

    # Noise Spectrum: 0.0 to 0.1
    scales = torch.linspace(0, 0.1, num_candidates, device=device)
    scales_expanded = scales.repeat(num_layers).view(-1, 1)

    # Apply noise
    noise = torch.rand_like(w_expanded) * w_expanded * scales_expanded
    # Ensure Candidate 0 (Pure LPT) has no noise
    noise.view(num_layers, num_candidates, num_groups)[:, 0, :] = 0
    sort_keys = w_expanded + noise

    # Sort descending [L*C, N]
    _, sorted_indices = sort_keys.sort(dim=-1, descending=True)
    sorted_weight = torch.gather(w_expanded, 1, sorted_indices)

    # --- Phase 1: Vectorized Greedy LPT with Offsets ---
    # Randomized Bin Capacity Offsets: Initialize pack_weights with noise
    # for half of the candidates to force diverse greedy decisions.
    pack_weights = torch.zeros(num_problems, num_packs, device=device, dtype=weight.dtype)

    # Offsets for second half of candidates
    avg_weight = weight.mean(dim=1, keepdim=True).repeat_interleave(num_candidates, dim=0)
    offsets = torch.rand(num_problems, num_packs, device=device, dtype=weight.dtype) * avg_weight * 1.5

    # Mask offsets for first half
    cand_ids = torch.arange(num_candidates, device=device).repeat(num_layers)
    offsets[cand_ids < (num_candidates // 2)] = 0

    # Apply offsets
    pack_weights += offsets

    pack_counts = torch.zeros(num_problems, num_packs, device=device, dtype=torch.int64)
    sorted_pack_index = torch.zeros_like(sorted_indices)

    # We fill items one by one into the emptiest available pack
    for i in range(num_groups):
        w_item = sorted_weight[:, i:i+1] # [L*C, 1]

        # Mask full packs
        is_full = (pack_counts >= groups_per_pack)
        masked_w = pack_weights.clone()
        masked_w[is_full] = float('inf')

        # Choose pack with min weight (including offset)
        chosen_pack = masked_w.argmin(dim=1, keepdim=True)

        sorted_pack_index[:, i:i+1] = chosen_pack
        pack_weights.scatter_add_(1, chosen_pack, w_item)
        pack_counts.scatter_add_(1, chosen_pack, torch.ones_like(chosen_pack))
>>>>>>> REPLACE
</DIFF>