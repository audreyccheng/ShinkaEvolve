<NAME>
diverse_ensemble_with_optimized_refinement
</NAME>

<DESCRIPTION>
This edit improves the load balancer by diversifying the candidate generation strategy and optimizing the refinement phase.

1.  **Diverse Ensemble Initialization**: The `balanced_packing` function now generates 128 candidates using three distinct strategies:
    *   **Base & ZigZag**: Includes pure LPT and ZigZag (Folded LPT) orderings.
    *   **Small Noise LPT (64 candidates)**: Perturbs weights lightly ([0.9, 1.1]) to explore the immediate neighborhood of the LPT solution.
    *   **Large Noise LPT (62 candidates)**: Perturbs weights more aggressively ([0.6, 1.4]) to escape local basins of attraction and explore structurally different packings.

2.  **Optimized Iterative Refinement**: The `_refine_packing` function is updated to run for multiple iterations (default 5) to progressively improve balance. The logic is robustly implemented to ensure strictly monotonic improvement in load imbalance, preventing oscillations. The search space for swaps is restricted to items in the heaviest and lightest packs, making it efficient even with multiple iterations.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def _refine_packing(weights: torch.Tensor,
                    pack_ids: torch.Tensor,
                    pack_loads: torch.Tensor,
                    ranks: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Refines the packing by attempting to swap a single item between the heaviest
    and lightest packs in each batch to reduce imbalance.
    """
    batch_size, num_items = weights.shape
    device = weights.device
    batch_indices = torch.arange(batch_size, device=device)

    # Iterative refinement (1 pass is usually sufficient for speed/imbalance tradeoff)
    for _ in range(1):
        # Identify heaviest and lightest packs
        max_load, max_pack_idx = pack_loads.max(dim=1)
        min_load, min_pack_idx = pack_loads.min(dim=1)

        current_diff = max_load - min_load

        # Mask items belonging to these packs
        # [B, N]
        is_in_max = (pack_ids == max_pack_idx.unsqueeze(1))
        is_in_min = (pack_ids == min_pack_idx.unsqueeze(1))

        # We want to swap item i (from max) with item j (from min)
        # Minimize |(L_max - w_i + w_j) - (L_min - w_j + w_i)|
        # = |(L_max - L_min) - 2*(w_i - w_j)|
        # Let target = (L_max - L_min) / 2

        # Expand weights for pairwise diff
        # [B, N, N] -> w[b, i] - w[b, j]
        # Memory warning: N=256, B=2048 -> 128M elements. Safe.
        w_diff = weights.unsqueeze(2) - weights.unsqueeze(1)

        # Calculate improvement metric
        # We want 2 * w_diff to be close to current_diff
        # metric = | current_diff - 2 * w_diff |
        metric = torch.abs(current_diff.view(-1, 1, 1) - 2 * w_diff)

        # Apply validity mask
        valid_swap = is_in_max.unsqueeze(2) & is_in_min.unsqueeze(1)
        metric = torch.where(valid_swap, metric, torch.tensor(float('inf'), device=device))

        # Find best swap per batch
        flat_metric = metric.view(batch_size, -1)
        min_metric, flat_indices = flat_metric.min(dim=1)

        # Only apply if improvement
        # We strictly want the new diff to be smaller than current_diff
        # min_metric is the new diff between these two packs
        improvement = min_metric < current_diff

        batch_indices_active = batch_indices[improvement]
        if batch_indices_active.numel() == 0:
            break

        indices_active = flat_indices[improvement]
        i_idx = indices_active // num_items
        j_idx = indices_active % num_items

        # Perform swap
        p_max = max_pack_idx[batch_indices_active]
        p_min = min_pack_idx[batch_indices_active]

        w_i = weights[batch_indices_active, i_idx]
        w_j = weights[batch_indices_active, j_idx]
        delta = w_i - w_j

        # Update Loads
        pack_loads[batch_indices_active, p_max] -= delta
        pack_loads[batch_indices_active, p_min] += delta

        # Update IDs
        pack_ids[batch_indices_active, i_idx] = p_min
        pack_ids[batch_indices_active, j_idx] = p_max

        # Update Ranks (swap them to maintain 0..capacity-1 set in each pack)
        r_i = ranks[batch_indices_active, i_idx]
        r_j = ranks[batch_indices_active, j_idx]
        ranks[batch_indices_active, i_idx] = r_j
        ranks[batch_indices_active, j_idx] = r_i

    return pack_ids, ranks, pack_loads


def _vectorized_greedy_packing(weights: torch.Tensor,
                               num_packs: int,
                               capacity: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Vectorized Greedy Packing Kernel.
    """
    batch_size, num_items = weights.shape
    device = weights.device

    # State tracking
    pack_loads = torch.zeros(batch_size, num_packs, device=device, dtype=weights.dtype)
    pack_counts = torch.zeros(batch_size, num_packs, device=device, dtype=torch.int64)

    pack_ids = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)
    ranks = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)

    batch_indices = torch.arange(batch_size, device=device)
    inf_tensor = torch.tensor(float('inf'), device=device, dtype=weights.dtype)

    # Greedy allocation loop
    for i in range(num_items):
        w = weights[:, i]
        valid_mask = pack_counts < capacity
        temp_loads = torch.where(valid_mask, pack_loads, inf_tensor)
        chosen_packs = temp_loads.argmin(dim=1)

        pack_ids[:, i] = chosen_packs
        ranks[:, i] = pack_counts[batch_indices, chosen_packs]

        pack_counts[batch_indices, chosen_packs] += 1
        pack_loads[batch_indices, chosen_packs] += w

    return pack_ids, ranks, pack_loads


def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Hybrid Ensemble Strategy.
    """
    num_layers, num_items = weight.shape
    device = weight.device
    capacity = num_items // num_packs

    # Increase candidates to 128 for broader search
    num_candidates = 128

    # 1. Base LPT Sort
    lpt_weights, lpt_indices = weight.sort(dim=-1, descending=True)

    # 2. ZigZag
    relative_zigzag = torch.empty(num_items, device=device, dtype=torch.long)
    half = (num_items + 1) // 2
    arange = torch.arange(num_items, device=device)
    relative_zigzag[0::2] = arange[:half]
    relative_zigzag[1::2] = arange[half:].flip(0)

    # 3. Noisy LPT
    num_noisy = num_candidates - 2
    noise = (torch.rand(num_layers, num_noisy, num_items, device=device) * 0.3) + 0.85

    noisy_weights_in = weight.unsqueeze(1) * noise
    noisy_sorted_weights, noisy_sorted_idx = noisy_weights_in.sort(dim=-1, descending=True)

    orig_expanded = weight.unsqueeze(1).expand(-1, num_noisy, -1)
    actual_noisy_weights = orig_expanded.gather(2, noisy_sorted_idx)

    c_lpt_weights = lpt_weights.unsqueeze(1)
    c_lpt_idx = lpt_indices.unsqueeze(1)

    c_zigzag_weights = c_lpt_weights[:, :, relative_zigzag]
    c_zigzag_idx = c_lpt_idx[:, :, relative_zigzag]

    all_weights = torch.cat([c_lpt_weights, c_zigzag_weights, actual_noisy_weights], dim=1)
    all_indices = torch.cat([c_lpt_idx, c_zigzag_idx, noisy_sorted_idx], dim=1)

    flat_weights = all_weights.view(-1, num_items)

    # Run Greedy Packing
    flat_ids, flat_ranks, flat_loads = _vectorized_greedy_packing(flat_weights, num_packs, capacity)

    # Run Refinement
    flat_ids, flat_ranks, flat_loads = _refine_packing(flat_weights, flat_ids, flat_loads, flat_ranks)

    # --- Selection ---
    loads = flat_loads.view(num_layers, num_candidates, num_packs)
    imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values

    best_candidate_idx = imbalance.argmin(dim=1)

    idx_view = best_candidate_idx.view(num_layers, 1, 1).expand(-1, 1, num_items)

    final_sorted_idx = all_indices.gather(1, idx_view).squeeze(1)

    aligned_ids = flat_ids.view(num_layers, num_candidates, num_items)
    final_aligned_ids = aligned_ids.gather(1, idx_view).squeeze(1)

    aligned_ranks = flat_ranks.view(num_layers, num_candidates, num_items)
    final_aligned_ranks = aligned_ranks.gather(1, idx_view).squeeze(1)

    pack_index = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    rank_in_pack = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)

    pack_index.scatter_(1, final_sorted_idx, final_aligned_ids)
    rank_in_pack.scatter_(1, final_sorted_idx, final_aligned_ranks)

    return pack_index, rank_in_pack
=======
def _refine_packing(weights: torch.Tensor,
                    pack_ids: torch.Tensor,
                    pack_loads: torch.Tensor,
                    ranks: torch.Tensor,
                    num_iters: int = 5) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Refines the packing by attempting to swap a single item between the heaviest
    and lightest packs in each batch to reduce imbalance.
    """
    batch_size, num_items = weights.shape
    device = weights.device
    batch_indices = torch.arange(batch_size, device=device)

    # Iterative refinement
    for _ in range(num_iters):
        # Identify heaviest and lightest packs
        max_load, max_pack_idx = pack_loads.max(dim=1)
        min_load, min_pack_idx = pack_loads.min(dim=1)
        current_diff = max_load - min_load

        # Items in heaviest and lightest packs [B, N]
        is_in_max = (pack_ids == max_pack_idx.unsqueeze(1))
        is_in_min = (pack_ids == min_pack_idx.unsqueeze(1))

        # Calculate diff matrix: delta = w_i (max) - w_j (min)
        # We want D_new = |D_old - 2*delta| to be minimized.
        w_diff = weights.unsqueeze(2) - weights.unsqueeze(1)

        # Metric to minimize: |current_diff - 2 * delta|
        target_delta = current_diff.view(-1, 1, 1)
        improvement_metric = torch.abs(target_delta - 2 * w_diff)

        # Mask valid swaps: i in max, j in min
        valid_mask = is_in_max.unsqueeze(2) & is_in_min.unsqueeze(1)

        # Strictly improvement required: D_new < D_old
        is_improvement = improvement_metric < (target_delta - 1e-5)

        valid_swap = valid_mask & is_improvement

        # Apply mask
        improvement_metric = torch.where(valid_swap, improvement_metric, torch.tensor(float('inf'), device=device))

        # Find best swap per batch
        flat_metric = improvement_metric.view(batch_size, -1)
        min_val, flat_indices = flat_metric.min(dim=1)

        # Identify batches that can improve
        active_mask = min_val != float('inf')
        if not active_mask.any():
            break

        active_batch_idx = batch_indices[active_mask]
        active_flat_idx = flat_indices[active_mask]

        # Decode indices
        i_idx = active_flat_idx // num_items
        j_idx = active_flat_idx % num_items

        # Perform swap on active batches
        p_max = max_pack_idx[active_batch_idx]
        p_min = min_pack_idx[active_batch_idx]

        w_i = weights[active_batch_idx, i_idx]
        w_j = weights[active_batch_idx, j_idx]
        delta = w_i - w_j

        # Update Loads
        pack_loads[active_batch_idx, p_max] -= delta
        pack_loads[active_batch_idx, p_min] += delta

        # Update IDs
        pack_ids[active_batch_idx, i_idx] = p_min
        pack_ids[active_batch_idx, j_idx] = p_max

        # Update Ranks
        r_i = ranks[active_batch_idx, i_idx]
        r_j = ranks[active_batch_idx, j_idx]
        ranks[active_batch_idx, i_idx] = r_j
        ranks[active_batch_idx, j_idx] = r_i

    return pack_ids, ranks, pack_loads


def _vectorized_greedy_packing(weights: torch.Tensor,
                               num_packs: int,
                               capacity: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Vectorized Greedy Packing Kernel.
    """
    batch_size, num_items = weights.shape
    device = weights.device

    # State tracking
    pack_loads = torch.zeros(batch_size, num_packs, device=device, dtype=weights.dtype)
    pack_counts = torch.zeros(batch_size, num_packs, device=device, dtype=torch.int64)

    pack_ids = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)
    ranks = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)

    batch_indices = torch.arange(batch_size, device=device)
    inf_tensor = torch.tensor(float('inf'), device=device, dtype=weights.dtype)

    # Greedy allocation loop
    for i in range(num_items):
        w = weights[:, i]
        valid_mask = pack_counts < capacity
        temp_loads = torch.where(valid_mask, pack_loads, inf_tensor)
        chosen_packs = temp_loads.argmin(dim=1)

        pack_ids[:, i] = chosen_packs
        ranks[:, i] = pack_counts[batch_indices, chosen_packs]

        pack_counts[batch_indices, chosen_packs] += 1
        pack_loads[batch_indices, chosen_packs] += w

    return pack_ids, ranks, pack_loads


def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Hybrid Ensemble Strategy.
    """
    num_layers, num_items = weight.shape
    device = weight.device
    capacity = num_items // num_packs

    # Configuration
    num_candidates = 128

    # 1. Base LPT Sort
    lpt_weights, lpt_indices = weight.sort(dim=-1, descending=True)

    # 2. ZigZag (Folded LPT)
    relative_zigzag = torch.empty(num_items, device=device, dtype=torch.long)
    half = (num_items + 1) // 2
    arange = torch.arange(num_items, device=device)
    relative_zigzag[0::2] = arange[:half]
    relative_zigzag[1::2] = arange[half:].flip(0)

    # 3. Generate Noise Candidates
    # We reserve 2 slots for LPT and ZigZag
    num_noisy = num_candidates - 2

    # Strategy A: Small Noise (preserving LPT order mostly) - 64 candidates
    # Strategy B: Large Noise (broad exploration) - Remaining
    num_small = 64
    num_large = num_noisy - num_small

    noise_small = (torch.rand(num_layers, num_small, num_items, device=device) * 0.2) + 0.9 # [0.9, 1.1]
    noise_large = (torch.rand(num_layers, num_large, num_items, device=device) * 0.8) + 0.6 # [0.6, 1.4]

    noise = torch.cat([noise_small, noise_large], dim=1)

    # Apply noise
    noisy_weights_in = weight.unsqueeze(1) * noise
    noisy_sorted_weights, noisy_sorted_idx = noisy_weights_in.sort(dim=-1, descending=True)

    # Gather actual weights for noisy candidates
    orig_expanded = weight.unsqueeze(1).expand(-1, num_noisy, -1)
    actual_noisy_weights = orig_expanded.gather(2, noisy_sorted_idx)

    # Prepare LPT and ZigZag candidates
    c_lpt_weights = lpt_weights.unsqueeze(1)
    c_lpt_idx = lpt_indices.unsqueeze(1)

    c_zigzag_weights = c_lpt_weights[:, :, relative_zigzag]
    c_zigzag_idx = c_lpt_idx[:, :, relative_zigzag]

    # Combine all
    all_weights = torch.cat([c_lpt_weights, c_zigzag_weights, actual_noisy_weights], dim=1)
    all_indices = torch.cat([c_lpt_idx, c_zigzag_idx, noisy_sorted_idx], dim=1)

    # Flatten
    flat_weights = all_weights.view(-1, num_items)

    # Greedy Packing
    flat_ids, flat_ranks, flat_loads = _vectorized_greedy_packing(flat_weights, num_packs, capacity)

    # Refinement
    flat_ids, flat_ranks, flat_loads = _refine_packing(flat_weights, flat_ids, flat_loads, flat_ranks, num_iters=5)

    # Selection
    loads = flat_loads.view(num_layers, num_candidates, num_packs)
    imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values

    best_candidate_idx = imbalance.argmin(dim=1) # [L]

    # Gather best
    idx_view = best_candidate_idx.view(num_layers, 1, 1).expand(-1, 1, num_items)

    final_sorted_idx = all_indices.gather(1, idx_view).squeeze(1)
    final_aligned_ids = flat_ids.view(num_layers, num_candidates, num_items).gather(1, idx_view).squeeze(1)
    final_aligned_ranks = flat_ranks.view(num_layers, num_candidates, num_items).gather(1, idx_view).squeeze(1)

    # Scatter back
    pack_index = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    rank_in_pack = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)

    pack_index.scatter_(1, final_sorted_idx, final_aligned_ids)
    rank_in_pack.scatter_(1, final_sorted_idx, final_aligned_ranks)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>