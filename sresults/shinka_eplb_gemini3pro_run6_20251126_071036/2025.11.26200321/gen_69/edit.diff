--- a/original.py
+++ b/original.py
@@ -1,431 +1,565 @@
 # EVOLVE-BLOCK-START
 """
 Expert parallelism load balancer (EPLB) for vLLM.
 
 This module implements the core rearrangement algorithm using a
-Hybrid Ensemble Greedy strategy that evaluates multiple packing
-heuristics in parallel to minimize load imbalance.
+Hybrid Ensemble Greedy strategy with Pairwise Partition Refinement.
 """
 
 import torch
 
 
-def _refine_packing(weights: torch.Tensor,
-                    pack_ids: torch.Tensor,
-                    pack_loads: torch.Tensor,
-                    ranks: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
-    """
-    Refines the packing by attempting to swap a single item between the heaviest
-    and lightest packs in each batch to reduce imbalance.
+def _pairwise_partition_refinement(weights: torch.Tensor,
+                                   pack_ids: torch.Tensor,
+                                   pack_loads: torch.Tensor,
+                                   ranks: torch.Tensor,
+                                   capacity: int,
+                                   num_iters: int = 10) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
+    """
+    Refines packing by iteratively identifying the heaviest and lightest packs,
+    and completely re-partitioning their combined items between them to balance load.
+    This acts as a vectorized Large Neighborhood Search (LNS) move.
     """
     batch_size, num_items = weights.shape
     device = weights.device
     batch_indices = torch.arange(batch_size, device=device)
-
-    # Iterative refinement (1 pass is usually sufficient for speed/imbalance tradeoff)
-    for _ in range(1):
-        # Identify heaviest and lightest packs
+    
+    # We repack items from 2 packs. 
+    # Max items to repack is roughly 2 * capacity. 
+    # To be safe and vector-friendly, we consider a fixed upper bound or mask.
+    # Since num_items is usually small (e.g. 256), we can sort masked weights.
+    
+    for _ in range(num_iters):
+        # 1. Identify Heaviest and Lightest packs
         max_load, max_pack_idx = pack_loads.max(dim=1)
         min_load, min_pack_idx = pack_loads.min(dim=1)
-
-        current_diff = max_load - min_load
-
-        # Mask items belonging to these packs
+        
+        diff = max_load - min_load
+        if diff.max() < 1e-4:
+            break
+            
+        # 2. Gather items belonging to these two packs
+        # Masks [B, N]
+        mask_max = (pack_ids == max_pack_idx.unsqueeze(1))
+        mask_min = (pack_ids == min_pack_idx.unsqueeze(1))
+        mask_union = mask_max | mask_min
+        
+        # We zero out weights not in the union to push them to the end during sort
+        masked_weights = torch.where(mask_union, weights, torch.tensor(-1.0, device=device, dtype=weights.dtype))
+        
+        # Sort descending. Valid items come first.
         # [B, N]
-        is_in_max = (pack_ids == max_pack_idx.unsqueeze(1))
-        is_in_min = (pack_ids == min_pack_idx.unsqueeze(1))
-
-        # We want to swap item i (from max) with item j (from min)
-        # Minimize |(L_max - w_i + w_j) - (L_min - w_j + w_i)|
-        # = |(L_max - L_min) - 2*(w_i - w_j)|
-        # Let target = (L_max - L_min) / 2
-
-        # Expand weights for pairwise diff
-        # [B, N, N] -> w[b, i] - w[b, j]
-        # Memory warning: N=256, B=2048 -> 128M elements. Safe.
-        w_diff = weights.unsqueeze(2) - weights.unsqueeze(1)
-
-        # Calculate improvement metric
-        # We want 2 * w_diff to be close to current_diff
-        # metric = | current_diff - 2 * w_diff |
-        metric = torch.abs(current_diff.view(-1, 1, 1) - 2 * w_diff)
-
-        # Apply validity mask
-        valid_swap = is_in_max.unsqueeze(2) & is_in_min.unsqueeze(1)
-        metric = torch.where(valid_swap, metric, torch.tensor(float('inf'), device=device))
-
-        # Find best swap per batch
-        flat_metric = metric.view(batch_size, -1)
-        min_metric, flat_indices = flat_metric.min(dim=1)
-
-        # Only apply if improvement
-        # We strictly want the new diff to be smaller than current_diff
-        # min_metric is the new diff between these two packs
-        improvement = min_metric < current_diff
-
-        batch_indices_active = batch_indices[improvement]
-        if batch_indices_active.numel() == 0:
-            break
-
-        indices_active = flat_indices[improvement]
-        i_idx = indices_active // num_items
-        j_idx = indices_active % num_items
-
-        # Perform swap
-        p_max = max_pack_idx[batch_indices_active]
-        p_min = min_pack_idx[batch_indices_active]
-
-        w_i = weights[batch_indices_active, i_idx]
-        w_j = weights[batch_indices_active, j_idx]
-        delta = w_i - w_j
-
+        sorted_w, sorted_idx = masked_weights.sort(dim=1, descending=True)
+        
+        # Determine how many items to process per batch
+        # This is the count of items in the union
+        counts = mask_union.sum(dim=1)
+        max_count = int(counts.max().item())
+        
+        # 3. Local Greedy Partitioning into 2 bins
+        # bin 0 -> max_pack_idx, bin 1 -> min_pack_idx
+        local_loads = torch.zeros(batch_size, 2, device=device, dtype=weights.dtype)
+        local_counts = torch.zeros(batch_size, 2, device=device, dtype=torch.int64)
+        
+        # We need to track decisions to scatter back later
+        # We will store: 0 for bin 0, 1 for bin 1
+        # decisions shape: [B, max_count]
+        decisions = torch.zeros(batch_size, max_count, device=device, dtype=torch.int64)
+        
+        # Loop over the relevant subset of items
+        for i in range(max_count):
+            # Items: sorted_w[:, i]
+            w = sorted_w[:, i]
+            
+            # Mask for batches where this item is valid (weight != -1)
+            valid_item = w > -0.5 
+            
+            # Check capacity constraints
+            # Global capacity is 'capacity'.
+            full_0 = local_counts[:, 0] >= capacity
+            full_1 = local_counts[:, 1] >= capacity
+            
+            # Greedy choice: pick bin with lower load
+            # If loads equal, pick 0
+            prefer_0 = local_loads[:, 0] <= local_loads[:, 1]
+            
+            # Logic:
+            # If 0 not full AND (1 is full OR prefer 0), pick 0.
+            # Else pick 1.
+            # Note: if both full, logic defaults to 1, but this shouldn't happen if sum(items) <= 2*capacity
+            pick_0 = (~full_0) & (full_1 | prefer_0)
+            
+            choice = torch.where(pick_0, 
+                                 torch.tensor(0, device=device), 
+                                 torch.tensor(1, device=device))
+            
+            decisions[:, i] = choice
+            
+            # Update loads/counts only for valid items
+            # We use scatter_add or simpler indexing since 'choice' is 0 or 1
+            # But we must mask update for inactive batches (where valid_item is False)
+            
+            target_load = local_loads.gather(1, choice.unsqueeze(1)).squeeze(1)
+            new_load = target_load + w
+            local_loads.scatter_(1, choice.unsqueeze(1), new_load.unsqueeze(1))
+            
+            # We only want to increment load/count if valid_item is True.
+            # However, for vectorization speed, we can compute everything and then mask out the tail effects?
+            # Better: Multiply w by valid_item.
+            # But counts?
+            # Actually, just condition the update.
+            
+            # Correct vectorized update:
+            # We need to update local_loads and local_counts at index 'choice'
+            # creating a one-hot-like update
+            delta_load = torch.zeros_like(local_loads)
+            delta_load.scatter_(1, choice.unsqueeze(1), w.unsqueeze(1))
+            
+            delta_count = torch.zeros_like(local_counts)
+            delta_count.scatter_(1, choice.unsqueeze(1), torch.ones_like(w, dtype=torch.int64).unsqueeze(1))
+            
+            # Mask out invalid items (padding -1s)
+            delta_load = torch.where(valid_item.unsqueeze(1), delta_load, torch.zeros_like(delta_load))
+            delta_count = torch.where(valid_item.unsqueeze(1), delta_count, torch.zeros_like(delta_count))
+            
+            local_loads += delta_load
+            local_counts += delta_count
+
+        # 4. Write back to global state
+        
+        # Decisions [B, max_count] correspond to sorted_idx [B, max_count]
+        # We need to map these back to global packs
+        
+        # Only process up to max_count columns of sorted_idx
+        relevant_indices = sorted_idx[:, :max_count]
+        
+        # Map 0 -> max_pack_idx, 1 -> min_pack_idx
+        global_pack_decisions = torch.where(decisions == 0, 
+                                            max_pack_idx.unsqueeze(1), 
+                                            min_pack_idx.unsqueeze(1))
+        
+        # Compute ranks within the new local bins
+        # Rank is cumulative sum of decision==k minus 1
+        rank_0 = (decisions == 0).cumsum(dim=1) - 1
+        rank_1 = (decisions == 1).cumsum(dim=1) - 1
+        local_ranks = torch.where(decisions == 0, rank_0, rank_1)
+        
+        # To handle the variable length properly during scatter, 
+        # we construct a mask of valid items in the subset
+        valid_subset = masked_weights[:, :max_count] > -0.5
+        
+        # We need to scatter 'global_pack_decisions' and 'local_ranks' back to 'pack_ids' and 'ranks'
+        # at positions 'relevant_indices'.
+        
+        # Pack IDs
+        pack_ids.scatter_(1, relevant_indices, global_pack_decisions)
+        
+        # For ranks, we need to be careful. The new ranks are 0-based within the partition.
+        # But are they valid globally? 
+        # The 'ranks' tensor in the system usually represents the order within the pack.
+        # Since we rebuilt the pack from scratch locally, these are the correct new ranks for these items.
+        ranks.scatter_(1, relevant_indices, local_ranks)
+        
+        # Ideally, we should not touch entries where valid_subset is False, 
+        # but sorted_idx for those point to garbage or valid items?
+        # sorted_idx includes all items. The tail are the items NOT in the mask_union.
+        # We must NOT scatter to those.
+        # However, scatter_ takes everything. 
+        # We should only scatter active items.
+        
+        # Workaround: Use the valid_subset mask to mix old and new, then scatter.
+        # Or simpler: The sorted_idx puts the -1 items at the end.
+        # The logic above computed decisions for them (likely bin 1), but we shouldn't apply them.
+        # We can construct a "sparse" update.
+        
+        # Let's perform the scatter only where valid.
+        # Since we can't easily conditional scatter, we can restore the old values for invalid positions 
+        # in the source tensor before scattering, OR gather old values, update, scatter back.
+        
+        # Actually, sorted_idx points to specific items. The tail items are those with weight -1.
+        # These are items NOT in the union. We definitely don't want to change their pack_ids.
+        # So we must ensure that we don't scatter to indices corresponding to items not in union.
+        
+        # Implementation:
+        # Create a temporary tensor for updates
+        # Only update indices where valid_subset is True
+        
+        # Expand valid_subset to use as mask
+        # We can just iterate or use index_put_ if strictly needed, but let's try masking the update values.
+        # But we need to mask the indices or the operation.
+        
+        # Safe way:
+        # 1. Gather current pack_ids at relevant_indices.
+        # 2. Where valid_subset is True, use global_pack_decisions. Else use gathered.
+        # 3. Scatter back.
+        
+        current_ids_subset = pack_ids.gather(1, relevant_indices)
+        current_ranks_subset = ranks.gather(1, relevant_indices)
+        
+        new_ids_subset = torch.where(valid_subset, global_pack_decisions, current_ids_subset)
+        new_ranks_subset = torch.where(valid_subset, local_ranks, current_ranks_subset)
+        
+        pack_ids.scatter_(1, relevant_indices, new_ids_subset)
+        ranks.scatter_(1, relevant_indices, new_ranks_subset)
+        
         # Update Loads
-        pack_loads[batch_indices_active, p_max] -= delta
-        pack_loads[batch_indices_active, p_min] += delta
-
-        # Update IDs
-        pack_ids[batch_indices_active, i_idx] = p_min
-        pack_ids[batch_indices_active, j_idx] = p_max
-
-        # Update Ranks (swap them to maintain 0..capacity-1 set in each pack)
-        r_i = ranks[batch_indices_active, i_idx]
-        r_j = ranks[batch_indices_active, j_idx]
-        ranks[batch_indices_active, i_idx] = r_j
-        ranks[batch_indices_active, j_idx] = r_i
+        pack_loads[batch_indices, max_pack_idx] = local_loads[:, 0]
+        pack_loads[batch_indices, min_pack_idx] = local_loads[:, 1]
 
     return pack_ids, ranks, pack_loads
 
 
 def _vectorized_greedy_packing(weights: torch.Tensor,
                                num_packs: int,
                                capacity: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
-    Vectorized Greedy Packing Kernel.
+    Standard Vectorized Greedy Packing Kernel.
     """
     batch_size, num_items = weights.shape
     device = weights.device
 
-    # State tracking
     pack_loads = torch.zeros(batch_size, num_packs, device=device, dtype=weights.dtype)
     pack_counts = torch.zeros(batch_size, num_packs, device=device, dtype=torch.int64)
-
     pack_ids = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)
     ranks = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)
 
     batch_indices = torch.arange(batch_size, device=device)
     inf_tensor = torch.tensor(float('inf'), device=device, dtype=weights.dtype)
 
-    # Greedy allocation loop
     for i in range(num_items):
         w = weights[:, i]
         valid_mask = pack_counts < capacity
         temp_loads = torch.where(valid_mask, pack_loads, inf_tensor)
         chosen_packs = temp_loads.argmin(dim=1)
 
         pack_ids[:, i] = chosen_packs
         ranks[:, i] = pack_counts[batch_indices, chosen_packs]
 
         pack_counts[batch_indices, chosen_packs] += 1
         pack_loads[batch_indices, chosen_packs] += w
 
     return pack_ids, ranks, pack_loads
 
 
 def balanced_packing(weight: torch.Tensor,
                      num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
     """
-    Pack n weighted objects to m packs using a Hybrid Ensemble Strategy.
+    Pack n weighted objects to m packs using Hybrid Ensemble and Partition Refinement.
     """
     num_layers, num_items = weight.shape
     device = weight.device
+    num_candidates = 128
     capacity = num_items // num_packs
-
-    # Increase candidates to 128 for broader search
-    num_candidates = 128
-
-    # 1. Base LPT Sort
-    lpt_weights, lpt_indices = weight.sort(dim=-1, descending=True)
-
-    # 2. ZigZag
-    relative_zigzag = torch.empty(num_items, device=device, dtype=torch.long)
+    
+    # --- 1. Candidate Generation ---
+    
+    # A. LPT
+    lpt_val, lpt_idx = weight.sort(dim=-1, descending=True)
+    c_lpt = lpt_idx.unsqueeze(1)
+    
+    # B. Interleaved (Heavy-Light-Heavy-Light)
+    # 0, N-1, 1, N-2, ...
+    interleaved_perm = torch.empty(num_items, device=device, dtype=torch.long)
     half = (num_items + 1) // 2
     arange = torch.arange(num_items, device=device)
-    relative_zigzag[0::2] = arange[:half]
-    relative_zigzag[1::2] = arange[half:].flip(0)
-
-    # 3. Noisy LPT
-    num_noisy = num_candidates - 2
-    noise = (torch.rand(num_layers, num_noisy, num_items, device=device) * 0.3) + 0.85
-
-    noisy_weights_in = weight.unsqueeze(1) * noise
-    noisy_sorted_weights, noisy_sorted_idx = noisy_weights_in.sort(dim=-1, descending=True)
-
-    orig_expanded = weight.unsqueeze(1).expand(-1, num_noisy, -1)
-    actual_noisy_weights = orig_expanded.gather(2, noisy_sorted_idx)
-
-    c_lpt_weights = lpt_weights.unsqueeze(1)
-    c_lpt_idx = lpt_indices.unsqueeze(1)
-
-    c_zigzag_weights = c_lpt_weights[:, :, relative_zigzag]
-    c_zigzag_idx = c_lpt_idx[:, :, relative_zigzag]
-
-    all_weights = torch.cat([c_lpt_weights, c_zigzag_weights, actual_noisy_weights], dim=1)
-    all_indices = torch.cat([c_lpt_idx, c_zigzag_idx, noisy_sorted_idx], dim=1)
-
-    flat_weights = all_weights.view(-1, num_items)
-
-    # Run Greedy Packing
+    interleaved_perm[0::2] = arange[:half]
+    interleaved_perm[1::2] = arange[half:].flip(0)
+    
+    c_interleaved = lpt_idx.gather(1, interleaved_perm.unsqueeze(0).expand(num_layers, -1)).unsqueeze(1)
+    
+    # C. Random Shuffles (Diversification)
+    num_random = 30
+    rand_perm = torch.rand(num_layers, num_random, num_items, device=device).argsort(dim=-1)
+    c_random = rand_perm
+    
+    # D. Noisy LPT (Exploitation around LPT)
+    num_noisy = num_candidates - 2 - num_random
+    noise = torch.rand(num_layers, num_noisy, num_items, device=device) * 0.4 + 0.8
+    noisy_w = weight.unsqueeze(1) * noise
+    _, c_noisy = noisy_w.sort(dim=-1, descending=True)
+    
+    all_indices = torch.cat([c_lpt, c_interleaved, c_random, c_noisy], dim=1)
+    
+    # Gather weights
+    expanded_weight = weight.unsqueeze(1).expand(-1, num_candidates, -1)
+    ordered_weights = expanded_weight.gather(2, all_indices)
+    
+    # Flatten
+    flat_weights = ordered_weights.view(-1, num_items)
+    
+    # --- 2. Greedy Packing ---
     flat_ids, flat_ranks, flat_loads = _vectorized_greedy_packing(flat_weights, num_packs, capacity)
-
-    # Run Refinement
-    flat_ids, flat_ranks, flat_loads = _refine_packing(flat_weights, flat_ids, flat_loads, flat_ranks)
-
-    # --- Selection ---
+    
+    # --- 3. Top-K Selection ---
     loads = flat_loads.view(num_layers, num_candidates, num_packs)
     imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values
-
-    best_candidate_idx = imbalance.argmin(dim=1)
-
-    idx_view = best_candidate_idx.view(num_layers, 1, 1).expand(-1, 1, num_items)
-
+    
+    # Select Top 8
+    k = 8
+    best_vals, best_k_indices = imbalance.topk(k, dim=1, largest=False)
+    
+    layer_offsets = (torch.arange(num_layers, device=device) * num_candidates).unsqueeze(1)
+    flat_selected_indices = (best_k_indices + layer_offsets).flatten()
+    
+    refined_weights = flat_weights[flat_selected_indices]
+    refined_ids = flat_ids[flat_selected_indices]
+    refined_ranks = flat_ranks[flat_selected_indices]
+    refined_loads = flat_loads[flat_selected_indices]
+    
+    # --- 4. Partition Refinement ---
+    refined_ids, refined_ranks, refined_loads = _pairwise_partition_refinement(
+        refined_weights, refined_ids, refined_loads, refined_ranks, capacity, num_iters=10
+    )
+    
+    # --- 5. Final Selection ---
+    loads_final = refined_loads.view(num_layers, k, num_packs)
+    imbalance_final = loads_final.max(dim=-1).values - loads_final.min(dim=-1).values
+    best_in_k = imbalance_final.argmin(dim=1)
+    
+    # Reconstruct result
+    
+    # Get the original permutation indices for the winner
+    best_cand_idx = best_k_indices.gather(1, best_in_k.unsqueeze(1)).squeeze(1)
+    idx_view = best_cand_idx.view(num_layers, 1, 1).expand(-1, 1, num_items)
     final_sorted_idx = all_indices.gather(1, idx_view).squeeze(1)
-
-    aligned_ids = flat_ids.view(num_layers, num_candidates, num_items)
-    final_aligned_ids = aligned_ids.gather(1, idx_view).squeeze(1)
-
-    aligned_ranks = flat_ranks.view(num_layers, num_candidates, num_items)
-    final_aligned_ranks = aligned_ranks.gather(1, idx_view).squeeze(1)
-
+    
+    # Get packing info
+    idx_in_refined = (torch.arange(num_layers, device=device) * k) + best_in_k
+    final_ids = refined_ids[idx_in_refined]
+    final_ranks = refined_ranks[idx_in_refined]
+    
+    # Scatter
     pack_index = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
     rank_in_pack = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
-
-    pack_index.scatter_(1, final_sorted_idx, final_aligned_ids)
-    rank_in_pack.scatter_(1, final_sorted_idx, final_aligned_ranks)
-
+    
+    pack_index.scatter_(1, final_sorted_idx, final_ids)
+    rank_in_pack.scatter_(1, final_sorted_idx, final_ranks)
+    
     return pack_index, rank_in_pack
 
 
 def replicate_experts(
         weight: torch.Tensor,
         num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Replicate experts using Binary Search on Max Load followed by Greedy Refinement.
     """
     num_layers, num_log = weight.shape
     device = weight.device
 
-    # Trivial case
     if num_phy == num_log:
         phy2log = torch.arange(num_log, device=device).expand(num_layers, -1)
         rank = torch.zeros(num_layers, num_phy, dtype=torch.int64, device=device)
         logcnt = torch.ones(num_layers, num_log, dtype=torch.int64, device=device)
         return phy2log, rank, logcnt
 
     # Binary Search
     low = weight.sum(dim=-1, keepdim=True) / num_phy
     high = weight.max(dim=-1, keepdim=True).values
     low = torch.max(low, torch.tensor(1e-6, device=device))
 
     for _ in range(15):
         mid = (low + high) * 0.5
         counts = torch.ceil(weight / mid)
         total = counts.sum(dim=-1, keepdim=True)
         mask = total <= num_phy
         high = torch.where(mask, mid, high)
         low = torch.where(mask, low, mid)
 
     logcnt = torch.ceil(weight / high).long().clamp(min=1)
 
     # Correct sum
     current_sum = logcnt.sum(dim=-1)
     diff = num_phy - current_sum
 
-    # Under-allocation: Add to max density
+    # Under-allocation
     max_diff = int(diff.max().item())
     if max_diff > 0:
         rows = torch.arange(num_layers, device=device)
         for _ in range(max_diff):
             active = current_sum < num_phy
             if not active.any(): break
-
+            
             density = weight / logcnt.float()
+            density[~active] = -1.0
             target_idx = density.argmax(dim=-1)
-
+            
             active_rows = rows[active]
             active_targets = target_idx[active]
-
-            logcnt.index_put_((active_rows, active_targets),
-                              torch.tensor(1, device=device, dtype=torch.int64),
+            
+            logcnt.index_put_((active_rows, active_targets), 
+                              torch.tensor(1, device=device, dtype=torch.int64), 
                               accumulate=True)
             current_sum[active] += 1
 
-    # Over-allocation: Remove from min cost
+    # Over-allocation
     min_diff = int(diff.min().item())
     if min_diff < 0:
         rows = torch.arange(num_layers, device=device)
         for _ in range(abs(min_diff)):
             active = current_sum > num_phy
             if not active.any(): break
-
+            
             valid = logcnt > 1
             cost = weight / (logcnt - 1).float()
             cost[~valid] = float('inf')
-
+            cost[~active] = float('inf')
+            
             target_idx = cost.argmin(dim=-1)
-
+            
             active_rows = rows[active]
             active_targets = target_idx[active]
-
-            logcnt.index_put_((active_rows, active_targets),
-                              torch.tensor(-1, device=device, dtype=torch.int64),
+            
+            logcnt.index_put_((active_rows, active_targets), 
+                              torch.tensor(-1, device=device, dtype=torch.int64), 
                               accumulate=True)
             current_sum[active] -= 1
 
     # Construct maps
     flat_log_ids = torch.arange(num_log, device=device).repeat(num_layers)
     flat_counts = logcnt.flatten()
     flat_phy2log = torch.repeat_interleave(flat_log_ids, flat_counts)
-
+    
     target_size = num_layers * num_phy
     if flat_phy2log.numel() != target_size:
         if flat_phy2log.numel() < target_size:
             flat_phy2log = torch.cat([flat_phy2log, torch.zeros(target_size - flat_phy2log.numel(), device=device, dtype=torch.long)])
         else:
             flat_phy2log = flat_phy2log[:target_size]
-
+            
     phy2log = flat_phy2log.view(num_layers, num_phy)
-
+    
     offsets = torch.zeros_like(logcnt)
     offsets[:, 1:] = logcnt[:, :-1].cumsum(dim=1)
     mapped_offsets = offsets.gather(1, phy2log)
     phy_indices = torch.arange(num_phy, device=device).expand(num_layers, -1)
     rank = phy_indices - mapped_offsets
 
     return phy2log, rank, logcnt
 
 
 def rebalance_experts_hierarchical(
     weight: torch.Tensor,
     num_physical_experts: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ):
     """
     Hierarchical rebalancing.
     """
     num_layers, num_logical_experts = weight.shape
     group_size = num_logical_experts // num_groups
     groups_per_node = num_groups // num_nodes
     phy_experts_per_gpu = num_physical_experts // num_gpus
 
     def inverse(perm: torch.Tensor) -> torch.Tensor:
         inv = torch.empty_like(perm)
         inv.scatter_(
             1,
             perm,
             torch.arange(perm.size(1), dtype=torch.int64,
                          device=perm.device).expand(perm.shape),
         )
         return inv
 
     # 1. Groups -> Nodes
     tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)
     group_pack_index, group_rank_in_pack = balanced_packing(
         tokens_per_group, num_nodes)
-
+        
     log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *
                  group_size).unsqueeze(-1) +
                 torch.arange(group_size,
                              dtype=torch.int64,
                              device=group_pack_index.device)).flatten(-2)
     mlog2log = inverse(log2mlog)
 
     # 2. Replicate within nodes
     tokens_per_mlog = weight.gather(-1, mlog2log).view(
         -1, num_logical_experts // num_nodes)
     phy2mlog, phyrank, mlogcnt = replicate_experts(
         tokens_per_mlog, num_physical_experts // num_nodes)
 
     # 3. Physical -> GPUs
     tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
     pack_index, rank_in_pack = balanced_packing(tokens_per_phy,
                                                 num_gpus // num_nodes)
-
+    
     phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
     pphy2phy = inverse(phy2pphy)
 
     pphy2mlog = phy2mlog.gather(-1, pphy2phy)
-
+    
     node_offsets = torch.arange(
         0,
         num_logical_experts,
         num_logical_experts // num_nodes,
         device=weight.device,
     ).view(1, -1, 1)
-
+    
     pphy2mlog_restored = (pphy2mlog.view(num_layers, num_nodes, -1) + node_offsets).flatten(-2)
-
+    
     pphy2log = mlog2log.gather(-1, pphy2mlog_restored)
     pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)
     logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)
-
+    
     return pphy2log, pphyrank, logcnt
 
 
 def rebalance_experts(
     weight: torch.Tensor,
     num_replicas: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Entry point.
     """
     num_layers, num_logical_experts = weight.shape
     weight = weight.float().cpu()
-
+    
     if num_groups % num_nodes == 0:
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, num_groups, num_nodes, num_gpus)
     else:
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, 1, 1, num_gpus)
-
+            
     max_replicas = int(logcnt.max().item())
-
+    
     log2phy = torch.full(
         (num_layers, num_logical_experts, max_replicas),
         -1,
         dtype=torch.int64,
         device=logcnt.device,
     )
-
+    
     flat_layer_idx = torch.arange(num_layers, device=logcnt.device).unsqueeze(-1).expand(-1, num_replicas).flatten()
     flat_log_idx = phy2log.flatten()
     flat_rank_idx = phyrank.flatten()
     flat_phy_ids = torch.arange(num_replicas, dtype=torch.int64, device=logcnt.device).expand(num_layers, -1).flatten()
-
+    
     indices = (flat_layer_idx * num_logical_experts * max_replicas) + \
               (flat_log_idx * max_replicas) + \
               flat_rank_idx
-
+              
     log2phy.view(-1).scatter_(0, indices, flat_phy_ids)
-
+    
     return phy2log, log2phy, logcnt
 
 # EVOLVE-BLOCK-END
 
 
 # This part remains fixed (not evolved)
 def run_eplb(weight: torch.Tensor, num_replicas: int, num_groups: int,
              num_nodes: int, num_gpus: int):
     """Run the expert parallelism load balancer"""
     phy2log, log2phy, logcnt = rebalance_experts(
         weight, num_replicas, num_groups, num_nodes, num_gpus
     )
     return phy2log, log2phy, logcnt
 
 
 __all__ = ["rebalance_experts", "run_eplb"]