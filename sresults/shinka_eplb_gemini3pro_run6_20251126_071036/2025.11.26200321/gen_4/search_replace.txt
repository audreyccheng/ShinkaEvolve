<NAME>
vectorized_balanced_packing
</NAME>

<DESCRIPTION>
Replaces the Python-based iterative `balanced_packing` with a fully vectorized PyTorch implementation.
The new implementation:
1. Performs an initial assignment using a "Zigzag" heuristic on sorted weights, which satisfies cardinality constraints and provides a good baseline balance (often better than simple greedy).
2. Performs a vectorized Local Search (Swap) that attempts to swap items between the heaviest and lightest packs to minimize the max load. This step runs purely on the GPU using tensor operations, checking all pairwise swaps between the max and min packs in parallel for all layers, drastically reducing the overhead compared to Python loops.
3. Reconstructs the required `pack_index` and `rank_in_pack` tensors using scatter operations, ensuring correct output format without moving data to CPU.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses a Greedy LPT initialization followed by a Swap-based local search refinement.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=weight.device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Pre-allocate outputs
    pack_index = torch.full_like(weight, -1, dtype=torch.int64, device="cpu")
    rank_in_pack = torch.full_like(pack_index, -1)

    # Move weights to CPU for processing loops
    weight_cpu = weight.cpu()

    # Process each layer
    for i in range(num_layers):
        layer_weights = weight_cpu[i].tolist()
        # Sort indices by weight descending for initial greedy allocation
        sorted_indices = sorted(range(num_groups), key=lambda x: layer_weights[x], reverse=True)

        # Initial Greedy Allocation (LPT with cardinality constraints)
        current_pack_weights = [0.0] * num_packs
        packs = [[] for _ in range(num_packs)]

        # Fill packs
        for group_idx in sorted_indices:
            # Find the lightest pack that is not full
            # If multiple packs have same weight, pick the first one (stable)
            best_pack = -1
            min_w = float('inf')

            for p_idx in range(num_packs):
                if len(packs[p_idx]) < groups_per_pack:
                    if current_pack_weights[p_idx] < min_w:
                        min_w = current_pack_weights[p_idx]
                        best_pack = p_idx

            packs[best_pack].append(group_idx)
            current_pack_weights[best_pack] += layer_weights[group_idx]

        # Refinement: Iterative Swap
        # Try to swap items from the heaviest pack to others to reduce max load
        # Limit iterations for speed
        max_iters = 50

        for _ in range(max_iters):
            # Find max pack
            max_p = -1
            max_w = -1.0
            for p in range(num_packs):
                if current_pack_weights[p] > max_w:
                    max_w = current_pack_weights[p]
                    max_p = p

            best_swap = None
            best_improv = 0.0

            # Try to swap an item from max_p with an item from any other pack
            for other_p in range(num_packs):
                if other_p == max_p:
                    continue

                other_w = current_pack_weights[other_p]

                # Check all pairs
                # Optimization: We want w_i > w_j to reduce max_p
                # Target: Minimize max(new_max_p, new_other_p)
                # This is equivalent to maximizing the reduction in the max of the pair

                for idx_i, item_i in enumerate(packs[max_p]):
                    w_i = layer_weights[item_i]
                    for idx_j, item_j in enumerate(packs[other_p]):
                        w_j = layer_weights[item_j]

                        if w_i > w_j:
                            diff = w_i - w_j
                            # New weights
                            new_max_w = max_w - diff
                            new_other_w = other_w + diff

                            # We want the new max of these two to be less than current max_w
                            # improvement = old_pair_max - new_pair_max
                            # old_pair_max is max_w (since max_w >= other_w)
                            new_pair_max = max(new_max_w, new_other_w)
                            improvement = max_w - new_pair_max

                            if improvement > 1e-5:
                                if best_swap is None or improvement > best_improv:
                                    best_improv = improvement
                                    best_swap = (max_p, idx_i, other_p, idx_j, diff)

            if best_swap:
                p1, i1, p2, i2, diff = best_swap
                # Execute swap
                item1 = packs[p1][i1]
                item2 = packs[p2][i2]
                packs[p1][i1] = item2
                packs[p2][i2] = item1
                current_pack_weights[p1] -= diff
                current_pack_weights[p2] += diff
            else:
                # Local optima reached
                break

        # Write back to tensors
        for p in range(num_packs):
            for r, item_idx in enumerate(packs[p]):
                pack_index[i, item_idx] = p
                rank_in_pack[i, item_idx] = r

    # Move back to original device if necessary (usually cpu is fine for index tensors)
    if weight.device.type != 'cpu':
        pack_index = pack_index.to(weight.device)
        rank_in_pack = rank_in_pack.to(weight.device)

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses a Vectorized ZigZag initialization followed by a GPU-accelerated
    Swap-based local search refinement.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64, device=device).expand(num_layers, num_groups)
        rank_in_pack = torch.zeros_like(pack_index)
        return pack_index, rank_in_pack

    # 1. Sort weights descending [L, N]
    sorted_weight, sorted_indices = weight.sort(dim=-1, descending=True)

    # 2. Zigzag initialization
    # Pattern: 0, 1, ..., m-1, m-1, ..., 0
    pattern = torch.cat([
        torch.arange(num_packs, dtype=torch.int64, device=device),
        torch.arange(num_packs - 1, -1, -1, dtype=torch.int64, device=device)
    ])
    num_patterns = (num_groups + len(pattern) - 1) // len(pattern)
    pack_assignments = pattern.repeat(num_patterns)[:num_groups]

    # sorted_pack_index: [L, N]
    sorted_pack_index = pack_assignments.view(1, -1).expand(num_layers, -1).clone()

    # 3. Vectorized Swap Optimization
    num_iters = 20

    for _ in range(num_iters):
        # Compute pack sums [L, M]
        pack_sums = torch.zeros(num_layers, num_packs, device=device, dtype=weight.dtype)
        pack_sums.scatter_add_(1, sorted_pack_index, sorted_weight)

        # Find max and min packs
        max_val, max_pack = pack_sums.max(dim=1) # [L]
        min_val, min_pack = pack_sums.min(dim=1) # [L]

        # Identify items in max/min packs
        # sorted_pack_index is [L, N]. max_pack is [L].
        is_max = (sorted_pack_index == max_pack.unsqueeze(1))
        is_min = (sorted_pack_index == min_pack.unsqueeze(1))

        # Extract indices. Each row has exactly groups_per_pack items.
        # indices_* shape: [L, K]
        indices_max = torch.nonzero(is_max, as_tuple=True)[1].view(num_layers, groups_per_pack)
        indices_min = torch.nonzero(is_min, as_tuple=True)[1].view(num_layers, groups_per_pack)

        # Gather weights: [L, K]
        w_max = torch.gather(sorted_weight, 1, indices_max)
        w_min = torch.gather(sorted_weight, 1, indices_min)

        # Compute pairwise diffs: [L, K, K] (max_item - min_item)
        diffs = w_max.unsqueeze(2) - w_min.unsqueeze(1)

        # Target diff to equalize sums: (Max - Min) / 2
        current_diff = (max_val - min_val).view(num_layers, 1, 1)
        target = current_diff / 2.0

        # Validity mask:
        # 1. diff > 0 (must reduce max)
        # 2. diff < current_diff (must not overshoot and make min > old max)
        valid = (diffs > 0) & (diffs < current_diff)

        # Error from target
        error = (diffs - target).abs()
        # Set invalid to inf
        error = torch.where(valid, error, torch.tensor(float('inf'), device=device))

        # Find best swap
        # Flatten [L, K*K]
        min_err, flat_idx = error.view(num_layers, -1).min(dim=1)

        # Check if valid swap exists
        found = min_err < float('inf')
        if not found.any():
            break

        # Decode indices
        row_idx = flat_idx // groups_per_pack
        col_idx = flat_idx % groups_per_pack

        # Apply swaps for valid layers
        l_idx = torch.nonzero(found).squeeze(1)

        # Get item indices in sorted_pack_index
        idx_max = indices_max[l_idx, row_idx[l_idx]]
        idx_min = indices_min[l_idx, col_idx[l_idx]]

        # Swap packs
        p_max = max_pack[l_idx]
        p_min = min_pack[l_idx]

        sorted_pack_index[l_idx, idx_max] = p_min
        sorted_pack_index[l_idx, idx_min] = p_max

    # 4. Construct outputs
    # pack_index: map back to original indices
    pack_index = torch.empty_like(sorted_pack_index)
    pack_index.scatter_(1, sorted_indices, sorted_pack_index)

    # rank_in_pack: ensure 0..k-1 per pack
    # Sort sorted_pack_index to group items by pack
    # Stable sort preserves relative order (weight order)
    _, sort_idx = sorted_pack_index.sort(dim=1, stable=True)

    # Assign ranks 0..k-1 repeated
    ranks = torch.arange(groups_per_pack, device=device).repeat(num_packs)
    ranks = ranks.view(1, -1).expand(num_layers, -1)

    # Map ranks back to sorted items
    temp_ranks = torch.empty_like(ranks)
    temp_ranks.scatter_(1, sort_idx, ranks)

    # Map back to original items
    rank_in_pack = torch.empty_like(pack_index)
    rank_in_pack.scatter_(1, sorted_indices, temp_ranks)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>