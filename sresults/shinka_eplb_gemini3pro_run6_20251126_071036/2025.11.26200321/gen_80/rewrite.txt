# EVOLVE-BLOCK-START
"""
Expert parallelism load balancer (EPLB) for vLLM.

This module implements the core rearrangement algorithm using a
Big-Rocks Initialized Ensemble with L2-Norm Refinement.
"""

import torch

def _refine_packing_l2(weights: torch.Tensor,
                       pack_ids: torch.Tensor,
                       pack_loads: torch.Tensor,
                       ranks: torch.Tensor,
                       num_packs: int,
                       num_iters: int = 20) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Refines packing by attempting to swap items to minimize the L2 norm of pack loads
    (Sum of Squared Loads), which implicitly minimizes Max Load and Variance.
    
    Checks swaps between the Max Load pack and ALL other packs.
    """
    batch_size, num_items = weights.shape
    device = weights.device
    batch_range = torch.arange(batch_size, device=device)

    # Precompute pairwise weight differences: [B, N, N]
    # diff[b, i, j] = w[b, i] - w[b, j]
    w_diff = weights.unsqueeze(2) - weights.unsqueeze(1)

    for _ in range(num_iters):
        # Current State
        # max_pack_idx: [B]
        max_vals, max_pack_idx = pack_loads.max(dim=1)
        
        # We try to swap items from max_pack with items from any other pack p.
        # Objective: Minimize Sum(Loads^2).
        # Change in objective when swapping i (from Max) and j (from P):
        # L_max_new = L_max - (w_i - w_j) = L_max - delta
        # L_p_new   = L_p + (w_i - w_j)   = L_p + delta
        # Delta_Cost = (L_max - delta)^2 + (L_p + delta)^2 - (L_max^2 + L_p^2)
        #            = L_max^2 - 2*L_max*delta + delta^2 + L_p^2 + 2*L_p*delta + delta^2 - L_max^2 - L_p^2
        #            = 2*delta^2 - 2*delta*(L_max - L_p)
        # We want Delta_Cost < 0 => delta^2 < delta * (L_max - L_p)
        # If delta > 0: delta < (L_max - L_p)
        # If delta < 0: delta > (L_max - L_p) (Impossible since L_max >= L_p)
        # So essentially we need 0 < delta < (L_max - L_p).
        # And we want to minimize 2*delta^2 - 2*delta*Gap
        # This is a parabola opening upwards. Minimum is at delta = Gap / 2.
        # So optimal swap has w_i - w_j close to (L_max - L_p) / 2.
        
        # We look for the swap that gives the most negative Delta_Cost.
        
        best_gain = torch.zeros(batch_size, device=device, dtype=weights.dtype)
        best_swap_flat = torch.zeros(batch_size, device=device, dtype=torch.int64)
        best_target_p = torch.full((batch_size,), -1, device=device, dtype=torch.int64)
        
        mask_max = (pack_ids == max_pack_idx.unsqueeze(1)) # [B, N]

        for p in range(num_packs):
            # Target Load [B]
            load_p = pack_loads[:, p]
            gap = max_vals - load_p # [B]
            
            # Filter batches where p is max pack or gap is tiny
            active_p = (gap > 1e-5)
            if not active_p.any():
                continue
                
            mask_p = (pack_ids == torch.tensor(p, device=device)) # [B, N]
            
            # Construct Delta tensor [B, N, N]
            delta = w_diff
            
            # Valid mask: i in Max, j in P
            valid_swap = mask_max.unsqueeze(2) & mask_p.unsqueeze(1) & active_p.view(-1, 1, 1)
            
            # Calculate Improvement (Negative Delta Cost)
            # We maximize Gain = -Delta_Cost = 2*delta*(Gap) - 2*delta^2
            #                  = 2*delta * (Gap - delta)
            gap_view = gap.view(-1, 1, 1)
            gain = 2 * delta * (gap_view - delta)
            
            # Enforce constraints
            # 1. Gain > 0
            # 2. Valid swap pair
            # 3. delta > 0 (Strictly move weight out of max pack? Not strictly necessary for L2, but good for convergence)
            # Actually, L2 allows delta < 0 if Gap is negative, but Gap is Max-P >= 0.
            # So delta must be < Gap. And delta must be > 0 for Gain > 0.
            
            valid_swap = valid_swap & (gain > 1e-5) & (delta > 0)
            
            # Mask invalid
            gain = torch.where(valid_swap, gain, torch.tensor(-1.0, device=device, dtype=weights.dtype))
            
            # Max gain for this pack p
            flat_gain = gain.view(batch_size, -1)
            p_max_gain, p_flat_idx = flat_gain.max(dim=1)
            
            # Update best across packs
            update = p_max_gain > best_gain
            if update.any():
                best_gain = torch.where(update, p_max_gain, best_gain)
                best_swap_flat = torch.where(update, p_flat_idx, best_swap_flat)
                best_target_p = torch.where(update, torch.tensor(p, device=device), best_target_p)
                
        # Apply best swaps
        active_batch = (best_target_p != -1)
        if not active_batch.any():
            break
            
        b_idx = batch_range[active_batch]
        swap_idx = best_swap_flat[active_batch]
        p_target = best_target_p[active_batch]
        p_max = max_pack_idx[active_batch]
        
        i_idx = swap_idx // num_items
        j_idx = swap_idx % num_items
        
        w_i = weights[b_idx, i_idx]
        w_j = weights[b_idx, j_idx]
        d = w_i - w_j
        
        # Update
        pack_loads[b_idx, p_max] -= d
        pack_loads[b_idx, p_target] += d
        
        pack_ids[b_idx, i_idx] = p_target
        pack_ids[b_idx, j_idx] = p_max
        
        r_i = ranks[b_idx, i_idx]
        r_j = ranks[b_idx, j_idx]
        ranks[b_idx, i_idx] = r_j
        ranks[b_idx, j_idx] = r_i
        
    return pack_ids, ranks, pack_loads

def _vectorized_greedy_packing_with_preassign(weights: torch.Tensor,
                                              num_packs: int,
                                              capacity: int,
                                              preassign_mask: torch.Tensor = None) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Vectorized Greedy Packing with optional pre-assignment logic.
    """
    batch_size, num_items = weights.shape
    device = weights.device
    
    pack_loads = torch.zeros(batch_size, num_packs, device=device, dtype=weights.dtype)
    pack_counts = torch.zeros(batch_size, num_packs, device=device, dtype=torch.int64)
    pack_ids = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)
    ranks = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)
    
    batch_range = torch.arange(batch_size, device=device)
    inf = torch.tensor(float('inf'), device=device, dtype=weights.dtype)

    # If preassign_mask is present, it indicates batches where the first 'num_packs' items
    # are assigned to packs 0..num_packs-1 respectively.
    start_item_idx = 0
    if preassign_mask is not None and preassign_mask.any():
        # Pre-assign first 'num_packs' items for marked batches
        # We'll do this by iterating 0..num_packs-1 manually or masking
        # For simplicity, let's just do the loop but force the choice for active batches
        pass

    for i in range(num_items):
        w = weights[:, i]
        
        # Logic for pre-assigned batches
        # If batch is in preassign_mask AND i < num_packs:
        #   force chosen_pack = i
        # Else:
        #   greedy choice
        
        valid_mask = pack_counts < capacity
        temp_loads = torch.where(valid_mask, pack_loads, inf)
        chosen_packs = temp_loads.argmin(dim=1)
        
        if preassign_mask is not None and i < num_packs:
             # Force assignment for specific batches
             forced_pack = torch.tensor(i, device=device)
             chosen_packs = torch.where(preassign_mask, forced_pack, chosen_packs)
        
        pack_ids[:, i] = chosen_packs
        ranks[:, i] = pack_counts[batch_range, chosen_packs]
        
        pack_loads[batch_range, chosen_packs] += w
        pack_counts[batch_range, chosen_packs] += 1
        
    return pack_ids, ranks, pack_loads

def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using Big-Rocks Ensemble Strategy.
    """
    num_layers, num_items = weight.shape
    device = weight.device
    num_candidates = 256
    capacity = num_items // num_packs
    
    # --- 1. Candidate Generation ---
    # Sort LPT
    lpt_val, lpt_idx = weight.sort(dim=-1, descending=True)
    
    # ZigZag
    zigzag_perm = torch.empty(num_items, device=device, dtype=torch.long)
    half = (num_items + 1) // 2
    arange = torch.arange(num_items, device=device)
    zigzag_perm[0::2] = arange[:half]
    zigzag_perm[1::2] = arange[half:].flip(0)
    
    # Candidates
    # A. Standard LPT (with and without Big Rocks)
    # B. ZigZag
    # C. Random Shuffles
    # D. Noisy Weights
    
    # Indices construction
    # Base LPT [L, 1, N]
    c_lpt = lpt_idx.unsqueeze(1)
    
    # ZigZag [L, 1, N]
    c_zigzag = lpt_idx.gather(1, zigzag_perm.view(1, -1).expand(num_layers, -1)).unsqueeze(1)
    
    # Random Shuffles (64)
    rand_perm = torch.rand(num_layers, 64, num_items, device=device).argsort(dim=-1)
    
    # Noisy Weights (190)
    # Mix of low and high noise
    num_noisy = 256 - 2 - 64 # 190
    noise = torch.rand(num_layers, num_noisy, num_items, device=device) * 0.5 + 0.75
    noisy_w = weight.unsqueeze(1) * noise
    _, c_noisy = noisy_w.sort(dim=-1, descending=True)
    
    all_indices = torch.cat([c_lpt, c_zigzag, rand_perm, c_noisy], dim=1)
    
    # Gather weights
    expanded_weight = weight.unsqueeze(1).expand(-1, num_candidates, -1)
    ordered_weights = expanded_weight.gather(2, all_indices)
    
    # Flatten
    batch_size = num_layers * num_candidates
    flat_weights = ordered_weights.view(batch_size, num_items)
    
    # --- 2. Big Rocks Logic ---
    # We want a subset of candidates to enforce "Big Rocks" separation.
    # Let's apply this to the first 64 candidates (LPT, ZigZag, some Random/Noisy).
    # num_packs must be <= items for this to make sense (always true).
    
    preassign_mask = torch.zeros(batch_size, device=device, dtype=torch.bool)
    # Mark first 64 candidates per layer
    # Reshape mask to [L, C]
    mask_view = preassign_mask.view(num_layers, num_candidates)
    mask_view[:, :64] = True
    preassign_mask = mask_view.flatten()
    
    # --- 3. Greedy Packing ---
    flat_ids, flat_ranks, flat_loads = _vectorized_greedy_packing_with_preassign(
        flat_weights, num_packs, capacity, preassign_mask
    )
    
    # --- 4. Top-K Selection ---
    loads = flat_loads.view(num_layers, num_candidates, num_packs)
    imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values
    
    k = 16
    _, best_k_indices = imbalance.topk(k, dim=1, largest=False) # [L, K]
    
    layer_offsets = (torch.arange(num_layers, device=device) * num_candidates).unsqueeze(1)
    flat_selected_idx = (layer_offsets + best_k_indices).flatten()
    
    refined_weights = flat_weights[flat_selected_idx]
    refined_ids = flat_ids[flat_selected_idx]
    refined_ranks = flat_ranks[flat_selected_idx]
    refined_loads = flat_loads[flat_selected_idx]
    
    # --- 5. L2 Refinement ---
    refined_ids, refined_ranks, refined_loads = _refine_packing_l2(
        refined_weights, refined_ids, refined_loads, refined_ranks, num_packs, num_iters=20
    )
    
    # --- 6. Final Selection ---
    loads_final = refined_loads.view(num_layers, k, num_packs)
    imbalance_final = loads_final.max(dim=-1).values - loads_final.min(dim=-1).values
    best_in_k = imbalance_final.argmin(dim=1)
    
    # Scatter back
    winner_cand_idx = best_k_indices.gather(1, best_in_k.unsqueeze(1)).squeeze(1)
    
    winner_flat_idx = (torch.arange(num_layers, device=device) * k) + best_in_k
    final_aligned_ids = refined_ids[winner_flat_idx]
    final_aligned_ranks = refined_ranks[winner_flat_idx]
    
    idx_view = winner_cand_idx.view(num_layers, 1, 1).expand(-1, 1, num_items)
    final_sorted_idx = all_indices.gather(1, idx_view).squeeze(1)
    
    pack_index = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    rank_in_pack = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    
    pack_index.scatter_(1, final_sorted_idx, final_aligned_ids)
    rank_in_pack.scatter_(1, final_sorted_idx, final_aligned_ranks)
    
    return pack_index, rank_in_pack

def replicate_experts(
        weight: torch.Tensor,
        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Replicate experts using Binary Search on Max Load followed by Greedy Refinement.
    """
    num_layers, num_log = weight.shape
    device = weight.device

    if num_phy == num_log:
        phy2log = torch.arange(num_log, device=device).expand(num_layers, -1)
        rank = torch.zeros(num_layers, num_phy, dtype=torch.int64, device=device)
        logcnt = torch.ones(num_layers, num_log, dtype=torch.int64, device=device)
        return phy2log, rank, logcnt

    # Binary Search
    low = weight.sum(dim=-1, keepdim=True) / num_phy
    high = weight.max(dim=-1, keepdim=True).values
    low = torch.max(low, torch.tensor(1e-6, device=device))

    for _ in range(15):
        mid = (low + high) * 0.5
        counts = torch.ceil(weight / mid)
        total = counts.sum(dim=-1, keepdim=True)
        mask = total <= num_phy
        high = torch.where(mask, mid, high)
        low = torch.where(mask, low, mid)

    logcnt = torch.ceil(weight / high).long().clamp(min=1)
    current_sum = logcnt.sum(dim=-1)
    diff = num_phy - current_sum

    max_diff = int(diff.max().item())
    if max_diff > 0:
        rows = torch.arange(num_layers, device=device)
        for _ in range(max_diff):
            active = current_sum < num_phy
            if not active.any(): break
            density = weight / logcnt.float()
            density[~active] = -1.0
            target_idx = density.argmax(dim=-1)
            active_rows = rows[active]
            active_targets = target_idx[active]
            logcnt.index_put_((active_rows, active_targets), 
                              torch.tensor(1, device=device, dtype=torch.int64), accumulate=True)
            current_sum[active] += 1

    min_diff = int(diff.min().item())
    if min_diff < 0:
        rows = torch.arange(num_layers, device=device)
        for _ in range(abs(min_diff)):
            active = current_sum > num_phy
            if not active.any(): break
            valid = logcnt > 1
            cost = weight / (logcnt - 1).float()
            cost[~valid] = float('inf')
            cost[~active] = float('inf')
            target_idx = cost.argmin(dim=-1)
            active_rows = rows[active]
            active_targets = target_idx[active]
            logcnt.index_put_((active_rows, active_targets), 
                              torch.tensor(-1, device=device, dtype=torch.int64), accumulate=True)
            current_sum[active] -= 1

    flat_log_ids = torch.arange(num_log, device=device).repeat(num_layers)
    flat_counts = logcnt.flatten()
    flat_phy2log = torch.repeat_interleave(flat_log_ids, flat_counts)
    
    target_size = num_layers * num_phy
    if flat_phy2log.numel() < target_size:
        flat_phy2log = torch.cat([flat_phy2log, torch.zeros(target_size - flat_phy2log.numel(), device=device, dtype=torch.long)])
    else:
        flat_phy2log = flat_phy2log[:target_size]
            
    phy2log = flat_phy2log.view(num_layers, num_phy)
    offsets = torch.zeros_like(logcnt)
    offsets[:, 1:] = logcnt[:, :-1].cumsum(dim=1)
    mapped_offsets = offsets.gather(1, phy2log)
    phy_indices = torch.arange(num_phy, device=device).expand(num_layers, -1)
    rank = phy_indices - mapped_offsets

    return phy2log, rank, logcnt

def rebalance_experts_hierarchical(
    weight: torch.Tensor,
    num_physical_experts: int,
    num_groups: int,
    num_nodes: int,
    num_gpus: int,
):
    num_layers, num_logical_experts = weight.shape
    group_size = num_logical_experts // num_groups
    groups_per_node = num_groups // num_nodes
    phy_experts_per_gpu = num_physical_experts // num_gpus

    def inverse(perm: torch.Tensor) -> torch.Tensor:
        inv = torch.empty_like(perm)
        inv.scatter_(1, perm, torch.arange(perm.size(1), dtype=torch.int64, device=perm.device).expand(perm.shape))
        return inv

    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)
    group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)
        
    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +
                torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)
    mlog2log = inverse(log2mlog)

    tokens_per_mlog = weight.gather(-1, mlog2log).view(-1, num_logical_experts // num_nodes)
    phy2mlog, phyrank, mlogcnt = replicate_experts(tokens_per_mlog, num_physical_experts // num_nodes)

    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
    pack_index, rank_in_pack = balanced_packing(tokens_per_phy, num_gpus // num_nodes)
    
    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
    pphy2phy = inverse(phy2pphy)

    pphy2mlog = phy2mlog.gather(-1, pphy2phy)
    node_offsets = torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=weight.device).view(1, -1, 1)
    pphy2mlog_restored = (pphy2mlog.view(num_layers, num_nodes, -1) + node_offsets).flatten(-2)
    
    pphy2log = mlog2log.gather(-1, pphy2mlog_restored)
    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)
    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)
    
    return pphy2log, pphyrank, logcnt

def rebalance_experts(
    weight: torch.Tensor,
    num_replicas: int,
    num_groups: int,
    num_nodes: int,
    num_gpus: int,
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    num_layers, num_logical_experts = weight.shape
    weight = weight.float().cpu()
    
    if num_groups % num_nodes == 0:
        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(weight, num_replicas, num_groups, num_nodes, num_gpus)
    else:
        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(weight, num_replicas, 1, 1, num_gpus)
            
    max_replicas = int(logcnt.max().item())
    log2phy = torch.full((num_layers, num_logical_experts, max_replicas), -1, dtype=torch.int64, device=logcnt.device)
    
    flat_layer_idx = torch.arange(num_layers, device=logcnt.device).unsqueeze(-1).expand(-1, num_replicas).flatten()
    flat_log_idx = phy2log.flatten()
    flat_rank_idx = phyrank.flatten()
    flat_phy_ids = torch.arange(num_replicas, dtype=torch.int64, device=logcnt.device).expand(num_layers, -1).flatten()
    
    indices = (flat_layer_idx * num_logical_experts * max_replicas) + (flat_log_idx * max_replicas) + flat_rank_idx
    log2phy.view(-1).scatter_(0, indices, flat_phy_ids)
    
    return phy2log, log2phy, logcnt
# EVOLVE-BLOCK-END