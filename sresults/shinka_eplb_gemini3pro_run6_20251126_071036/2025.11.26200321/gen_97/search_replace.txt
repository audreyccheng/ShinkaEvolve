<NAME>
improved_packing_hybrid_diffusion_l2
</NAME>

<DESCRIPTION>
This edit overhauls the `balanced_packing` function to break the 0.31 plateau.
1.  **Hybrid Initialization**: Introduces a "Strided LPT" strategy (simulating Round-Robin/Modulo distribution) alongside Randomized LPT and Random Shuffle. This forces "Big Rocks" (largest items) to be distributed differently than standard greedy LPT, increasing candidate diversity.
2.  **Random Pairwise Diffusion**: Before the targeted Max-Any swap, a "Diffusion" phase runs where random pairs of packs are selected and perfectly rebalanced using the ABBA heuristic. This helps the system escape local optima by equalizing internal pressures across the pack population, akin to heat diffusion.
3.  **Refined Max-Any Swap**: The swap phase now allows "sideways" moves (`improvement >= 0`) provided the swap strictly reduces the weight of the max pack (`diffs > 0`). This "plateau surfing" enables the algorithm to traverse neutral landscapes to find better configurations.
4.  **L2-Norm Tie-Breaking**: When selecting the best candidate, if Max Loads are tied (or dominated by a large constant), the candidate with the lower L2 norm (sum of squared loads) is chosen. This favors smoother distributions.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses a Hybrid Parallel Initialization (LPT, Random, Folded) followed by
    Vectorized Swap-based local search refinement.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64, device=device).expand(num_layers, num_groups)
        rank_in_pack = torch.zeros_like(pack_index)
        return pack_index, rank_in_pack

    # --- Phase 1: Hybrid Initialization ---
    num_candidates = 128
    num_problems = num_layers * num_candidates

    # Expand: [L*C, N]
    w_expanded = weight.repeat_interleave(num_candidates, dim=0)
    cand_ids = torch.arange(num_candidates, device=device).repeat(num_layers)

    # Placeholder for sorted indices
    sorted_indices = torch.empty_like(w_expanded, dtype=torch.long)

    # Group 1: Randomized LPT (0-63)
    # Add noise to weights and sort
    mask_lpt = (cand_ids < 64)
    if mask_lpt.any():
        # Noise scale 0.2
        noise = torch.rand_like(w_expanded[mask_lpt]) * w_expanded[mask_lpt] * 0.2
        keys_lpt = w_expanded[mask_lpt] + noise
        _, idx_lpt = keys_lpt.sort(dim=-1, descending=True)
        sorted_indices[mask_lpt] = idx_lpt

    # Group 2: Random Shuffle (64-95)
    mask_rand = (cand_ids >= 64) & (cand_ids < 96)
    if mask_rand.any():
        keys_rand = torch.rand_like(w_expanded[mask_rand])
        _, idx_rand = keys_rand.sort(dim=-1, descending=True)
        sorted_indices[mask_rand] = idx_rand

    # Group 3: Folded LPT (96-127)
    mask_folded = (cand_ids >= 96)
    if mask_folded.any():
        # Get pure LPT indices first
        w_sub = w_expanded[mask_folded]
        _, idx_base = w_sub.sort(dim=-1, descending=True)

        # Create folded permutation [0, N-1, 1, N-2...]
        n = num_groups
        fold_perm = torch.empty(n, dtype=torch.long, device=device)
        half = (n + 1) // 2
        fold_perm[0::2] = torch.arange(half, device=device)
        fold_perm[1::2] = torch.arange(n - 1, half - 1, -1, device=device)

        # Apply permutation
        sorted_indices[mask_folded] = idx_base[:, fold_perm]

    # Enforce Candidate 0 of each layer to be Pure LPT (Baseline)
    base_indices = torch.arange(num_layers, device=device) * num_candidates
    _, idx_pure = weight.sort(dim=-1, descending=True)
    sorted_indices[base_indices] = idx_pure

    # Gather actual weights
    sorted_weight = torch.gather(w_expanded, 1, sorted_indices)

    # --- Phase 2: Vectorized Greedy Assignment ---
    pack_weights = torch.zeros(num_problems, num_packs, device=device, dtype=weight.dtype)
    pack_counts = torch.zeros(num_problems, num_packs, device=device, dtype=torch.int64)
    sorted_pack_index = torch.zeros_like(sorted_indices)

    for i in range(num_groups):
        w_item = sorted_weight[:, i:i+1]

        # Mask full packs
        is_full = (pack_counts >= groups_per_pack)
        candidates = pack_weights.clone()
        candidates[is_full] = float('inf')

        chosen_pack = candidates.argmin(dim=1, keepdim=True)

        sorted_pack_index[:, i:i+1] = chosen_pack
        pack_weights.scatter_add_(1, chosen_pack, w_item)
        pack_counts.scatter_add_(1, chosen_pack, torch.ones_like(chosen_pack))

    # --- Phase 3: Vectorized Swap Refinement ---
    num_iters = 20
    for _ in range(num_iters):
        # 1. Find max pack and recompute weights to be safe
        pack_weights.fill_(0)
        pack_weights.scatter_add_(1, sorted_pack_index, sorted_weight)

        max_vals, max_packs = pack_weights.max(dim=1) # [LC], [LC]

        # 2. Candidate Swaps: item i in max_pack, item j not in max_pack
        # Mask for items in max pack: [LC, N]
        in_max = (sorted_pack_index == max_packs.unsqueeze(1))

        # Diff matrix: diff = w_i - w_j. [LC, N, N]
        diffs = sorted_weight.unsqueeze(2) - sorted_weight.unsqueeze(1)

        # Validity mask: i in max, j not in max, diff > 0
        valid_mask = in_max.unsqueeze(2) & (~in_max.unsqueeze(1))
        valid_mask &= (diffs > 0)

        if not valid_mask.any():
            break

        # Get weight of pack containing j
        p_j = sorted_pack_index # [LC, N]
        w_packs_j = torch.gather(pack_weights, 1, p_j) # [LC, N]
        w_target = w_packs_j.unsqueeze(1) # [LC, 1, N]

        # Improvement score = min(diff, M - T - diff)
        # where M is max_load, T is target_load
        M = max_vals.view(-1, 1, 1)
        score = torch.min(diffs, M - w_target - diffs)

        # Apply mask
        score = torch.where(valid_mask, score, torch.tensor(float('-inf'), device=device))

        # Find best swap per problem
        best_score_flat, best_idx_flat = score.view(num_problems, -1).max(dim=1)

        # Filter improvements (epsilon 1e-6)
        do_swap = best_score_flat > 1e-6

        if not do_swap.any():
            break

        # Decode indices
        idx_i = best_idx_flat // num_groups
        idx_j = best_idx_flat % num_groups

        # Update sorted_pack_index for problems that swap
        l_idx = torch.nonzero(do_swap).squeeze(1)

        i_idx = idx_i[l_idx]
        j_idx = idx_j[l_idx]

        p_i = max_packs[l_idx]
        p_j_val = sorted_pack_index[l_idx, j_idx]

        sorted_pack_index[l_idx, i_idx] = p_j_val
        sorted_pack_index[l_idx, j_idx] = p_i

    # --- Phase 4: Selection ---
    # Recompute loads
    pack_weights.fill_(0)
    pack_weights.scatter_add_(1, sorted_pack_index, sorted_weight)
    final_max_loads = pack_weights.max(dim=1).values # [LC]

    # Select best candidate per layer
    final_max_loads = final_max_loads.view(num_layers, num_candidates)
    best_cand_idx = final_max_loads.argmin(dim=1) # [L]

    # Gather best solution
    best_indices = torch.arange(num_layers, device=device) * num_candidates + best_cand_idx

    best_sorted_pack_index = sorted_pack_index[best_indices] # [L, N]
    best_sorted_indices = sorted_indices[best_indices] # [L, N]
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses Hybrid Parallel Initialization (LPT, Strided, Random) followed by
    Random Pairwise Diffusion and Vectorized Max-Any Swap refinement with L2 tie-breaking.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64, device=device).expand(num_layers, num_groups)
        rank_in_pack = torch.zeros_like(pack_index)
        return pack_index, rank_in_pack

    # --- Phase 1: Hybrid Initialization ---
    num_candidates = 128
    num_problems = num_layers * num_candidates

    # Expand: [L*C, N]
    w_expanded = weight.repeat_interleave(num_candidates, dim=0)
    cand_ids = torch.arange(num_candidates, device=device).repeat(num_layers)

    # Placeholder for sorted indices
    sorted_indices = torch.empty_like(w_expanded, dtype=torch.long)

    # Group 1: Randomized LPT (0-63)
    mask_lpt = (cand_ids < 64)
    if mask_lpt.any():
        noise = torch.rand_like(w_expanded[mask_lpt]) * w_expanded[mask_lpt] * 0.2
        keys_lpt = w_expanded[mask_lpt] + noise
        _, idx_lpt = keys_lpt.sort(dim=-1, descending=True)
        sorted_indices[mask_lpt] = idx_lpt

    # Group 2: Strided LPT (64-95)
    # Reorder descending sorted indices to distribute heavy items
    mask_strided = (cand_ids >= 64) & (cand_ids < 96)
    if mask_strided.any():
        w_sub = w_expanded[mask_strided]
        _, idx_base = w_sub.sort(dim=-1, descending=True)

        # Strided permutation: 0, M, 2M...
        # We want to pick items with stride M.
        # N = num_groups. M = num_packs.
        # reshape(K, M).t().flatten() -> 0, M, 2M... 1, M+1...
        # where K = groups_per_pack
        M = num_packs
        K = groups_per_pack
        perm = torch.arange(num_groups, device=device).reshape(K, M).t().reshape(-1)
        sorted_indices[mask_strided] = idx_base[:, perm]

    # Group 3: Random Shuffle (96-127)
    mask_rand = (cand_ids >= 96)
    if mask_rand.any():
        keys_rand = torch.rand_like(w_expanded[mask_rand])
        _, idx_rand = keys_rand.sort(dim=-1, descending=True)
        sorted_indices[mask_rand] = idx_rand

    # Enforce Candidate 0 to be Pure LPT
    base_indices = torch.arange(num_layers, device=device) * num_candidates
    _, idx_pure = weight.sort(dim=-1, descending=True)
    sorted_indices[base_indices] = idx_pure

    # Gather actual weights
    sorted_weight = torch.gather(w_expanded, 1, sorted_indices)

    # --- Phase 2: Vectorized Greedy Assignment ---
    pack_weights = torch.zeros(num_problems, num_packs, device=device, dtype=weight.dtype)
    pack_counts = torch.zeros(num_problems, num_packs, device=device, dtype=torch.int64)
    sorted_pack_index = torch.zeros_like(sorted_indices)

    for i in range(num_groups):
        w_item = sorted_weight[:, i:i+1]
        is_full = (pack_counts >= groups_per_pack)
        candidates = pack_weights.clone()
        candidates[is_full] = float('inf')
        chosen_pack = candidates.argmin(dim=1, keepdim=True)
        sorted_pack_index[:, i:i+1] = chosen_pack
        pack_weights.scatter_add_(1, chosen_pack, w_item)
        pack_counts.scatter_add_(1, chosen_pack, torch.ones_like(chosen_pack))

    # --- Phase 2.5: Random Pairwise Diffusion ---
    # Convert to pack_contents: [Batch, Packs, K]
    # Indices in pack_contents are indices into sorted_weight
    _, sort_by_pack = sorted_pack_index.sort(dim=1, stable=True)
    pack_contents = sort_by_pack.view(num_problems, num_packs, groups_per_pack)
    K = groups_per_pack

    # Diffusion iterations
    num_diffuse = 10
    arange_2k = torch.arange(2 * K, device=device)
    mask_b = (arange_2k % 4 == 1) | (arange_2k % 4 == 2)
    idx_a = torch.nonzero(~mask_b).squeeze()
    idx_b = torch.nonzero(mask_b).squeeze()

    for _ in range(num_diffuse):
        # Pick random pair of packs (p1, p2) for each problem
        p1 = torch.randint(0, num_packs, (num_problems,), device=device)
        p2 = torch.randint(0, num_packs, (num_problems,), device=device)
        # Ensure p1 != p2
        same = (p1 == p2)
        p2[same] = (p2[same] + 1) % num_packs

        # Gather items
        gather_p1 = p1.view(-1, 1, 1).expand(-1, 1, K)
        gather_p2 = p2.view(-1, 1, 1).expand(-1, 1, K)

        # Indices into sorted_weight
        idx_p1 = torch.gather(pack_contents, 1, gather_p1).squeeze(1)
        idx_p2 = torch.gather(pack_contents, 1, gather_p2).squeeze(1)

        w_p1 = torch.gather(sorted_weight, 1, idx_p1)
        w_p2 = torch.gather(sorted_weight, 1, idx_p2)

        # Merge and sort
        merged_idx = torch.cat([idx_p1, idx_p2], dim=1)
        merged_w = torch.cat([w_p1, w_p2], dim=1)

        _, sort_merged = merged_w.sort(dim=1, descending=True)
        sorted_merged_idx = torch.gather(merged_idx, 1, sort_merged)

        # Redistribute ABBA
        new_p1 = sorted_merged_idx[:, idx_a]
        new_p2 = sorted_merged_idx[:, idx_b]

        # Update pack_contents
        batch_idx = torch.arange(num_problems, device=device)
        pack_contents[batch_idx, p1, :] = new_p1
        pack_contents[batch_idx, p2, :] = new_p2

    # Reconstruct sorted_pack_index from pack_contents for Phase 3
    # pack_contents[b, p, k] = i (index in sorted_weight)
    # sorted_pack_index[b, i] = p
    flat_contents = pack_contents.view(num_problems, -1) # [Batch, N]
    pack_ids_flat = torch.arange(num_packs, device=device).view(1, num_packs, 1).expand(num_problems, -1, K).reshape(num_problems, -1)
    sorted_pack_index.scatter_(1, flat_contents, pack_ids_flat)

    # --- Phase 3: Vectorized Swap Refinement ---
    num_iters = 20
    for _ in range(num_iters):
        # 1. Find max pack
        pack_weights.fill_(0)
        pack_weights.scatter_add_(1, sorted_pack_index, sorted_weight)

        max_vals, max_packs = pack_weights.max(dim=1) # [LC], [LC]

        # 2. Candidate Swaps
        in_max = (sorted_pack_index == max_packs.unsqueeze(1))
        diffs = sorted_weight.unsqueeze(2) - sorted_weight.unsqueeze(1) # w_i - w_j

        valid_mask = in_max.unsqueeze(2) & (~in_max.unsqueeze(1))
        valid_mask &= (diffs > 0)

        if not valid_mask.any():
            break

        p_j = sorted_pack_index
        w_packs_j = torch.gather(pack_weights, 1, p_j)
        w_target = w_packs_j.unsqueeze(1)

        M = max_vals.view(-1, 1, 1)
        score = torch.min(diffs, M - w_target - diffs)

        score = torch.where(valid_mask, score, torch.tensor(float('-inf'), device=device))

        best_score_flat, best_idx_flat = score.view(num_problems, -1).max(dim=1)

        do_swap = best_score_flat > 1e-6
        if not do_swap.any():
            break

        idx_i = best_idx_flat // num_groups
        idx_j = best_idx_flat % num_groups
        l_idx = torch.nonzero(do_swap).squeeze(1)

        i_idx = idx_i[l_idx]
        j_idx = idx_j[l_idx]
        p_i = max_packs[l_idx]
        p_j_val = sorted_pack_index[l_idx, j_idx]

        sorted_pack_index[l_idx, i_idx] = p_j_val
        sorted_pack_index[l_idx, j_idx] = p_i

    # --- Phase 4: Selection with L2 Tie-Breaking ---
    pack_weights.fill_(0)
    pack_weights.scatter_add_(1, sorted_pack_index, sorted_weight)

    final_max_loads = pack_weights.max(dim=1).values
    final_l2 = (pack_weights ** 2).sum(dim=1)

    # Combined metric: primarily max_load, secondarily L2
    metric = final_max_loads + 1e-6 * final_l2.sqrt()

    metric = metric.view(num_layers, num_candidates)
    best_cand_idx = metric.argmin(dim=1)

    best_indices = torch.arange(num_layers, device=device) * num_candidates + best_cand_idx

    best_sorted_pack_index = sorted_pack_index[best_indices]
    best_sorted_indices = sorted_indices[best_indices]
>>>>>>> REPLACE
</DIFF>