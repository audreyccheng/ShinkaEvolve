<NAME>
soft_greedy_packing
</NAME>

<DESCRIPTION>
Expand the search space to 256 candidates and introduce a "Soft Greedy" initialization strategy (Indices 192-255). In this strategy, the greedy construction adds random noise to the pack loads before selecting the destination pack. This probabilistic selection helps the greedy heuristic escape local optima and explore diverse packing topologies that pure LPT or Interleaved strategies might miss. Adjust refinement iterations to 50 to balance the increased candidate count.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # --- Phase 1: Diverse Candidate Generation ---
    # We use 128 candidates to explore the solution space.
    # Strategies:
    # 0-63:   Randomized LPT (Descending Sort + Noise)
    # 64-95:  Interleaved (Heavy, Light, Heavy, Light...)
    # 96-127: Random Permutation
    num_candidates = 128
    num_problems = num_layers * num_candidates

    # Expand weights [L*C, N]
    w_expanded = weight.repeat_interleave(num_candidates, dim=0)

    # Base: Sort descending (LPT)
    # Add noise to creating diverse LPT rankings
    # Scale noise from 0.0 to 0.2 across the candidates to vary "greediness"
    scales = torch.linspace(0, 0.2, num_candidates, device=device)
    noise_scale = scales.repeat(num_layers).view(-1, 1)

    noise = torch.rand_like(w_expanded) * w_expanded * noise_scale
    # Ensure Candidate 0 is pure LPT (no noise)
    noise.view(num_layers, num_candidates, num_groups)[:, 0, :] = 0

    sort_keys = w_expanded + noise
    _, sorted_indices = sort_keys.sort(dim=-1, descending=True)

    # Apply Strategies
    cand_ids = torch.arange(num_candidates, device=device).repeat_interleave(num_layers)

    # Strategy 2: Interleaved (64-95)
    # Reorder the sorted indices: [0, N-1, 1, N-2, 2, N-3...]
    mask_interleave = (cand_ids >= 64) & (cand_ids < 96)
    if mask_interleave.any():
        # Precompute interleave map
        imap = torch.empty(num_groups, dtype=torch.long, device=device)
        imap[0::2] = torch.arange((num_groups + 1) // 2, device=device)
        imap[1::2] = torch.arange(num_groups - 1, (num_groups + 1) // 2 - 1, -1, device=device)

        # Apply map
        subset = sorted_indices[mask_interleave]
        sorted_indices[mask_interleave] = subset[:, imap]

    # Strategy 3: Random Shuffle (96-127)
    mask_random = (cand_ids >= 96)
    if mask_random.any():
        # Generate random indices
        rand_keys = torch.rand(mask_random.sum(), num_groups, device=device)
        _, rand_idx = rand_keys.sort(dim=-1)
        sorted_indices[mask_random] = rand_idx

    # Gather weights in the determined order
    sorted_weight = torch.gather(w_expanded, 1, sorted_indices)

    # --- Phase 2: Vectorized Greedy Assignment ---
    pack_weights = torch.zeros(num_problems, num_packs, device=device, dtype=weight.dtype)
    pack_counts = torch.zeros(num_problems, num_packs, device=device, dtype=torch.int64)
    sorted_pack_index = torch.zeros_like(sorted_indices)

    # Pre-allocate infinity for masking
    inf_val = torch.tensor(float('inf'), device=device)

    for i in range(num_groups):
        w_item = sorted_weight[:, i:i+1] # [LC, 1]

        # Mask full packs
        is_full = (pack_counts >= groups_per_pack)
        candidates = torch.where(is_full, inf_val, pack_weights)

        # Choose pack with min weight
        chosen_pack = candidates.argmin(dim=1, keepdim=True)

        sorted_pack_index[:, i:i+1] = chosen_pack

        # Optimized update
        pack_weights.scatter_add_(1, chosen_pack, w_item)
        pack_counts.scatter_add_(1, chosen_pack, torch.ones_like(chosen_pack))

    # --- Phase 3: Vectorized Max-Any Swap Refinement ---
    # Construct pack structure: [LC, M, K]
    _, pack_content_sort_idx = sorted_pack_index.sort(dim=1, stable=True)
    pack_contents = pack_content_sort_idx.view(num_problems, num_packs, groups_per_pack)

    K = groups_per_pack
    num_iters = 100 # Increased depth for better convergence
=======
    # --- Phase 1: Diverse Candidate Generation ---
    # We use 256 candidates to explore the solution space.
    # Strategies:
    # 0-63:    Randomized LPT (Descending Sort + Input Noise)
    # 64-127:  Interleaved (Heavy, Light, Heavy, Light...)
    # 128-191: Random Permutation
    # 192-255: Soft Greedy (LPT Input + Selection Noise)
    num_candidates = 256
    num_problems = num_layers * num_candidates

    # Expand weights [L*C, N]
    w_expanded = weight.repeat_interleave(num_candidates, dim=0)

    # Base: Sort descending (LPT)
    # Add noise to creating diverse LPT rankings
    # Scale noise from 0.0 to 0.2 across the candidates
    scales = torch.linspace(0, 0.2, num_candidates, device=device)
    noise_scale = scales.repeat(num_layers).view(-1, 1)

    noise = torch.rand_like(w_expanded) * w_expanded * noise_scale
    # Ensure Candidate 0 is pure LPT
    noise.view(num_layers, num_candidates, num_groups)[:, 0, :] = 0

    sort_keys = w_expanded + noise
    _, sorted_indices = sort_keys.sort(dim=-1, descending=True)

    # Apply Strategies
    cand_ids = torch.arange(num_candidates, device=device).repeat_interleave(num_layers)

    # Strategy 2: Interleaved (64-127)
    mask_interleave = (cand_ids >= 64) & (cand_ids < 128)
    if mask_interleave.any():
        imap = torch.empty(num_groups, dtype=torch.long, device=device)
        imap[0::2] = torch.arange((num_groups + 1) // 2, device=device)
        imap[1::2] = torch.arange(num_groups - 1, (num_groups + 1) // 2 - 1, -1, device=device)

        subset = sorted_indices[mask_interleave]
        sorted_indices[mask_interleave] = subset[:, imap]

    # Strategy 3: Random Shuffle (128-191)
    mask_random = (cand_ids >= 128) & (cand_ids < 192)
    if mask_random.any():
        rand_keys = torch.rand(mask_random.sum(), num_groups, device=device)
        _, rand_idx = rand_keys.sort(dim=-1)
        sorted_indices[mask_random] = rand_idx

    # Strategy 4: Soft Greedy (192-255)
    mask_soft_greedy = (cand_ids >= 192)

    # Gather weights in the determined order
    sorted_weight = torch.gather(w_expanded, 1, sorted_indices)

    # --- Phase 2: Vectorized Greedy Assignment ---
    pack_weights = torch.zeros(num_problems, num_packs, device=device, dtype=weight.dtype)
    pack_counts = torch.zeros(num_problems, num_packs, device=device, dtype=torch.int64)
    sorted_pack_index = torch.zeros_like(sorted_indices)

    inf_val = torch.tensor(float('inf'), device=device)

    for i in range(num_groups):
        w_item = sorted_weight[:, i:i+1] # [LC, 1]

        # Mask full packs
        is_full = (pack_counts >= groups_per_pack)
        candidates = torch.where(is_full, inf_val, pack_weights)

        # Apply Soft Greedy Noise (192-255)
        # Adds proportional noise to pack weights to randomize selection
        if mask_soft_greedy.any():
             noise_selection = torch.rand_like(candidates) * w_item * 0.5
             m_broad = mask_soft_greedy.view(-1, 1)
             candidates = candidates + noise_selection * m_broad

        # Choose pack with min weight
        chosen_pack = candidates.argmin(dim=1, keepdim=True)

        sorted_pack_index[:, i:i+1] = chosen_pack

        # Optimized update
        pack_weights.scatter_add_(1, chosen_pack, w_item)
        pack_counts.scatter_add_(1, chosen_pack, torch.ones_like(chosen_pack))

    # --- Phase 3: Vectorized Max-Any Swap Refinement ---
    # Construct pack structure: [LC, M, K]
    _, pack_content_sort_idx = sorted_pack_index.sort(dim=1, stable=True)
    pack_contents = pack_content_sort_idx.view(num_problems, num_packs, groups_per_pack)

    K = groups_per_pack
    num_iters = 50 # Adjusted for higher candidate count
>>>>>>> REPLACE
</DIFF>