

<DIFF>
<<<<<<< SEARCH
    # Binary Search
    low = weight.sum(dim=-1, keepdim=True) / num_phy
    high = weight.max(dim=-1, keepdim=True).values
    low = torch.max(low, torch.tensor(1e-6, device=device))

    for _ in range(15):
        mid = (low + high) * 0.5
        counts = torch.ceil(weight / mid)
=======
    # Binary Search
    low = weight.sum(dim=-1, keepdim=True) / num_phy
    high = weight.max(dim=-1, keepdim=True).values
    low = torch.max(low, torch.tensor(1e-4, device=device))

    for _ in range(15):
        mid = (low + high) * 0.5
        counts = torch.ceil(weight / mid)
>>>>>>> REPLACE
</DIFF>

The changes are consistent.
I replaced `_refine_packing` with a gathered implementation.
I updated `balanced_packing` to use the new refinement and added "Random" candidates (replacing some Noisy LPT).
I tweaked `replicate_experts` slightly (epsilon 1e-6 -> 1e-4) to be safe, though 1e-6 is fine. 1e-4 is safer for float32 precision near zero load.

Note: `_refine_packing` now relies on `capacity` calculation. I derived it from `(num_items + num_packs - 1) // num_packs` which matches `ceil(N/M)`. This ensures `ranks` fit in `0..capacity-1`.

One logic verification:
In `balanced_packing` I used `all_weights` constructed by gathering.
`all_weights` matches the order of `all_indices`.
`flat_weights` passed to `_refine_packing` corresponds to `weights` in that function.
`pack_ids`, `ranks` correspond to `weights`.
In `_refine_packing`: `bin_weights_flat.scatter_(1, flat_pos, weights)`.
This correctly places the weight of item `k` (which is `weights[k]`) into the bin `pack_ids[k]` at rank `ranks[k]`.
This is correct.

Double check `flat_pos` update:
`flat_pos` is `[Batch, N]`.
`flat_pos[batch_active, item_i] = p_min * capacity + r_j`.
`p_min` is the NEW pack for `item_i`. `r_j` is the NEW rank for `item_i` (formerly rank of item_j).
This is correct.

I am confident in this edit.