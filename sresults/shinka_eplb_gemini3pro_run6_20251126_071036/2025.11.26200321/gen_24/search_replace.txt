<NAME>
gpu_parallel_candidates
</NAME>

<DESCRIPTION>
This edit replaces the CPU-based `balanced_packing` function with a high-performance GPU-accelerated version using massive candidate parallelism and vectorized operations.

1.  **Massive Parallel Candidate Expansion**: The input problem is replicated `num_candidates = 64` times.
2.  **Noise Spectrum Initialization**: A noise spectrum (from 0.0 to 0.2) is applied to the weights of these candidates before sorting (Randomized LPT). This allows exploring diverse initial greedy packings, from pure LPT (0 noise) to more randomized configurations.
3.  **Vectorized Greedy Assignment**: Replaces the nested loops with a vectorized loop over items, filling packs efficiently on the GPU.
4.  **Vectorized Max-Min Local Search**: Implements a batched local search that attempts to swap items between the heaviest and lightest packs to minimize the L2 norm of pack weights (which correlates with minimizing max load).
5.  **Device Awareness**: Modifies `rebalance_experts` to keep tensors on the GPU (removing the forced `.cpu()` transfer), allowing the entire pipeline to run on the accelerator.

This approach exploits the perfect speed score capability to significantly improve the balancedness score by exploring a much larger solution space in parallel.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=weight.device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    indices = weight.float().sort(-1, descending=True).indices.cpu()
    pack_index = torch.full_like(weight,
                                 fill_value=-1,
                                 dtype=torch.int64,
                                 device="cpu")
    rank_in_pack = torch.full_like(pack_index, fill_value=-1)
    for i in range(num_layers):
        pack_weights = [0] * num_packs
        pack_items = [0] * num_packs
        for group in indices[i]:
            pack = min(
                (i
                 for i in range(num_packs) if pack_items[i] < groups_per_pack),
                key=pack_weights.__getitem__,
            )
            assert pack_items[pack] < groups_per_pack
            pack_index[i, group] = pack
            rank_in_pack[i, group] = pack_items[pack]
            pack_weights[pack] += weight[i, group]
            pack_items[pack] += 1
    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses a Parallel Randomized Greedy LPT initialization followed by a
    Vectorized Swap-based local search refinement on GPU.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64, device=device).expand(num_layers, num_groups)
        rank_in_pack = torch.zeros_like(pack_index)
        return pack_index, rank_in_pack

    # --- Parallel Candidate Expansion ---
    num_candidates = 64
    num_problems = num_layers * num_candidates

    # [L*C, N]
    w_expanded = weight.repeat_interleave(num_candidates, dim=0)

    # Noise Spectrum: 0.0 to 0.2
    # Candidates 0 in each layer group will have 0 noise (Pure LPT)
    scales = torch.linspace(0, 0.2, num_candidates, device=device)
    scales_expanded = scales.repeat(num_layers).view(-1, 1)

    # Apply noise
    noise = torch.rand_like(w_expanded) * w_expanded * scales_expanded
    sort_keys = w_expanded + noise

    # Sort descending [L*C, N]
    _, sorted_indices = sort_keys.sort(dim=-1, descending=True)
    sorted_weight = torch.gather(w_expanded, 1, sorted_indices)

    # --- Phase 1: Vectorized Greedy LPT ---
    pack_weights = torch.zeros(num_problems, num_packs, device=device, dtype=weight.dtype)
    pack_counts = torch.zeros(num_problems, num_packs, device=device, dtype=torch.int64)
    sorted_pack_index = torch.zeros_like(sorted_indices)

    # We fill items one by one into the emptiest available pack
    # This loop is N iterations, where N is usually small (e.g. 64-256)
    for i in range(num_groups):
        w_item = sorted_weight[:, i:i+1] # [L*C, 1]

        # Mask full packs
        is_full = (pack_counts >= groups_per_pack)
        masked_w = pack_weights.clone()
        masked_w[is_full] = float('inf')

        # Choose pack with min weight
        chosen_pack = masked_w.argmin(dim=1, keepdim=True)

        sorted_pack_index[:, i:i+1] = chosen_pack
        pack_weights.scatter_add_(1, chosen_pack, w_item)
        pack_counts.scatter_add_(1, chosen_pack, torch.ones_like(chosen_pack))

    # --- Phase 2: Vectorized Swap Refinement (Max-Min) ---
    # We want to swap items between the heaviest and lightest packs to balance them.

    # Reconstruct pack contents for swapping: [L*C, M, K]
    # We sort the 'sorted_pack_index' to group items by pack.
    _, pack_content_idx = sorted_pack_index.sort(dim=1, stable=True)
    pack_contents = pack_content_idx.view(num_problems, num_packs, groups_per_pack)

    num_iters = 20
    for _ in range(num_iters):
        # Current weights of items in packs
        flat_contents = pack_contents.view(num_problems, -1)
        curr_items = torch.gather(sorted_weight, 1, flat_contents).view(num_problems, num_packs, groups_per_pack)

        pack_sums = curr_items.sum(dim=2)
        val_max, idx_max = pack_sums.max(dim=1)
        val_min, idx_min = pack_sums.min(dim=1)

        # Gather items from max/min packs
        # idx_max: [LC] -> [LC, 1, K]
        max_items = torch.gather(curr_items, 1, idx_max.view(-1, 1, 1).expand(-1, 1, groups_per_pack)).squeeze(1)
        min_items = torch.gather(curr_items, 1, idx_min.view(-1, 1, 1).expand(-1, 1, groups_per_pack)).squeeze(1)

        # Calculate cost change for swapping max_items[i] with min_items[j]
        # Delta = w_max - w_min
        # Cost Change (L2) = 2 * Delta * (P_min - P_max + Delta)

        deltas = max_items.unsqueeze(2) - min_items.unsqueeze(1) # [LC, K, K]
        p_diff = (val_min - val_max).view(-1, 1, 1)

        change = 2 * deltas * (p_diff + deltas)

        # Find best swap
        best_change, flat_idx = change.view(num_problems, -1).min(dim=1)

        should_swap = best_change < -1e-6
        if not should_swap.any():
            break

        # Execute swaps
        valid_idx = torch.nonzero(should_swap).squeeze(1)

        sel_flat_idx = flat_idx[valid_idx]
        row = sel_flat_idx // groups_per_pack # index in max pack
        col = sel_flat_idx % groups_per_pack  # index in min pack

        p_max = idx_max[valid_idx]
        p_min = idx_min[valid_idx]

        # Swap indices in pack_contents
        v_max = pack_contents[valid_idx, p_max, row]
        v_min = pack_contents[valid_idx, p_min, col]

        pack_contents[valid_idx, p_max, row] = v_min
        pack_contents[valid_idx, p_min, col] = v_max

    # --- Phase 3: Selection ---
    # Select best candidate based on Max Load
    flat_contents = pack_contents.view(num_problems, -1)
    final_items = torch.gather(sorted_weight, 1, flat_contents).view(num_problems, num_packs, groups_per_pack)
    final_max_load = final_items.sum(dim=2).max(dim=1).values

    # Reshape to find min max_load per layer
    final_max_load = final_max_load.view(num_layers, num_candidates)
    best_cand = final_max_load.argmin(dim=1)

    # Gather best solution
    best_indices = torch.arange(num_layers, device=device) * num_candidates + best_cand

    best_contents = pack_contents[best_indices] # [L, M, K]
    best_sorted_idx = sorted_indices[best_indices] # [L, N]

    # Map back to original
    flat_best = best_contents.view(num_layers, -1)
    original_idx = torch.gather(best_sorted_idx, 1, flat_best)

    # Generate pack/rank IDs
    pack_ids = torch.arange(num_packs, device=device).view(1, num_packs, 1).expand(num_layers, -1, groups_per_pack).reshape(num_layers, -1)
    rank_ids = torch.arange(groups_per_pack, device=device).view(1, 1, groups_per_pack).expand(num_layers, num_packs, -1).reshape(num_layers, -1)

    pack_index = torch.empty_like(pack_ids)
    rank_in_pack = torch.empty_like(rank_ids)

    pack_index.scatter_(1, original_idx, pack_ids)
    rank_in_pack.scatter_(1, original_idx, rank_ids)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
<<<<<<< SEARCH
def rebalance_experts(
    weight: torch.Tensor,
    num_replicas: int,
    num_groups: int,
    num_nodes: int,
    num_gpus: int,
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Entry point for expert-parallelism load balancer.

    Parameters:
        weight: [layers, num_logical_experts], the load statistics for all
            logical experts
        num_replicas: number of physical experts, must be a multiple of
            `num_gpus`
        num_groups: number of expert groups
        num_nodes: number of server nodes, where the intra-node network
            (e.g, NVLink) is faster
        num_gpus: number of GPUs, must be a multiple of `num_nodes`

    Returns:
        physical_to_logical_map: [layers, num_replicas], the expert index of
            each replica
        logical_to_physical_map: [layers, num_logical_experts, X], the replica
            indices for each expert
        expert_count: [layers, num_logical_experts], number of physical
            replicas for each logical expert
    """
    num_layers, num_logical_experts = weight.shape
    weight = weight.float().cpu()
    if num_groups % num_nodes == 0:
        # use hierarchical load-balance policy
        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
            weight, num_replicas, num_groups, num_nodes, num_gpus)
    else:
        # use global load-balance policy
        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
            weight, num_replicas, 1, 1, num_gpus)
    num_redundant_experts = num_replicas - num_logical_experts
    maxlogcnt = num_redundant_experts + 1
    log2phy: torch.Tensor = torch.full(
        (num_layers, num_logical_experts, maxlogcnt),
        -1,
        dtype=torch.int64,
        device=logcnt.device,
    )
    log2phy.view(num_layers, -1).scatter_(
        -1,
        phy2log * maxlogcnt + phyrank,
        torch.arange(num_replicas, dtype=torch.int64,
                     device=log2phy.device).expand(num_layers, -1),
    )
    return phy2log, log2phy, logcnt
=======
def rebalance_experts(
    weight: torch.Tensor,
    num_replicas: int,
    num_groups: int,
    num_nodes: int,
    num_gpus: int,
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Entry point for expert-parallelism load balancer.

    Parameters:
        weight: [layers, num_logical_experts], the load statistics for all
            logical experts
        num_replicas: number of physical experts, must be a multiple of
            `num_gpus`
        num_groups: number of expert groups
        num_nodes: number of server nodes, where the intra-node network
            (e.g, NVLink) is faster
        num_gpus: number of GPUs, must be a multiple of `num_nodes`

    Returns:
        physical_to_logical_map: [layers, num_replicas], the expert index of
            each replica
        logical_to_physical_map: [layers, num_logical_experts, X], the replica
            indices for each expert
        expert_count: [layers, num_logical_experts], number of physical
            replicas for each logical expert
    """
    num_layers, num_logical_experts = weight.shape
    # Ensure weight is float for calculations. Keep on original device for speed.
    weight = weight.float()

    if num_groups % num_nodes == 0:
        # use hierarchical load-balance policy
        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
            weight, num_replicas, num_groups, num_nodes, num_gpus)
    else:
        # use global load-balance policy
        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
            weight, num_replicas, 1, 1, num_gpus)

    num_redundant_experts = num_replicas - num_logical_experts
    maxlogcnt = num_redundant_experts + 1
    log2phy: torch.Tensor = torch.full(
        (num_layers, num_logical_experts, maxlogcnt),
        -1,
        dtype=torch.int64,
        device=logcnt.device,
    )

    # Create the reverse map
    # phy2log * maxlogcnt + phyrank gives a unique index for (expert, replica_id)
    # We scatter the physical index (0..num_replicas) into this location
    log2phy.view(num_layers, -1).scatter_(
        -1,
        phy2log * maxlogcnt + phyrank,
        torch.arange(num_replicas, dtype=torch.int64,
                     device=log2phy.device).expand(num_layers, -1),
    )
    return phy2log, log2phy, logcnt
>>>>>>> REPLACE
</DIFF>