<NAME>
refine_packing
</NAME>

<DESCRIPTION>
This edit introduces a "Vectorized Swap Refinement" step to the packing algorithm.

Current limitation: The algorithm relies solely on greedy construction heuristics (LPT, ZigZag, Noisy). Once items are assigned, they are never moved, meaning the algorithm cannot fix suboptimal decisions made early in the greedy process.

Proposed improvement:
1.  **Refinement**: Implement `_refine_packing`, which performs iterative improvement on the initial greedy solutions. It identifies the heaviest and lightest packs and checks all `Capacity * Capacity` pairs of items between them to find a swap that maximally reduces load imbalance. This is essentially a steepest descent local search in the neighborhood of "swapping one item from the heaviest pack with one from the lightest".
2.  **Parallelism**: This refinement is fully vectorized across the batch (Layer * Candidates), utilizing the GPU efficiently.
3.  **Expansion**: Increase `num_candidates` from 64 to 128 to explore a wider initial search space, given that we have a perfect speed score (1.0) and can afford slightly more computation.

The combination of massive parallel candidate generation plus gradient-descent-like refinement on the discrete packing problem should significantly improve the `balancedness_score` while remaining extremely fast due to vectorization.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def _vectorized_greedy_packing(weights: torch.Tensor,
                               num_packs: int,
                               capacity: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Vectorized Greedy Packing Kernel.

    Assigns items to packs such that the current load is minimized,
    respecting the capacity constraint.

    Args:
        weights: [Batch, Items], weights of items (ordered by processing order).
        num_packs: int
        capacity: int

    Returns:
        pack_ids: [Batch, Items]
        ranks: [Batch, Items]
        pack_loads: [Batch, Packs]
    """
    batch_size, num_items = weights.shape
    device = weights.device

    # State tracking
    pack_loads = torch.zeros(batch_size, num_packs, device=device, dtype=weights.dtype)
    pack_counts = torch.zeros(batch_size, num_packs, device=device, dtype=torch.int64)

    pack_ids = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)
    ranks = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)

    batch_indices = torch.arange(batch_size, device=device)
    inf_tensor = torch.tensor(float('inf'), device=device, dtype=weights.dtype)

    # Greedy allocation loop
    for i in range(num_items):
        w = weights[:, i]

        # Identify valid packs (not full)
        valid_mask = pack_counts < capacity

        # Select pack with min load among valid packs
        # Mask invalid packs with infinity using torch.where for speed
        temp_loads = torch.where(valid_mask, pack_loads, inf_tensor)

        # Choose best pack (Argmin)
        chosen_packs = temp_loads.argmin(dim=1)

        # Record assignment
        pack_ids[:, i] = chosen_packs
        ranks[:, i] = pack_counts[batch_indices, chosen_packs]

        # Update state
        # We use advanced indexing which is efficient for these shapes
        pack_counts[batch_indices, chosen_packs] += 1
        pack_loads[batch_indices, chosen_packs] += w

    return pack_ids, ranks, pack_loads


def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Hybrid Ensemble Strategy.

    Generates multiple candidate solutions based on:
    1. LPT (Longest Processing Time) greedy.
    2. ZigZag LPT (Interleaved heavy/light).
    3. Noisy LPT (Randomized) greedy.

    Selects the best candidate per layer based on load imbalance.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_items = weight.shape
    device = weight.device
    capacity = num_items // num_packs

    # --- Ensemble Configuration ---
    # We will generate a total of 64 candidates per layer
    num_candidates = 64

    # 1. Base LPT Sort
    # [L, N]
    lpt_weights, lpt_indices = weight.sort(dim=-1, descending=True)

    # 2. Generate ZigZag Indices (relative to LPT sorted)
    # Interleave: 0, N-1, 1, N-2 ...
    # This helps balance heavy and light items immediately
    relative_zigzag = torch.empty(num_items, device=device, dtype=torch.long)
    half = (num_items + 1) // 2
    arange = torch.arange(num_items, device=device)
    relative_zigzag[0::2] = arange[:half]
    relative_zigzag[1::2] = arange[half:].flip(0)

    # 3. Generate Noisy Scales
    # Candidate 0: Pure LPT (scale 1.0)
    # Candidate 1: ZigZag (implemented via permutation of LPT, handled below)
    # Candidates 2..63: Noisy LPT
    num_noisy = num_candidates - 2

    # Noise: Random in [0.85, 1.15]
    # This range is sufficient to flip close weights but preserve general order
    noise = (torch.rand(num_layers, num_noisy, num_items, device=device) * 0.3) + 0.85

    # Prepend ones for LPT and ZigZag (placeholders)
    # We will overwrite the ZigZag weights with LPT weights but permuted later
    # Actually, simpler to treat ZigZag as a permutation of Pure LPT

    # Prepare expanded weights for noise sorting
    # [L, Num_Noisy, N]
    noisy_weights_in = weight.unsqueeze(1) * noise
    noisy_sorted_weights, noisy_sorted_idx = noisy_weights_in.sort(dim=-1, descending=True)

    # We need the *actual* weights for the packing kernel, not the perturbed ones
    orig_expanded = weight.unsqueeze(1).expand(-1, num_noisy, -1)
    # [L, Num_Noisy, N]
    actual_noisy_weights = orig_expanded.gather(2, noisy_sorted_idx)

    # Prepare Pure LPT [L, 1, N]
    c_lpt_weights = lpt_weights.unsqueeze(1)
    c_lpt_idx = lpt_indices.unsqueeze(1)

    # Prepare ZigZag [L, 1, N]
    # ZigZag permutes the already sorted LPT weights
    c_zigzag_weights = c_lpt_weights[:, :, relative_zigzag]
    c_zigzag_idx = c_lpt_idx[:, :, relative_zigzag]

    # Concatenate all candidates
    # Weights: [L, C, N]
    # Indices: [L, C, N] (These are indices into the original weight tensor)
    all_weights = torch.cat([c_lpt_weights, c_zigzag_weights, actual_noisy_weights], dim=1)
    all_indices = torch.cat([c_lpt_idx, c_zigzag_idx, noisy_sorted_idx], dim=1)

    # Flatten for Vectorized Kernel
    # [L*C, N]
    flat_weights = all_weights.view(-1, num_items)

    # Run Greedy Packing
    flat_ids, flat_ranks, flat_loads = _vectorized_greedy_packing(flat_weights, num_packs, capacity)

    # --- Selection ---
    # Reshape loads: [L, C, Packs]
    loads = flat_loads.view(num_layers, num_candidates, num_packs)

    # Calculate Imbalance: Max - Min
    imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values # [L, C]

    # Find Best Candidate Index
    best_candidate_idx = imbalance.argmin(dim=1) # [L]

    # Gather Results
    # We need to extract the specific row corresponding to best_candidate_idx for each layer

    # Shape helpers
    idx_view = best_candidate_idx.view(num_layers, 1, 1).expand(-1, 1, num_items)

    # 1. Best Permutation of original indices
    final_sorted_idx = all_indices.gather(1, idx_view).squeeze(1) # [L, N]

    # 2. Best Pack IDs (aligned to the permutation)
    aligned_ids = flat_ids.view(num_layers, num_candidates, num_items)
    final_aligned_ids = aligned_ids.gather(1, idx_view).squeeze(1)

    # 3. Best Ranks (aligned to the permutation)
    aligned_ranks = flat_ranks.view(num_layers, num_candidates, num_items)
    final_aligned_ranks = aligned_ranks.gather(1, idx_view).squeeze(1)

    # Scatter back to original item order
    # Output[l, original_idx] = assigned_val
    # We have: original_idx = final_sorted_idx[l, i]
    #          value = final_aligned_ids[l, i]

    pack_index = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    rank_in_pack = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)

    pack_index.scatter_(1, final_sorted_idx, final_aligned_ids)
    rank_in_pack.scatter_(1, final_sorted_idx, final_aligned_ranks)

    return pack_index, rank_in_pack
=======
def _refine_packing(weights: torch.Tensor,
                    pack_ids: torch.Tensor,
                    pack_loads: torch.Tensor,
                    ranks: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Refines the packing by attempting to swap a single item between the heaviest
    and lightest packs in each batch to reduce imbalance.
    """
    batch_size, num_items = weights.shape
    device = weights.device
    batch_indices = torch.arange(batch_size, device=device)

    # Iterative refinement (1 pass is usually sufficient for speed/imbalance tradeoff)
    for _ in range(1):
        # Identify heaviest and lightest packs
        max_load, max_pack_idx = pack_loads.max(dim=1)
        min_load, min_pack_idx = pack_loads.min(dim=1)

        current_diff = max_load - min_load

        # Mask items belonging to these packs
        # [B, N]
        is_in_max = (pack_ids == max_pack_idx.unsqueeze(1))
        is_in_min = (pack_ids == min_pack_idx.unsqueeze(1))

        # We want to swap item i (from max) with item j (from min)
        # Minimize |(L_max - w_i + w_j) - (L_min - w_j + w_i)|
        # = |(L_max - L_min) - 2*(w_i - w_j)|
        # Let target = (L_max - L_min) / 2

        # Expand weights for pairwise diff
        # [B, N, N] -> w[b, i] - w[b, j]
        # Memory warning: N=256, B=2048 -> 128M elements. Safe.
        w_diff = weights.unsqueeze(2) - weights.unsqueeze(1)

        # Calculate improvement metric
        # We want 2 * w_diff to be close to current_diff
        # metric = | current_diff - 2 * w_diff |
        metric = torch.abs(current_diff.view(-1, 1, 1) - 2 * w_diff)

        # Apply validity mask
        valid_swap = is_in_max.unsqueeze(2) & is_in_min.unsqueeze(1)
        metric = torch.where(valid_swap, metric, torch.tensor(float('inf'), device=device))

        # Find best swap per batch
        flat_metric = metric.view(batch_size, -1)
        min_metric, flat_indices = flat_metric.min(dim=1)

        # Only apply if improvement
        # We strictly want the new diff to be smaller than current_diff
        # min_metric is the new diff between these two packs
        improvement = min_metric < current_diff

        batch_indices_active = batch_indices[improvement]
        if batch_indices_active.numel() == 0:
            break

        indices_active = flat_indices[improvement]
        i_idx = indices_active // num_items
        j_idx = indices_active % num_items

        # Perform swap
        p_max = max_pack_idx[batch_indices_active]
        p_min = min_pack_idx[batch_indices_active]

        w_i = weights[batch_indices_active, i_idx]
        w_j = weights[batch_indices_active, j_idx]
        delta = w_i - w_j

        # Update Loads
        pack_loads[batch_indices_active, p_max] -= delta
        pack_loads[batch_indices_active, p_min] += delta

        # Update IDs
        pack_ids[batch_indices_active, i_idx] = p_min
        pack_ids[batch_indices_active, j_idx] = p_max

        # Update Ranks (swap them to maintain 0..capacity-1 set in each pack)
        r_i = ranks[batch_indices_active, i_idx]
        r_j = ranks[batch_indices_active, j_idx]
        ranks[batch_indices_active, i_idx] = r_j
        ranks[batch_indices_active, j_idx] = r_i

    return pack_ids, ranks, pack_loads


def _vectorized_greedy_packing(weights: torch.Tensor,
                               num_packs: int,
                               capacity: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Vectorized Greedy Packing Kernel.
    """
    batch_size, num_items = weights.shape
    device = weights.device

    # State tracking
    pack_loads = torch.zeros(batch_size, num_packs, device=device, dtype=weights.dtype)
    pack_counts = torch.zeros(batch_size, num_packs, device=device, dtype=torch.int64)

    pack_ids = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)
    ranks = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)

    batch_indices = torch.arange(batch_size, device=device)
    inf_tensor = torch.tensor(float('inf'), device=device, dtype=weights.dtype)

    # Greedy allocation loop
    for i in range(num_items):
        w = weights[:, i]
        valid_mask = pack_counts < capacity
        temp_loads = torch.where(valid_mask, pack_loads, inf_tensor)
        chosen_packs = temp_loads.argmin(dim=1)

        pack_ids[:, i] = chosen_packs
        ranks[:, i] = pack_counts[batch_indices, chosen_packs]

        pack_counts[batch_indices, chosen_packs] += 1
        pack_loads[batch_indices, chosen_packs] += w

    return pack_ids, ranks, pack_loads


def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Hybrid Ensemble Strategy.
    """
    num_layers, num_items = weight.shape
    device = weight.device
    capacity = num_items // num_packs

    # Increase candidates to 128 for broader search
    num_candidates = 128

    # 1. Base LPT Sort
    lpt_weights, lpt_indices = weight.sort(dim=-1, descending=True)

    # 2. ZigZag
    relative_zigzag = torch.empty(num_items, device=device, dtype=torch.long)
    half = (num_items + 1) // 2
    arange = torch.arange(num_items, device=device)
    relative_zigzag[0::2] = arange[:half]
    relative_zigzag[1::2] = arange[half:].flip(0)

    # 3. Noisy LPT
    num_noisy = num_candidates - 2
    noise = (torch.rand(num_layers, num_noisy, num_items, device=device) * 0.3) + 0.85

    noisy_weights_in = weight.unsqueeze(1) * noise
    noisy_sorted_weights, noisy_sorted_idx = noisy_weights_in.sort(dim=-1, descending=True)

    orig_expanded = weight.unsqueeze(1).expand(-1, num_noisy, -1)
    actual_noisy_weights = orig_expanded.gather(2, noisy_sorted_idx)

    c_lpt_weights = lpt_weights.unsqueeze(1)
    c_lpt_idx = lpt_indices.unsqueeze(1)

    c_zigzag_weights = c_lpt_weights[:, :, relative_zigzag]
    c_zigzag_idx = c_lpt_idx[:, :, relative_zigzag]

    all_weights = torch.cat([c_lpt_weights, c_zigzag_weights, actual_noisy_weights], dim=1)
    all_indices = torch.cat([c_lpt_idx, c_zigzag_idx, noisy_sorted_idx], dim=1)

    flat_weights = all_weights.view(-1, num_items)

    # Run Greedy Packing
    flat_ids, flat_ranks, flat_loads = _vectorized_greedy_packing(flat_weights, num_packs, capacity)

    # Run Refinement
    flat_ids, flat_ranks, flat_loads = _refine_packing(flat_weights, flat_ids, flat_loads, flat_ranks)

    # --- Selection ---
    loads = flat_loads.view(num_layers, num_candidates, num_packs)
    imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values

    best_candidate_idx = imbalance.argmin(dim=1)

    idx_view = best_candidate_idx.view(num_layers, 1, 1).expand(-1, 1, num_items)

    final_sorted_idx = all_indices.gather(1, idx_view).squeeze(1)

    aligned_ids = flat_ids.view(num_layers, num_candidates, num_items)
    final_aligned_ids = aligned_ids.gather(1, idx_view).squeeze(1)

    aligned_ranks = flat_ranks.view(num_layers, num_candidates, num_items)
    final_aligned_ranks = aligned_ranks.gather(1, idx_view).squeeze(1)

    pack_index = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    rank_in_pack = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)

    pack_index.scatter_(1, final_sorted_idx, final_aligned_ids)
    rank_in_pack.scatter_(1, final_sorted_idx, final_aligned_ranks)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>