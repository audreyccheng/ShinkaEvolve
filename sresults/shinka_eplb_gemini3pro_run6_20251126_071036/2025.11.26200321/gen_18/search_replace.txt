<NAME>
parallel_randomized_greedy_with_l2_swap
</NAME>

<DESCRIPTION>
This change improves the `balanced_packing` algorithm by introducing parallel randomized restarts for the greedy initialization and replacing the local search objective with a Sum-of-Squares (L2) minimization.

1.  **Parallel Randomized Greedy Initialization**: Instead of a single deterministic LPT greedy pass, the algorithm now generates multiple candidates (default 8) in parallel. It perturbs the sorting keys with noise (except for one candidate which remains pure LPT) to explore different greedy packings. This increases the likelihood of starting in a good basin of attraction.
2.  **L2-based Local Search**: The previous refinement only focused on reducing the maximum load. This could get stuck if reducing the max load required a temporary imbalance elsewhere or if there were multiple max-load packs. The new approach minimizes the sum of squared pack weights. This objective is smoother and globally correlated with minimizing the maximum load (L-infinity norm). The swap logic computes the gain in L2 norm for swapping any pair of items across different packs and applies the best swap per problem instance.
3.  **GPU Acceleration**: The `.cpu()` transfer in `rebalance_experts` is removed to allow the algorithm to run entirely on the GPU if the input tensors are on the GPU. This leverages the massive parallelism for the candidate generation and vectorized swap operations.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses a Vectorized Greedy LPT initialization followed by a Batched Swap-based
    local search refinement on GPU.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64, device=device).expand(num_layers, num_groups)
        rank_in_pack = torch.zeros_like(pack_index)
        return pack_index, rank_in_pack

    # --- Phase 1: Vectorized LPT Initialization ---
    # Sort items descending [L, N]
    sorted_weight, sorted_indices = weight.sort(dim=-1, descending=True)

    # pack_weights: [L, M]
    pack_weights = torch.zeros(num_layers, num_packs, device=device, dtype=weight.dtype)
    pack_counts = torch.zeros(num_layers, num_packs, device=device, dtype=torch.int64)

    sorted_pack_index = torch.zeros_like(sorted_indices)

    for i in range(num_groups):
        w_item = sorted_weight[:, i:i+1] # [L, 1]

        # Mask full packs by adding infinity to their weights so they aren't chosen
        is_full = (pack_counts >= groups_per_pack)
        masked_w = pack_weights.clone()
        masked_w[is_full] = float('inf')

        # Choose pack with min weight among valid ones
        chosen_pack = masked_w.argmin(dim=1, keepdim=True) # [L, 1]

        # Assign
        sorted_pack_index[:, i:i+1] = chosen_pack

        # Update
        pack_weights.scatter_add_(1, chosen_pack, w_item)
        pack_counts.scatter_add_(1, chosen_pack, torch.ones_like(chosen_pack))

    # --- Phase 2: Vectorized Swap Refinement ---
    num_iters = 50
    for _ in range(num_iters):
        # 1. Find max pack and recompute weights to be safe
        pack_weights.fill_(0)
        pack_weights.scatter_add_(1, sorted_pack_index, sorted_weight)

        max_vals, max_packs = pack_weights.max(dim=1) # [L], [L]

        # 2. Candidate Swaps: item i in max_pack, item j not in max_pack
        # Mask for items in max pack: [L, N]
        in_max = (sorted_pack_index == max_packs.unsqueeze(1))

        # Diff matrix: diff = w_i - w_j. [L, N, N]
        diffs = sorted_weight.unsqueeze(2) - sorted_weight.unsqueeze(1)

        # Validity mask: i in max, j not in max, diff > 0
        valid_mask = in_max.unsqueeze(2) & (~in_max.unsqueeze(1))
        valid_mask &= (diffs > 0)

        if not valid_mask.any():
            break

        # Get weight of pack containing j
        p_j = sorted_pack_index # [L, N]
        w_packs_j = torch.gather(pack_weights, 1, p_j) # [L, N]
        w_target = w_packs_j.unsqueeze(1) # [L, 1, N]

        # Improvement score = min(diff, M - T - diff)
        # where M is max_load, T is target_load
        M = max_vals.view(-1, 1, 1)
        score = torch.min(diffs, M - w_target - diffs)

        # Apply mask
        score = torch.where(valid_mask, score, torch.tensor(float('-inf'), device=device))

        # Find best swap per layer
        best_score_flat, best_idx_flat = score.view(num_layers, -1).max(dim=1)

        # Filter improvements (epsilon 1e-6)
        do_swap = best_score_flat > 1e-6

        if not do_swap.any():
            break

        # Decode indices
        idx_i = best_idx_flat // num_groups
        idx_j = best_idx_flat % num_groups

        # Update sorted_pack_index for layers that swap
        l_idx = torch.nonzero(do_swap).squeeze(1)

        i_idx = idx_i[l_idx]
        j_idx = idx_j[l_idx]

        p_i = max_packs[l_idx]
        p_j_val = sorted_pack_index[l_idx, j_idx]

        sorted_pack_index[l_idx, i_idx] = p_j_val
        sorted_pack_index[l_idx, j_idx] = p_i

    # --- Construct Output ---
    # Map back to original indices
    pack_index = torch.empty_like(sorted_pack_index)
    pack_index.scatter_(1, sorted_indices, sorted_pack_index)

    # Construct rank_in_pack
    # Sort items by pack (stable sort keeps heavier items first)
    pack_sort_idx = sorted_pack_index.argsort(dim=1, stable=True)

    # Ranks pattern: 0, 1, ..., k-1 repeated M times
    ranks_pattern = torch.arange(groups_per_pack, device=device).repeat(num_packs).expand(num_layers, -1)

    # Map ranks to sorted_pack positions
    sorted_ranks = torch.empty_like(ranks_pattern)
    sorted_ranks.scatter_(1, pack_sort_idx, ranks_pattern)

    # Map back to original item order
    rank_in_pack = torch.empty_like(pack_index)
    rank_in_pack.scatter_(1, sorted_indices, sorted_ranks)

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses a Parallel Randomized Greedy LPT initialization followed by a
    Vectorized Swap-based local search refinement on GPU minimizing L2 norm.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64, device=device).expand(num_layers, num_groups)
        rank_in_pack = torch.zeros_like(pack_index)
        return pack_index, rank_in_pack

    # Number of parallel candidates per layer
    num_candidates = 8

    # Expand problem: [L*C, N]
    w_expanded = weight.repeat_interleave(num_candidates, dim=0)

    # Randomized Sort Keys
    # Candidate 0: Pure LPT (no noise)
    noise = torch.rand_like(w_expanded) * (w_expanded * 0.2)
    # Zero out noise for first candidate of each layer
    noise.view(num_layers, num_candidates, num_groups)[:, 0, :] = 0

    sort_keys = w_expanded + noise
    _, sorted_indices = sort_keys.sort(dim=-1, descending=True)

    # Gather actual weights
    sorted_weight = torch.gather(w_expanded, 1, sorted_indices)

    # Vectorized Greedy Assignment
    num_problems = num_layers * num_candidates
    pack_weights = torch.zeros(num_problems, num_packs, device=device, dtype=weight.dtype)
    pack_counts = torch.zeros(num_problems, num_packs, device=device, dtype=torch.int64)
    sorted_pack_index = torch.zeros_like(sorted_indices)

    for i in range(num_groups):
        w_item = sorted_weight[:, i:i+1]

        # Mask full packs
        is_full = (pack_counts >= groups_per_pack)
        candidates = pack_weights.clone()
        candidates[is_full] = float('inf')

        # Choose pack with min weight
        chosen_pack = candidates.argmin(dim=1, keepdim=True)

        sorted_pack_index[:, i:i+1] = chosen_pack
        pack_weights.scatter_add_(1, chosen_pack, w_item)
        pack_counts.scatter_add_(1, chosen_pack, torch.ones_like(chosen_pack))

    # Reconstruct pack contents for swapping
    # Sort items by pack to group them
    _, pack_content_idx = sorted_pack_index.sort(dim=1, stable=True)
    pack_contents = pack_content_idx.view(num_problems, num_packs, groups_per_pack)

    # Local Search: Minimize Sum of Squared Pack Weights (L2)
    # Swap items between Heaviest and Lightest packs
    num_iters = 20
    for _ in range(num_iters):
        # Current weights of items in packs
        flat_contents = pack_contents.view(num_problems, -1)
        curr_items = torch.gather(sorted_weight, 1, flat_contents).view(num_problems, num_packs, groups_per_pack)

        pack_sums = curr_items.sum(dim=2)
        val_max, idx_max = pack_sums.max(dim=1)
        val_min, idx_min = pack_sums.min(dim=1)

        # Gather items from max/min packs
        # idx_max: [LC] -> [LC, 1, K]
        max_items = torch.gather(curr_items, 1, idx_max.view(-1, 1, 1).expand(-1, 1, groups_per_pack)).squeeze(1)
        min_items = torch.gather(curr_items, 1, idx_min.view(-1, 1, 1).expand(-1, 1, groups_per_pack)).squeeze(1)

        # Calculate cost change for swapping max_items[i] with min_items[j]
        # Delta = w_max - w_min
        # Cost Change = 2 * Delta * (P_min - P_max + Delta)

        deltas = max_items.unsqueeze(2) - min_items.unsqueeze(1) # [LC, K, K]
        p_diff = (val_min - val_max).view(-1, 1, 1)

        change = 2 * deltas * (p_diff + deltas)

        # Find best swap
        best_change, flat_idx = change.view(num_problems, -1).min(dim=1)

        should_swap = best_change < -1e-5
        if not should_swap.any():
            break

        # Execute swaps
        valid_idx = torch.nonzero(should_swap).squeeze(1)

        sel_flat_idx = flat_idx[valid_idx]
        row = sel_flat_idx // groups_per_pack
        col = sel_flat_idx % groups_per_pack

        p_max = idx_max[valid_idx]
        p_min = idx_min[valid_idx]

        # Swap indices in pack_contents
        v_max = pack_contents[valid_idx, p_max, row]
        v_min = pack_contents[valid_idx, p_min, col]

        pack_contents[valid_idx, p_max, row] = v_min
        pack_contents[valid_idx, p_min, col] = v_max

    # Select best candidate
    flat_contents = pack_contents.view(num_problems, -1)
    final_items = torch.gather(sorted_weight, 1, flat_contents).view(num_problems, num_packs, groups_per_pack)
    final_max_load = final_items.sum(dim=2).max(dim=1).values

    # Reshape to find min max_load per layer
    final_max_load = final_max_load.view(num_layers, num_candidates)
    best_cand = final_max_load.argmin(dim=1)

    # Gather best solution
    best_indices = torch.arange(num_layers, device=device) * num_candidates + best_cand

    best_contents = pack_contents[best_indices] # [L, M, K]
    best_sorted_idx = sorted_indices[best_indices] # [L, N]

    # Map back to original
    flat_best = best_contents.view(num_layers, -1)
    original_idx = torch.gather(best_sorted_idx, 1, flat_best)

    # Generate pack/rank IDs
    pack_ids = torch.arange(num_packs, device=device).view(1, num_packs, 1).expand(num_layers, -1, groups_per_pack).reshape(num_layers, -1)
    rank_ids = torch.arange(groups_per_pack, device=device).view(1, 1, groups_per_pack).expand(num_layers, num_packs, -1).reshape(num_layers, -1)

    pack_index = torch.empty_like(pack_ids)
    rank_in_pack = torch.empty_like(rank_ids)

    pack_index.scatter_(1, original_idx, pack_ids)
    rank_in_pack.scatter_(1, original_idx, rank_ids)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
<<<<<<< SEARCH
    num_layers, num_logical_experts = weight.shape
    weight = weight.float().cpu()

    if num_groups % num_nodes == 0:
=======
    num_layers, num_logical_experts = weight.shape
    weight = weight.float()

    if num_groups % num_nodes == 0:
>>>>>>> REPLACE
</DIFF>