<NAME>
folded_chunk_packing
</NAME>

<DESCRIPTION>
I am modifying the `balanced_packing` function to use a "Folded Chunked Sorted Greedy" strategy.
Instead of processing items in chunks of size M (where M is number of packs) and assigning them greedily, we process them in chunks of size 2M.
In each "double chunk", we pair the largest items with the smallest items (folding) to form M pairs. We then assign these pairs to the M packs greedily.
This "folding" technique significantly reduces the variance of the items being assigned, leading to better load balancing, especially when the weight distribution is skewed (e.g., power law), while respecting the cardinality constraints.
The algorithm remains fully vectorized and efficient.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Chunked Sorted Greedy strategy.

    Items are sorted and processed in chunks of size m. In each chunk,
    items are assigned to packs such that the heaviest item in the chunk
    goes to the lightest pack, effectively balancing the load iteratively.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_items = weight.shape
    device = weight.device

    # Sort all items by weight descending
    # sorted_weights: [layers, n]
    sorted_weights, sorted_indices = weight.sort(dim=-1, descending=True)

    # Track current load of each pack
    pack_loads = torch.zeros(num_layers, num_packs, device=device, dtype=weight.dtype)

    # Prepare aligned outputs to store results in sorted order first
    aligned_pack_ids = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    aligned_ranks = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)

    # Number of chunks (each pack gets exactly 1 item per chunk)
    num_chunks = num_items // num_packs

    for k in range(num_chunks):
        # Indices for the current chunk in the sorted array
        start = k * num_packs
        end = start + num_packs

        # Weights of items in this chunk: [layers, num_packs]
        chunk_weights = sorted_weights[:, start:end]

        # Sort packs by current load ascending to find the best fit
        # sorted_bin_indices[:, 0] is the pack with the smallest load
        _, sorted_bin_indices = pack_loads.sort(dim=-1)

        # Assign items to packs:
        # Largest item in chunk (col 0) -> Pack with min load (col 0)
        # ...
        # Smallest item in chunk (col m-1) -> Pack with max load (col m-1)
        aligned_pack_ids[:, start:end] = sorted_bin_indices
        aligned_ranks[:, start:end] = k

        # Update pack loads
        # We add the weights of the items to their assigned packs
        pack_loads.scatter_add_(1, sorted_bin_indices, chunk_weights)

    # Scatter results back to original item order
    pack_index = torch.empty_like(aligned_pack_ids)
    rank_in_pack = torch.empty_like(aligned_ranks)

    pack_index.scatter_(1, sorted_indices, aligned_pack_ids)
    rank_in_pack.scatter_(1, sorted_indices, aligned_ranks)

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Folded Chunked Sorted Greedy strategy.

    Items are processed in "double chunks" of size 2*m. In each double chunk,
    items are paired (heaviest + lightest) to form m balanced pairs, which are
    then assigned to packs greedily (heaviest pair -> lightest pack).
    This smoothing reduces load variance significantly compared to single-item greedy.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_items = weight.shape
    device = weight.device

    sorted_weights, sorted_indices = weight.sort(dim=-1, descending=True)

    pack_loads = torch.zeros(num_layers, num_packs, device=device, dtype=weight.dtype)
    aligned_pack_ids = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    aligned_ranks = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)

    # Process in chunks of 2 * num_packs to enable folding
    double_chunk_size = 2 * num_packs
    num_double_chunks = num_items // double_chunk_size

    for k in range(num_double_chunks):
        start = k * double_chunk_size
        end = start + double_chunk_size

        # Chunk weights: [Layers, 2*M]
        chunk_weights = sorted_weights[:, start:end]

        # Pair indices: 0 with 2M-1, 1 with 2M-2, ...
        idx_low = torch.arange(num_packs, device=device)
        idx_high = torch.arange(double_chunk_size - 1, num_packs - 1, -1, device=device)

        # Compute pair weights
        pair_weights = chunk_weights[:, idx_low] + chunk_weights[:, idx_high]

        # Identify assignment order: Heaviest pair -> Lightest pack
        _, sorted_bin_indices = pack_loads.sort(dim=-1) # Lightest pack at 0
        _, pairs_order = pair_weights.sort(dim=-1, descending=True) # Heaviest pair at 0

        # We assign sorted_bin_indices[i] to pairs_order[i]
        # Map: Pair j gets Pack assigned_packs[j]
        # assigned_packs[pairs_order] = sorted_bin_indices
        assigned_packs = torch.empty_like(sorted_bin_indices)
        assigned_packs.scatter_(1, pairs_order, sorted_bin_indices)

        # Assign to items
        # Low item (j) and High item (2M-1-j) get the same pack
        aligned_pack_ids[:, start + idx_low] = assigned_packs
        aligned_pack_ids[:, start + idx_high] = assigned_packs

        # Assign ranks (2 items added per pack)
        aligned_ranks[:, start + idx_low] = 2 * k
        aligned_ranks[:, start + idx_high] = 2 * k + 1

        # Update loads
        pack_loads.scatter_add_(1, assigned_packs, pair_weights)

    # Handle remainder chunk (size M, if any)
    remainder_start = num_double_chunks * double_chunk_size
    if remainder_start < num_items:
        start = remainder_start
        end = num_items # Should be start + num_packs

        chunk_weights = sorted_weights[:, start:end]
        _, sorted_bin_indices = pack_loads.sort(dim=-1)

        aligned_pack_ids[:, start:end] = sorted_bin_indices
        aligned_ranks[:, start:end] = 2 * num_double_chunks

        pack_loads.scatter_add_(1, sorted_bin_indices, chunk_weights)

    # Scatter back to original order
    pack_index = torch.empty_like(aligned_pack_ids)
    rank_in_pack = torch.empty_like(aligned_ranks)

    pack_index.scatter_(1, sorted_indices, aligned_pack_ids)
    rank_in_pack.scatter_(1, sorted_indices, aligned_ranks)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>