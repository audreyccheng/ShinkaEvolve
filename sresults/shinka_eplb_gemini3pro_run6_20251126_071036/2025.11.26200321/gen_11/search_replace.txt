<NAME>
improved_balanced_packing_v2
</NAME>

<DESCRIPTION>
Improved `balanced_packing` function by replacing the "Max-Min Swap" local search with a "Max-Any Swap" local search.
The previous algorithm only attempted to swap items between the heaviest pack and the lightest pack.
The new algorithm attempts to swap items from the heaviest pack with items from *any* other pack, provided the swap reduces the maximum load.
This is implemented using fully vectorized tensor operations on GPU, evaluating all possible swaps from the max pack in parallel.
The initialization remains ZigZag, which is fast and provides a decent starting point.
This change significantly improves the `balancedness_score` by escaping local optima that Max-Min swap cannot, without sacrificing speed (vectorized).
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses ZigZag initialization followed by a vectorized Max-Min Swap local search.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_packs, dtype=torch.int64, device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(pack_index)
        return pack_index, rank_in_pack

    # 1. Sort weights descending: [L, N]
    sorted_weight, sorted_indices = weight.sort(dim=-1, descending=True)

    # 2. ZigZag Initialization
    # Pattern: 0, 1, ..., m-1, m-1, ..., 0
    # This distributes large items across packs evenly.
    pattern = torch.cat([
        torch.arange(num_packs, device=device),
        torch.arange(num_packs - 1, -1, -1, device=device)
    ])
    num_patterns = (num_groups + len(pattern) - 1) // len(pattern)
    assignments = pattern.repeat(num_patterns)[:num_groups] # [N]

    # Convert assignments to pack_contents: [M, K]
    # This tells us which sorted-index is in which pack.
    # We sort the assignments to group by pack id.
    # Since ZigZag is deterministic, we can compute the indices once.
    # We want indices such that sorted_weight[indices] gives the items in each pack.
    # To get that, we just need the indices 0..N grouped by their assignment.
    # Stable sort of assignments gives us the permutation of 0..N that groups by pack.
    _, perm = assignments.sort(stable=True)

    # pack_contents_indices: [M, K] contains indices into sorted_weight dim 1
    pack_contents_indices = perm.view(num_packs, groups_per_pack)

    # Expand to all layers: [L, M, K]
    pack_contents = pack_contents_indices.unsqueeze(0).expand(num_layers, -1, -1).clone()

    # 3. Vectorized Local Search
    # Iteratively swap items between the heaviest (Max) and lightest (Min) packs.
    # Maximize K (groups_per_pack) is usually small (e.g., 4-32). M (num_packs) is small (e.g., 8).
    # Operations are mostly on small matrices [L, K, K].

    num_iters = 20
    layer_arange = torch.arange(num_layers, device=device)

    for _ in range(num_iters):
        # Gather weights: [L, M, K]
        # Flatten for gather: [L, M*K]
        flat_contents = pack_contents.view(num_layers, -1)
        current_weights = torch.gather(sorted_weight, 1, flat_contents).view(num_layers, num_packs, groups_per_pack)

        # Compute sums: [L, M]
        pack_sums = current_weights.sum(dim=2)

        # Find Max and Min packs: [L]
        val_max, idx_max_pack = pack_sums.max(dim=1)
        val_min, idx_min_pack = pack_sums.min(dim=1)

        gap = val_max - val_min # [L]

        # If gap is small, we might break early? (Optional, but vectorization makes it hard to break per-layer)

        # Get weights in Max and Min packs: [L, K]
        # We need to gather from current_weights using the pack indices
        # idx_max_pack: [L] -> expand to [L, 1, K] for gather is complex on [L, M, K]
        # Easier to gather from the sorted_weight using the indices in pack_contents

        # Get indices: [L, K]
        # pack_contents: [L, M, K]
        indices_max = pack_contents[layer_arange, idx_max_pack, :] # [L, K]
        indices_min = pack_contents[layer_arange, idx_min_pack, :] # [L, K]

        # Get weights: [L, K]
        w_max = torch.gather(sorted_weight, 1, indices_max)
        w_min = torch.gather(sorted_weight, 1, indices_min)

        # Compute diff matrix: [L, K, K]
        # D[l, i, j] = w_max[l, i] - w_min[l, j]
        diffs = w_max.unsqueeze(2) - w_min.unsqueeze(1)

        # We want to swap if it reduces the max load and doesn't make min load > old max load.
        # Ideally, we want to minimize abs((SumMax - diff) - (SumMin + diff)) = abs(Gap - 2*diff)
        # Constraint: diff > 0 (to reduce Max)
        # Constraint: SumMax - diff >= SumMin + diff => 2*diff <= Gap (technically we can go slightly beyond, but safely 2*diff < Gap is good)
        # Actually, best swap minimizes max(SumMax - diff, SumMin + diff).
        # This is equivalent to minimizing abs(Gap - 2*diff) if diff < Gap.

        cost = (gap.view(num_layers, 1, 1) - 2 * diffs).abs()

        # Mask invalid swaps:
        # We only want swaps where diff > 0 (reduce Max).
        # Also, strictly speaking, we shouldn't overshoot too much, but minimizing cost usually handles it.
        # Just ensure diff > 0.
        mask = diffs > 0
        cost = torch.where(mask, cost, torch.tensor(float('inf'), device=device))

        # Find best swap per layer
        # flatten last 2 dims: [L, K*K]
        flat_cost = cost.view(num_layers, -1)
        min_cost, flat_idx = flat_cost.min(dim=1)

        # Determine which layers have a valid swap
        valid_swap = min_cost < float('inf')
        # If no layer has a valid swap, we could break, but let's just continue (no-op for invalid)

        if not valid_swap.any():
            break

        # Decode indices
        # idx_in_max_pack (0..K-1), idx_in_min_pack (0..K-1)
        idx_i = flat_idx // groups_per_pack
        idx_j = flat_idx % groups_per_pack

        # Apply swaps for valid layers
        # We need to swap elements in pack_contents
        # pack_contents[l, max_p, i] <-> pack_contents[l, min_p, j]

        l_indices_valid = torch.nonzero(valid_swap).squeeze(1)

        if len(l_indices_valid) > 0:
            p_max = idx_max_pack[l_indices_valid]
            p_min = idx_min_pack[l_indices_valid]
            i_idx = idx_i[l_indices_valid]
            j_idx = idx_j[l_indices_valid]

            # Values to swap (indices into sorted_weight)
            val_i = pack_contents[l_indices_valid, p_max, i_idx]
            val_j = pack_contents[l_indices_valid, p_min, j_idx]

            # Perform swap
            pack_contents[l_indices_valid, p_max, i_idx] = val_j
            pack_contents[l_indices_valid, p_min, j_idx] = val_i

    # 4. Construct outputs
    # pack_contents [L, M, K] contains indices into sorted_weight.
    # We need to map back to original indices.

    # Flatten pack_contents to [L, N] (but strictly speaking order is M blocks of K)
    # The value at pack_contents[l, m, k] is the index 's' in sorted_weight.
    # The original item index is sorted_indices[l, s].
    # That item belongs to pack 'm'.

    # Create pack_ids tensor corresponding to pack_contents structure
    # [L, M, K] filled with m
    pack_ids = torch.arange(num_packs, device=device).view(1, num_packs, 1).expand(num_layers, -1, groups_per_pack)

    # Create rank_ids tensor
    # [L, M, K] filled with k
    rank_ids = torch.arange(groups_per_pack, device=device).view(1, 1, groups_per_pack).expand(num_layers, num_packs, -1)

    # Flatten everything
    flat_sorted_idx_ptr = pack_contents.view(num_layers, -1) # [L, N] pointers to sorted array columns
    flat_pack_ids = pack_ids.reshape(num_layers, -1) # [L, N]
    flat_rank_ids = rank_ids.reshape(num_layers, -1) # [L, N]

    # Resolve to original item indices
    # original_idx[l, i] = sorted_indices[l, flat_sorted_idx_ptr[l, i]]
    original_idx = torch.gather(sorted_indices, 1, flat_sorted_idx_ptr)

    # Now scatter pack_ids and rank_ids to their original positions
    pack_index = torch.empty_like(flat_pack_ids)
    rank_in_pack = torch.empty_like(flat_rank_ids)

    pack_index.scatter_(1, original_idx, flat_pack_ids)
    rank_in_pack.scatter_(1, original_idx, flat_rank_ids)

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses ZigZag initialization followed by a vectorized Max-Any Swap local search.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_packs, dtype=torch.int64, device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(pack_index)
        return pack_index, rank_in_pack

    # 1. Sort weights descending: [L, N]
    sorted_weight, sorted_indices = weight.sort(dim=-1, descending=True)

    # 2. ZigZag Initialization
    pattern = torch.cat([
        torch.arange(num_packs, device=device),
        torch.arange(num_packs - 1, -1, -1, device=device)
    ])
    num_patterns = (num_groups + len(pattern) - 1) // len(pattern)
    assignments = pattern.repeat(num_patterns)[:num_groups] # [N]

    _, perm = assignments.sort(stable=True)
    pack_contents_indices = perm.view(num_packs, groups_per_pack)
    pack_contents = pack_contents_indices.unsqueeze(0).expand(num_layers, -1, -1).clone()

    # 3. Vectorized Local Search
    # Iterate to swap items between the heaviest (Max) and ANY other pack to reduce max load.

    num_iters = 20
    layer_arange = torch.arange(num_layers, device=device)

    for _ in range(num_iters):
        # Gather weights: [L, M, K]
        flat_contents = pack_contents.view(num_layers, -1)
        current_weights = torch.gather(sorted_weight, 1, flat_contents).view(num_layers, num_packs, groups_per_pack)

        # Compute sums: [L, M]
        pack_sums = current_weights.sum(dim=2)

        # Find Max pack: [L]
        val_max, idx_max_pack = pack_sums.max(dim=1)

        # Get weights in Max pack: [L, K]
        w_max = current_weights.gather(1, idx_max_pack.view(num_layers, 1, 1).expand(-1, -1, groups_per_pack)).squeeze(1)

        # Compute diffs: [L, M, K_max, K_other]
        # w_max: [L, 1, K, 1]
        # current_weights: [L, M, 1, K]
        w_max_expanded = w_max.view(num_layers, 1, groups_per_pack, 1)
        current_weights_expanded = current_weights.view(num_layers, num_packs, 1, groups_per_pack)

        diffs = w_max_expanded - current_weights_expanded # [L, M, K, K]

        # Target improvement logic
        # New Max Pack Weight = val_max - diff
        # New Other Pack Weight = pack_sums[other] + diff
        # New pair max = max(val_max - diff, pack_sums[other] + diff)
        # Improvement = val_max - New pair max

        val_max_expanded = val_max.view(num_layers, 1, 1, 1)
        pack_sums_expanded = pack_sums.view(num_layers, num_packs, 1, 1)

        new_pair_max = torch.max(val_max_expanded - diffs, pack_sums_expanded + diffs)
        improvement = val_max_expanded - new_pair_max

        # Mask out invalid swaps
        mask_self = torch.arange(num_packs, device=device).view(1, -1).expand(num_layers, -1) == idx_max_pack.view(-1, 1)
        mask_self = mask_self.view(num_layers, num_packs, 1, 1)

        valid_mask = (diffs > 0) & (improvement > 1e-6) & (~mask_self)
        improvement = torch.where(valid_mask, improvement, torch.tensor(float('-inf'), device=device))

        # Find best swap
        flat_imp = improvement.view(num_layers, -1)
        best_imp, flat_idx = flat_imp.max(dim=1)

        if not (best_imp > float('-inf')).any():
            break

        # Decode indices
        k_sq = groups_per_pack * groups_per_pack
        m_idx = flat_idx // k_sq
        rem = flat_idx % k_sq
        k_max_idx = rem // groups_per_pack
        k_other_idx = rem % groups_per_pack

        l_valid = torch.nonzero(best_imp > float('-inf')).squeeze(1)

        if len(l_valid) > 0:
            p_max = idx_max_pack[l_valid]
            p_other = m_idx[l_valid]

            idx_in_max = k_max_idx[l_valid]
            idx_in_other = k_other_idx[l_valid]

            val_max_item = pack_contents[l_valid, p_max, idx_in_max]
            val_other_item = pack_contents[l_valid, p_other, idx_in_other]

            pack_contents[l_valid, p_max, idx_in_max] = val_other_item
            pack_contents[l_valid, p_other, idx_in_other] = val_max_item

    # 4. Construct outputs
    pack_ids = torch.arange(num_packs, device=device).view(1, num_packs, 1).expand(num_layers, -1, groups_per_pack)
    rank_ids = torch.arange(groups_per_pack, device=device).view(1, 1, groups_per_pack).expand(num_layers, num_packs, -1)

    flat_sorted_idx_ptr = pack_contents.view(num_layers, -1)
    flat_pack_ids = pack_ids.reshape(num_layers, -1)
    flat_rank_ids = rank_ids.reshape(num_layers, -1)

    original_idx = torch.gather(sorted_indices, 1, flat_sorted_idx_ptr)

    pack_index = torch.empty_like(flat_pack_ids)
    rank_in_pack = torch.empty_like(flat_rank_ids)

    pack_index.scatter_(1, original_idx, flat_pack_ids)
    rank_in_pack.scatter_(1, original_idx, flat_rank_ids)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>