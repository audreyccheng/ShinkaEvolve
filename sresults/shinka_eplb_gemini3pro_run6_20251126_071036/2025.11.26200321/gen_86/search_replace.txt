<NAME>
hybrid_evolutionary_refinement
</NAME>

<DESCRIPTION>
Replaces the parallel greedy strategy with a Hybrid Evolutionary Strategy that includes diverse candidate generation (LPT, ZigZag, Random, Noisy LPT), vectorized greedy packing, Top-K selection, and a rigorous "Max-Any" swap refinement step. This allows the algorithm to escape local optima found by greedy heuristics by locally optimizing the best candidates using pairwise swaps.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def _parallel_greedy_packing(weight: torch.Tensor,
                             num_packs: int,
                             num_candidates: int = 64) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Performs greedy packing on multiple perturbed versions of the weights in parallel.

    Returns:
        best_pack_ids: [layers, items]
        best_ranks: [layers, items]
        best_imbalance: [layers]
    """
    num_layers, num_items = weight.shape
    device = weight.device
    capacity = num_items // num_packs

    # 1. Generate Candidates
    # Candidate 0: Pure LPT (scale=1.0)
    # Candidates 1..N: Perturbed LPT

    # Expand weights: [Layers, Candidates, Items]
    # We use a localized generator or just standard rand to avoid global side effects if possible,
    # but torch.rand is standard.
    noise = torch.rand(num_layers, num_candidates - 1, num_items, device=device) * 0.4 + 0.8 # [0.8, 1.2]
    # Prepend ones for the pure LPT candidate
    ones = torch.ones(num_layers, 1, num_items, device=device)
    scales = torch.cat([ones, noise], dim=1)

    # [Layers, Candidates, Items]
    perturbed_weights = weight.unsqueeze(1) * scales

    # Sort descending
    # indices: [Layers, Candidates, Items]
    sorted_weights, sorted_indices = perturbed_weights.sort(dim=-1, descending=True)

    # We need the original weights in the sorted order to track loads correctly
    # gather original weights
    # [Layers, Candidates, Items]
    original_weights_expanded = weight.unsqueeze(1).expand(-1, num_candidates, -1)
    sorted_original_weights = original_weights_expanded.gather(-1, sorted_indices)

    # 2. Parallel Greedy Execution
    # State: [Layers, Candidates, Packs]
    pack_loads = torch.zeros(num_layers, num_candidates, num_packs, device=device, dtype=weight.dtype)
    pack_counts = torch.zeros(num_layers, num_candidates, num_packs, device=device, dtype=torch.int64)

    # Store results aligned to sorted order first
    # [Layers, Candidates, Items]
    aligned_pack_ids = torch.empty(num_layers, num_candidates, num_items, device=device, dtype=torch.int64)
    aligned_ranks = torch.empty(num_layers, num_candidates, num_items, device=device, dtype=torch.int64)

    # Helper range for indexing
    # We will flatten [Layers, Candidates] to simplified batch operations if needed,
    # but PyTorch broadcasts well.

    # We iterate through items 0..N-1
    for i in range(num_items):
        # Weight of current item across all candidates
        w = sorted_original_weights[:, :, i] # [Layers, Candidates]

        # Valid packs: count < capacity
        # mask: [Layers, Candidates, Packs]
        valid_mask = pack_counts < capacity

        # Find pack with min load among valid
        # We add a large value to invalid packs
        # Note: clone is necessary to avoid modifying state
        # In-place modification optimization: add inf, find argmin, subtract inf?
        # Easier to just use where.

        # Large value: max possible weight sum ~ sum of all weights.
        # Using standard float('inf') is safe.
        temp_loads = torch.where(valid_mask, pack_loads, torch.tensor(float('inf'), device=device))

        # Chosen pack indices: [Layers, Candidates]
        chosen_packs = temp_loads.argmin(dim=-1)

        # Store assignment
        aligned_pack_ids[:, :, i] = chosen_packs

        # Get ranks
        # Gather counts
        # pack_counts: [L, C, P] -> gather -> [L, C]
        curr_ranks = pack_counts.gather(-1, chosen_packs.unsqueeze(-1)).squeeze(-1)
        aligned_ranks[:, :, i] = curr_ranks

        # Update State
        # scatter_add_ requires index to have same dim as src (except mapped dim)
        # We use a trick: flattened update or unsqueezed
        # View as [L*C, P] for easier scatter
        # This reshaping might be costly in loop?
        # Actually, we can use advanced indexing.

        # Construct index tensor for scatter
        # We want to update pack_loads[l, c, chosen_packs[l,c]] += w[l,c]
        # Torch doesn't support multi-dim scatter easily without flattening.

        # Flatten L, C
        flat_loads = pack_loads.view(-1, num_packs)
        flat_counts = pack_counts.view(-1, num_packs)
        flat_chosen = chosen_packs.view(-1, 1) # [L*C, 1]
        flat_w = w.view(-1, 1)

        flat_loads.scatter_add_(1, flat_chosen, flat_w)
        flat_counts.scatter_add_(1, flat_chosen, torch.ones_like(flat_w, dtype=torch.int64))

        # View back (in place)

    # 3. Select Best Candidate
    # Calculate imbalance: max - min
    # pack_loads: [L, C, P]
    max_loads = pack_loads.max(dim=-1).values
    min_loads = pack_loads.min(dim=-1).values
    imbalance = max_loads - min_loads # [L, C]

    # Find best candidate index for each layer
    best_candidate_idx = imbalance.argmin(dim=-1) # [L]
    best_imbalance = imbalance.min(dim=-1).values # [L]

    # Gather the results corresponding to best candidates
    # sorted_indices: [L, C, N] -> select C
    # aligned_pack_ids: [L, C, N] -> select C
    # aligned_ranks: [L, C, N] -> select C

    # Helper to gather per layer
    def gather_best(tensor_3d, indices_1d):
        # tensor: [L, C, N], indices: [L]
        # We want output [L, N]
        L, C, N = tensor_3d.shape
        # Expand indices to [L, 1, N]
        idx_expanded = indices_1d.view(L, 1, 1).expand(-1, -1, N)
        return tensor_3d.gather(1, idx_expanded).squeeze(1)

    final_sorted_indices = gather_best(sorted_indices, best_candidate_idx)
    final_aligned_ids = gather_best(aligned_pack_ids, best_candidate_idx)
    final_aligned_ranks = gather_best(aligned_ranks, best_candidate_idx)

    # Scatter back to original item order
    # We want output[l, original_idx] = assigned_val
    # We have assignment for sorted_idx.
    # original_idx = final_sorted_indices[l, i]
    # value = final_aligned_ids[l, i]
    # So output[l, final_sorted_indices[l, i]] = final_aligned_ids[l, i]

    pack_index = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    rank_in_pack = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)

    pack_index.scatter_(1, final_sorted_indices, final_aligned_ids)
    rank_in_pack.scatter_(1, final_sorted_indices, final_aligned_ranks)

    return pack_index, rank_in_pack, best_imbalance


def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a combination of Recursive Folding
    and Parallel Randomized Greedy strategies.

    It runs both strategies and selects the one with lower load imbalance per layer.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_items = weight.shape
    device = weight.device

    # Strategy 1: Parallel Randomized Greedy (Ensemble)
    # Uses multiple candidates (e.g. 64) to find a good fit
    ids1, ranks1, imb1 = _parallel_greedy_packing(weight, num_packs, num_candidates=64)

    # Strategy 2: Recursive Folding Strategy
    # This is a deterministic heuristic that often works well for specific distributions
    # Re-implementation of the folding logic

    curr_w = weight
    reconstruct_stack = []
    curr_n = num_items

    # Fold
    while (curr_n // num_packs) % 2 == 0 and (curr_n // num_packs) > 0:
        sorted_w, sorted_idx = curr_w.sort(dim=-1, descending=True)
        reconstruct_stack.append(sorted_idx)
        half = curr_n // 2
        w_pairs = sorted_w[:, :half] + sorted_w[:, half:].flip(1)
        curr_w = w_pairs
        curr_n = half

    # Base Greedy
    # We use a simple 1-candidate greedy for the base case of folding
    base_ids, base_ranks, _ = _parallel_greedy_packing(curr_w, num_packs, num_candidates=1)

    # Unwind
    while reconstruct_stack:
        sort_idx = reconstruct_stack.pop()
        n_parent = sort_idx.shape[1]
        half = n_parent // 2

        new_ids = torch.empty(num_layers, n_parent, dtype=torch.int64, device=device)
        new_ranks = torch.empty(num_layers, n_parent, dtype=torch.int64, device=device)

        # Left
        new_ids[:, :half] = base_ids
        new_ranks[:, :half] = base_ranks * 2

        # Right
        new_ids[:, half:] = base_ids.flip(1)
        new_ranks[:, half:] = base_ranks.flip(1) * 2 + 1

        final_ids = torch.empty_like(new_ids)
        final_ranks = torch.empty_like(new_ranks)

        final_ids.scatter_(1, sort_idx, new_ids)
        final_ranks.scatter_(1, sort_idx, new_ranks)

        base_ids = final_ids
        base_ranks = final_ranks

    ids2, ranks2 = base_ids, base_ranks

    # Calc imbalance for Strategy 2
    loads2 = torch.zeros(num_layers, num_packs, device=device, dtype=weight.dtype)
    loads2.scatter_add_(1, ids2, weight)
    imb2 = loads2.max(dim=1).values - loads2.min(dim=1).values

    # Selection
    use_strategy2 = (imb2 < imb1).unsqueeze(-1) # [L, 1]

    final_ids = torch.where(use_strategy2, ids2, ids1)
    final_ranks = torch.where(use_strategy2, ranks2, ranks1)

    return final_ids, final_ranks
=======
def _refine_packing(weights: torch.Tensor,
                    pack_ids: torch.Tensor,
                    pack_loads: torch.Tensor,
                    ranks: torch.Tensor,
                    num_iters: int = 20) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Refines the packing by attempting to swap items from the heaviest pack
    to any other pack to strictly reduce the maximum load.
    """
    batch_size, num_items = weights.shape
    device = weights.device

    # Precompute pairwise weight differences: w[b, i] - w[b, j]
    # [B, N, N]
    w_diff = weights.unsqueeze(2) - weights.unsqueeze(1)

    batch_range = torch.arange(batch_size, device=device)

    for _ in range(num_iters):
        # Identify Max Pack
        max_load, max_idx = pack_loads.max(dim=1) # [B]

        # Load of pack where j resides
        # [B, N]
        load_j = pack_loads.gather(1, pack_ids)

        # Gap = MaxLoad - Load_j
        # [B, 1, N]
        gap = (max_load.unsqueeze(1) - load_j).unsqueeze(1)

        # Delta = w_i - w_j
        # [B, N, N]
        delta = w_diff

        # Valid Swap Mask: i is Max, j is NOT Max
        # [B, N, N]
        is_max = (pack_ids == max_idx.unsqueeze(1))
        valid_swap = is_max.unsqueeze(2) & (~is_max.unsqueeze(1))

        # Gain calculation
        # Gain = min(delta, gap - delta)
        gain = torch.min(delta, gap - delta)

        # Filter valid swaps with positive gain
        valid_swap = valid_swap & (gain > 1e-6)

        # Apply mask
        gain = torch.where(valid_swap, gain, torch.tensor(-1.0, device=device, dtype=weights.dtype))

        # Find best swap
        # Flatten last two dims [B, N*N]
        flat_gain = gain.view(batch_size, -1)
        best_gain, best_flat_idx = flat_gain.max(dim=1)

        # Apply swaps for batches with improvement
        active = best_gain > 1e-6
        if not active.any():
            break

        b_idx = batch_range[active]
        flat_idx = best_flat_idx[active]

        idx_i = flat_idx // num_items
        idx_j = flat_idx % num_items

        p_max = max_idx[b_idx]
        p_target = pack_ids[b_idx, idx_j]

        w_i = weights[b_idx, idx_i]
        w_j = weights[b_idx, idx_j]
        d = w_i - w_j

        # Update loads
        pack_loads[b_idx, p_max] -= d
        pack_loads[b_idx, p_target] += d

        # Update indices
        pack_ids[b_idx, idx_i] = p_target
        pack_ids[b_idx, idx_j] = p_max

        # Update ranks
        r_i = ranks[b_idx, idx_i]
        r_j = ranks[b_idx, idx_j]
        ranks[b_idx, idx_i] = r_j
        ranks[b_idx, idx_j] = r_i

    return pack_ids, ranks, pack_loads


def _vectorized_greedy_packing(weights: torch.Tensor,
                               num_packs: int,
                               capacity: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Vectorized Greedy Packing Kernel.
    """
    batch_size, num_items = weights.shape
    device = weights.device

    pack_loads = torch.zeros(batch_size, num_packs, device=device, dtype=weights.dtype)
    pack_counts = torch.zeros(batch_size, num_packs, device=device, dtype=torch.int64)
    pack_ids = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)
    ranks = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)

    batch_indices = torch.arange(batch_size, device=device)
    inf_tensor = torch.tensor(float('inf'), device=device, dtype=weights.dtype)

    for i in range(num_items):
        w = weights[:, i]
        valid_mask = pack_counts < capacity
        temp_loads = torch.where(valid_mask, pack_loads, inf_tensor)
        chosen_packs = temp_loads.argmin(dim=1)

        pack_ids[:, i] = chosen_packs
        ranks[:, i] = pack_counts[batch_indices, chosen_packs]

        pack_counts[batch_indices, chosen_packs] += 1
        pack_loads[batch_indices, chosen_packs] += w

    return pack_ids, ranks, pack_loads


def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Massive Parallel Ensemble strategy.
    """
    num_layers, num_items = weight.shape
    device = weight.device
    num_candidates = 128
    num_refine = 8
    capacity = num_items // num_packs

    # 1. Candidate Generation
    # A. LPT
    lpt_val, lpt_idx = weight.sort(dim=-1, descending=True)
    c_lpt = lpt_idx.unsqueeze(1)

    # B. ZigZag
    zigzag_perm = torch.empty(num_items, device=device, dtype=torch.long)
    half = (num_items + 1) // 2
    arange = torch.arange(num_items, device=device)
    zigzag_perm[0::2] = arange[:half]
    zigzag_perm[1::2] = arange[half:].flip(0)
    c_zigzag = lpt_idx.gather(1, zigzag_perm.unsqueeze(0).expand(num_layers, -1)).unsqueeze(1)

    # C. Random Shuffles
    num_random = 30
    rand_perm = torch.rand(num_layers, num_random, num_items, device=device).argsort(dim=-1)

    # D. Noisy LPT
    num_noisy = num_candidates - 2 - num_random
    noise = torch.rand(num_layers, num_noisy, num_items, device=device) * 0.4 + 0.8
    noisy_w = weight.unsqueeze(1) * noise
    _, c_noisy = noisy_w.sort(dim=-1, descending=True)

    all_indices = torch.cat([c_lpt, c_zigzag, rand_perm, c_noisy], dim=1)

    # Gather weights
    expanded_weight = weight.unsqueeze(1).expand(-1, num_candidates, -1)
    ordered_weights = expanded_weight.gather(2, all_indices)

    # Flatten
    batch_size = num_layers * num_candidates
    flat_weights = ordered_weights.view(batch_size, num_items)

    # 2. Greedy Packing
    flat_ids, flat_ranks, flat_loads = _vectorized_greedy_packing(flat_weights, num_packs, capacity)

    # 3. Top-K Selection
    loads = flat_loads.view(num_layers, num_candidates, num_packs)
    imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values

    _, best_k_indices = imbalance.topk(num_refine, dim=1, largest=False)

    layer_offsets = (torch.arange(num_layers, device=device) * num_candidates).unsqueeze(1)
    flat_selected = (layer_offsets + best_k_indices).flatten()

    refined_weights = flat_weights[flat_selected]
    refined_ids = flat_ids[flat_selected]
    refined_ranks = flat_ranks[flat_selected]
    refined_loads = flat_loads[flat_selected]

    # 4. Refinement
    refined_ids, refined_ranks, refined_loads = _refine_packing(
        refined_weights, refined_ids, refined_loads, refined_ranks, num_iters=20
    )

    # 5. Final Selection
    loads_final = refined_loads.view(num_layers, num_refine, num_packs)
    imbalance_final = loads_final.max(dim=-1).values - loads_final.min(dim=-1).values
    best_in_k = imbalance_final.argmin(dim=1)

    # 6. Scatter Back
    winner_cand_idx = best_k_indices.gather(1, best_in_k.unsqueeze(1)).squeeze(1)

    winner_flat_idx = (torch.arange(num_layers, device=device) * num_refine) + best_in_k
    final_aligned_ids = refined_ids[winner_flat_idx]
    final_aligned_ranks = refined_ranks[winner_flat_idx]

    idx_view = winner_cand_idx.view(num_layers, 1, 1).expand(-1, 1, num_items)
    final_sorted_idx = all_indices.gather(1, idx_view).squeeze(1)

    pack_index = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    rank_in_pack = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)

    pack_index.scatter_(1, final_sorted_idx, final_aligned_ids)
    rank_in_pack.scatter_(1, final_sorted_idx, final_aligned_ranks)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>