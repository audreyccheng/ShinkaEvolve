--- a/original.py
+++ b/original.py
@@ -1,494 +1,482 @@
 # EVOLVE-BLOCK-START
 """
 Expert parallelism load balancer (EPLB) for vLLM.
 
 This module implements the core rearrangement algorithm using a
-Hybrid Ensemble strategy with Massive Parallelism and Top-K Refinement.
+Hybrid Ensemble Greedy strategy with Massive Parallelism and Top-K Refinement.
 """
 import torch
+
 
 def _refine_packing(weights: torch.Tensor,
                     pack_ids: torch.Tensor,
                     pack_loads: torch.Tensor,
                     ranks: torch.Tensor,
                     num_iters: int = 10) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Refines the packing by iteratively attempting to swap a single item between 
     the heaviest and lightest packs to reduce imbalance.
     """
     batch_size, num_items = weights.shape
     device = weights.device
     batch_indices = torch.arange(batch_size, device=device)
     
-    # Pre-compute constant memory for diff expansion if possible, but B*N*N might be too big for 128 candidates.
-    # Since we prune to Top-K (e.g. 8), B is small (Layers * 8).
-    # If Layers=32, B=256. N=256. 256^3 * 4 bytes = 64MB. Very safe.
-
+    # Iterate to reduce imbalance
     for _ in range(num_iters):
         # Identify heaviest and lightest packs
         max_load, max_pack_idx = pack_loads.max(dim=1)
         min_load, min_pack_idx = pack_loads.min(dim=1)
         current_diff = max_load - min_load
 
-        # Stop if imbalance is negligible
+        # Early exit if perfectly balanced (or close enough)
         if current_diff.max() < 1e-5:
             break
 
         # Masks for items in max/min packs
+        # [B, N]
         is_in_max = (pack_ids == max_pack_idx.unsqueeze(1))
         is_in_min = (pack_ids == min_pack_idx.unsqueeze(1))
 
         # Weight diffs: w[i] - w[j]
         # [B, N, 1] - [B, 1, N] = [B, N, N]
         w_diff = weights.unsqueeze(2) - weights.unsqueeze(1)
 
         # Metric: |(diff) - 2*(w_i - w_j)|
         target = current_diff.view(-1, 1, 1)
         new_diff_metric = torch.abs(target - 2 * w_diff)
 
         # Valid mask: i in Max, j in Min
         valid_swap = is_in_max.unsqueeze(2) & is_in_min.unsqueeze(1)
         
-        # Improvement check: New diff < Current diff
+        # Improvement check: New diff < Current diff - epsilon
         # We mask invalid swaps with infinity
-        # We also want strict improvement to avoid cycling 
-        # (though abs metric prevents cycling usually, float precision issues can cause flip-flopping)
+        improved = new_diff_metric < (target - 1e-6)
+        valid_swap = valid_swap & improved
+        
         new_diff_metric = torch.where(valid_swap, new_diff_metric, torch.tensor(float('inf'), device=device))
 
         # Find best swap per batch
         # flatten last two dims
         flat_metric = new_diff_metric.view(batch_size, -1)
         min_val, flat_idx = flat_metric.min(dim=1)
 
-        # Check improvement
-        improve_mask = min_val < (current_diff - 1e-6)
+        # Check if any batch improved
+        improve_mask = min_val != float('inf')
         
         if not improve_mask.any():
             break
 
         # Apply swaps
         active_batch = batch_indices[improve_mask]
         active_idx = flat_idx[improve_mask]
         
         i_idx = active_idx // num_items
         j_idx = active_idx % num_items
         
         p_max = max_pack_idx[active_batch]
         p_min = min_pack_idx[active_batch]
         
         w_i = weights[active_batch, i_idx]
         w_j = weights[active_batch, j_idx]
         delta = w_i - w_j
         
         # Update loads
         pack_loads[active_batch, p_max] -= delta
         pack_loads[active_batch, p_min] += delta
         
         # Update IDs
         pack_ids[active_batch, i_idx] = p_min
         pack_ids[active_batch, j_idx] = p_max
         
-        # Update Ranks
-        # Swap ranks to maintain valid rank set
+        # Update Ranks (swap)
         r_i = ranks[active_batch, i_idx]
         r_j = ranks[active_batch, j_idx]
         ranks[active_batch, i_idx] = r_j
         ranks[active_batch, j_idx] = r_i
 
     return pack_ids, ranks, pack_loads
 
 
 def _vectorized_greedy_packing(weights: torch.Tensor,
                                num_packs: int,
                                capacity: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Vectorized Greedy Packing Kernel.
+    Assigns each item to the valid pack with the minimum current load.
     """
     batch_size, num_items = weights.shape
     device = weights.device
 
     pack_loads = torch.zeros(batch_size, num_packs, device=device, dtype=weights.dtype)
     pack_counts = torch.zeros(batch_size, num_packs, device=device, dtype=torch.int64)
     pack_ids = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)
     ranks = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)
 
     batch_indices = torch.arange(batch_size, device=device)
     inf_tensor = torch.tensor(float('inf'), device=device, dtype=weights.dtype)
 
     for i in range(num_items):
         w = weights[:, i]
         valid_mask = pack_counts < capacity
         temp_loads = torch.where(valid_mask, pack_loads, inf_tensor)
         chosen_packs = temp_loads.argmin(dim=1)
 
         pack_ids[:, i] = chosen_packs
         ranks[:, i] = pack_counts[batch_indices, chosen_packs]
 
         pack_counts[batch_indices, chosen_packs] += 1
         pack_loads[batch_indices, chosen_packs] += w
 
     return pack_ids, ranks, pack_loads
 
 
 def balanced_packing(weight: torch.Tensor,
                      num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
     """
     Pack n weighted objects to m packs using a Hybrid Ensemble Strategy with Top-K Refinement.
+    
+    Strategies:
+    - LPT (1)
+    - ZigZag (1)
+    - Small Noise LPT (40): Perturbation [0.9, 1.1]
+    - Large Noise LPT (40): Perturbation [0.6, 1.4]
+    - Random Shuffle (46): Total 128 candidates.
+    
+    Flow:
+    1. Generate 128 candidate permutations.
+    2. Run Vectorized Greedy Packing on all.
+    3. Select Top-16 candidates with lowest load imbalance.
+    4. Run Refinement (15 iters) on Top-16.
+    5. Select Best.
     """
     num_layers, num_items = weight.shape
     device = weight.device
     capacity = num_items // num_packs
+    num_candidates = 128
     
     # 1. Candidate Generation
-    # Total candidates: 128
-    # Distribution:
-    # - 1 Pure LPT
-    # - 31 Folded/ZigZag variants (shifted/permuted)
-    # - 32 Random Shuffles
-    # - 64 Noisy LPT
-    
-    num_candidates = 128
     
     # A. LPT
     lpt_val, lpt_idx = weight.sort(dim=-1, descending=True)
     c_lpt = lpt_idx.unsqueeze(1) # [L, 1, N]
     
-    # B. Folded / ZigZag
-    # Create base zigzag permutation
+    # B. ZigZag
     zigzag_perm = torch.empty(num_items, device=device, dtype=torch.long)
     half = (num_items + 1) // 2
     arange = torch.arange(num_items, device=device)
     zigzag_perm[0::2] = arange[:half]
     zigzag_perm[1::2] = arange[half:].flip(0)
     
-    # Generate variants of zigzag by rolling? Or just one. 
-    # Let's do one main ZigZag and some shifted versions of it for diversity?
-    # Simpler: just 1 ZigZag, and give more slots to Noisy LPT or Random.
-    # Let's follow recommendation: 32 "Folded LPT".
-    # Implementation: Just using different "folding" strides or offsets? 
-    # Or just 1 Folded, and fill the rest with Random.
-    # Recommendation said "32 Folded LPT". We can simulate diversity by permuting LPT blocks.
-    # For now, let's just use 1 ZigZag, and 31 "Block Shuffled LPT".
-    
-    # Actually, let's stick to the high-level recommendation:
-    # 64 Noisy, 32 Random, 32 Folded-ish.
-    # To generate 32 Folded candidates, we can apply the ZigZag perm logic to Noisy LPT sorts?
-    # Or just use 1 ZigZag + 31 Random.
-    # Let's implement: 1 LPT, 1 ZigZag, 62 Noisy LPT, 64 Random Shuffle.
-    
-    # ZigZag Indices
     c_zigzag = lpt_idx.gather(1, zigzag_perm.unsqueeze(0).expand(num_layers, -1)).unsqueeze(1)
     
-    # Random Shuffles (64)
-    # Just random permutations
-    rand_keys = torch.randn(num_layers, 64, num_items, device=device)
-    _, c_random = rand_keys.sort(dim=-1)
-    
-    # Noisy LPT (62)
-    # Weights * Noise
-    noise = torch.rand(num_layers, 62, num_items, device=device) * 0.4 + 0.8
+    # C. Noise & Random
+    num_small = 40
+    num_large = 40
+    num_random = num_candidates - 2 - num_small - num_large # 46
+    
+    # Small Noise: [0.9, 1.1]
+    noise_small = (torch.rand(num_layers, num_small, num_items, device=device) * 0.2) + 0.9
+    
+    # Large Noise: [0.6, 1.4]
+    noise_large = (torch.rand(num_layers, num_large, num_items, device=device) * 0.8) + 0.6
+    
+    # Apply Noise
+    noise = torch.cat([noise_small, noise_large], dim=1)
     noisy_w = weight.unsqueeze(1) * noise
     _, c_noisy = noisy_w.sort(dim=-1, descending=True)
     
-    all_indices = torch.cat([c_lpt, c_zigzag, c_random, c_noisy], dim=1) # 1+1+64+62 = 128
-    
-    # Gather weights for kernel
+    # Random Shuffles
+    rand_keys = torch.randn(num_layers, num_random, num_items, device=device)
+    _, c_random = rand_keys.sort(dim=-1)
+    
+    # Combine Indices
+    all_indices = torch.cat([c_lpt, c_zigzag, c_noisy, c_random], dim=1)
+    
+    # Gather weights for packing
     expanded_weight = weight.unsqueeze(1).expand(-1, num_candidates, -1)
     ordered_weights = expanded_weight.gather(2, all_indices)
     
-    # Flatten
+    # Flatten for vectorized processing
     flat_weights = ordered_weights.view(-1, num_items)
     
-    # 2. Greedy Packing (Batched)
+    # 2. Greedy Packing
     flat_ids, flat_ranks, flat_loads = _vectorized_greedy_packing(flat_weights, num_packs, capacity)
     
-    # 3. Top-K Selection
-    # Calculate imbalance
+    # 3. Selection of Top-K
     loads = flat_loads.view(num_layers, num_candidates, num_packs)
-    imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values # [L, C]
-    
-    # Select Top 8 candidates per layer
-    k = 8
+    imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values
+    
+    # Select Top 16
+    k = 16
+    # If num_candidates < k (e.g. small tests), handle it
+    k = min(k, num_candidates)
+    
     best_vals, best_k_indices = imbalance.topk(k, dim=1, largest=False) # [L, K]
     
-    # Extract data for these Top K
-    # We need to flatten properly.
-    # Global index in flat_weights = layer_idx * 128 + candidate_idx
-    layer_offsets = (torch.arange(num_layers, device=device) * num_candidates).unsqueeze(1) # [L, 1]
-    global_indices = (layer_offsets + best_k_indices).view(-1) # [L*K]
-    
-    refined_weights = flat_weights[global_indices] # [L*K, N] (This is sorted/permuted weights)
-    # BUT, refine_packing requires weights to match pack_ids column-wise.
-    # flat_ids is [L*C, N] aligned with flat_weights.
-    # So refined_weights and refined_ids are aligned.
+    # Extract data for Top-K
+    layer_offsets = (torch.arange(num_layers, device=device) * num_candidates).unsqueeze(1)
+    global_indices = (layer_offsets + best_k_indices).view(-1)
+    
+    refined_weights = flat_weights[global_indices]
     refined_ids = flat_ids[global_indices]
     refined_ranks = flat_ranks[global_indices]
     refined_loads = flat_loads[global_indices]
     
-    # 4. Refinement on Top K
-    # Run iterative 1-item swap
+    # 4. Refinement on Top-K
     refined_ids, refined_ranks, refined_loads = _refine_packing(
-        refined_weights, refined_ids, refined_loads, refined_ranks, num_iters=20
+        refined_weights, refined_ids, refined_loads, refined_ranks, num_iters=15
     )
     
     # 5. Final Selection
-    # Re-calc imbalance
     final_imbalance = refined_loads.max(dim=1).values - refined_loads.min(dim=1).values # [L*K]
     final_imbalance = final_imbalance.view(num_layers, k)
     
     best_in_k = final_imbalance.argmin(dim=1) # [L]
     
-    # 6. Gather and Scatter Back
-    # We need to reconstruct the mapping to original items.
-    # The refined_ids are aligned to the candidate's permutation.
-    # We need the candidate's permutation indices.
-    
-    # Get the original permutation index for the winner
-    # best_k_indices[l, best_in_k[l]] gives the candidate index c within 0..127
+    # 6. Scatter Back
+    # Identify winner candidate index in original 128 list
     winner_cand_idx = best_k_indices.gather(1, best_in_k.unsqueeze(1)).squeeze(1) # [L]
     
-    # Get indices [L, N] from all_indices [L, 128, N]
     idx_view = winner_cand_idx.view(num_layers, 1, 1).expand(-1, 1, num_items)
-    final_sorted_idx = all_indices.gather(1, idx_view).squeeze(1) # [L, N]
-    
-    # Get the refined pack IDs and ranks for the winner
-    # We have refined_ids [L*K, N]. The winner is at index (l*k + best_in_k[l])
+    final_sorted_idx = all_indices.gather(1, idx_view).squeeze(1)
+    
+    # Identify winner index in refined arrays
     winner_flat_idx = (torch.arange(num_layers, device=device) * k) + best_in_k
-    final_aligned_ids = refined_ids[winner_flat_idx] # [L, N]
-    final_aligned_ranks = refined_ranks[winner_flat_idx] # [L, N]
-    
-    # Scatter back
+    
+    final_aligned_ids = refined_ids[winner_flat_idx]
+    final_aligned_ranks = refined_ranks[winner_flat_idx]
+    
     pack_index = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
     rank_in_pack = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
     
     pack_index.scatter_(1, final_sorted_idx, final_aligned_ids)
     rank_in_pack.scatter_(1, final_sorted_idx, final_aligned_ranks)
     
     return pack_index, rank_in_pack
+
 
 def replicate_experts(
         weight: torch.Tensor,
         num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Replicate experts using Binary Search on Max Load followed by Greedy Refinement.
     """
     num_layers, num_log = weight.shape
     device = weight.device
 
     if num_phy == num_log:
         phy2log = torch.arange(num_log, device=device).expand(num_layers, -1)
         rank = torch.zeros(num_layers, num_phy, dtype=torch.int64, device=device)
         logcnt = torch.ones(num_layers, num_log, dtype=torch.int64, device=device)
         return phy2log, rank, logcnt
 
     # Binary Search
     low = weight.sum(dim=-1, keepdim=True) / num_phy
     high = weight.max(dim=-1, keepdim=True).values
     low = torch.max(low, torch.tensor(1e-6, device=device))
 
     for _ in range(15):
         mid = (low + high) * 0.5
         counts = torch.ceil(weight / mid)
         total = counts.sum(dim=-1, keepdim=True)
         mask = total <= num_phy
         high = torch.where(mask, mid, high)
         low = torch.where(mask, low, mid)
 
     logcnt = torch.ceil(weight / high).long().clamp(min=1)
 
     # Correct sum
     current_sum = logcnt.sum(dim=-1)
     diff = num_phy - current_sum
 
-    # Under-allocation
+    # Under-allocation: Add to max density
     max_diff = int(diff.max().item())
     if max_diff > 0:
         rows = torch.arange(num_layers, device=device)
         for _ in range(max_diff):
             active = current_sum < num_phy
             if not active.any(): break
             
             density = weight / logcnt.float()
-            # mask inactive rows to avoid selecting them
-            density[~active] = -1.0
+            density[~active] = -1.0 # Mask inactive
             target_idx = density.argmax(dim=-1)
             
             active_rows = rows[active]
             active_targets = target_idx[active]
             
             logcnt.index_put_((active_rows, active_targets), 
                               torch.tensor(1, device=device, dtype=torch.int64), 
                               accumulate=True)
             current_sum[active] += 1
 
-    # Over-allocation
+    # Over-allocation: Remove from min cost
     min_diff = int(diff.min().item())
     if min_diff < 0:
         rows = torch.arange(num_layers, device=device)
         for _ in range(abs(min_diff)):
             active = current_sum > num_phy
             if not active.any(): break
             
             valid = logcnt > 1
             cost = weight / (logcnt - 1).float()
             cost[~valid] = float('inf')
             cost[~active] = float('inf')
             
             target_idx = cost.argmin(dim=-1)
             
             active_rows = rows[active]
             active_targets = target_idx[active]
             
             logcnt.index_put_((active_rows, active_targets), 
                               torch.tensor(-1, device=device, dtype=torch.int64), 
                               accumulate=True)
             current_sum[active] -= 1
 
     # Construct maps
     flat_log_ids = torch.arange(num_log, device=device).repeat(num_layers)
     flat_counts = logcnt.flatten()
     flat_phy2log = torch.repeat_interleave(flat_log_ids, flat_counts)
     
     target_size = num_layers * num_phy
     if flat_phy2log.numel() != target_size:
         if flat_phy2log.numel() < target_size:
             flat_phy2log = torch.cat([flat_phy2log, torch.zeros(target_size - flat_phy2log.numel(), device=device, dtype=torch.long)])
         else:
             flat_phy2log = flat_phy2log[:target_size]
             
     phy2log = flat_phy2log.view(num_layers, num_phy)
     
     offsets = torch.zeros_like(logcnt)
     offsets[:, 1:] = logcnt[:, :-1].cumsum(dim=1)
     mapped_offsets = offsets.gather(1, phy2log)
     phy_indices = torch.arange(num_phy, device=device).expand(num_layers, -1)
     rank = phy_indices - mapped_offsets
 
     return phy2log, rank, logcnt
 
 
 def rebalance_experts_hierarchical(
     weight: torch.Tensor,
     num_physical_experts: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ):
     """
     Hierarchical rebalancing.
     """
     num_layers, num_logical_experts = weight.shape
     group_size = num_logical_experts // num_groups
     groups_per_node = num_groups // num_nodes
     phy_experts_per_gpu = num_physical_experts // num_gpus
 
     def inverse(perm: torch.Tensor) -> torch.Tensor:
         inv = torch.empty_like(perm)
         inv.scatter_(
             1,
             perm,
             torch.arange(perm.size(1), dtype=torch.int64,
                          device=perm.device).expand(perm.shape),
         )
         return inv
 
     # 1. Groups -> Nodes
     tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)
     group_pack_index, group_rank_in_pack = balanced_packing(
         tokens_per_group, num_nodes)
         
     log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *
                  group_size).unsqueeze(-1) +
                 torch.arange(group_size,
                              dtype=torch.int64,
                              device=group_pack_index.device)).flatten(-2)
     mlog2log = inverse(log2mlog)
 
     # 2. Replicate within nodes
     tokens_per_mlog = weight.gather(-1, mlog2log).view(
         -1, num_logical_experts // num_nodes)
     phy2mlog, phyrank, mlogcnt = replicate_experts(
         tokens_per_mlog, num_physical_experts // num_nodes)
 
     # 3. Physical -> GPUs
     tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
     pack_index, rank_in_pack = balanced_packing(tokens_per_phy,
                                                 num_gpus // num_nodes)
     
     phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
     pphy2phy = inverse(phy2pphy)
 
     pphy2mlog = phy2mlog.gather(-1, pphy2phy)
     
     node_offsets = torch.arange(
         0,
         num_logical_experts,
         num_logical_experts // num_nodes,
         device=weight.device,
     ).view(1, -1, 1)
     
     pphy2mlog_restored = (pphy2mlog.view(num_layers, num_nodes, -1) + node_offsets).flatten(-2)
     
     pphy2log = mlog2log.gather(-1, pphy2mlog_restored)
     pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)
     logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)
     
     return pphy2log, pphyrank, logcnt
 
 
 def rebalance_experts(
     weight: torch.Tensor,
     num_replicas: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Entry point.
     """
     num_layers, num_logical_experts = weight.shape
     weight = weight.float().cpu()
     
     if num_groups % num_nodes == 0:
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, num_groups, num_nodes, num_gpus)
     else:
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, 1, 1, num_gpus)
             
     max_replicas = int(logcnt.max().item())
     
     log2phy = torch.full(
         (num_layers, num_logical_experts, max_replicas),
         -1,
         dtype=torch.int64,
         device=logcnt.device,
     )
     
     flat_layer_idx = torch.arange(num_layers, device=logcnt.device).unsqueeze(-1).expand(-1, num_replicas).flatten()
     flat_log_idx = phy2log.flatten()
     flat_rank_idx = phyrank.flatten()
     flat_phy_ids = torch.arange(num_replicas, dtype=torch.int64, device=logcnt.device).expand(num_layers, -1).flatten()
     
     indices = (flat_layer_idx * num_logical_experts * max_replicas) + \
               (flat_log_idx * max_replicas) + \
               flat_rank_idx
               
     log2phy.view(-1).scatter_(0, indices, flat_phy_ids)
     
     return phy2log, log2phy, logcnt
 
 # EVOLVE-BLOCK-END
 
 
 # This part remains fixed (not evolved)
 def run_eplb(weight: torch.Tensor, num_replicas: int, num_groups: int,
              num_nodes: int, num_gpus: int):
     """Run the expert parallelism load balancer"""
     phy2log, log2phy, logcnt = rebalance_experts(
         weight, num_replicas, num_groups, num_nodes, num_gpus
     )
     return phy2log, log2phy, logcnt
 
 
 __all__ = ["rebalance_experts", "run_eplb"]