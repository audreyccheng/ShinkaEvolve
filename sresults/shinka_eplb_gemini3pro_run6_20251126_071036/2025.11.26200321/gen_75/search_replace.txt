<NAME>
exhaustive_swap_refinement
</NAME>

<DESCRIPTION>
Improves the load balancing algorithm by:
1.  Upgrading `_refine_packing` to check for swaps between the heaviest pack and *any* other pack (exhaustive search), maximizing the reduction of the maximum load. This overcomes the limitation of only swapping with the lightest pack, which can get stuck in local optima. The swap logic minimizes the local maximum load of the pair involved.
2.  Increasing the number of candidates in `balanced_packing` from 128 to 256 to explore a wider search space.
3.  Optimizing the refinement logic to handle the larger candidate pool efficiently by focusing on the Top-8 candidates.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def _refine_packing(weights: torch.Tensor,
                    pack_ids: torch.Tensor,
                    pack_loads: torch.Tensor,
                    ranks: torch.Tensor,
                    num_iters: int = 5) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Refines the packing by attempting to swap a single item between the heaviest
    and lightest packs in each batch to reduce imbalance.
    """
    batch_size, num_items = weights.shape
    device = weights.device
    batch_indices = torch.arange(batch_size, device=device)

    # Iterative refinement
    for _ in range(num_iters):
        # Identify heaviest and lightest packs
        max_load, max_pack_idx = pack_loads.max(dim=1)
        min_load, min_pack_idx = pack_loads.min(dim=1)
        current_diff = max_load - min_load

        # Items in heaviest and lightest packs [B, N]
        is_in_max = (pack_ids == max_pack_idx.unsqueeze(1))
        is_in_min = (pack_ids == min_pack_idx.unsqueeze(1))

        # Calculate diff matrix: delta = w_i (max) - w_j (min)
        # We want D_new = |D_old - 2*delta| to be minimized.
        w_diff = weights.unsqueeze(2) - weights.unsqueeze(1)

        # Metric to minimize: |current_diff - 2 * delta|
        target_delta = current_diff.view(-1, 1, 1)
        improvement_metric = torch.abs(target_delta - 2 * w_diff)

        # Mask valid swaps: i in max, j in min
        valid_mask = is_in_max.unsqueeze(2) & is_in_min.unsqueeze(1)

        # Strictly improvement required: D_new < D_old
        is_improvement = improvement_metric < (target_delta - 1e-5)

        valid_swap = valid_mask & is_improvement

        # Apply mask
        improvement_metric = torch.where(valid_swap, improvement_metric, torch.tensor(float('inf'), device=device))

        # Find best swap per batch
        flat_metric = improvement_metric.view(batch_size, -1)
        min_val, flat_indices = flat_metric.min(dim=1)

        # Identify batches that can improve
        active_mask = min_val != float('inf')
        if not active_mask.any():
            break

        active_batch_idx = batch_indices[active_mask]
        active_flat_idx = flat_indices[active_mask]

        # Decode indices
        i_idx = active_flat_idx // num_items
        j_idx = active_flat_idx % num_items

        # Perform swap on active batches
        p_max = max_pack_idx[active_batch_idx]
        p_min = min_pack_idx[active_batch_idx]

        w_i = weights[active_batch_idx, i_idx]
        w_j = weights[active_batch_idx, j_idx]
        delta = w_i - w_j

        # Update Loads
        pack_loads[active_batch_idx, p_max] -= delta
        pack_loads[active_batch_idx, p_min] += delta

        # Update IDs
        pack_ids[active_batch_idx, i_idx] = p_min
        pack_ids[active_batch_idx, j_idx] = p_max

        # Update Ranks
        r_i = ranks[active_batch_idx, i_idx]
        r_j = ranks[active_batch_idx, j_idx]
        ranks[active_batch_idx, i_idx] = r_j
        ranks[active_batch_idx, j_idx] = r_i

    return pack_ids, ranks, pack_loads
=======
def _refine_packing(weights: torch.Tensor,
                    pack_ids: torch.Tensor,
                    pack_loads: torch.Tensor,
                    ranks: torch.Tensor,
                    num_iters: int = 10) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Refines the packing by attempting to swap a single item between the heaviest
    pack and ANY other pack to reduce the maximum load.
    """
    batch_size, num_items = weights.shape
    num_packs = pack_loads.size(1)
    device = weights.device
    batch_indices = torch.arange(batch_size, device=device)

    # Precompute pairwise weight differences: [B, N, N]
    w_diff = weights.unsqueeze(2) - weights.unsqueeze(1)

    for _ in range(num_iters):
        max_load, max_pack_idx = pack_loads.max(dim=1)

        # Track best swap per batch
        best_gain = torch.full((batch_size,), -1.0, device=device, dtype=weights.dtype)
        best_swap_flat = torch.zeros(batch_size, device=device, dtype=torch.int64)
        best_target_pack = torch.full((batch_size,), -1, device=device, dtype=torch.int64)

        # Mask items in heaviest pack
        is_in_max = (pack_ids == max_pack_idx.unsqueeze(1))

        # Try swapping with every other pack
        for p in range(num_packs):
            target_load = pack_loads[:, p]
            gap = max_load - target_load

            # Active batches where moving mass to p is possible
            active_p_mask = gap > 1e-5
            if not active_p_mask.any():
                continue

            is_in_target = (pack_ids == torch.tensor(p, device=device))

            # Valid pair: i in Max, j in Target
            valid_pair = is_in_max.unsqueeze(2) & is_in_target.unsqueeze(1)

            # Constraints:
            # 1. w_i - w_j > 0 (reduce max load)
            # 2. w_i - w_j < gap (target load < old max load)
            delta = w_diff
            valid_swap = valid_pair & (delta > 1e-5) & (delta < (gap.view(-1, 1, 1) - 1e-5))

            # Gain metric: maximize min(reduction_in_max, margin_in_target)
            gain = torch.min(delta, gap.view(-1, 1, 1) - delta)
            gain = torch.where(valid_swap, gain, torch.tensor(-1.0, device=device, dtype=weights.dtype))

            # Best gain for this pack
            p_max_gain, p_flat_idx = gain.view(batch_size, -1).max(dim=1)

            # Update global best
            improve = (p_max_gain > best_gain) & active_p_mask
            if improve.any():
                best_gain = torch.where(improve, p_max_gain, best_gain)
                best_swap_flat = torch.where(improve, p_flat_idx, best_swap_flat)
                best_target_pack = torch.where(improve, torch.tensor(p, device=device), best_target_pack)

        # Apply swaps
        active_mask = best_target_pack != -1
        if not active_mask.any():
            break

        active_batch_idx = batch_indices[active_mask]
        active_flat = best_swap_flat[active_mask]
        active_p = best_target_pack[active_mask]
        active_max_p = max_pack_idx[active_mask]

        i_idx = active_flat // num_items
        j_idx = active_flat % num_items

        w_i = weights[active_batch_idx, i_idx]
        w_j = weights[active_batch_idx, j_idx]
        delta_val = w_i - w_j

        # Update state
        pack_loads[active_batch_idx, active_max_p] -= delta_val
        pack_loads[active_batch_idx, active_p] += delta_val

        pack_ids[active_batch_idx, i_idx] = active_p
        pack_ids[active_batch_idx, j_idx] = active_max_p

        r_i = ranks[active_batch_idx, i_idx]
        r_j = ranks[active_batch_idx, j_idx]
        ranks[active_batch_idx, i_idx] = r_j
        ranks[active_batch_idx, j_idx] = r_i

    return pack_ids, ranks, pack_loads
>>>>>>> REPLACE
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Hybrid Ensemble Strategy.
    """
    num_layers, num_items = weight.shape
    device = weight.device
    capacity = num_items // num_packs

    # Configuration
    num_candidates = 128
    num_refine_candidates = 8  # Only refine the top K candidates per layer

    # 1. Base LPT Sort
    lpt_weights, lpt_indices = weight.sort(dim=-1, descending=True)

    # 2. ZigZag (Folded LPT)
    relative_zigzag = torch.empty(num_items, device=device, dtype=torch.long)
    half = (num_items + 1) // 2
    arange = torch.arange(num_items, device=device)
    relative_zigzag[0::2] = arange[:half]
    relative_zigzag[1::2] = arange[half:].flip(0)

    c_zigzag_weights = lpt_weights[:, relative_zigzag].unsqueeze(1)
    c_zigzag_idx = lpt_indices[:, relative_zigzag].unsqueeze(1)

    c_lpt_weights = lpt_weights.unsqueeze(1)
    c_lpt_idx = lpt_indices.unsqueeze(1)

    # 3. Random Shuffles (Exploration)
    num_shuffle = 30
    rand_perm = torch.rand(num_layers, num_shuffle, num_items,
                           device=device).argsort(dim=-1)

    orig_expanded_shuffle = weight.unsqueeze(1).expand(-1, num_shuffle, -1)
    c_shuffle_weights = orig_expanded_shuffle.gather(2, rand_perm)
    c_shuffle_idx = rand_perm

    # 4. Noisy LPT
    num_noisy = num_candidates - 2 - num_shuffle  # 96
    num_small = num_noisy // 2
    num_large = num_noisy - num_small

    noise_small = (torch.rand(num_layers, num_small, num_items, device=device) *
                   0.2) + 0.9
    noise_large = (torch.rand(num_layers, num_large, num_items, device=device) *
                   0.8) + 0.6

    noise = torch.cat([noise_small, noise_large], dim=1)
    noisy_weights_in = weight.unsqueeze(1) * noise
    _, noisy_sorted_idx = noisy_weights_in.sort(dim=-1, descending=True)

    orig_expanded_noisy = weight.unsqueeze(1).expand(-1, num_noisy, -1)
    actual_noisy_weights = orig_expanded_noisy.gather(2, noisy_sorted_idx)

    # Combine all
    all_weights = torch.cat(
        [c_lpt_weights, c_zigzag_weights, c_shuffle_weights, actual_noisy_weights],
        dim=1)
    all_indices = torch.cat(
        [c_lpt_idx, c_zigzag_idx, c_shuffle_idx, noisy_sorted_idx], dim=1)

    # Flatten for Greedy
    flat_weights = all_weights.view(-1, num_items)

    # Greedy Packing
    flat_ids, flat_ranks, flat_loads = _vectorized_greedy_packing(
        flat_weights, num_packs, capacity)

    # --- Pre-Selection for Refinement ---
    loads = flat_loads.view(num_layers, num_candidates, num_packs)
    imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values

    # Select top K candidates
    _, best_k_indices = imbalance.topk(num_refine_candidates,
                                       dim=1,
                                       largest=False)  # [L, K]

    # Flatten indices to select from flat tensors
    layer_offsets = (torch.arange(num_layers, device=device) *
                     num_candidates).unsqueeze(1)
    flat_selected_indices = (best_k_indices + layer_offsets).flatten()

    refined_weights = flat_weights[flat_selected_indices]
    refined_ids = flat_ids[flat_selected_indices]
    refined_ranks = flat_ranks[flat_selected_indices]
    refined_loads = flat_loads[flat_selected_indices]

    # Run Refinement on top K
    refined_ids, refined_ranks, refined_loads = _refine_packing(
        refined_weights, refined_ids, refined_loads, refined_ranks, num_iters=5)

    # --- Final Selection ---
    loads_final = refined_loads.view(num_layers, num_refine_candidates,
                                     num_packs)
    imbalance_final = loads_final.max(dim=-1).values - loads_final.min(
        dim=-1).values

    best_in_k_idx = imbalance_final.argmin(dim=1)  # [L]

    # Gather results
    idx_view = best_in_k_idx.view(num_layers, 1, 1).expand(-1, 1, num_items)
    final_aligned_ids = refined_ids.view(num_layers, num_refine_candidates,
                                         num_items).gather(1, idx_view).squeeze(1)
    final_aligned_ranks = refined_ranks.view(
        num_layers, num_refine_candidates,
        num_items).gather(1, idx_view).squeeze(1)

    # Recover global indices for scatter
    best_global_cand_idx = best_k_indices.gather(
        1, best_in_k_idx.unsqueeze(1)).squeeze(1)
    global_idx_view = best_global_cand_idx.view(num_layers, 1,
                                                1).expand(-1, 1, num_items)
    final_sorted_idx = all_indices.gather(1, global_idx_view).squeeze(1)

    # Scatter back
    pack_index = torch.empty(num_layers,
                             num_items,
                             device=device,
                             dtype=torch.int64)
    rank_in_pack = torch.empty(num_layers,
                               num_items,
                               device=device,
                               dtype=torch.int64)

    pack_index.scatter_(1, final_sorted_idx, final_aligned_ids)
    rank_in_pack.scatter_(1, final_sorted_idx, final_aligned_ranks)

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Hybrid Ensemble Strategy.
    """
    num_layers, num_items = weight.shape
    device = weight.device
    capacity = num_items // num_packs

    # Configuration
    num_candidates = 256
    num_refine_candidates = 8

    # 1. Base LPT Sort
    lpt_weights, lpt_indices = weight.sort(dim=-1, descending=True)

    # 2. ZigZag (Folded LPT)
    relative_zigzag = torch.empty(num_items, device=device, dtype=torch.long)
    half = (num_items + 1) // 2
    arange = torch.arange(num_items, device=device)
    relative_zigzag[0::2] = arange[:half]
    relative_zigzag[1::2] = arange[half:].flip(0)

    c_zigzag_weights = lpt_weights[:, relative_zigzag].unsqueeze(1)
    c_zigzag_idx = lpt_indices[:, relative_zigzag].unsqueeze(1)

    c_lpt_weights = lpt_weights.unsqueeze(1)
    c_lpt_idx = lpt_indices.unsqueeze(1)

    # 3. Random Shuffles (Exploration)
    num_shuffle = 64
    rand_perm = torch.rand(num_layers, num_shuffle, num_items,
                           device=device).argsort(dim=-1)

    orig_expanded_shuffle = weight.unsqueeze(1).expand(-1, num_shuffle, -1)
    c_shuffle_weights = orig_expanded_shuffle.gather(2, rand_perm)
    c_shuffle_idx = rand_perm

    # 4. Noisy LPT
    num_noisy = num_candidates - 2 - num_shuffle
    num_small = num_noisy // 2
    num_large = num_noisy - num_small

    noise_small = (torch.rand(num_layers, num_small, num_items, device=device) *
                   0.2) + 0.9
    noise_large = (torch.rand(num_layers, num_large, num_items, device=device) *
                   0.8) + 0.6

    noise = torch.cat([noise_small, noise_large], dim=1)
    noisy_weights_in = weight.unsqueeze(1) * noise
    _, noisy_sorted_idx = noisy_weights_in.sort(dim=-1, descending=True)

    orig_expanded_noisy = weight.unsqueeze(1).expand(-1, num_noisy, -1)
    actual_noisy_weights = orig_expanded_noisy.gather(2, noisy_sorted_idx)

    # Combine all
    all_weights = torch.cat(
        [c_lpt_weights, c_zigzag_weights, c_shuffle_weights, actual_noisy_weights],
        dim=1)
    all_indices = torch.cat(
        [c_lpt_idx, c_zigzag_idx, c_shuffle_idx, noisy_sorted_idx], dim=1)

    # Flatten for Greedy
    flat_weights = all_weights.view(-1, num_items)

    # Greedy Packing
    flat_ids, flat_ranks, flat_loads = _vectorized_greedy_packing(
        flat_weights, num_packs, capacity)

    # --- Pre-Selection for Refinement ---
    loads = flat_loads.view(num_layers, num_candidates, num_packs)
    imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values

    # Select top K candidates
    _, best_k_indices = imbalance.topk(num_refine_candidates,
                                       dim=1,
                                       largest=False)  # [L, K]

    # Flatten indices to select from flat tensors
    layer_offsets = (torch.arange(num_layers, device=device) *
                     num_candidates).unsqueeze(1)
    flat_selected_indices = (best_k_indices + layer_offsets).flatten()

    refined_weights = flat_weights[flat_selected_indices]
    refined_ids = flat_ids[flat_selected_indices]
    refined_ranks = flat_ranks[flat_selected_indices]
    refined_loads = flat_loads[flat_selected_indices]

    # Run Refinement on top K
    refined_ids, refined_ranks, refined_loads = _refine_packing(
        refined_weights, refined_ids, refined_loads, refined_ranks, num_iters=10)

    # --- Final Selection ---
    loads_final = refined_loads.view(num_layers, num_refine_candidates,
                                     num_packs)
    imbalance_final = loads_final.max(dim=-1).values - loads_final.min(
        dim=-1).values

    best_in_k_idx = imbalance_final.argmin(dim=1)  # [L]

    # Gather results
    idx_view = best_in_k_idx.view(num_layers, 1, 1).expand(-1, 1, num_items)
    final_aligned_ids = refined_ids.view(num_layers, num_refine_candidates,
                                         num_items).gather(1, idx_view).squeeze(1)
    final_aligned_ranks = refined_ranks.view(
        num_layers, num_refine_candidates,
        num_items).gather(1, idx_view).squeeze(1)

    # Recover global indices for scatter
    best_global_cand_idx = best_k_indices.gather(
        1, best_in_k_idx.unsqueeze(1)).squeeze(1)
    global_idx_view = best_global_cand_idx.view(num_layers, 1,
                                                1).expand(-1, 1, num_items)
    final_sorted_idx = all_indices.gather(1, global_idx_view).squeeze(1)

    # Scatter back
    pack_index = torch.empty(num_layers,
                             num_items,
                             device=device,
                             dtype=torch.int64)
    rank_in_pack = torch.empty(num_layers,
                               num_items,
                               device=device,
                               dtype=torch.int64)

    pack_index.scatter_(1, final_sorted_idx, final_aligned_ids)
    rank_in_pack.scatter_(1, final_sorted_idx, final_aligned_ranks)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>