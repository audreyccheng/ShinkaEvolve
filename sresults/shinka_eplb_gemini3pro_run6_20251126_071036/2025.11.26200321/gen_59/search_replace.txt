<NAME>
diverse_init_and_2swap
</NAME>

<DESCRIPTION>
1.  **Diverse Initialization**: Introduce a mix of initialization strategies. Candidates 0-95 use Randomized LPT (preserving the gradient of noise from 0.0 to 0.4), while candidates 96-127 use pure Random keys. This ensures we explore the search space broadly, avoiding basins of attraction specific to LPT.
2.  **Phase 2 Local Search (2-Swap)**: Implement a vectorized Max-Min 2-Swap refinement using `torch.triu_indices` to avoid `itertools`. After the single-item Max-Any swap converges, this new phase attempts to swap *pairs* of items between the heaviest pack and the lightest pack. This helps escape local optima where no single item swap can reduce the maximum load. This is applied only when group size K is between 2 and 32.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses a Massive Parallel Greedy LPT initialization followed by a
    Vectorized Max-Any Swap local search refinement.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_packs, dtype=torch.int64, device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(pack_index)
        return pack_index, rank_in_pack

    # --- 1. Massive Parallel Initialization ---
    # Increased candidates for better exploration
    num_candidates = 128
    num_total_problems = num_layers * num_candidates

    # [L*C, N]
    w_expanded = weight.repeat_interleave(num_candidates, dim=0)

    # Noise Spectrum: 0.0 to 0.4
    # Candidate 0 of each layer group is pure LPT (0 noise)
    scales = torch.linspace(0, 0.4, num_candidates, device=device)
    noise_scale = scales.repeat(num_layers).view(-1, 1)

    # Add noise
    noise = torch.rand_like(w_expanded, dtype=torch.float32) * (w_expanded.float() * noise_scale)
    noise = noise.to(dtype=weight.dtype)
    noise.view(num_layers, num_candidates, num_groups)[:, 0, :] = 0

    sort_keys = w_expanded + noise
    _, sorted_indices = sort_keys.sort(dim=-1, descending=True)
    sorted_weight = torch.gather(w_expanded, 1, sorted_indices)

    # --- 2. Vectorized Greedy LPT Construction ---
    # Fill packs greedily item by item
    pack_weights = torch.zeros(num_total_problems, num_packs, device=device, dtype=weight.dtype)
    pack_counts = torch.zeros(num_total_problems, num_packs, device=device, dtype=torch.int64)
    sorted_pack_index = torch.zeros_like(sorted_indices)

    # Optimization: Pre-allocate constants
    ones = torch.ones(num_total_problems, 1, device=device, dtype=torch.int64)

    for i in range(num_groups):
        w_item = sorted_weight[:, i:i+1] # [LC, 1]

        # Mask full packs by setting their current weight to infinity
        is_full = (pack_counts >= groups_per_pack)
        candidates = pack_weights.clone()
        candidates[is_full] = float('inf')

        # Choose pack with min weight
        chosen_pack = candidates.argmin(dim=1, keepdim=True)

        # Record decision
        sorted_pack_index[:, i:i+1] = chosen_pack

        # Update state
        pack_weights.scatter_add_(1, chosen_pack, w_item)
        pack_counts.scatter_add_(1, chosen_pack, ones)

    # --- 3. Vectorized Local Search: Max-Any Swap ---
    # Convert linear assignment to structured [Batch, Pack, Group]
    # We sort the 'sorted_pack_index' to group items by pack
    _, pack_content_sort_idx = sorted_pack_index.sort(dim=1, stable=True)

    # pack_contents contains indices (0..N-1) referring to sorted_weight
    pack_contents = pack_content_sort_idx.view(num_total_problems, num_packs, groups_per_pack)

    K = groups_per_pack
    num_iters = 20

    for _ in range(num_iters):
        # 1. Recompute Weights & Identify Max Pack
        flat_contents = pack_contents.view(num_total_problems, -1)
        current_weights = torch.gather(sorted_weight, 1, flat_contents).view(num_total_problems, num_packs, K)

        pack_sums = current_weights.sum(dim=2) # [LC, M]
        val_max, idx_max_pack = pack_sums.max(dim=1) # [LC]

        # 2. Gather items
        # Max Pack Items: [LC, 1, K]
        gather_idx = idx_max_pack.view(-1, 1, 1).expand(-1, 1, K)
        w_max = torch.gather(current_weights, 1, gather_idx).squeeze(1) # [LC, K]

        # 3. Compute Swap Diffs against ALL packs
        # w_max: [LC, 1, K, 1]
        # current_weights: [LC, M, 1, K]
        # diffs: [LC, M, K, K] -> diff[b, m, i, j] = w_max[i] - w_pack_m[j]
        diffs = w_max.view(num_total_problems, 1, K, 1) - current_weights.view(num_total_problems, num_packs, 1, K)

        # 4. Compute Improvement
        # New Max Pack Load = val_max - diff
        # New Other Pack Load = pack_sums[other] + diff
        # Objective = max(New Max Pack Load, New Other Pack Load)

        val_max_exp = val_max.view(num_total_problems, 1, 1, 1)
        pack_sums_exp = pack_sums.view(num_total_problems, num_packs, 1, 1)

        new_pair_max = torch.max(val_max_exp - diffs, pack_sums_exp + diffs)
        improvement = val_max_exp - new_pair_max

        # 5. Masking
        # Mask self-swaps (swapping with same pack)
        mask_self = (torch.arange(num_packs, device=device).view(1, -1) == idx_max_pack.view(-1, 1))
        mask_self = mask_self.view(num_total_problems, num_packs, 1, 1)

        # Validity: Must reduce weight of max pack (diff > 0) AND reduce global max (improvement > 0)
        valid_mask = (diffs > 0) & (improvement > 1e-6) & (~mask_self)

        scores = torch.where(valid_mask, improvement, torch.tensor(float('-inf'), device=device))

        # 6. Select Best Swap
        flat_scores = scores.view(num_total_problems, -1)
        best_imp, flat_idx = flat_scores.max(dim=1)

        if not (best_imp > float('-inf')).any():
            break

        # 7. Execute Swaps
        valid_layers = torch.nonzero(best_imp > float('-inf')).squeeze(1)

        if len(valid_layers) == 0:
            break

        f_idx = flat_idx[valid_layers]

        # Decode flattened index: M * K * K
        K2 = K * K
        p_other = f_idx // K2
        rem = f_idx % K2
        idx_in_max = rem // K
        idx_in_other = rem % K

        p_max = idx_max_pack[valid_layers]

        # Swap indices in pack_contents
        val_max_item = pack_contents[valid_layers, p_max, idx_in_max].clone()
        val_other_item = pack_contents[valid_layers, p_other, idx_in_other].clone()

        pack_contents[valid_layers, p_max, idx_in_max] = val_other_item
        pack_contents[valid_layers, p_other, idx_in_other] = val_max_item

    # --- 4. Select Best Candidate ---
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses a Massive Parallel Diverse Initialization followed by a
    Vectorized Max-Any Swap (1-item) and Max-Min Swap (2-item) local search.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_packs, dtype=torch.int64, device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(pack_index)
        return pack_index, rank_in_pack

    # --- 1. Massive Parallel Initialization ---
    num_candidates = 128
    num_total_problems = num_layers * num_candidates

    # [L*C, N]
    w_expanded = weight.repeat_interleave(num_candidates, dim=0)

    # Strategy:
    # 0-95: Randomized LPT (LPT + Noise 0.0 to 0.4)
    # 96-127: Random Shuffle (Pure Random keys)

    # Create base noise
    base_noise = torch.rand_like(w_expanded, dtype=torch.float32)

    # Create scales [128]
    cand_scales = torch.zeros(num_candidates, device=device)
    cand_scales[:96] = torch.linspace(0, 0.4, 96, device=device)

    # Expand scales to [L*C, 1]
    scales_expanded = cand_scales.repeat(num_layers).view(-1, 1)

    # Apply noise for LPT part
    noise_lpt = base_noise * w_expanded.float() * scales_expanded
    sort_keys = w_expanded + noise_lpt.to(dtype=weight.dtype)

    # Fix Candidate 0 of each layer to be Pure LPT
    sort_keys.view(num_layers, num_candidates, num_groups)[:, 0, :] = w_expanded.view(num_layers, num_candidates, num_groups)[:, 0, :]

    # Part 2: Random Shuffle for 96-127
    random_keys = torch.rand_like(w_expanded)

    # Mask for last 32 candidates
    mask_random = torch.zeros(num_candidates, dtype=torch.bool, device=device)
    mask_random[96:] = True
    mask_random_expanded = mask_random.repeat(num_layers)

    sort_keys[mask_random_expanded] = random_keys[mask_random_expanded]

    # Sort
    _, sorted_indices = sort_keys.sort(dim=-1, descending=True)
    sorted_weight = torch.gather(w_expanded, 1, sorted_indices)

    # --- 2. Vectorized Greedy LPT Construction ---
    # Fill packs greedily item by item
    pack_weights = torch.zeros(num_total_problems, num_packs, device=device, dtype=weight.dtype)
    pack_counts = torch.zeros(num_total_problems, num_packs, device=device, dtype=torch.int64)
    sorted_pack_index = torch.zeros_like(sorted_indices)

    # Optimization: Pre-allocate constants
    ones = torch.ones(num_total_problems, 1, device=device, dtype=torch.int64)

    for i in range(num_groups):
        w_item = sorted_weight[:, i:i+1] # [LC, 1]

        # Mask full packs by setting their current weight to infinity
        is_full = (pack_counts >= groups_per_pack)
        candidates = pack_weights.clone()
        candidates[is_full] = float('inf')

        # Choose pack with min weight
        chosen_pack = candidates.argmin(dim=1, keepdim=True)

        # Record decision
        sorted_pack_index[:, i:i+1] = chosen_pack

        # Update state
        pack_weights.scatter_add_(1, chosen_pack, w_item)
        pack_counts.scatter_add_(1, chosen_pack, ones)

    # --- 3. Vectorized Local Search: Max-Any Swap (1-Item) ---
    _, pack_content_sort_idx = sorted_pack_index.sort(dim=1, stable=True)
    pack_contents = pack_content_sort_idx.view(num_total_problems, num_packs, groups_per_pack)

    K = groups_per_pack
    num_iters_1 = 20

    for _ in range(num_iters_1):
        # Recompute Weights & Identify Max Pack
        flat_contents = pack_contents.view(num_total_problems, -1)
        current_weights = torch.gather(sorted_weight, 1, flat_contents).view(num_total_problems, num_packs, K)
        pack_sums = current_weights.sum(dim=2)
        val_max, idx_max_pack = pack_sums.max(dim=1)

        gather_idx = idx_max_pack.view(-1, 1, 1).expand(-1, 1, K)
        w_max = torch.gather(current_weights, 1, gather_idx).squeeze(1)

        # Diff against all packs
        diffs = w_max.view(num_total_problems, 1, K, 1) - current_weights.view(num_total_problems, num_packs, 1, K)

        val_max_exp = val_max.view(num_total_problems, 1, 1, 1)
        pack_sums_exp = pack_sums.view(num_total_problems, num_packs, 1, 1)

        new_pair_max = torch.max(val_max_exp - diffs, pack_sums_exp + diffs)
        improvement = val_max_exp - new_pair_max

        mask_self = (torch.arange(num_packs, device=device).view(1, -1) == idx_max_pack.view(-1, 1))
        mask_self = mask_self.view(num_total_problems, num_packs, 1, 1)

        valid_mask = (diffs > 0) & (improvement > 1e-6) & (~mask_self)

        scores = torch.where(valid_mask, improvement, torch.tensor(float('-inf'), device=device))
        best_imp, flat_idx = scores.view(num_total_problems, -1).max(dim=1)

        if not (best_imp > float('-inf')).any():
            break

        valid_layers = torch.nonzero(best_imp > float('-inf')).squeeze(1)
        if len(valid_layers) == 0:
            break

        f_idx = flat_idx[valid_layers]
        K2 = K * K
        p_other = f_idx // K2
        rem = f_idx % K2
        idx_in_max = rem // K
        idx_in_other = rem % K
        p_max = idx_max_pack[valid_layers]

        val_max_item = pack_contents[valid_layers, p_max, idx_in_max].clone()
        val_other_item = pack_contents[valid_layers, p_other, idx_in_other].clone()
        pack_contents[valid_layers, p_max, idx_in_max] = val_other_item
        pack_contents[valid_layers, p_other, idx_in_other] = val_max_item

    # --- 4. Vectorized Max-Min Swap (2-Item) ---
    if K >= 2 and K <= 32:
        pair_idx = torch.triu_indices(K, K, offset=1, device=device)
        idx_1 = pair_idx[0]
        idx_2 = pair_idx[1]
        num_pairs = pair_idx.shape[1]

        num_iters_2 = 10
        for _ in range(num_iters_2):
            flat_contents = pack_contents.view(num_total_problems, -1)
            current_weights = torch.gather(sorted_weight, 1, flat_contents).view(num_total_problems, num_packs, K)
            pack_sums = current_weights.sum(dim=2)
            val_max, idx_max_pack = pack_sums.max(dim=1)
            val_min, idx_min_pack = pack_sums.min(dim=1)

            gather_max = idx_max_pack.view(-1, 1, 1).expand(-1, 1, K)
            gather_min = idx_min_pack.view(-1, 1, 1).expand(-1, 1, K)

            items_max = torch.gather(current_weights, 1, gather_max).squeeze(1)
            items_min = torch.gather(current_weights, 1, gather_min).squeeze(1)

            pairs_max_w = items_max[:, idx_1] + items_max[:, idx_2]
            pairs_min_w = items_min[:, idx_1] + items_min[:, idx_2]

            diffs = pairs_max_w.unsqueeze(2) - pairs_min_w.unsqueeze(1)

            val_max_exp = val_max.view(-1, 1, 1)
            val_min_exp = val_min.view(-1, 1, 1)

            new_peak = torch.max(val_max_exp - diffs, val_min_exp + diffs)
            improvement = val_max_exp - new_peak

            valid_mask = (diffs > 0) & (improvement > 1e-6)

            scores = torch.where(valid_mask, improvement, torch.tensor(float('-inf'), device=device))
            best_imp, flat_idx = scores.view(num_total_problems, -1).max(dim=1)

            if not (best_imp > float('-inf')).any():
                break

            valid_layers = torch.nonzero(best_imp > float('-inf')).squeeze(1)
            if len(valid_layers) == 0:
                break

            f_idx = flat_idx[valid_layers]
            p_idx_max_pair = f_idx // num_pairs
            p_idx_min_pair = f_idx % num_pairs

            i1_max = idx_1[p_idx_max_pair]
            i2_max = idx_2[p_idx_max_pair]
            i1_min = idx_1[p_idx_min_pair]
            i2_min = idx_2[p_idx_min_pair]

            p_max = idx_max_pack[valid_layers]
            p_min = idx_min_pack[valid_layers]

            val_max_1 = pack_contents[valid_layers, p_max, i1_max].clone()
            val_min_1 = pack_contents[valid_layers, p_min, i1_min].clone()
            pack_contents[valid_layers, p_max, i1_max] = val_min_1
            pack_contents[valid_layers, p_min, i1_min] = val_max_1

            val_max_2 = pack_contents[valid_layers, p_max, i2_max].clone()
            val_min_2 = pack_contents[valid_layers, p_min, i2_min].clone()
            pack_contents[valid_layers, p_max, i2_max] = val_min_2
            pack_contents[valid_layers, p_min, i2_min] = val_max_2

    # --- 5. Select Best Candidate ---
>>>>>>> REPLACE
</DIFF>