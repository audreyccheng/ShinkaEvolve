<NAME>
refine_ensemble
</NAME>

<DESCRIPTION>
Diversify the greedy ensemble by introducing random shuffle candidates alongside LPT and ZigZag heuristics to escape local optima. Additionally, implement a vectorized local search refinement step (pairwise swapping between heaviest and lightest packs) on the best selected candidate to further reduce load imbalance efficiently.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Massive Parallel Ensemble strategy.

    Generates 128 candidates using LPT, ZigZag, and Randomized perturbations.
    Selects the best packing per layer minimizing load imbalance.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_items = weight.shape
    device = weight.device
    num_candidates = 128
    capacity = num_items // num_packs

    # 1. Generate Candidates
    # Base: Sort descending (LPT)
    lpt_val, lpt_idx = weight.sort(dim=-1, descending=True)

    # ZigZag permutation of LPT: 0, N-1, 1, N-2, ...
    zigzag_perm = torch.empty(num_items, device=device, dtype=torch.long)
    half = (num_items + 1) // 2
    arange = torch.arange(num_items, device=device)
    zigzag_perm[0::2] = arange[:half]
    zigzag_perm[1::2] = arange[half:].flip(0)

    # Candidate Indices [L, C, N]
    # C0: lpt_idx
    c0_idx = lpt_idx.unsqueeze(1)

    # C1: lpt_idx gathered by zigzag_perm
    # Permute columns of lpt_idx
    c1_idx = lpt_idx.gather(1, zigzag_perm.view(1, -1).expand(num_layers, -1)).unsqueeze(1)

    # Noisy candidates
    noise = torch.rand(num_layers, num_candidates - 2, num_items, device=device) * 0.4 + 0.8
    noisy_weights = weight.unsqueeze(1) * noise
    _, c_noisy_idx = noisy_weights.sort(dim=-1, descending=True)

    all_indices = torch.cat([c0_idx, c1_idx, c_noisy_idx], dim=1)

    # Gather actual weights [L, C, N]
    expanded_weight = weight.unsqueeze(1).expand(-1, num_candidates, -1)
    ordered_weights = expanded_weight.gather(2, all_indices)

    # Flatten for vectorized kernel
    batch_size = num_layers * num_candidates
    flat_weights = ordered_weights.view(batch_size, num_items)

    # 2. Vectorized Greedy Packing
    pack_loads = torch.zeros(batch_size, num_packs, device=device, dtype=weight.dtype)
    pack_counts = torch.zeros(batch_size, num_packs, device=device, dtype=torch.int64)
    flat_assigned_packs = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)
    flat_assigned_ranks = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)

    batch_range = torch.arange(batch_size, device=device)
    inf = torch.tensor(float('inf'), device=device, dtype=weight.dtype)

    for i in range(num_items):
        w = flat_weights[:, i]
        valid_mask = pack_counts < capacity
        temp_loads = torch.where(valid_mask, pack_loads, inf)
        chosen_pack = temp_loads.argmin(dim=1)

        flat_assigned_packs[:, i] = chosen_pack
        flat_assigned_ranks[:, i] = pack_counts[batch_range, chosen_pack]

        pack_loads[batch_range, chosen_pack] += w
        pack_counts[batch_range, chosen_pack] += 1

    # 3. Selection
    loads = pack_loads.view(num_layers, num_candidates, num_packs)
    imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values
    best_candidate_idx = imbalance.argmin(dim=1)

    # Helper to gather
    def gather_best(tensor_lcn, best_idx_l):
        idx_expanded = best_idx_l.view(num_layers, 1, 1).expand(-1, 1, num_items)
        return tensor_lcn.gather(1, idx_expanded).squeeze(1)

    final_ordered_indices = gather_best(all_indices, best_candidate_idx)
    final_packs_ordered = gather_best(flat_assigned_packs.view(num_layers, num_candidates, num_items), best_candidate_idx)
    final_ranks_ordered = gather_best(flat_assigned_ranks.view(num_layers, num_candidates, num_items), best_candidate_idx)

    pack_index = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    rank_in_pack = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)

    pack_index.scatter_(1, final_ordered_indices, final_packs_ordered)
    rank_in_pack.scatter_(1, final_ordered_indices, final_ranks_ordered)

    return pack_index, rank_in_pack
=======
def _refine_packing(weights: torch.Tensor,
                    pack_ids: torch.Tensor,
                    ranks: torch.Tensor,
                    num_packs: int,
                    num_iters: int = 5) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Refines the packing by attempting to swap a single item between the heaviest
    and lightest packs in each layer.
    """
    batch_size, num_items = weights.shape
    device = weights.device
    batch_indices = torch.arange(batch_size, device=device)
    inf = torch.tensor(float('inf'), device=device, dtype=weights.dtype)

    # Track loads
    pack_loads = torch.zeros(batch_size, num_packs, device=device, dtype=weights.dtype)
    pack_loads.scatter_add_(1, pack_ids, weights)

    for _ in range(num_iters):
        max_load, max_pack_idx = pack_loads.max(dim=1)
        min_load, min_pack_idx = pack_loads.min(dim=1)
        curr_diff = max_load - min_load

        # Mask items in max/min packs
        in_max = (pack_ids == max_pack_idx.unsqueeze(1))
        in_min = (pack_ids == min_pack_idx.unsqueeze(1))

        # w[b,i] - w[b,j]
        w_diff = weights.unsqueeze(2) - weights.unsqueeze(1)

        # Metric: |curr_diff - 2*(wi - wj)|
        metric = torch.abs(curr_diff.view(-1, 1, 1) - 2 * w_diff)

        # Valid mask
        valid = in_max.unsqueeze(2) & in_min.unsqueeze(1)
        metric = torch.where(valid, metric, inf)

        # Best swap
        best_val, best_flat_idx = metric.view(batch_size, -1).min(dim=1)

        improve = best_val < curr_diff
        if not improve.any():
            break

        active_b = batch_indices[improve]
        active_idx = best_flat_idx[improve]

        i_idx = active_idx // num_items
        j_idx = active_idx % num_items

        # Apply swap
        p_max = max_pack_idx[active_b]
        p_min = min_pack_idx[active_b]

        # IDs
        pack_ids[active_b, i_idx] = p_min
        pack_ids[active_b, j_idx] = p_max

        # Ranks
        r_i = ranks[active_b, i_idx]
        r_j = ranks[active_b, j_idx]
        ranks[active_b, i_idx] = r_j
        ranks[active_b, j_idx] = r_i

        # Loads
        w_i = weights[active_b, i_idx]
        w_j = weights[active_b, j_idx]
        delta = w_i - w_j
        pack_loads[active_b, p_max] -= delta
        pack_loads[active_b, p_min] += delta

    return pack_ids, ranks


def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Massive Parallel Ensemble strategy.

    Includes:
    - LPT (Sorted Descending)
    - ZigZag (Interleaved Sorted)
    - Random Shuffle (Random Order)
    - Noisy LPT (Perturbed Weights)

    Followed by local search refinement on the best candidate.
    """
    num_layers, num_items = weight.shape
    device = weight.device
    num_candidates = 128
    capacity = num_items // num_packs

    # --- 1. Candidate Generation ---
    # LPT Base
    lpt_val, lpt_idx = weight.sort(dim=-1, descending=True)

    # ZigZag Base
    zigzag_perm = torch.empty(num_items, device=device, dtype=torch.long)
    half = (num_items + 1) // 2
    arange = torch.arange(num_items, device=device)
    zigzag_perm[0::2] = arange[:half]
    zigzag_perm[1::2] = arange[half:].flip(0)

    c0_idx = lpt_idx.unsqueeze(1) # LPT

    # ZigZag: permute lpt_idx
    c1_idx = lpt_idx.gather(1, zigzag_perm.view(1, -1).expand(num_layers, -1)).unsqueeze(1)

    # Random Shuffles (25% of candidates approx, say 32)
    num_shuffle = 32
    rand_keys = torch.randn(num_layers, num_shuffle, num_items, device=device)
    _, c_shuffle_idx = rand_keys.sort(dim=-1)

    # Noisy LPT (Rest)
    num_noisy = num_candidates - 2 - num_shuffle
    noise = torch.rand(num_layers, num_noisy, num_items, device=device) * 0.4 + 0.8
    noisy_w = weight.unsqueeze(1) * noise
    _, c_noisy_idx = noisy_w.sort(dim=-1, descending=True)

    # Combine
    all_indices = torch.cat([c0_idx, c1_idx, c_shuffle_idx, c_noisy_idx], dim=1)

    # Gather actual weights
    # [L, C, N]
    expanded_weight = weight.unsqueeze(1).expand(-1, num_candidates, -1)
    ordered_weights = expanded_weight.gather(2, all_indices)

    # --- 2. Vectorized Greedy Packing ---
    batch_size = num_layers * num_candidates
    flat_weights = ordered_weights.view(batch_size, num_items)

    pack_loads = torch.zeros(batch_size, num_packs, device=device, dtype=weight.dtype)
    pack_counts = torch.zeros(batch_size, num_packs, device=device, dtype=torch.int64)
    flat_assigned_packs = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)
    flat_assigned_ranks = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)

    batch_range = torch.arange(batch_size, device=device)
    inf = torch.tensor(float('inf'), device=device, dtype=weight.dtype)

    for i in range(num_items):
        w = flat_weights[:, i]
        valid_mask = pack_counts < capacity
        temp_loads = torch.where(valid_mask, pack_loads, inf)
        chosen_pack = temp_loads.argmin(dim=1)

        flat_assigned_packs[:, i] = chosen_pack
        flat_assigned_ranks[:, i] = pack_counts[batch_range, chosen_pack]

        pack_loads[batch_range, chosen_pack] += w
        pack_counts[batch_range, chosen_pack] += 1

    # --- 3. Selection ---
    loads = pack_loads.view(num_layers, num_candidates, num_packs)
    imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values
    best_candidate_idx = imbalance.argmin(dim=1)

    def gather_best(tensor_lcn, best_idx_l):
        idx_expanded = best_idx_l.view(num_layers, 1, 1).expand(-1, 1, num_items)
        return tensor_lcn.gather(1, idx_expanded).squeeze(1)

    # Get the packing configuration of the best candidate
    # These are in the *sorted/permuted* order of that candidate
    final_ordered_indices = gather_best(all_indices, best_candidate_idx)
    final_packs_ordered = gather_best(flat_assigned_packs.view(num_layers, num_candidates, num_items), best_candidate_idx)
    final_ranks_ordered = gather_best(flat_assigned_ranks.view(num_layers, num_candidates, num_items), best_candidate_idx)

    # Scatter back to original item order
    # pack_index[l, original_idx] = pack_id
    pack_index = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    rank_in_pack = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)

    pack_index.scatter_(1, final_ordered_indices, final_packs_ordered)
    rank_in_pack.scatter_(1, final_ordered_indices, final_ranks_ordered)

    # --- 4. Refinement ---
    # Apply refinement on the best selected packing
    # We pass 'weight' which is in original order, matching pack_index
    pack_index, rank_in_pack = _refine_packing(weight, pack_index, rank_in_pack, num_packs, num_iters=5)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>