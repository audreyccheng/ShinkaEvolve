{
  "unprocessed_programs": [],
  "meta_summary": "**Program Name: Hierarchical Greedy Expert Parallelism Load Balancer**\n- **Implementation**: This approach employs a hierarchical strategy that assigns expert groups to nodes using greedy packing, iteratively replicates high-load experts, and finally distributes physical replicas across GPUs.\n- **Performance**: The program achieves a combined score of 0.66, driven by a perfect speed score (1.0) but limited by a moderate balancedness score (0.31).\n- **Feedback**: The algorithm is highly efficient and correct, but the greedy packing heuristics result in suboptimal load distribution, suggesting that more advanced optimization techniques could improve balance.\n**Program Identifier:** Generation 0 - Patch Name initial_program - Correct Program: True\n\n**Program Name: Greedy LPT Packing with Swap-Based Refinement**\n- **Implementation**: The algorithm utilizes a Greedy Longest Processing Time (LPT) strategy to initialize expert assignments, followed by a CPU-based iterative local search that swaps items between the heaviest packs and others to minimize maximum load.\n- **Performance**: The solution achieved a combined score of 0.66, maximizing the speed metric (1.00) while yielding a balancedness score of 0.31 across evaluated workloads.\n- **Feedback**: The approach is computationally efficient and scales well, achieving top marks for speed, though the heuristic nature of the packing logic results in moderate load distribution balance compared to more exhaustive methods.\n**Program Identifier:** Generation 1 - Patch Name refined_eplb - Correct Program: True\n\n**Program Name: Hierarchical EPLB with Zig-Zag Packing and Binary Search**\n- **Implementation**: The algorithm employs a hierarchical strategy using sorted Zig-Zag packing to assign expert groups to nodes and a vectorized binary search with greedy density-based refinement to calculate expert replication counts.\n- **Performance**: It achieves a combined score of 0.63, characterized by perfect execution speed (1.0) but a lower balancedness score (0.27).\n- **Feedback**: The implementation is highly optimized for speed using vectorized operations, but the Zig-Zag packing heuristic yields suboptimal load distribution compared to more robust partitioning strategies like Longest Processing Time (LPT).\n**Program Identifier:** Generation 2 - Patch Name sorted_zigzag_eplb - Correct Program: True\n\n**Program Name: Hierarchical Greedy-Swap Expert Load Balancer**\n- **Implementation**: This solution employs a hierarchical rebalancing strategy that uses greedy Longest Processing Time (LPT) packing followed by iterative swap refinement to distribute expert groups across nodes and GPUs.\n- **Performance**: The program received a score of 0.0, failing to pass validation tests.\n- **Feedback**: The solution is functionally incorrect, likely due to errors in the complex index manipulation or tensor scattering logic required to map logical experts to physical replicas during the hierarchical transformation.\n**Program Identifier:** Generation 3 - Patch Name eplb_greedy_swap - Correct Program: False\n\n**Program Name: Vectorized ZigZag Packing for Expert Parallelism Load Balancing**\n- **Implementation**: The solution implements a hierarchical load balancing strategy using a GPU-accelerated ZigZag initialization followed by a vectorized, iterative swap-based local search to distribute expert weights across resources.\n- **Performance**: It achieved a combined score of 0.66, distinguished by a perfect speed score of 1.00 and a balancedness score of 0.31.\n- **Feedback**: The fully vectorized implementation using tensor operations ensures exceptional runtime speed, though the heuristic swap approach prioritizes low latency over finding the theoretically optimal packing configuration.\n**Program Identifier:** Generation 4 - Patch Name vectorized_balanced_packing - Correct Program: True\n\n**Program Name: Vectorized Greedy LPT with Hierarchical Proportional Replication**\n- **Implementation**: This solution implements a hierarchical load balancer using a vectorized greedy Longest Processing Time (LPT) heuristic for bin packing and proportional allocation with greedy residual correction for expert replication. The logic is fully vectorized across model layers using PyTorch CPU tensors to maximize computational throughput during the rebalancing step.\n- **Performance**: The program achieved a combined score of 0.65, distinguishing itself with a perfect speed score (1.0) but a moderate balancedness score of 0.31.\n- **Feedback**: The vectorized implementation ensures minimal overhead, resulting in exceptional execution speed; however, the greedy heuristic struggles to achieve optimal load distribution compared to more exhaustive combinatorial solvers.\n**Program Identifier:** Generation 5 - Patch Name vectorized_greedy_eplb - Correct Program: True\n\n**Program Name: Hierarchical Greedy Expert Load Balancer**\n- **Implementation**: This approach implements DeepSeek's EPLB algorithm using a custom `balanced_packing` function that combines greedy Longest Processing Time (LPT) initialization with a swap-based refinement loop to distribute expert groups and replicas hierarchically across nodes and GPUs.\n- **Performance**: The solution achieved a combined score of 0.0, failing to pass the required validation tests.\n- **Feedback**: The failure suggests critical logic errors in the packing algorithm's constraint handling or the final mapping transformations, preventing the generation of valid expert assignments.\n**Program Identifier:** Generation 6 - Patch Name optimize_packing_lpt_swap - Correct Program: False\n\n**Program Name: Chunked Greedy Packing with Binary Search Replication EPLB**\n- **Implementation**: Implements hierarchical load balancing using a chunked sorted greedy approach for packing and a binary search algorithm to efficiently determine expert replication counts on the CPU.\n- **Performance**: The solution attains a combined score of 0.66, characterized by maximum execution speed (1.0) but a suboptimal balancedness score (0.31).\n- **Feedback**: While the vectorized binary search and chunking strategy dramatically reduce computational overhead, the segmented packing approach restricts global optimization, negatively impacting the final load balance quality.\n**Program Identifier:** Generation 7 - Patch Name moe_eplb_chunked_bs - Correct Program: True\n\n**Program Name: Vectorized Greedy LPT with Batched Swap Refinement**\n- **Implementation**: Utilizes a GPU-vectorized Greedy Longest Processing Time (LPT) initialization followed by an iterative, batched swap-based local search algorithm to optimize expert placement.\n- **Performance**: Achieved a perfect speed score (1.0) but a lower balancedness score (0.31), resulting in a combined score of 0.66.\n- **Feedback**: While the vectorized implementation yields excellent execution speed, the greedy initialization and simple local search heuristics struggle to escape local optima, limiting the final load balance quality.\n**Program Identifier:** Generation 8 - Patch Name vectorized_lpt_and_swap - Correct Program: True\n\n**Program Name: Vectorized ZigZag Packing with Max-Min Local Search**\n- **Implementation**: The solution combines a deterministic ZigZag initialization for expert assignment with a vectorized local search that iteratively swaps experts between the heaviest and lightest packs using efficient tensor operations.\n- **Performance**: It achieves a perfect speed score (1.0) but a lower balancedness score (0.31), resulting in a combined score of 0.66.\n- **Feedback**: The high speed confirms the effectiveness of vectorizing the swap logic to minimize runtime overhead, though the heuristic nature of the local search trades some load balancing precision for computational throughput.\n**Program Identifier:** Generation 9 - Patch Name moe_eplb_opt - Correct Program: True\n\n**Program Name: GPU-Vectorized Greedy LPT with Swap Refinement for EPLB**\n- **Implementation**: Initializes assignments via a vectorized greedy Longest Processing Time (LPT) method and refines them using a GPU-accelerated iterative swap algorithm to reduce the maximum pack weight.\n- **Performance**: Achieves a perfect speed score (1.0) with a moderate balancedness score (0.31).\n- **Feedback**: The fully vectorized approach ensures minimal overhead suitable for runtime execution, though the reliance on local search refinement limits the algorithm's ability to find globally optimal balanced packings compared to slower solvers.\n**Program Identifier:** Generation 10 - Patch Name eplb_lpt_swap - Correct Program: True\n\n**Program Name: Vectorized Hierarchical Expert Load Balancer**\n- **Implementation**: Implements hierarchical rebalancing using ZigZag initialization and vectorized Max-Any swap local search for packing, combined with greedy expert replication.\n- **Performance**: Maximizes execution speed (1.0) but struggles with load distribution quality (balancedness 0.31), yielding a combined score of 0.66.\n- **Feedback**: The highly vectorized approach ensures efficiency, but the greedy heuristics and limited search iterations likely limit the algorithm's ability to find globally optimal load distributions.\n**Program Identifier:** Generation 11 - Patch Name improved_balanced_packing_v2 - Correct Program: True\n\n**Program Name: Hierarchical EPLB with Folded Chunked Greedy Packing**\n- **Implementation**: This solution implements hierarchical load balancing using a \"folded chunked sorted greedy\" strategy that pairs heavy and light items to smooth variance, alongside a binary search algorithm for optimizing expert replication counts.\n- **Performance**: The algorithm is extremely fast (speed score 1.0) but achieves moderate load balancing (balancedness score 0.31), resulting in a combined score of 0.66.\n- **Feedback**: The vectorized chunked approach facilitates high-speed execution by processing items in batches, though the heuristic nature of the packing and replication refinement limits the attainable load balance compared to more exhaustive methods.\n**Program Identifier:** Generation 12 - Patch Name folded_chunk_packing - Correct Program: True\n\n**Program Name: Hierarchical EPLB with ZigZag-Greedy Initialization**\n- **Implementation**: This solution employs a hierarchical load balancing strategy using a hybrid ZigZag and constrained Greedy initialization, refined by a vectorized \"Max-Any Swap\" local search to optimize expert distribution across GPUs.\n- **Performance**: The program achieved a combined score of 0.0, failing to pass validation tests.\n- **Feedback**: Despite the sophisticated heuristic approach, the algorithm is functionally incorrect; critical bugs likely exist within the complex tensor manipulations of the local search or the hierarchical index mapping, causing it to produce invalid assignment plans.\n**Program Identifier:** Generation 13 - Patch Name moe_eplb_hybrid_greedy_swap - Correct Program: False\n\n**Program Name: Hybrid Greedy Packing and Binary Search Replication for EPLB**\n- **Implementation**: Implements a hybrid packing strategy selecting between Folded Chunked Sorted Greedy and constrained Global Greedy, coupled with a binary search-based allocation for expert replication. The approach applies these algorithms hierarchically (nodes then GPUs) using efficient vectorized PyTorch operations on the CPU.\n- **Performance**: The solution attained a combined score of 0.66, characterized by a perfect speed score (1.0) and a moderate balancedness score (0.31).\n- **Feedback**: The vectorized heuristic approach is extremely fast and robust, though the moderate balancedness score suggests that the greedy packing strategies may not fully resolve complex load skewing as effectively as more expensive iterative solvers.\n**Program Identifier:** Generation 14 - Patch Name hybrid_greedy_packing - Correct Program: True\n\n**Program Name: Vectorized Greedy LPT and L2-Swap Expert Load Balancer**\n- **Implementation**: The solution utilizes a vectorized Longest Processing Time (LPT) greedy initialization followed by a GPU-accelerated pairwise swap algorithm to minimize the L2-norm of pack weights. The hierarchical strategy first assigns expert groups to nodes and then packs replicated experts onto GPUs using this logic.\n- **Performance**: The solution achieved a combined score of 0.66, characterized by a perfect speed score (1.00) and a balancedness score of 0.31.\n- **Feedback**: The fully vectorized implementation on the GPU provides exceptional runtime efficiency, achieving the maximum speed score. However, the moderate balancedness score suggests that the greedy heuristic combined with local search may settle in local optima, leaving room for improvement in load distribution uniformity.\n**Program Identifier:** Generation 15 - Patch Name l2_opt_packing_gpu - Correct Program: True\n\n**Program Name: Vectorized Randomized LPT with Max-Any Swap Local Search**\n- **Implementation**: The solution implements a parallelized greedy Longest Processing Time (LPT) initialization with randomized restarts, followed by a fully vectorized \"Max-Any\" local search that iteratively swaps items to reduce the maximum load.\n- **Performance**: The program achieved a perfect speed score (1.0) and a balancedness score of 0.31, yielding a combined score of 0.66.\n- **Feedback**: While the highly vectorized approach ensures exceptional runtime efficiency, the lower balancedness score indicates that the greedy initialization and single-item swap strategy may be insufficient for finding optimal packing configurations in complex distributions.\n**Program Identifier:** Generation 16 - Patch Name parallel_greedy_lpt_refinement - Correct Program: True\n\n**Program Name: Hybrid Recursive Folded Packing with Binary Search Replication**\n- **Implementation**: Combines Greedy LPT with a recursive \"folding\" strategy that pairs heavy and light items to minimize variance, using binary search with density-based refinement for replica allocation.\n- **Performance**: Achieved a combined score of 0.66, with perfect speed (1.0) but a modest balancedness score (0.31).\n- **Feedback**: While the vectorized CPU implementation is extremely fast, the heuristic packing strategies fail to achieve high load balance, suggesting a need for more robust global optimization techniques like minimum-cost flow.\n**Program Identifier:** Generation 17 - Patch Name recursive_folded_packing - Correct Program: True\n\n**Program Name: Vectorized Randomized Greedy EPLB with L2 Local Search**\n- **Implementation**: Uses 8 parallel randomized Longest Processing Time (LPT) greedy initializations followed by a vectorized swap-based local search on GPU to minimize the L2 norm of pack weights.\n- **Performance**: Achieves a perfect speed score of 1.0 and a balancedness score of 0.31, resulting in a combined score of 0.66.\n- **Feedback**: The highly vectorized implementation ensures maximum speed, but the moderate balancedness score suggests that the randomized greedy initialization with simple pairwise swapping may struggle to find optimal solutions for complex weight distributions.\n**Program Identifier:** Generation 18 - Patch Name parallel_randomized_greedy_with_l2_swap - Correct Program: True\n\n**Program Name: Vectorized Greedy LPT with Randomized Restarts and Local Search**\n- **Implementation**: Uses a vectorized Parallel Greedy LPT initialization with 4 candidates (1 deterministic, 3 randomized) to generate diverse packings, followed by a vectorized Max-Any Swap local search to iteratively reduce the maximum load.\n- **Performance**: Achieved a combined score of 0.66, characterized by a perfect speed score of 1.0 but a moderate balancedness score of 0.31.\n- **Feedback**: The heavy use of PyTorch vectorization ensures exceptional execution speed, though the randomized greedy approach with simple swaps prioritizes low latency over finding the theoretically optimal packing configuration.\n**Program Identifier:** Generation 19 - Patch Name randomized_greedy_lpt_vectorized_swap - Correct Program: True\n\n**Program Name: Vectorized Randomized ZigZag EPLB with Max-Min Local Search**\n- **Implementation**: Implements `balanced_packing` using parallel randomized ZigZag initialization across multiple candidates, followed by a fully vectorized Max-Min swap local search to refine load distribution.\n- **Performance**: Achieved a combined score of 0.66 with perfect speed (1.0) and moderate balancedness (0.31).\n- **Feedback**: The vectorized approach efficiently handles multiple candidates and local search iterations, ensuring high throughput, though the balancedness score indicates room for further optimization in the swapping heuristic.\n**Program Identifier:** Generation 20 - Patch Name randomized_zigzag_packing - Correct Program: True\n\n**Program Name: Parallel Randomized Greedy LPT with Vectorized Swap Refinement**\n- **Implementation**: The algorithm employs parallelized randomized greedy Longest Processing Time (LPT) initialization across multiple noisy candidates, followed by a fully vectorized GPU-based swap refinement phase to minimize maximum pack loads.\n- **Performance**: It achieved a combined score of 0.66, characterized by a perfect speed score (1.0) due to efficient vectorization, though balancedness (0.31) was moderate.\n- **Feedback**: The extensive use of PyTorch vectorization allows for evaluating many candidate solutions rapidly, ensuring high throughput; however, the randomized greedy approach struggles to find optimal packing solutions compared to more exhaustive methods, impacting the final balance.\n**Program Identifier:** Generation 21 - Patch Name parallel_candidates_and_gpu_fix - Correct Program: True\n\n**Program Name: Parallel Ensemble Greedy and Recursive Folding Load Balancer**\n- **Implementation**: The solution employs a hybrid strategy that selects the best outcome between a parallelized randomized greedy packing algorithm using perturbed weight candidates and a deterministic recursive folding heuristic.\n- **Performance**: It achieved a combined score of 0.66, maximizing execution speed (1.00) while maintaining a moderate balancedness score of 0.31.\n- **Feedback**: The highly vectorized ensemble approach ensures exceptional runtime efficiency, but the reliance on greedy heuristics limits the ability to find globally optimal load distributions compared to slower iterative solvers.\n**Program Identifier:** Generation 22 - Patch Name parallel_ensemble_greedy_eplb - Correct Program: True\n\n**Program Name: Vectorized Randomized Greedy LPT with Swap Refinement**\n- **Implementation**: Utilizes a massively parallelized greedy LPT approach with noise injection to generate candidates, followed by a vectorized local search that swaps items between highest and lowest load bins.\n- **Performance**: Achieved a combined score of 0.66, demonstrating maximum speed (1.0) but poor balancedness (0.31).\n- **Feedback**: The fully vectorized implementation provides exceptional speed, but the randomized greedy strategy with limited local swaps yields suboptimal packing quality compared to stronger optimization techniques.\n**Program Identifier:** Generation 23 - Patch Name parallel_ensemble_greedy_packing - Correct Program: True\n\n**Program Name: Parallel Randomized Greedy LPT with Vectorized Swap Refinement**\n- **Implementation**: The algorithm employs parallelized randomized greedy LPT to generate multiple candidate packings on the GPU, followed by a vectorized Max-Min swap refinement step that iteratively exchanges items between heaviest and lightest packs.\n- **Performance**: It achieves a combined score of 0.66, distinguished by a perfect speed score of 1.0 but a moderate balancedness score of 0.31.\n- **Feedback**: While the fully vectorized approach ensures maximum throughput and scalability, the balancedness score suggests that the local search heuristic or randomization parameters could be tuned further to better escape local optima.\n**Program Identifier:** Generation 24 - Patch Name gpu_parallel_candidates - Correct Program: True\n\n**Program Name: Parallel Randomized Greedy Packing with Vectorized Local Search**\n- **Implementation**: The algorithm generates 128 diverse candidates per layer using randomized Greedy LPT, concurrently refining them via vectorized 1-item and conditional 2-item swaps to minimize load variance.\n- **Performance**: It achieved a combined score of 0.66, effectively maximizing speed (1.0) while maintaining moderate packing balance (0.31).\n- **Feedback**: Vectorizing the local search across many random initializations is highly efficient, though the reliance on greedy heuristics limits the absolute optimal balance achievable in complex cases.\n**Program Identifier:** Generation 25 - Patch Name massive_parallel_eplb_v2 - Correct Program: True\n\n**Program Name: Vectorized Hybrid Ensemble Greedy Strategy for Expert Load Balancing**\n- **Implementation**: The algorithm employs a vectorized ensemble approach that concurrently evaluates LPT, ZigZag, and Noisy greedy packing heuristics to select the optimal configuration per layer, coupled with a binary search for expert replication.\n- **Performance**: It achieved a perfect speed score (1.0) but a lower balancedness score (0.31), yielding a combined score of 0.66.\n- **Feedback**: The high speed confirms the efficiency of the vectorized ensemble, but the moderate balancedness suggests that simple greedy heuristics alone are insufficient for optimal load distribution compared to iterative or flow-based solvers.\n**Program Identifier:** Generation 26 - Patch Name ensemble_hybrid_eplb - Correct Program: True\n\n**Program Name: Vectorized Randomized Greedy LPT with Local Search EPLB**\n- **Implementation**: Uses massively parallel randomized greedy LPT initialization across 128 candidates per layer, followed by vectorized 1-item and 2-item swap local search refinements to balance expert weights.\n- **Performance**: Achieves perfect speed (1.0) due to efficient tensor operations but yields a modest balancedness score (0.31), totaling 0.66.\n- **Feedback**: The highly parallelized approach is exceptionally fast but trades off packing precision, indicating that the randomized local search struggles to escape local optima compared to more rigorous solvers.\n**Program Identifier:** Generation 27 - Patch Name eplb_hybrid_zigzag_blockswap - Correct Program: True\n\n**Program Name: Vectorized Hybrid Ensemble Greedy Packing with Single-Pass Refinement**\n- **Implementation**: The algorithm generates 128 sorting permutations (LPT, ZigZag, Noisy) and applies a vectorized greedy packing kernel, followed by a constrained single-iteration swap refinement between the heaviest and lightest packs.\n- **Performance**: It maximizes computational efficiency with a perfect speed score (1.0) but produces suboptimal load distribution (balancedness 0.31), resulting in a combined score of 0.66.\n- **Feedback**: While the vectorized ensemble approach is extremely fast, the refinement phase is too shallow (only one pass) to escape local optima; deeper iterative improvement is necessary to significantly improve the balancedness score.\n**Program Identifier:** Generation 28 - Patch Name refine_packing - Correct Program: True\n\n**Program Name: Vectorized DeepSeek EPLB with Noise Spectrum and Local Search**\n- **Implementation**: The algorithm employs a massive parallel greedy initialization using a noise spectrum across 64 candidates, followed by a vectorized Max-Any Swap local search to optimize expert allocation. All operations, including candidate generation and iterative swapping, are fully batched using PyTorch to ensure high throughput across all layers simultaneously.\n- **Performance**: The program achieves a perfect speed score (1.0) and a balancedness score of 0.31, resulting in a combined score of 0.66.\n- **Feedback**: The perfect speed score confirms that the vectorized approach successfully handles the computational load of exploring multiple solution candidates in parallel. However, the moderate balancedness score suggests that the greedy initialization combined with local swapping may converge to local optima that are difficult to escape without more aggressive perturbation or optimization techniques.\n**Program Identifier:** Generation 29 - Patch Name massive_parallel_max_any_swap - Correct Program: True\n\n**Program Name: Ensemble Chunked Greedy Packing with Binary Search Replication**\n- **Implementation**: This solution employs a parallelized ensemble of chunked sorted greedy strategies for packing and a binary search method with greedy refinement to determine expert replication counts.\n- **Performance**: The program achieves a combined score of 0.66, characterized by perfect execution speed (1.0) but a relatively low balancedness score (0.31).\n- **Feedback**: While the vectorized implementation and parallel candidate generation ensure high throughput, the chunked heuristic limits the global optimization capability, resulting in suboptimal load distribution compared to standard greedy approaches.\n**Program Identifier:** Generation 30 - Patch Name ensemble_chunked_greedy - Correct Program: True\n\n**Program Name: Vectorized EPLB with Mixed-Strategy Initialization and Max-Any Swap**\n- **Implementation**: Features a massively parallel initialization strategy combining perturbed LPT, random, and ZigZag patterns, followed by a fully vectorized iterative swap algorithm to optimize expert distribution on the GPU.\n- **Performance**: Attained a combined score of 0.66, driven by a perfect speed score (1.0) despite a lower balancedness metric (0.31).\n- **Feedback**: The heavy reliance on vectorization and parallel candidate evaluation ensures the algorithm is extremely fast for real-time use, although the heuristic packing approach yields suboptimal load distribution compared to exact solvers.\n**Program Identifier:** Generation 31 - Patch Name mixed_init_max_any_swap - Correct Program: True\n\n**Program Name: Parallel Randomized Greedy LPT with Vectorized Swap Refinement**\n- **Implementation**: Uses parallelized Greedy Longest Processing Time (LPT) with noise injection and capacity offsets across 128 candidates, followed by a vectorized Max-Min swap-based local search on GPU.\n- **Performance**: Achieves a combined score of 0.66, characterized by a perfect speed score (1.0) and a moderate balancedness score (0.31).\n- **Feedback**: The highly vectorized design ensures maximum throughput, though the randomized greedy heuristic combined with simple local swapping limits the solution's ability to escape local optima for better load balancing.\n**Program Identifier:** Generation 32 - Patch Name mixed_offsets_greedy - Correct Program: True\n\n**Program Name: Vectorized Ensemble Greedy Packing with Binary Search Replication**\n- **Implementation**: The algorithm generates 128 packing candidates in parallel using LPT, ZigZag, and random perturbations, selecting the best via vectorized evaluation, while using binary search to optimize expert replication counts.\n- **Performance**: It achieves a perfect speed score (1.0) but poor balancedness (0.31), totaling a 0.66 combined score.\n- **Feedback**: The highly vectorized implementation ensures maximum throughput, but the reliance on simple greedy heuristics for packing results in suboptimal load distribution compared to more complex iterative solvers.\n**Program Identifier:** Generation 33 - Patch Name ensemble_greedy_packing - Correct Program: True\n\n**Program Name: Hybrid Folded Greedy and Randomized Local Search Load Balancer**\n- **Implementation**: The algorithm combines a deterministic folded-chunk packing strategy with a parallelized randomized greedy search that includes swap-based local refinement. It employs a binary search with density-based adjustments for determining expert replication counts and dynamically selects the packing strategy that minimizes imbalance for each layer.\n- **Performance**: The solution achieved a combined score of 0.66, demonstrating perfect execution speed (1.0) but moderate load balancing effectiveness (0.31).\n- **Feedback**: The vectorized parallel search and simple heuristics ensure the algorithm is extremely fast, achieving the maximum speed score. However, the lower balancedness score indicates that the greedy and swap-based heuristics may struggle to find near-optimal distributions for highly skewed workloads compared to more complex optimization approaches.\n**Program Identifier:** Generation 34 - Patch Name parallel_hybrid_packing - Correct Program: True\n\n**Program Name: Hierarchical Parallel Ensemble Greedy Load Balancer**\n- **Implementation**: This solution implements a hierarchical rebalancing strategy using a vectorized ensemble of greedy packing heuristics (LPT, ZigZag, Noisy) processed in parallel chunks to assign experts to GPUs.\n- **Performance**: The program achieves a combined score of 0.66, distinguished by a perfect speed score (1.0) but a modest balancedness score (0.31).\n- **Feedback**: While the highly vectorized ensemble approach ensures minimal runtime overhead, the chunked approximation and hierarchical constraints appear to limit the solver's ability to find globally optimal distributions, negatively impacting the balance score.\n**Program Identifier:** Generation 35 - Patch Name massive_ensemble_greedy_eplb - Correct Program: True\n\n**Program Name: Vectorized EPLB with Parallel Initialization and Swap Refinement**\n- **Implementation**: The algorithm leverages GPU parallelism to simultaneously evaluate 128 diverse packing candidates initialized via randomized LPT and folded sorting, followed by a vectorized local search that iteratively swaps items from the heaviest pack to reduce maximum load.\n- **Performance**: The solution achieves a combined score of 0.66, distinguishing itself with a perfect speed score (1.0) while maintaining a balancedness score of 0.31.\n- **Feedback**: The implementation's heavy reliance on vectorization and massive parallel candidates makes it extremely fast and suitable for real-time constraints, though the heuristic nature of the greedy-plus-refinement approach trades some packing optimality for this execution speed.\n**Program Identifier:** Generation 36 - Patch Name diverse_init_max_any_swap - Correct Program: True\n\n**Program Name: Hybrid Randomized Greedy-ZigZag EPLB with Vectorized Refinement**\n- **Implementation**: This approach employs a hybrid initialization strategy using 128 parallel candidates generated via randomized Greedy LPT and ZigZag methods with noise, followed by a vectorized L2-norm local search to refine pack weights.\n- **Performance**: The solution achieved a score of 0.0 as it failed to pass validation tests.\n- **Feedback**: The program is functionally incorrect, suggesting that the complex vectorized logic for candidate generation or swap refinement likely introduced errors in the final expert-to-device mapping.\n**Program Identifier:** Generation 37 - Patch Name hybrid_greedy_zigzag_l2 - Correct Program: False\n\n**Program Name: Vectorized Hybrid Ensemble Greedy Load Balancer**\n- **Implementation**: The algorithm utilizes a vectorized greedy packing strategy applied to an ensemble of heuristic sorts (LPT, ZigZag, Noisy) followed by a single-pass swap refinement to adjust loads. Expert replication is handled via a binary search on max load with greedy adjustments for over/under-allocation.\n- **Performance**: It achieves a combined score of 0.66, demonstrating perfect execution speed (1.0) but poor load balancing effectiveness (0.31).\n- **Feedback**: The heavy reliance on vectorization and limited refinement iterations ensures minimal runtime overhead, but the resulting packing quality is suboptimal; significantly more rigorous local search or stronger packing heuristics are required to improve the balance score.\n**Program Identifier:** Generation 38 - Patch Name none - Correct Program: True\n\n**Program Name: Parallel Ensemble Greedy Packing with Vectorized Refinement**\n- **Implementation**: Generates 128 candidate permutations per layer using LPT variants and noise, processes them via a vectorized chunked greedy packer, and optimizes loads using iterative 2-opt swaps.\n- **Performance**: Achieved a combined score of 0.66, attaining a perfect speed score (1.0) but a lower balancedness score (0.31).\n- **Feedback**: The extensive vectorization yields maximum efficiency, but the greedy heuristics combined with local search struggle to find globally optimal packings compared to more complex approaches.\n**Program Identifier:** Generation 39 - Patch Name vectorized_ensemble_refinement_v2 - Correct Program: True\n\n**Program Name: Parallel Ensemble Packing with Binary Search Replication**\n- **Implementation**: This approach utilizes a vectorized ensemble strategy to generate and evaluate 128 packing candidates (LPT, ZigZag, Noisy) concurrently, followed by local search refinement on the best candidate. Expert replication is calculated using binary search on load thresholds with greedy adjustments to ensure exact physical expert constraints.\n- **Performance**: It achieves a perfect speed score (1.00) with a moderate balancedness score (0.31), yielding a combined score of 0.66.\n- **Feedback**: The implementation prioritizes computational efficiency through massive parallelization of simple heuristics, resulting in extremely fast execution but suboptimal load distribution compared to more iterative solvers.\n**Program Identifier:** Generation 40 - Patch Name refine_ensemble - Correct Program: True\n\n**Program Name: Hybrid Ensemble Greedy with Vectorized Refinement**\n- **Implementation**: Utilizes a parallelized ensemble of 128 packing heuristics (including LPT, ZigZag, and noise) coupled with a vectorized iterative swap refinement step to optimize load distribution.\n- **Performance**: Achieved a combined score of 0.66, with perfect speed (1.0) but moderate balancedness (0.31).\n- **Feedback**: The highly vectorized implementation minimizes runtime overhead allowing for broad state space exploration, though the heuristic ensemble yielded only marginal improvements in load balancing effectiveness.\n**Program Identifier:** Generation 41 - Patch Name diverse_ensemble_with_optimized_refinement - Correct Program: True\n\n**Program Name: Parallel Hybrid Initialization with Vectorized Local Search EPLB**\n- **Implementation**: Uses massive parallel initialization with 128 noisy candidates per layer and a two-stage vectorized local search (single and pair swaps) to optimize pack assignments efficiently on GPU.\n- **Performance**: Achieves a combined score of 0.66, characterized by a perfect speed score (1.0) but a moderate balancedness score (0.31).\n- **Feedback**: The fully vectorized approach eliminates CPU bottlenecks yielding excellent speed, but the greedy swap heuristics may limit the algorithm's ability to escape local optima for better load balancing.\n**Program Identifier:** Generation 42 - Patch Name hybrid_parallel_eplb_v2 - Correct Program: True\n\n**Program Name: Ensemble Greedy Packing with 2-Opt Refinement and Binary Replication**\n- **Implementation**: The algorithm generates 128 candidate packings using LPT, ZigZag, and noisy permutations, selects the best via vectorized simulation, and improves it with iterative 2-opt local search. Replication counts are determined by binary searching for a max-load threshold, followed by greedy adjustments to match physical constraints.\n- **Performance**: It achieves a combined score of 0.66, characterized by perfect execution speed (1.0) but moderate load balancing effectiveness (0.31).\n- **Feedback**: The highly vectorized ensemble ensures minimal runtime overhead, but the underlying greedy heuristics struggle to achieve the granular load distribution required for higher balancedness scores compared to more complex solvers.\n**Program Identifier:** Generation 43 - Patch Name ensemble_packing_with_refinement - Correct Program: True\n\n**Program Name: Vectorized Hybrid Local Search EPLB**\n- **Implementation**: This approach employs 128 parallel candidates initialized via LPT, Random, and Folded strategies, refined by a vectorized two-phase local search minimizing L2 norm and peak loads.\n- **Performance**: It achieves a combined score of 0.66, characterized by perfect execution speed (1.0) and moderate load balancing effectiveness (0.31).\n- **Feedback**: The heavy reliance on vectorized operations allows for extensive search space exploration without compromising runtime, though the moderate balancedness score suggests the local search or initialization diversity could be further tuned for difficult packing scenarios.\n**Program Identifier:** Generation 44 - Patch Name eplb_hybrid_folded - Correct Program: True\n\n**Program Name: Vectorized Hybrid Initialization EPLB**\n- **Implementation**: Uses a fully vectorized PyTorch approach combining randomized LPT, random shuffle, and folded LPT initializations with a Max-Any Swap local search.\n- **Performance**: Achieved a combined score of 0.66, characterized by a perfect speed score (1.0) but moderate balancedness (0.31).\n- **Feedback**: The high speed confirms the effectiveness of vectorization for this workload, though the lower balancedness score suggests the greedy and local search heuristics could be further optimized for better load distribution.\n**Program Identifier:** Generation 45 - Patch Name hybrid_init_balanced_packing - Correct Program: True\n\n**Program Name: Hybrid Ensemble Greedy Packing with Iterative Refinement**\n- **Implementation**: Utilizes a vectorized ensemble of packing heuristics (LPT, ZigZag, Noisy LPT) followed by greedy allocation and an iterative pairwise swap refinement step to optimize expert load distribution.\n- **Performance**: The solution achieved a combined score of 0.66, characterized by perfect execution speed (1.0) but sub-optimal balancedness (0.31).\n- **Feedback**: The implementation prioritizes runtime efficiency through heavy vectorization, but the heuristic-based approach struggles to find tight packings for highly skewed distributions compared to more complex solvers.\n**Program Identifier:** Generation 46 - Patch Name ensemble_shuffle_refine_filtering - Correct Program: True\n\n**Program Name: Vectorized Hybrid LPT with Parallel 1-Item and 2-Item Swaps**\n- **Implementation**: This approach initializes 128 parallel candidates using noisy LPT and random strategies, refines them via vectorized 1-item swaps, prunes to the top 8, and concludes with 2-item swaps.\n- **Performance**: It attains a combined score of 0.66, driven by a perfect speed score (1.00) while balancedness remains moderate (0.31).\n- **Feedback**: The implementation successfully maximizes computational efficiency through vectorization and pruning, but the lower balancedness score indicates that the local search heuristics may settle into suboptimal packings for difficult distributions.\n**Program Identifier:** Generation 47 - Patch Name hierarchical_hybrid_eplb - Correct Program: True\n\n**Program Name: Vectorized Hybrid Randomized Greedy with Two-Phase Local Search**\n- **Implementation**: Initializes 128 candidate solutions in parallel using perturbed weight sorting (hybrid LPT/random), followed by vectorized refinement passes performing global single-item swaps and pairwise Max-Min swaps.\n- **Performance**: Achieves a combined score of 0.66, excelling in speed (1.0) but with moderate balancedness (0.31).\n- **Feedback**: The heavy vectorization effectively minimizes runtime overhead, but the packing quality suggests the randomized greedy heuristic provides a limited starting point that the current local search cannot fully optimize.\n**Program Identifier:** Generation 48 - Patch Name hybrid_init_global_swap - Correct Program: True\n\n**Program Name: Hybrid Ensemble Greedy Packing with Top-K Refinement**\n- **Implementation**: This algorithm generates 128 candidate permutations (including LPT, ZigZag, Noisy LPT, and Random), evaluates them simultaneously using a vectorized greedy packing kernel, and applies iterative swap refinement to the top 8 candidates.\n- **Performance**: It achieved a combined score of 0.66, characterized by a perfect speed score (1.0) but a moderate balancedness score (0.31).\n- **Feedback**: The approach demonstrates that vectorizing the evaluation of many stochastic heuristics is highly efficient for runtime, but the reliance on greedy construction and limited local refinement results in suboptimal load distribution compared to more exhaustive methods.\n**Program Identifier:** Generation 49 - Patch Name hybrid_pairwise_eplb - Correct Program: True\n\n**Program Name: Vectorized Hybrid Initialization with Swap-Based Refinement Load Balancer**\n- **Implementation**: This approach generates 128 parallel candidate solutions per layer using randomized LPT, shuffling, and folded strategies, then refines them simultaneously via vectorized greedy assignment and swap-based local search using PyTorch operations.\n- **Performance**: The algorithm achieves a combined score of 0.66, distinguished by a perfect speed score (1.0) but a moderate balancedness score (0.31).\n- **Feedback**: The vectorized implementation successfully minimizes overhead by evaluating diverse heuristics in parallel, ensuring optimal speed. However, the moderate balancedness suggests that while the greedy-swap strategy is fast, it may occasionally settle for local optima compared to more exhaustive search methods.\n**Program Identifier:** Generation 50 - Patch Name hybrid_init_packing - Correct Program: True\n\n**Program Name: Genetic Algorithm with Vectorized Local Search for EPLB**\n- **Implementation**: The solution implements a genetic algorithm featuring diverse initialization strategies (Noisy LPT, Interleaved) and a vectorized \"Max-Any Swap\" local search to optimize expert packing hierarchically. It utilizes efficient PyTorch tensor operations for both the greedy construction phase and the evolutionary resampling of elite candidates to handle large-scale inputs.\n- **Performance**: The algorithm achieves a perfect speed score (1.0) but a moderate balancedness score (0.31), yielding a combined score of 0.66.\n- **Feedback**: The heavy use of vectorization ensures excellent runtime performance, but the lower balancedness score indicates that the local search heuristics or mutation strategies may need further refinement to handle difficult load distributions effectively.\n**Program Identifier:** Generation 51 - Patch Name genetic_prio_packing - Correct Program: True\n\n**Program Name: Vectorized Two-Stage Evolutionary Load Balancer**\n- **Implementation**: The solution employs a vectorized two-stage evolutionary algorithm using randomized LPT initialization and rank-based mutations, followed by a custom \"Max-Any\" swap local search to iteratively minimize peak loads.\n- **Performance**: It achieves a perfect speed score of 1.00 but a lower balancedness score of 0.31, resulting in an overall score of 0.66.\n- **Feedback**: While the vectorized implementation is highly efficient, the heuristic approach sacrifices some packing quality, suggesting that more aggressive search strategies or diverse initializations could improve balance.\n**Program Identifier:** Generation 52 - Patch Name evolutionary_interleaved_eplb - Correct Program: True\n\n**Program Name: Vectorized Hybrid Ensemble Greedy Load Balancer**\n- **Implementation**: Generates 128 candidate permutations using strategies like LPT, ZigZag, and noise injection, evaluates them via a vectorized greedy packing kernel, and applies iterative swap-based refinement to the top 16 candidates.\n- **Performance**: Achieved a combined score of 0.66, with a perfect speed score (1.0) but moderate balancedness (0.31).\n- **Feedback**: The vectorized approach enables rapid evaluation of diverse heuristics, ensuring low overhead, though the greedy foundation limits the algorithm's ability to achieve perfect load distribution compared to slower optimization solvers.\n**Program Identifier:** Generation 53 - Patch Name eplb_hybrid_refined - Correct Program: True\n\n**Program Name: Vectorized EPLB with Parallel LPT and Local Search**\n- **Implementation**: Uses massively parallel greedy Longest Processing Time (LPT) initialization with noise injection for candidate generation, followed by a vectorized Max-Any Swap local search algorithm to refine allocations.\n- **Performance**: Achieved a combined score of 0.66, securing a perfect speed score (1.0) with a balancedness score of 0.31.\n- **Feedback**: The highly vectorized design allows for rapid evaluation of numerous packing candidates, prioritizing execution speed while maintaining acceptable load distribution quality through local search refinement.\n**Program Identifier:** Generation 54 - Patch Name greedy_lpt_max_any_swap - Correct Program: True\n\n**Program Name: Vectorized Hybrid EPLB with Noise and Local Search**\n- **Implementation**: Generates 128 parallel candidates using LPT and Interleaved-LPT with noise injection, followed by a vectorized greedy assignment and iterative 1-item and 2-item swap local searches to refine pack balance.\n- **Performance**: Achieved a combined score of 0.66, characterized by a perfect speed score (1.0) but a lower balancedness score (0.31).\n- **Feedback**: The fully vectorized approach yields exceptional runtime performance, but the moderate balancedness suggests the local search heuristics or noise strategies may need further tuning to escape local optima in difficult packing scenarios.\n**Program Identifier:** Generation 55 - Patch Name parallel_hybrid_sort_greedy_plus_local_search - Correct Program: True\n\n**Program Name: Vectorized Ensemble Greedy Packing with Binary Search Replication**\n- **Implementation**: The solution utilizes vectorized evaluation of 64 candidate permutations (LPT, Interleaved, Noisy) for greedy packing, followed by swap-based refinement and a binary-search strategy for expert replication.\n- **Performance**: Achieved a combined score of 0.66, delivering perfect speed (1.0) with moderate load balancing efficacy (0.31).\n- **Feedback**: The vectorized ensemble strategy ensures high throughput and adaptability, though the reliance on greedy heuristics limits the maximum achievable balance compared to more computationally intensive global optimization methods.\n**Program Identifier:** Generation 56 - Patch Name ensemble_greedy_refine - Correct Program: True\n\n**Program Name: Massive Parallel Randomized Greedy LPT with Vectorized Swap Refinement**\n- **Implementation**: The algorithm employs a massive parallel strategy generating 64 randomized greedy LPT candidates with noise injection, followed by a vectorized Max-Any swap local search on the GPU to refine pack assignments.\n- **Performance**: The solution attained a perfect speed score (1.00) but limited balancedness (0.31), resulting in a combined score of 0.66.\n- **Feedback**: While the fully vectorized GPU design maximizes execution speed and throughput, the randomized heuristic and simple swap logic sacrificed significant packing precision compared to more robust optimization techniques.\n**Program Identifier:** Generation 57 - Patch Name gpu_ensemble_packing - Correct Program: True\n\n**Program Name: Vectorized Hybrid Initialization with Max-Any Swap EPLB**\n- **Implementation**: This solution employs massive parallel initialization using perturbed LPT, Random, and ZigZag strategies across 128 candidates, refined by a vectorized Max-Any swap local search to optimize packing.\n- **Performance**: The program achieves a combined score of 0.66, driven by a perfect speed score (1.00) but a moderate balancedness score (0.31).\n- **Feedback**: The fully vectorized design ensures optimal execution speed; however, the lower balancedness suggests that increasing the local search depth or refining the heuristic diversity could improve the final load distribution quality.\n**Program Identifier:** Generation 58 - Patch Name hybrid_parallel_eplb - Correct Program: True\n\n**Program Name: Parallel Randomized LPT with Vectorized Swap Local Search**\n- **Implementation**: Generates 128 parallel candidates using randomized LPT and diverse shuffles, refined by vectorized greedy packing and GPU-accelerated 1-item and 2-item local swap searches to minimize max load.\n- **Performance**: Achieves a combined score of 0.66, demonstrating maximum speed (1.0) but moderate load balancing effectiveness (0.31).\n- **Feedback**: The vectorized parallel approach maximizes computational throughput, but the relatively low balancedness score suggests the greedy construction and limited local search depth may struggle to escape local optima in complex packing scenarios.\n**Program Identifier:** Generation 59 - Patch Name diverse_init_and_2swap - Correct Program: True\n\n**Program Name: Vectorized Evolutionary Strategy with Max-Any Swap Search**\n- **Implementation**: The solution generates 128 diverse candidates per layer using randomized LPT and interleaved sorting, utilizing fully vectorized greedy packing and a \"Max-Any\" swap local search to iteratively reduce the peak load.\n- **Performance**: It achieved a combined score of 0.66, characterized by a perfect speed score (1.0) but a lower balancedness score (0.31).\n- **Feedback**: The implementation effectively leverages GPU parallelism to explore a wide solution space quickly, but the reliance on simple swaps limits its ability to escape local optima compared to more complex combinatorial moves.\n**Program Identifier:** Generation 60 - Patch Name evolutionary_interleaved_packing - Correct Program: True\n\n**Program Name: Vectorized EPLB with Massive Parallel Initialization and Local Search**\n- **Implementation**: Deploys 128 parallel candidates using diverse initializations (Noisy LPT, Random, Interleaved) and noisy-greedy packing, refined by vectorized 1-item and 2-item local swap heuristics to minimize max load.\n- **Performance**: The solution achieves a combined score of 0.66, obtaining a perfect speed score (1.0) but a lower balancedness score (0.31).\n- **Feedback**: The vectorized approach significantly optimizes execution speed by evaluating many candidates simultaneously, but the local search heuristics appear insufficient to fully converge to an optimal load balance for complex weight distributions.\n**Program Identifier:** Generation 61 - Patch Name init_interleaved_and_noisy_greedy_v2 - Correct Program: True\n\n**Program Name: Massive Parallel Hybrid Initialization with Vectorized Max-Any Swap EPLB**\n- **Implementation**: The solution generates 256 parallel candidates using randomized LPT, interleaved, and random permutations, followed by a vectorized greedy assignment and a max-any swap local search to minimize the maximum load.\n- **Performance**: It achieves a combined score of 0.66, excelling in speed (1.0) while providing moderate balancedness (0.31).\n- **Feedback**: High speed indicates successful vectorization, but the moderate balancedness suggests the local search or greedy heuristics struggle to find the globally optimal packing for highly skewed distributions compared to more complex algorithms.\n**Program Identifier:** Generation 62 - Patch Name parallel_hybrid_packing_v2 - Correct Program: True\n\n**Program Name: Vectorized Hybrid Randomized Greedy with Two-Phase Local Search**\n- **Implementation**: Utilizes massively parallel randomized greedy initialization mixing LPT and random strategies, followed by a refined local search performing global single-item swaps and targeted max-min pair swaps.\n- **Performance**: Achieved a combined score of 0.66, maximizing speed (1.0) with a balancedness score of 0.31.\n- **Feedback**: The fully vectorized implementation yields excellent runtime efficiency by leveraging GPU parallelism to explore many candidates, though the load balancing quality remains moderate compared to the optimal speed.\n**Program Identifier:** Generation 63 - Patch Name zoom_in_perturbation - Correct Program: True\n\n**Program Name: Vectorized Hybrid Ensemble Greedy Packing with Load Refinement**\n- **Implementation**: The algorithm generates diverse candidate packings via vectorized heuristics (LPT, ZigZag, Noisy, Shuffle) and refines the best allocations by iteratively swapping items from the heaviest pack to reduce maximum load.\n- **Performance**: The solution achieved a combined score of 0.66, delivering perfect speed (1.0) but a lower balancedness score (0.31).\n- **Feedback**: While the vectorized implementation ensures maximal execution speed, the moderate balancedness score indicates that the greedy ensemble and single-swap refinement heuristics may be insufficient for finding highly optimized packings in difficult weight distributions.\n**Program Identifier:** Generation 64 - Patch Name global_swap_refinement - Correct Program: True\n\n**Program Name: Ensemble Greedy Packing with Destruct-Reconstruct Refinement**\n- **Implementation**: The algorithm generates 128 parallel packing candidates using varied sorting strategies (LPT, ZigZag, Noisy) and selects the best one, followed by a local iterative refinement that rebalances the heaviest and lightest packs. Expert replication is handled via a binary search on max load thresholds to determine replica counts efficiently.\n- **Performance**: It achieves a combined score of 0.66, with a perfect speed score of 1.0 but a moderate balancedness score of 0.31.\n- **Feedback**: The vectorized ensemble approach provides exceptional execution speed, making it highly scalable. However, the reliance on greedy heuristics and localized refinement limits its ability to escape local optima, resulting in suboptimal load balancing compared to more computationally intensive global solvers.\n**Program Identifier:** Generation 65 - Patch Name reconstruct_refinement - Correct Program: True\n\n**Program Name: Hybrid Soft-Greedy Ensemble with Vectorized Swap Refinement**\n- **Implementation**: Utilizes 256 parallel candidates combining LPT and interleaved sorting with stochastic soft-greedy placement, followed by a vectorized local search that swaps items from maximally loaded packs to improve balance.\n- **Performance**: Achieves a combined score of 0.66, delivering perfect speed (1.0) but a lower balancedness score (0.31).\n- **Feedback**: The vectorized ensemble ensures high throughput and efficiency, but the stochastic greedy approach coupled with simple swaps struggles to find globally optimal packings compared to more complex iterative solvers.\n**Program Identifier:** Generation 66 - Patch Name hybrid_soft_greedy_eplb - Correct Program: True\n\n**Program Name: Massively Parallel Hybrid LPT with Vectorized Swap Refinement**\n- **Implementation**: The algorithm generates 256 parallel candidate solutions per layer using hybrid initialization strategies (Spectrum LPT, Interleaved, Random), executes a vectorized greedy packing, and refines results via a vectorized \"Max-Any\" swap local search to minimize bottleneck loads.\n- **Performance**: It achieves a perfect speed score (1.0) and a balancedness score of 0.31, yielding a combined score of 0.66.\n- **Feedback**: The implementation leverages GPU parallelism effectively for high throughput and low latency, though the heuristic nature limits the optimality of load balancing compared to more computationally intensive solvers.\n**Program Identifier:** Generation 67 - Patch Name eplb_hybrid_spectrum - Correct Program: True\n\n**Program Name: Hybrid Vectorized EPLB with ABBA and Max-Any Swap Refinement**\n- **Implementation**: This approach utilizes vectorized greedy packing across 256 candidates initialized via LPT with noise and interleaving, subsequently refined using pairwise ABBA redistribution and Max-Any item swaps.\n- **Performance**: Achieved a combined score of 0.66, characterized by perfect speed (1.0) but moderate balancedness (0.31).\n- **Feedback**: The highly vectorized implementation ensures minimal runtime overhead, but the moderate balancedness score suggests that local search refinements are insufficient to fully escape local optima inherent to the initial greedy assignments.\n**Program Identifier:** Generation 68 - Patch Name stochastic_abba_rebalance - Correct Program: True\n\n**Program Name: Hybrid Ensemble Greedy with Pairwise Refinement**\n- **Implementation**: This approach generates multiple candidate packings using various sorting heuristics (LPT, interleaved, random) for a vectorized greedy allocator, followed by an iterative pairwise partition refinement step to rebalance heavy and light packs.\n- **Performance**: Combined score of 0.0, as the solution failed validation tests.\n- **Feedback**: The implementation is functionally incorrect; the complex vectorized scatter/gather logic in the refinement and replication phases likely creates invalid mappings or corrupts state, causing it to fail basic correctness checks.\n**Program Identifier:** Generation 69 - Patch Name partition_refined_greedy - Correct Program: False\n\n**Program Name: Vectorized Evolutionary Load Balancer with Greedy Noise and Local Search**\n- **Implementation**: This solution implements a vectorized two-stage evolutionary algorithm using randomized LPT and interleaved sorting for exploration, followed by rank-based mutation with greedy decision noise and a Max-Any swap local search.\n- **Performance**: The program achieves a combined score of 0.66, characterized by a perfect speed score of 1.0 but a moderate balancedness score of 0.31.\n- **Feedback**: The highly vectorized approach ensures maximum execution speed, but the lower balancedness score indicates that the heuristic diversity or local search depth might be insufficient for finding optimal packings in complex distributions.\n**Program Identifier:** Generation 70 - Patch Name decision_noise_greedy - Correct Program: True\n\n**Program Name: Hybrid Ensemble Load Balancer with Top-K Refinement**\n- **Implementation**: Generates 128 parallel packing candidates (LPT, ZigZag, Random, Noisy) and iteratively refines the top 8 using a vectorized greedy swap heuristic to reduce maximum load.\n- **Performance**: Achieved a combined score of 0.66, characterized by perfect speed (1.0) and moderate balancedness (0.31).\n- **Feedback**: The ensemble-based generation with vectorized refinement proved highly efficient, though the local greedy search strategy limited the potential for finding more globally optimal balanced configurations.\n**Program Identifier:** Generation 71 - Patch Name improved_refinement_and_candidates - Correct Program: True\n\n**Program Name: Vectorized Evolutionary Max-Any Swap Load Balancer**\n- **Implementation**: Utilizes 256 parallelized evolutionary candidates initialized via diverse strategies (LPT, Interleaved, Random) and optimizes them using a vectorized greedy assignment followed by an iterative Max-Any swap local search on the GPU.\n- **Performance**: Achieves exceptional speed (1.0) with a moderate balancedness score (0.31), resulting in a combined score of 0.65.\n- **Feedback**: While the fully vectorized approach maximizes computational throughput, the local search heuristic\u2014restricted to swapping items only from the single heaviest pack\u2014likely limits exploration, preventing the algorithm from escaping local optima to achieve tighter packing.\n**Program Identifier:** Generation 72 - Patch Name soft_greedy_packing - Correct Program: True\n\n**Program Name: Massive Parallel LPT-SoftGreedy with L2-MinMax Refinement**\n- **Implementation**: The algorithm generates 512 parallel candidates using randomized LPT sorting and dynamic soft-greedy construction, then refines them via vectorized L2 descent to reduce variance and Min-Max polishing to minimize peak loads.\n- **Performance**: The solution achieves a perfect speed score (1.0) but a moderate balancedness score (0.31), yielding a combined score of 0.66.\n- **Feedback**: The highly vectorized approach ensures minimal overhead, though the lower balancedness score suggests that the local search strategy may yield diminishing returns or get trapped in local optima compared to more exhaustive methods.\n**Program Identifier:** Generation 73 - Patch Name soft_greedy_l2_descent_eplb - Correct Program: True\n\n**Program Name: Vectorized Hybrid Stochastic Search with Variance-Aware Swaps**\n- **Implementation**: This approach generates 256 parallel candidates using varied strategies (Noisy LPT, Z-Curve, Soft Greedy) and refines them via vectorized local swaps that lexicographically minimize maximum load and L2 norm.\n- **Performance**: It achieved a combined score of 0.66, characterized by a perfect speed score (1.0) and a balancedness score of 0.31.\n- **Feedback**: The highly parallelized candidate generation and refinement ensure exceptional speed, though the packing quality (balancedness) remains moderate compared to the theoretical optimum.\n**Program Identifier:** Generation 74 - Patch Name stochastic_snake_lpt_variance_packing - Correct Program: True\n\n**Program Name: Hybrid Ensemble Greedy with Vectorized Refinement**\n- **Implementation**: This module implements a Hybrid Ensemble Greedy strategy that generates diverse packing candidates via parallel heuristics (LPT, ZigZag, Noisy) and improves the best candidates using a vectorized swap-based refinement algorithm.\n- **Performance**: It achieves a perfect speed score (1.0) but a lower balancedness score (0.31), reflecting a trade-off favoring execution time over packing precision.\n- **Feedback**: While the vectorized ensemble approach is extremely fast, the reliance on greedy heuristics limits the algorithm's ability to resolve complex imbalances compared to more exhaustive optimization techniques.\n**Program Identifier:** Generation 75 - Patch Name exhaustive_swap_refinement - Correct Program: True\n\n**Program Name: Massive Parallel Stochastic Search EPLB**\n- **Implementation**: The solution employs a fully vectorized parallel search generating 256 candidate packings via multi-strategy initialization (LPT, Interleaved, Random) and soft-greedy construction, refined by a variance-aware local swap heuristic.\n- **Performance**: It achieves a perfect speed score (1.0) due to efficient tensor operations but moderate packing quality (0.31), resulting in a combined score of 0.66.\n- **Feedback**: The implementation prioritizes throughput with its highly parallelized candidate generation, though the lower balancedness score suggests the stochastic and greedy heuristics may not fully converge to optimal assignments for difficult weight distributions.\n**Program Identifier:** Generation 76 - Patch Name stochastic_lpt_variance_swap_eplb - Correct Program: True\n\n**Program Name: Vectorized Ensemble Packing with Binary Search Replication**\n- **Implementation**: Implements a massively parallel ensemble of packing strategies (LPT, ZigZag, Noisy) refined by iterative Max-Any swaps, and allocates replicas using binary search on max load density.\n- **Performance**: The solution excels in speed (1.0) but provides average load balancing (0.31), yielding a combined score of 0.66.\n- **Feedback**: The vectorized ensemble ensures rapid execution, yet the underlying greedy packing logic struggles to achieve high balancedness compared to more sophisticated optimization techniques like min-cost flow.\n**Program Identifier:** Generation 77 - Patch Name parallel_topk_refinement_max_any - Correct Program: True\n\n**Program Name: Massive Parallel Ensemble with Targeted Max-Reduction Refinement**\n- **Implementation**: The solution employs a vectorized ensemble strategy generating 256 candidates (LPT, ZigZag, Noisy) and refines the best ones using a \"Targeted Max-Reduction\" local search that optimally relocates items from the heaviest packs to strictly reduce max load.\n- **Performance**: Achieved a combined score of 0.66 with a perfect speed score (1.0) and a balancedness score of 0.31.\n- **Feedback**: The purely vectorized implementation yields excellent runtime performance, though the moderate balancedness score suggests that more aggressive or iterative refinement strategies could further minimize the maximum load gap.\n**Program Identifier:** Generation 78 - Patch Name ensemble_max_reduction_eplb - Correct Program: True\n\n**Program Name: Hybrid Stochastic Ensemble with Vectorized Swap Refinement**\n- **Implementation**: This approach generates diverse packing candidates using LPT, ZigZag patterns, and noise injection, processes them with a fast vectorized greedy packer, and iteratively refines the best results using a swap-based reduction of the maximum load.\n- **Performance**: The algorithm achieved a combined score of 0.66, characterized by a perfect speed score (1.0) but a lower balancedness score (0.31).\n- **Feedback**: The vectorized operations provide excellent runtime efficiency, though the reliance on greedy heuristics and stochastic search yields suboptimal load distribution compared to more rigorous optimization solvers.\n**Program Identifier:** Generation 79 - Patch Name eplb_hybrid_stochastic_refine - Correct Program: True\n\n**Program Name: Big-Rocks Ensemble Packing with L2-Norm Refinement**\n- **Implementation**: This solution employs a vectorized ensemble of 256 candidates (including LPT, ZigZag, and noisy variants) with \"Big-Rocks\" pre-assignment for heavy items, refined by an iterative swap strategy minimizing the L2 norm of loads.\n- **Performance**: It achieved a combined score of 0.66, with a perfect speed score (1.0) dominating a lower balancedness score (0.31).\n- **Feedback**: The purely vectorized approach yields exceptional speed, making it highly responsive; however, the lower balancedness suggests that the greedy ensemble and L2 local search struggle to fully resolve imbalances in highly skewed distributions compared to slower, more exhaustive algorithms.\n**Program Identifier:** Generation 80 - Patch Name big_rocks_l2_ensemble_eplb - Correct Program: True\n\n**Program Name: Vectorized Hybrid EPLB with ABBA and Swap Refinement**\n- **Implementation**: The algorithm generates 256 parallel candidates using strategies like noisy LPT and interleaved sorting with diversity offsets, followed by vectorized greedy packing. It refines solutions using 30 iterations of ABBA destruct-reconstruct on min/max packs and 15 iterations of Max-Any swapping to minimize load variance.\n- **Performance**: Achieved a combined score of 0.66, distinguishing itself with a perfect speed score (1.0) while maintaining a balancedness score of 0.31.\n- **Feedback**: The fully vectorized design allows for massive candidate exploration without sacrificing speed, though the balancedness score suggests that the local search heuristics (ABBA/Swap) may hit local optima on highly skewed distributions.\n**Program Identifier:** Generation 81 - Patch Name eplb_hybrid_abba_offset - Correct Program: True\n\n**Program Name: Massively Parallel Vectorized EPLB with Local Search**\n- **Implementation**: Uses 128 parallel candidates with diverse initialization (Noisy LPT, Random) and vectorized greedy construction, refined by swap-based and destruct-reconstruct local search strategies.\n- **Performance**: Achieved a combined score of 0.66, characterized by perfect speed (1.00) and moderate balancedness (0.31).\n- **Feedback**: The highly vectorized design ensures exceptional speed, but the lower balancedness indicates that the local search algorithms or candidate diversity could be tuned to better minimize the maximum load.\n**Program Identifier:** Generation 82 - Patch Name topk_destruct_reconstruct - Correct Program: True\n\n**Program Name: Vectorized EPLB with Snake Refinement and Diverse Candidates**\n- **Implementation**: The algorithm generates 256 diverse sorting candidates (LPT, noise, interleaving) and applies fully vectorized greedy packing. It subsequently refines loads using a multi-pack \"snake\" redistribution strategy and a pairwise swap heuristic to minimize the maximum pack weight.\n- **Performance**: It achieves a perfect speed score (1.0) and a balancedness score of 0.31, resulting in a combined metric of 0.66.\n- **Feedback**: The highly optimized, vectorized approach ensures exceptional execution speed, though the balancedness score indicates that the heuristic search effectively finds good local optima rather than global ones.\n**Program Identifier:** Generation 83 - Patch Name multipack_pool_snake_eplb - Correct Program: True\n\n**Program Name: Two-Stage Evolutionary Strategy for Hierarchical Expert Load Balancing**\n- **Implementation**: Uses a vectorized evolutionary approach generating 256 candidates via randomized LPT and interleaving heuristics, followed by a \"zoom-in\" refinement stage using ABBA destruct-reconstruct logic and max-any swapping.\n- **Performance**: Achieves a combined score of 0.66, characterized by perfect execution speed (1.0) but moderate load balancing effectiveness (0.31).\n- **Feedback**: The highly vectorized implementation and simple heuristics ensure minimal latency suitable for runtime usage, though the stochastic local search limits the ability to escape local optima compared to heavier solvers.\n**Program Identifier:** Generation 84 - Patch Name zoom_in_evolutionary_search - Correct Program: True\n\n**Program Name: Two-Stage Zoom-In Strategy with Vectorized Swapping for EPLB**\n- **Implementation**: The algorithm utilizes a two-stage process starting with randomized LPT exploration, followed by an exploitation phase using Top-4 pooling redistribution and vectorized max-any swaps to refine the packing.\n- **Performance**: The program achieved a score of 0.0, failing to pass validation tests.\n- **Feedback**: The sophisticated vectorized index manipulations in the pooling and swapping phases likely caused validity issues (such as item loss or duplication), leading to correctness failures.\n**Program Identifier:** Generation 85 - Patch Name two_stage_zoom_top4_pooling - Correct Program: False\n\n**Program Name: Parallel Ensemble Greedy Packing with Pairwise Refinement**\n- **Implementation**: Uses a massive parallel ensemble of sorting heuristics (LPT, ZigZag, Noisy) fed into a vectorized greedy packing kernel, followed by iterative pairwise swapping to refine the best candidates.\n- **Performance**: Achieved a perfect speed score (1.0) but a low balancedness score (0.31), resulting in a combined score of 0.66.\n- **Feedback**: The vectorized ensemble approach ensures extremely low latency, but the underlying greedy logic and local search struggle to find optimal packing configurations for complex load distributions.\n**Program Identifier:** Generation 86 - Patch Name hybrid_evolutionary_refinement - Correct Program: True\n\n**Program Name: Vectorized Vortex Search with Quad-Pooling Load Balancer**\n- **Implementation**: The algorithm utilizes a vectorized \"Vortex Search\" generating 256 parallel candidates via LPT and noise, refined by iterative Max-Any swaps and a \"Quad-Pooling\" strategy that redistributes items among the top-2 and bottom-2 packs.\n- **Performance**: It achieved a combined score of 0.66, with a perfect speed score (1.0) but a low balancedness score (0.31).\n- **Feedback**: While the fully vectorized approach utilizing tensor operations on the GPU is extremely fast, the heuristic refinement strategies were ineffective at achieving high load balance, indicating a trade-off where solution quality was sacrificed for speed.\n**Program Identifier:** Generation 87 - Patch Name eplb_vortex_quad - Correct Program: True\n\n**Program Name: Vectorized Two-Stage Evolutionary Strategy with Targeted Refinement**\n- **Implementation**: This solution employs a massively parallel evolutionary strategy, generating diverse heuristic candidates (LPT, ZigZag, Noisy) and refining the best ones using vectorized pairwise swaps to reduce maximum load. Expert replication is handled via binary search on load thresholds followed by greedy correction.\n- **Performance**: The approach attains a perfect speed score (1.0) and a balancedness score of 0.31, leading to a combined metric of 0.66.\n- **Feedback**: The fully vectorized candidate generation and refinement pipeline ensures minimal overhead, securing the best possible speed. However, the balancedness score suggests that while fast, the heuristic search space or refinement depth could be expanded to better escape local optima.\n**Program Identifier:** Generation 88 - Patch Name evolutionary_eplb - Correct Program: True\n\n**Program Name: Hybrid Ensemble Packing with Pairwise LPT Refinement**\n- **Implementation**: Generates multiple packing candidates using LPT, ZigZag, and randomized heuristics, then selects the best for iterative refinement via a pairwise constrained LPT algorithm that rebalances the heaviest and lightest packs.\n- **Performance**: Achieved a combined score of 0.66, excelling in speed (1.0) while attaining a balancedness score of 0.31.\n- **Feedback**: The approach is highly efficient computationally, but the pairwise refinement strategy limits the ability to escape local optima involving multi-pack dependencies, capping the balancing quality.\n**Program Identifier:** Generation 89 - Patch Name pairwise_lpt_refine_eplb - Correct Program: True\n\n**Program Name: Hybrid Ensemble Packing with Iterative Refinement**\n- **Implementation**: Generates multiple packing candidates using diverse heuristics (LPT, ZigZag, Noisy) and improves the best candidates via localized max-min repacking and targeted load-reducing swaps.\n- **Performance**: Achieves perfect speed (1.0) but suboptimal balancedness (0.31), leading to a combined score of 0.66.\n- **Feedback**: The highly vectorized ensemble approach ensures efficiency, but the refinement strategies focusing on local pairs and swaps appear insufficient for achieving high-quality load balancing across all workloads.\n**Program Identifier:** Generation 90 - Patch Name eplb_hybrid_redistribute - Correct Program: True\n\n**Program Name: Vectorized Hybrid Ensemble with Top-K Swap Refinement**\n- **Implementation**: Generates 256 candidate orderings using diverse heuristics (LPT, ZigZag, Noisy), evaluates them with a vectorized greedy packing kernel, and applies a tensor-based pairwise swap refinement on the top candidates to reduce maximum load.\n- **Performance**: Achieves a perfect speed score (1.0) but strictly moderate balancedness (0.31), resulting in a combined score of 0.66.\n- **Feedback**: The highly vectorized ensemble approach ensures exceptional runtime efficiency, but the reliance on greedy construction followed by local swap refinement limits the algorithm's ability to find globally optimal load distributions.\n**Program Identifier:** Generation 91 - Patch Name improved_refinement_and_selection - Correct Program: True\n\n**Program Name: Hybrid Ensemble with Pairwise LPT Refinement**\n- **Implementation:** Implements a hierarchical load balancer using a vectorized ensemble of packing heuristics (LPT, ZigZag, Random) selected via top-K scoring, followed by an iterative pairwise refinement algorithm that redistributes items between the heaviest and lightest packs.\n- **Performance:** 0.0 (Failed Validation).\n- **Feedback:** The program fails functional validation; the complex vectorized scatter-gather logic within the iterative refinement and candidate selection stages likely results in invalid indices or incorrect expert assignments.\n**Program Identifier:** Generation 92 - Patch Name pairwise_lpt_refinement - Correct Program: False\n\n**Program Name: Vectorized Evolutionary Strategy with Max-Any Swap for EPLB**\n- **Implementation**: The solution employs a vectorized evolutionary strategy generating 256 candidates via diverse sort keys (Randomized LPT, Interleaved), followed by greedy assignment, pairwise rebalancing, and a deep \"Max-Any\" swap local search.\n- **Performance**: Achieved a combined score of 0.66, characterized by perfect speed (1.0) but moderate balancedness (0.31).\n- **Feedback**: The highly parallelized GPU implementation ensures minimal overhead suitable for runtime usage, though the heuristic approach sacrifices some load-balancing precision for raw speed.\n**Program Identifier:** Generation 93 - Patch Name pairwise_greedy_rebalance_v2 - Correct Program: True\n\n**Program Name: Hybrid Ensemble Packing with Multi-Target Pairwise Refinement**\n- **Implementation**: Generates 64 candidate packings using diverse greedy heuristics (LPT, ZigZag, Noisy) and iteratively refines the top candidates via vectorized pairwise re-packing between maximum and minimum load packs.\n- **Performance**: Achieved a combined score of 0.66, delivering perfect speed (1.0) with moderate load balancing efficacy (0.31).\n- **Feedback**: The ensemble approach effectively diversifies initial solutions to avoid local optima, but the reliance on greedy foundations and limited refinement steps constrains the maximum achievable balance compared to exhaustive solvers.\n**Program Identifier:** Generation 94 - Patch Name pairwise_repack_lpt_plus - Correct Program: True\n\n**Program Name: Vectorized DeepSeek EPLB with Snake Rebalancing**\n- **Implementation**: The solution implements a vectorized version of the DeepSeek EPLB algorithm, generating 256 candidate packings in parallel using LPT, noise, and interleaved strategies, followed by a custom \"4-Way Snake\" redistribution and pairwise swap refinement.\n- **Performance**: Combined score of 0.0 (Failed validation).\n- **Feedback**: The complex vectorized refinement logic and custom snake redistribution likely introduced indexing errors or invalid assignments, causing the solution to produce incorrect packings that failed verification tests.\n**Program Identifier:** Generation 95 - Patch Name eplb_snake_refine - Correct Program: False\n\n**Program Name**: EPLB with Ensemble Packing and Targeted Refinement\n- **Implementation**: The algorithm generates diverse packing candidates using LPT, pattern-based, and noisy sorting strategies, then selects and optimizes the best candidate using a targeted pairwise swap refinement to minimize peak loads.\n- **Performance**: It achieved a combined score of 0.66, demonstrating perfect speed (1.0) but poor load balancing effectiveness (0.31).\n- **Feedback**: The highly vectorized approach ensures minimal overhead, resulting in a perfect speed score, but the heuristic packing and refinement steps struggle to significantly reduce load imbalance compared to optimal solutions.\n**Program Identifier:** Generation 96 - Patch Name brrr_eplb - Correct Program: True\n\n**Program Name: Vectorized Hybrid EPLB with Diffusion and Swap Refinement**\n- **Implementation**: Utilizes hybrid initialization (LPT, Strided, Random) followed by vectorized random pairwise diffusion and greedy max-any swap refinement to distribute expert loads efficiently.\n- **Performance**: Achieves a combined score of 0.66, characterized by perfect speed (1.0) but moderate balancedness (0.31).\n- **Feedback**: The fully vectorized design ensures maximal runtime efficiency, though the randomized refinement strategies struggle to achieve optimal load balance compared to more exhaustive approaches.\n**Program Identifier:** Generation 97 - Patch Name improved_packing_hybrid_diffusion_l2 - Correct Program: True\n\n**Program Name: Hybrid Ensemble LPT with Quad-Pack Refinement**\n- **Implementation**: This approach employs a parallelized ensemble of sorting heuristics (LPT, ZigZag, Noisy) to generate initial packings, which are then optimized using a specialized refinement kernel that iteratively redistributes items from the heaviest and lightest packs.\n- **Performance**: The solution achieves a combined score of 0.66, obtaining a perfect speed score (1.0) while maintaining a balancedness score of 0.31.\n- **Feedback**: The use of vectorized tensor operations and a diverse ensemble of heuristics allows the algorithm to find competitive load distributions extremely quickly, avoiding the computational cost of complex iterative solvers.\n**Program Identifier:** Generation 98 - Patch Name quad_lpt_ensemble - Correct Program: True\n\n**Program Name: Vectorized EPLB with Snake Init, Mutation, and Iterative Refinement**\n- **Implementation**: The algorithm generates 256 diverse packing candidates using noise and snake ordering, selects the best via L2 tie-breaking, and refines them using pairwise ABBA mutation and alternating Top-4 redistribution/swapping.\n- **Performance**: It achieves a combined score of 0.66, characterized by perfect speed (1.0) but moderate load balancing effectiveness (0.31).\n- **Feedback**: The highly vectorized design ensures exceptional speed; however, the complex heuristic refinements yield only marginal improvements in balance, suggesting a need for stronger global optimization techniques.\n**Program Identifier:** Generation 99 - Patch Name snake_zoom_eplb - Correct Program: True",
  "meta_scratch_pad": "# Analysis of Program Evaluation Results\n\n## Successful Algorithmic Patterns\n- **\"Zoom-In\" Evolutionary Strategy:** The **Current Best Program** effectively employs a two-phase approach: first generating 256 diverse candidates (Exploration), then selecting the single best candidate to replicate, mutate, and refine (Exploitation). This pattern allows the algorithm to focus GPU resources on the most promising area of the search space, consistently achieving perfect speed (1.0) while maximizing optimization opportunities.\n- **Broadcast-Based Max-Any Local Search:** **Generation 93** and the **Current Best Program** utilize a powerful refinement kernel that broadcasts the heaviest pack's contents against all other packs/items in a 4D tensor operation. This exhaustively identifies and applies the single best load-reducing swap in one step per iteration, proving robust and fast compared to random pairwise swaps.\n- **Deterministic ABBA Redistribution:** The pattern of refining solutions by redistributing items from the heaviest and lightest packs using a deterministic `A-B-B-A` modulo mask (seen in the **Current Best Program**) remains a stable and high-speed method for variance reduction. It efficiently flattens load distribution without the fragility of complex bin-packing logic.\n- **Hybrid Heuristic Initialization:** The use of mixed initialization strategies\u2014specifically combining **Randomized LPT**, **Interleaved Sorting**, and **Pure Random** (as seen in **Generation 90**, **Generation 94**, and the **Current Best Program**)\u2014ensures that the solver is not trapped by the pathologies of a single sorting order (like standard LPT) given the heavy-tailed item distribution.\n\n## Ineffective Approaches\n- **Complex Custom Redistribution Logic:** **Generation 92** (\"Pairwise LPT Refinement\") and **Generation 95** (\"Snake Rebalancing\") failed validation (Score 0.0) due to indexing errors. Attempts to implement complex scatter-gather logic for multi-pack redistribution or snake-like ordering often introduce subtle bugs in tensor indexing that lead to invalid expert assignments.\n- **Purely Randomized Diffusion:** **Generation 97** relied on random pairwise diffusion for refinement. While fast (Speed 1.0), it failed to break the 0.31 balancedness ceiling, demonstrating that blind randomization is less effective than targeted greedy swaps (Max-Any) or deterministic redistribution (ABBA) for this problem.\n- **Snake and ZigZag Sorting:** While **Generation 90** (ZigZag) and **Generation 99** (Snake Init) passed validation, they achieved the exact same balancedness score (0.31) as simpler LPT approaches. This confirms that alternative sorting orders alone cannot overcome the \"large item\" bottleneck inherent in the workload distribution.\n\n## Implementation Insights\n- **Mask-Based Branchless Execution:** The **Current Best Program** maximizes throughput by using boolean masks (e.g., `mask_b = (arange_2k % 4 == 1) | ...` for ABBA, `mask_interleave` for sorting) instead of Python control flow. This allows the GPU to execute refinement kernels fully in parallel across all 256 candidates/replicas.\n- **Monotonic \"Elitism\" in Mutation:** A critical detail in the **Current Best Program** is the explicit preservation of the best candidate during the mutation phase (`mask_mutate` ensures index 0 is skipped). This guarantees that the \"Zoom-In\" phase is monotonic\u2014the solution quality can never degrade below the initial greedy baseline.\n- **Tensor Broadcasting for Exhaustive Search:** The implementation of the Max-Any swap in the **Current Best Program** avoids explicit loops over target packs. Instead, it computes `diffs` using broadcasting (`w_max.unsqueeze(3) - curr_w.unsqueeze(2)`), allowing the logic to simultaneously evaluate every possible swap for the bottleneck pack across all candidates.\n\n## Performance Analysis\n- **The 0.31 Balancedness Ceiling:** A striking trend is that every valid program from **Generation 90** to **Generation 99** (including the **Current Best Program**) achieved a balancedness score of exactly ~0.31. This strongly implies that the test workload contains specific \"jumbo\" items that act as a hard lower bound on the maximum load, rendering further optimization mathematically impossible regardless of the refinement technique.\n- **Speed Saturation:** All programs achieved a perfect speed score of 1.0. This indicates that the current fully vectorized GPU implementations are extremely efficient, and the performance bottleneck is entirely determined by solution quality (balancedness) rather than runtime overhead.\n- **Robustness vs. Complexity Trade-off:** Simple, vectorized kernels like Pairwise Swap and ABBA (used in **Gen 91**, **Gen 94**, **Gen 96**) consistently pass validation with high speed. In contrast, complex custom logic (Snake, Hierarchical Pools) significantly increases the risk of correctness failures (Score 0.0) without yielding any improvement in the balancedness score.",
  "meta_recommendations": "Based on the analysis of the **Current Best Program (Generation 68)** and the **Global Insights**, here are 5 actionable recommendations for future mutations. These recommendations aim to overcome the \"0.31 balancedness plateau\" by diversifying the ensemble's initialization and deepening the local search capabilities without sacrificing the perfect speed score.\n\n1.  **Deterministic \"Big Rocks\" Round-Robin Initialization**\n    To explicitly break the \"0.31 ceiling\" likely caused by large item collisions, introduce a specialized initialization strategy for a subset of candidates (e.g., 32 of 256). For these candidates, force the **$N$ heaviest items** to be assigned to the $N$ distinct packs in a deterministic Round-Robin fashion (indices 0 to $N-1$) *before* the greedy filling process begins. This guarantees that the most difficult constraints are orthogonalized, preventing the greedy heuristic from accidentally clumping them.\n\n2.  **Expanded Top-4 Fixed-Pattern Refinement**\n    Extend the effective \"ABBA\" refinement step\u2014currently applied only to the single heaviest and single lightest packs\u2014to operate on the **Top-2 Heaviest** and **Top-2 Lightest** packs simultaneously. Pool items from these 4 packs and redistribute them using a precomputed, fixed `A-B-C-D-D-C-B-A` mask pattern. This allows the algorithm to resolve \"deadlocks\" where the absolute lightest pack is too full to accept a large item, but the \"second lightest\" pack has the necessary slack.\n\n3.  **L2-Norm Tie-Breaking for Candidate Selection**\n    In the \"Zoom-In\" phase, when selecting the best candidate to replicate, use the **L2 norm (sum of squared loads)** or load variance as a secondary sorting key. Since many candidates currently achieve the exact same maximum load (leading to the 0.31 plateau), favoring the candidate with the \"smoothest\" overall distribution selects a configuration that is less \"jammed\" and more amenable to subsequent swap mutations.\n\n4.  **Plateau-Surfing with Zero-Gain Swaps**\n    Modify the **Max-Any Swap** logic to accept swaps that result in **zero improvement** (`improvement >= 0` instead of `> 0`) with a certain probability or mask. Allowing these \"sideways moves\" on the fitness landscape helps reconfigure the items in the bottleneck pack without immediately reducing the max load, potentially unlocking a state where a beneficial swap becomes possible in the next iteration.\n\n5.  **Randomized Pairwise ABBA Mutation**\n    Replace or augment the simple \"random item swap\" mutation in the Zoom-In phase with a **Random Pairwise ABBA Rebalance**. Select two *random* packs (not necessarily Max/Min) from each replicated candidate, pool their items, and redistribute them using the ABBA pattern. This acts as a \"mini-greedy\" repair that locally optimizes random regions of the solution, providing much higher quality perturbations than blind single-item swaps.",
  "meta_recommendations_history": [
    "Based on the detailed analysis of the current",
    "Based on the analysis of the **Current Best Program (Generation 18)** and the consistent performance ceiling observed across generations, here are 5 actionable recommendations for future program mutations:\n\n1.  **Massive Parallel Candidate Expansion**: Capitalize on the perfect speed score (1.0) by significantly increasing `num_candidates` (e.g., from 8 to 64 or 128). Implement a **noise spectrum** across these candidates\u2014varying the randomization factor from 0.0 (pure LPT) to 0.5 (high randomness)\u2014to ensure the algorithm simultaneously explores the immediate greedy neighborhood and distant, diverse configurations.\n2.  **Vectorized Two-Item Block Swaps**: To break the 0.31 balancedness ceiling, extend the local search to evaluate swapping **pairs of items** between the heaviest and lightest packs. Since single-item swaps are getting stuck in local optima, calculating the benefit of `2-vs-2` exchanges (broadcasting over item pairs) can resolve imbalances where moving a single item is forbidden by the objective function but moving a group is beneficial.\n3.  **Sub-Problem Re-Optimization (Large Neighborhood Search)**: Implement a strategy that periodically identifies the $K$ heaviest and $K$ lightest packs, pools their items, and re-distributes them from scratch (e.g., using a fresh LPT pass or exhaustive search on this small subset). This \"destroy and repair\" approach disrupts the local structure more effectively than pairwise swaps and specifically targets the outliers defining the maximum load.\n4.  **Simulated Annealing on GPU**: Modify the swap acceptance criteria to allow \"uphill\" moves (moves that slightly increase the L2 cost) with a probability dependent on a decaying temperature. Using the existing fast vectorized cost calculation, this meta-heuristic refinement can help the algorithm navigate out of the wide, flat basins of attraction characterizing the 0.31 score plateau.\n5.  **Ensemble of Sorting Heuristics**: Instead of using Randomized LPT for all parallel candidates, diversify the initialization logic by assigning different sorting strategies to different batch indices. For example, have 50% of candidates use **Descending Sort (LPT)**, 25% use **Ascending Sort (SPT)**, and 25% use **Interleaved/Folded Sort** (pairing heaviest with lightest). This structural diversity prevents the entire batch from converging to the same greedy bias.",
    "Based on the analysis of the **Current Best Program (Generation 18)** and the global insights, here are 5 actionable recommendations for future program mutations:\n\n1.  **Massive Parallelism with Mixed Heuristics**: Scale `num_candidates` to 128 (leveraging the perfect 1.0 Speed Score) but diversify the initialization logic beyond simple LPT. Dedicate partitions of the candidate batch to different strategies: 50% **Randomized LPT**, 25% **Folded Sort** (interleaving heaviest and lightest items), and 25% **Random Shuffling**. This ensures the search explores structurally distinct regions of the solution space that LPT-based methods (even with noise) systematically miss.\n2.  **Vectorized Multi-Item Swaps (2-vs-1 and 2-vs-2)**: Extend the swap kernel to evaluate moving **two items** from the heaviest pack and exchanging them with one or two items from the lightest pack. By broadcasting the sum of item pairs (`max_items[i] + max_items[j]`), the algorithm can resolve imbalances where a single large item cannot be moved without violating the improvement condition, but replacing it with two smaller items (or vice-versa) yields a better balance.\n3.  **Simulated Annealing with Decay**: Introduce a \"Temperature\" parameter to the swap acceptance criteria, allowing moves that slightly increase the L2 cost (`change < temperature`) in early iterations. Decay the temperature to zero over the course of the local search loop. This meta-heuristic modification utilizes the available compute budget to help the algorithm climb out of the 0.31 local optimum basin before converging.\n4.  **Targeted \"Destroy and Repair\" (Large Neighborhood Search)**: Implement a refinement step that periodically identifies the $K$ heaviest and $K$ lightest packs, removes all their assigned items, and redistributes them into the empty slots using a different greedy rule (e.g., Best-Fit Decreasing). This strategy disrupts the local packing structure more aggressively than pairwise swaps, specifically targeting the outliers that define the maximum load.\n5.  **Randomized Bin Capacity Offsets**: During the greedy initialization phase, initialize the `pack_weights` tensor with random noise (e.g., `torch.randn(...) * scale`) instead of zeros. This creates \"virtual\" capacity constraints that force the greedy algorithm to avoid placing items in certain bins early in the process, generating diverse valid packings that are orthogonal to those produced by simply shuffling item weights.",
    "Based on the global insights and the performance of the **Current Best Program**, here are 5 actionable recommendations for future program mutations:\n\n1.  **Massive Parallelism with Hybrid Initialization**: Scale `num_candidates` from 8 to **128**, utilizing the perfect Speed Score (1.0) to drastically widen the search. Partition this larger batch to use **diverse initialization strategies**: assign 64 candidates to the current **Randomized LPT**, 32 to **Random Shuffling** (pure random order), and 32 to **Folded LPT** (alternating selection from the start and end of the sorted list). This ensures the local search starts from structurally distinct basins of attraction, unlike the current homogeneous LPT approach.\n\n2.  **Vectorized Multi-Item Swap Kernel (2-to-1 and 2-to-2)**: Extend the local search to evaluate swapping **pairs",
    "Based on the global insights and the analysis of the current best program (Generation 49), here are 5 actionable recommendations for future program mutations. These recommendations prioritize leveraging the system's proven capacity for massive parallelism and vectorized evaluation to break the current balancedness plateau (0.31).\n\n1.  **Two-Stage Evolutionary Resampling**: Extend the \"Generate-Filter-Refine\" pipeline into a \"Generate-Select-Mutate-Refine\" loop. Use the first pass of 128 candidates to identify the single best permutation, then immediately generate a **new population of 128 candidates** derived solely from perturbing that best permutation (using varying noise levels). This concentrates the massive parallel search power on \"zooming in\" on the most promising basin of attraction found in the initial coarse search, rather than spending resources on 127 clearly inferior starting points.\n\n2.  **Interleaved \"Heavy-Light\" Initialization**: Implement a deterministic \"Interleaved\" sorting strategy that alternates items from the extremes of the sorted list (e.g., `[heaviest, lightest, 2nd_heaviest, 2nd_lightest...]`). Unlike LPT (which packs all heavy items first) or Folded (which alternates blocks), this fine-grained interleaving forces the greedy packer to pair large items with complementary small items immediately, preventing the accumulation of difficult-to-pack \"dust\" (tiny items) at the end of the packing process.\n\n3.  **Vectorized \"Destruct-Reconstruct\" Refinement**: Augment the local search with a vectorized Large Neighborhood Search (LNS) move applied to the Top-K candidates. Specifically, identify the heaviest pack for each candidate, effectively \"empty\" it by setting its load to zero and its items to \"unassigned\", and then greedily re-insert those specific items into the remaining packs. This non-local move allows the algorithm to completely dismantle a problematic heavy pack that cannot be fixed by simple 1-item swaps.\n\n4.  **Soft-Greedy Construction Kernel**: Modify the `_vectorized_greedy_packing` kernel to introduce stochasticity directly into the bin selection. Instead of deterministically assigning an item to the absolute minimum-load pack, use `topk(3)` to identify the three least-loaded packs and select one randomly (weighted by their remaining capacity). This ensures that even identical input permutations can result in structurally different packings, maximizing the diversity of the 128-candidate ensemble.\n\n5.  **Relaxed Target Refinement (Max-to-Any)**: Broaden the local search neighborhood by attempting swaps between the heaviest pack and **randomly selected packs** (rather than just the lightest pack). The current strict \"Max-vs-Min\" swap often fails because the lightest pack is too small or full to accept large items from the heaviest pack; swapping with a \"medium\" load pack provides more geometric opportunities to reduce the peak load (the primary objective) without being bottlenecked by the constraints of the absolute lightest pack.",
    "Based on the analysis of the **Current Best Program** (Generation 49) and the **Global Insights**, here are 5 actionable recommendations for future mutations. These recommendations focus on breaking the \"0.31 balancedness wall\" by diversifying the construction process and expanding the local search neighborhood, while leveraging the system's proven capacity for massive parallelism.\n\n1.  **Stochastic \"Soft-Greedy\" Construction Kernel**: Modify the `_vectorized_greedy_packing` kernel to break the determinism of the \"Least Loaded\" heuristic. Instead of always selecting the pack with the absolute minimum load (`argmin`), introduce a \"Soft-Greedy\" approach where the algorithm selects randomly among the **Top-3 least loaded packs** (weighted by their capacity). This will allow the 128 parallel candidates to explore structurally different packing configurations even from similar input permutations, preventing the ensemble from collapsing into identical local optima.\n\n2.  **Vectorized \"Destruct-Reconstruct\" Refinement (LNS)**: Implement a vectorized Large Neighborhood Search (LNS) step on the filtered Top-K candidates. Specifically, identify the **heaviest pack** and the **lightest pack** for each candidate, effectively \"empty\" them by resetting their loads to zero and their items to \"unassigned\", and then re-insert those specific items into the existing partial packing using the greedy logic. This non-local move allows the algorithm to dismantle a problematic heavy pack that cannot be fixed by simple 1-item swaps, leveraging the GPU's speed to perform this expensive operation on the elite subset.\n\n3.  **Relaxed \"Max-to-Any\" Refinement Strategy**: Broaden the local search criteria in `_refine_packing` by attempting swaps between the heaviest pack and **randomly selected packs** (or the \"second lightest\"), rather than strictly the absolute lightest pack. The current \"Max-vs-Min\" constraint often fails because the lightest pack may lack the capacity to accept a large item; allowing swaps with \"medium\" loaded packs creates more geometric opportunities to reduce the global maximum load (the primary objective) without being bottlenecked by the minimum load constraint.\n\n4.  **Two-Stage \"Zoom-In\" Search Pipeline**: Extend the pipeline to use the first pass of 128 candidates as a \"coarse search\" to identify the single best permutation, and then immediately trigger a **second generation of 128 candidates** derived solely from perturbing that winner (e.g., applying varying degrees of noise to the winner's weights). This concentrates the massive parallel compute power on exploiting the specific basin of attraction found in the first stage, rather than refining 127 inferior candidates that are likely to be discarded.\n\n5.  **Interleaved \"Heavy-Light\" Initialization**: Add a deterministic \"Interleaved\" sorting strategy to the candidate pool (alongside LPT and Random). Construct this by alternating items from the extremes of the sorted list (e.g., `[heaviest, lightest, 2nd_heaviest, 2nd_lightest...]`). Unlike LPT (which packs all heavy items first) or standard Random, this forces the greedy packer to immediately pair large items with complementary small items, preventing the accumulation of \"dust\" (tiny items) at the end of the packing process which often limits the efficacy of later refinement steps.",
    "Based on the analysis of the **Current Best Program** (Generation 68) and the **Global Insights**, here are 5 actionable recommendations for future mutations. These focus on breaking the \"0.31 balancedness plateau\" by diversifying the construction logic and deepening the local search capabilities.\n\n1.  **Stochastic \"Soft-Greedy\" Construction Kernel**:\n    Modify the `_vectorized_greedy_packing` loop to break the rigid determinism of the \"Least Loaded\" heuristic. Instead of selecting the pack strictly via `costs.argmin()`, introduce a randomized tie-breaking mechanism by adding small noise to the `costs` tensor before selection (e.g., `(costs + torch.rand_like(costs) * noise_level).argmin()`). This ensures that the 256 parallel candidates explore structurally distinct topologies even when input sorting permutations are similar, preventing the ensemble from collapsing into identical local optima.\n\n2.  **Multi-Pack Vectorized Destruct-Reconstruct (Top-4)**:\n    Expand the \"ABBA\" refinement strategy from just the Max and Min packs to the **Top-2 Heaviest** and **Top-2 Lightest** packs simultaneously (4 packs total). By pooling items from 4 packs, sorting them, and redistributing using a precomputed `A-B-C-D-D-C-B-A` pattern, the algorithm can resolve complex bottlenecks where a necessary swap is blocked because the receiving pack (the single lightest) is too small to accept a large item, but a \"medium-light\" pack could.\n\n3.  **Variance-Based Descent (L2 Norm Objective)**:\n    In the `_refine_packing` or swap phase, broaden the acceptance criteria to include moves that minimize the **Sum of Squared Loads (L2 Norm)**, even if they do not immediately reduce the Maximum Load. The Max-Load landscape is often a flat plateau; minimizing variance provides a smooth gradient that guides the configuration towards a more balanced state, enabling the algorithm to escape the \"0.31\" local optimum that the strict Min-Max greedy search is stuck in.\n\n4.  **Two-Stage \"Zoom-In\" Search Pipeline**:\n    Split the compute budget into a \"Coarse Search\" and a \"Fine Search\". First, run the existing 256 candidates to find the best packing. Then, immediately generate a **second generation of 256 candidates** by replicating that single best winner and applying random $N$-item swaps (mutations) to each replica. This concentrates the massive parallel compute power on exploiting the specific basin of attraction found by the best candidate, rather than refining 255 inferior candidates that will be discarded.\n\n5.  **\"Big Rocks\" Forced Separation Strategy**:\n    Introduce a new initialization strategy for a subset of candidates (e.g., indices 192-255) that explicitly assigns the **$M$ heaviest items** (where $M=$ `num_packs`) to the $M$ distinct packs *before* starting the greedy process. By hard-coding the separation of the largest items, this strategy prevents the greedy heuristic from accidentally clumping heavy items into the same bin early in the process\u2014a common cause of the imbalance that is difficult to fix with later local swaps.",
    "Based on the analysis of the **Current Best Program (Generation 68)** and the **Global Insights**, here are 5 actionable recommendations for future mutations. These recommendations aim to overcome the \"0.31 balancedness plateau\" by diversifying the ensemble's initialization and deepening the local search capabilities.\n\n1.  **Multi-Pack Vectorized Destruct-Reconstruct (Top-4 Pooling)**\n    Extend the effective \"ABBA\" refinement step\u2014currently applied only to the single heaviest and single lightest packs\u2014to operate on the **Top-2 Heaviest** and **Top-2 Lightest** packs simultaneously. By pooling items from 4 packs and redistributing them using a precomputed `A-B-C-D-D-C-B-A` pattern (or similar balanced mask), the algorithm can resolve \"deadlocks\" where the absolute lightest pack is too full to accept a large item from the heaviest pack, but a \"medium-light\" pack has the necessary slack.\n\n2.  **Forced \"Big Rocks\" Separation Initialization**\n    Introduce a specialized initialization strategy for a subset of the 256 candidates (e.g., indices 192\u2013255) that explicitly assigns the **$M$ heaviest items** to the $M$ distinct packs *before* the greedy filling process begins. This deterministic \"hard-coding\" prevents the greedy heuristic from accidentally clumping the largest constraints (the \"big rocks\") into the same bin early on\u2014a common failure mode in heavy-tailed distributions that is difficult to correct via local swaps later.\n\n3.  **Two-Stage \"Zoom-In\" Evolutionary Search**\n    Restructure the search budget into two phases: first, run the existing 256 diverse candidates and identify the single best configuration; second, immediately generate a **new population of 256 candidates** by replicating that winner and applying random $N$-item swap mutations (perturbations) to each replica. This focuses the massive parallel compute power on thoroughly exploring the \"basin of attraction\" of the most promising solution, rather than wasting cycles refining 255 inferior candidates.\n\n4.  **Plateau-Surfing with Zero-Gain Swaps**\n    Modify the **",
    "Based on the analysis of the **Current Best Program (Generation 68)** and the **Global Insights**, here are 5 actionable recommendations for future mutations. These recommendations aim to overcome the \"0.31 balancedness plateau\" by diversifying the ensemble's initialization and deepening the local search capabilities.\n\n1.  **Deterministic \"Big Rocks\" Round-Robin Initialization**\n    To explicitly break the \"0.31 ceiling\" caused by large item collisions, introduce a specialized initialization strategy for a subset of candidates (e.g., 32 of 256). For these candidates, force the **$N$ heaviest items** to be assigned to the $N$ distinct packs in a deterministic Round-Robin fashion (0 to $N-1$) *before* the greedy filling process begins. This guarantees that the most difficult constraints are orthogonalized, preventing the greedy heuristic from accidentally clumping them.\n\n2.  **Expanded Top-4 Fixed-Pattern Refinement**\n    Extend the effective \"ABBA\" refinement step\u2014currently applied only to the single heaviest and single lightest packs\u2014to operate on the **Top-2 Heaviest** and **Top-2 Lightest** packs simultaneously. Pool items from these 4 packs and redistribute them using a precomputed, fixed `A-B-C-D-D-C-B-A` mask pattern. This allows the algorithm to resolve \"deadlocks\" where the absolute lightest pack is too full to accept a large item, but the \"second lightest\" pack has the necessary slack.\n\n3.  **L2-Norm Tie-Breaking for Candidate Selection**\n    In the \"Zoom-In\" phase, when selecting the best candidate to replicate, use the **L2 norm (sum of squared loads)** or load variance as a secondary sorting key. Since many candidates currently achieve the exact same maximum load (leading to the 0.31 plateau), favoring the candidate with the \"smoothest\" overall distribution can select a configuration that is less \"jammed\" and more amenable to subsequent swap mutations.\n\n4.  **Plateau-Surfing with Zero-Gain Swaps**\n    Modify the **Max-Any Swap** logic to accept swaps that result in **zero improvement** (`improvement >= 0` instead of `> 0`) with a certain probability or mask. Allowing these \"sideways moves\" on the fitness landscape helps reconfigure the items in the bottleneck pack without immediately reducing the max load, potentially unlocking a state where a beneficial swap becomes possible in the next iteration.\n\n5.  **Randomized Pairwise ABBA Mutation**\n    Replace or augment the simple \"random item swap\" mutation in the Zoom-In phase with a **Random Pairwise ABBA Rebalance**. Select two *random* packs (not necessarily Max/Min) from each replicated candidate, pool their items, and redistribute them using the ABBA pattern. This acts as a \"mini-greedy\" repair that locally optimizes random regions of the solution, providing much higher quality perturbations than blind single-item swaps.",
    "Based on the analysis of the **Current Best Program (Generation 68)** and the **Global Insights**, here are 5 actionable recommendations for future mutations. These recommendations aim to overcome the \"0.31 balancedness plateau\" by diversifying the ensemble's initialization and deepening the local search capabilities without sacrificing the perfect speed score.\n\n1.  **Deterministic \"Big Rocks\" Round-Robin Initialization**\n    To explicitly break the \"0.31 ceiling\" likely caused by large item collisions, introduce a specialized initialization strategy for a subset of candidates (e.g., 32 of 256). For these candidates, force the **$N$ heaviest items** to be assigned to the $N$ distinct packs in a deterministic Round-Robin fashion (indices 0 to $N-1$) *before* the greedy filling process begins. This guarantees that the most difficult constraints are orthogonalized, preventing the greedy heuristic from accidentally clumping them.\n\n2.  **Expanded Top-4 Fixed-Pattern Refinement**\n    Extend the effective \"ABBA\" refinement step\u2014currently applied only to the single heaviest and single lightest packs\u2014to operate on the **Top-2 Heaviest** and **Top-2 Lightest** packs simultaneously. Pool items from these 4 packs and redistribute them using a precomputed, fixed `A-B-C-D-D-C-B-A` mask pattern. This allows the algorithm to resolve \"deadlocks\" where the absolute lightest pack is too full to accept a large item, but the \"second lightest\" pack has the necessary slack.\n\n3.  **L2-Norm Tie-Breaking for Candidate Selection**\n    In the \"Zoom-In\" phase, when selecting the best candidate to replicate, use the **L2 norm (sum of squared loads)** or load variance as a secondary sorting key. Since many candidates currently achieve the exact same maximum load (leading to the 0.31 plateau), favoring the candidate with the \"smoothest\" overall distribution selects a configuration that is less \"jammed\" and more amenable to subsequent swap mutations.\n\n4.  **Plateau-Surfing with Zero-Gain Swaps**\n    Modify the **Max-Any Swap** logic to accept swaps that result in **zero improvement** (`improvement >= 0` instead of `> 0`) with a certain probability or mask. Allowing these \"sideways moves\" on the fitness landscape helps reconfigure the items in the bottleneck pack without immediately reducing the max load, potentially unlocking a state where a beneficial swap becomes possible in the next iteration.\n\n5.  **Randomized Pairwise ABBA Mutation**\n    Replace or augment the simple \"random item swap\" mutation in the Zoom-In phase with a **Random Pairwise ABBA Rebalance**. Select two *random* packs (not necessarily Max/Min) from each replicated candidate, pool their items, and redistribute them using the ABBA pattern. This acts as a \"mini-greedy\" repair that locally optimizes random regions of the solution, providing much higher quality perturbations than blind single-item swaps."
  ],
  "total_programs_meta_processed": 100
}