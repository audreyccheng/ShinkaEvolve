<NAME>
parallel_ensemble_greedy_packing
</NAME>

<DESCRIPTION>
I propose to replace the single-candidate ZigZag initialization with a Massive Parallel Randomized Greedy LPT strategy.
1. Use 64 parallel candidates.
2. Apply a noise spectrum (from 0.0 to 0.2) to the weights before sorting, creating diverse item orderings.
3. Perform a Vectorized Greedy LPT assignment for all candidates simultaneously. This assigns items to the lightest available pack, respecting capacity constraints.
4. Follow up with a Vectorized Swap refinement that minimizes the maximum load (MinMax objective).
5. Select the best candidate that minimizes the max load.
6. Run the entire computation on GPU (if available) by removing the forced CPU transfer in `rebalance_experts` and adding a device check. This leverages the 1.0 speed score headroom to improve balancedness (currently 0.31).
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses a Vectorized ZigZag initialization followed by a GPU-accelerated
    Swap-based local search refinement.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64, device=device).expand(num_layers, num_groups)
        rank_in_pack = torch.zeros_like(pack_index)
        return pack_index, rank_in_pack

    # 1. Sort weights descending [L, N]
    sorted_weight, sorted_indices = weight.sort(dim=-1, descending=True)

    # 2. Zigzag initialization
    # Pattern: 0, 1, ..., m-1, m-1, ..., 0
    pattern = torch.cat([
        torch.arange(num_packs, dtype=torch.int64, device=device),
        torch.arange(num_packs - 1, -1, -1, dtype=torch.int64, device=device)
    ])
    num_patterns = (num_groups + len(pattern) - 1) // len(pattern)
    pack_assignments = pattern.repeat(num_patterns)[:num_groups]

    # sorted_pack_index: [L, N]
    sorted_pack_index = pack_assignments.view(1, -1).expand(num_layers, -1).clone()

    # 3. Vectorized Swap Optimization
    num_iters = 20

    for _ in range(num_iters):
        # Compute pack sums [L, M]
        pack_sums = torch.zeros(num_layers, num_packs, device=device, dtype=weight.dtype)
        pack_sums.scatter_add_(1, sorted_pack_index, sorted_weight)

        # Find max and min packs
        max_val, max_pack = pack_sums.max(dim=1) # [L]
        min_val, min_pack = pack_sums.min(dim=1) # [L]

        # Identify items in max/min packs
        # sorted_pack_index is [L, N]. max_pack is [L].
        is_max = (sorted_pack_index == max_pack.unsqueeze(1))
        is_min = (sorted_pack_index == min_pack.unsqueeze(1))

        # Extract indices. Each row has exactly groups_per_pack items.
        # indices_* shape: [L, K]
        indices_max = torch.nonzero(is_max, as_tuple=True)[1].view(num_layers, groups_per_pack)
        indices_min = torch.nonzero(is_min, as_tuple=True)[1].view(num_layers, groups_per_pack)

        # Gather weights: [L, K]
        w_max = torch.gather(sorted_weight, 1, indices_max)
        w_min = torch.gather(sorted_weight, 1, indices_min)

        # Compute pairwise diffs: [L, K, K] (max_item - min_item)
        diffs = w_max.unsqueeze(2) - w_min.unsqueeze(1)

        # Target diff to equalize sums: (Max - Min) / 2
        current_diff = (max_val - min_val).view(num_layers, 1, 1)
        target = current_diff / 2.0

        # Validity mask:
        # 1. diff > 0 (must reduce max)
        # 2. diff < current_diff (must not overshoot and make min > old max)
        valid = (diffs > 0) & (diffs < current_diff)

        # Error from target
        error = (diffs - target).abs()
        # Set invalid to inf
        error = torch.where(valid, error, torch.tensor(float('inf'), device=device))

        # Find best swap
        # Flatten [L, K*K]
        min_err, flat_idx = error.view(num_layers, -1).min(dim=1)

        # Check if valid swap exists
        found = min_err < float('inf')
        if not found.any():
            break

        # Decode indices
        row_idx = flat_idx // groups_per_pack
        col_idx = flat_idx % groups_per_pack

        # Apply swaps for valid layers
        l_idx = torch.nonzero(found).squeeze(1)

        # Get item indices in sorted_pack_index
        idx_max = indices_max[l_idx, row_idx[l_idx]]
        idx_min = indices_min[l_idx, col_idx[l_idx]]

        # Swap packs
        p_max = max_pack[l_idx]
        p_min = min_pack[l_idx]

        sorted_pack_index[l_idx, idx_max] = p_min
        sorted_pack_index[l_idx, idx_min] = p_max

    # 4. Construct outputs
    # pack_index: map back to original indices
    pack_index = torch.empty_like(sorted_pack_index)
    pack_index.scatter_(1, sorted_indices, sorted_pack_index)

    # rank_in_pack: ensure 0..k-1 per pack
    # Sort sorted_pack_index to group items by pack
    # Stable sort preserves relative order (weight order)
    _, sort_idx = sorted_pack_index.sort(dim=1, stable=True)

    # Assign ranks 0..k-1 repeated
    ranks = torch.arange(groups_per_pack, device=device).repeat(num_packs)
    ranks = ranks.view(1, -1).expand(num_layers, -1)

    # Map ranks back to sorted items
    temp_ranks = torch.empty_like(ranks)
    temp_ranks.scatter_(1, sort_idx, ranks)

    # Map back to original items
    rank_in_pack = torch.empty_like(pack_index)
    rank_in_pack.scatter_(1, sorted_indices, temp_ranks)

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses a Massive Parallel Randomized Greedy LPT initialization followed by
    a Vectorized Swap-based local search refinement.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64, device=device).expand(num_layers, num_groups)
        rank_in_pack = torch.zeros_like(pack_index)
        return pack_index, rank_in_pack

    # Massive Parallelism
    num_candidates = 64
    num_problems = num_layers * num_candidates

    # Expand weights [L*C, N]
    w_expanded = weight.repeat_interleave(num_candidates, dim=0)

    # Noise Injection for Diversity (0.0 to 0.2 range)
    # Candidate 0 of each layer has 0 noise (Pure LPT)
    scales = torch.linspace(0, 0.2, num_candidates, device=device)
    noise_scale = scales.repeat(num_layers).unsqueeze(1)

    noise = torch.rand_like(w_expanded) * w_expanded * noise_scale
    # Enforce pure LPT for the first candidate
    noise.view(num_layers, num_candidates, num_groups)[:, 0, :] = 0

    sort_keys = w_expanded + noise
    _, sorted_indices = sort_keys.sort(dim=-1, descending=True)
    sorted_weight = torch.gather(w_expanded, 1, sorted_indices)

    # --- Phase 1: Parallel Greedy LPT ---
    pack_weights = torch.zeros(num_problems, num_packs, device=device, dtype=weight.dtype)
    pack_counts = torch.zeros(num_problems, num_packs, device=device, dtype=torch.int64)
    sorted_pack_index = torch.zeros_like(sorted_indices)

    # Iterate over items (columns)
    # Vectorized across all batch problems
    for i in range(num_groups):
        w_item = sorted_weight[:, i:i+1] # [LC, 1]

        # Mask full packs
        # We add infinity to weights of full packs so argmin skips them
        is_full = (pack_counts >= groups_per_pack)
        candidates = pack_weights.clone()
        candidates[is_full] = float('inf')

        # Choose pack with min weight
        chosen_pack = candidates.argmin(dim=1, keepdim=True) # [LC, 1]

        # Assign
        sorted_pack_index[:, i:i+1] = chosen_pack

        # Update
        pack_weights.scatter_add_(1, chosen_pack, w_item)
        pack_counts.scatter_add_(1, chosen_pack, torch.ones_like(chosen_pack))

    # --- Phase 2: Vectorized Swap Refinement ---
    num_iters = 20
    for _ in range(num_iters):
        # Re-calculate max/min packs
        # pack_weights is already current from greedy loop, but to be safe/clean in loop:
        # (Optimized: we could maintain pack_weights incrementally, but re-calc avoids drift)
        pack_weights.fill_(0)
        pack_weights.scatter_add_(1, sorted_pack_index, sorted_weight)

        max_val, max_pack = pack_weights.max(dim=1) # [LC]
        min_val, min_pack = pack_weights.min(dim=1) # [LC]

        # Identify items
        # We want to swap item from max pack with item from min pack.
        # Mask for items in max pack: [LC, N]
        mask_max = (sorted_pack_index == max_pack.unsqueeze(1))
        mask_min = (sorted_pack_index == min_pack.unsqueeze(1))

        # diffs = w_i - w_j
        diffs = sorted_weight.unsqueeze(2) - sorted_weight.unsqueeze(1) # [LC, N, N]

        # Valid mask: i in max, j in min
        valid = mask_max.unsqueeze(2) & mask_min.unsqueeze(1)

        # Goal: Minimize max(new_max, new_min)
        # current max is max_val. current min is min_val.
        # new_max = max_val - (w_i - w_j) = max_val - diff
        # new_min = min_val + (w_i - w_j) = min_val + diff

        mv = max_val.view(-1, 1, 1)
        mnv = min_val.view(-1, 1, 1)

        new_max = mv - diffs
        new_min = mnv + diffs
        new_peak = torch.max(new_max, new_min)
        improvement = mv - new_peak

        # Filter valid
        improvement = torch.where(valid, improvement, torch.tensor(float('-inf'), device=device))

        # Find best
        best_imp_flat, best_idx_flat = improvement.view(num_problems, -1).max(dim=1)

        should_swap = best_imp_flat > 1e-6
        if not should_swap.any():
            break

        # Execute Swaps
        # Decode indices: best_idx_flat is i * N + j
        active_batch = torch.nonzero(should_swap).squeeze(1)
        flat_idx = best_idx_flat[active_batch]

        idx_i = flat_idx // num_groups
        idx_j = flat_idx % num_groups

        p_max_active = max_pack[active_batch]
        p_min_active = min_pack[active_batch]

        # Swap pack assignments
        sorted_pack_index[active_batch, idx_i] = p_min_active
        sorted_pack_index[active_batch, idx_j] = p_max_active

    # --- Phase 3: Select Best Candidate ---
    # Calc max loads
    pack_weights.fill_(0)
    pack_weights.scatter_add_(1, sorted_pack_index, sorted_weight)
    final_max, _ = pack_weights.max(dim=1)

    # Reshape [L, C]
    final_max = final_max.view(num_layers, num_candidates)
    best_cand = final_max.argmin(dim=1)

    # Gather best
    batch_indices = torch.arange(num_layers, device=device) * num_candidates + best_cand

    best_sorted_pack_index = sorted_pack_index[batch_indices] # [L, N]
    best_sorted_indices = sorted_indices[batch_indices]       # [L, N]

    # Map back to original
    pack_index = torch.empty_like(best_sorted_pack_index)
    pack_index.scatter_(1, best_sorted_indices, best_sorted_pack_index)

    # Compute rank_in_pack
    # Sort items by pack ID to group them
    _, pack_sort_idx = best_sorted_pack_index.sort(dim=1, stable=True)

    # Create ranks pattern [0, 1, ... k-1, 0, 1... k-1]
    ranks_pattern = torch.arange(groups_per_pack, device=device).repeat(num_packs).expand(num_layers, -1)

    # Scatter ranks to the position of the items
    sorted_ranks = torch.empty_like(ranks_pattern)
    sorted_ranks.scatter_(1, pack_sort_idx, ranks_pattern)

    # Now sorted_ranks[i] corresponds to item sorted_indices[i]
    rank_in_pack = torch.empty_like(pack_index)
    rank_in_pack.scatter_(1, best_sorted_indices, sorted_ranks)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
<<<<<<< SEARCH
def rebalance_experts(
    weight: torch.Tensor,
    num_replicas: int,
    num_groups: int,
    num_nodes: int,
    num_gpus: int,
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Entry point for expert-parallelism load balancer.

    Parameters:
        weight: [layers, num_logical_experts], the load statistics for all
            logical experts
        num_replicas: number of physical experts, must be a multiple of
            `num_gpus`
        num_groups: number of expert groups
        num_nodes: number of server nodes, where the intra-node network
            (e.g, NVLink) is faster
        num_gpus: number of GPUs, must be a multiple of `num_nodes`

    Returns:
        physical_to_logical_map: [layers, num_replicas], the expert index of
            each replica
        logical_to_physical_map: [layers, num_logical_experts, X], the replica
            indices for each expert
        expert_count: [layers, num_logical_experts], number of physical
            replicas for each logical expert
    """
    num_layers, num_logical_experts = weight.shape
    weight = weight.float().cpu()

    if num_groups % num_nodes == 0:
        # use hierarchical load-balance policy
        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
            weight, num_replicas, num_groups, num_nodes, num_gpus)
    else:
        # use global load-balance policy
        # Treating as if 1 huge group per layer, so packing step 1 is trivial
        # But here logic passes num_groups=1, so group_size=all experts.
        # Step 1 packs 1 item to 1 node? No, step 1 uses num_nodes=1.
        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
            weight, num_replicas, 1, 1, num_gpus)

    num_redundant_experts = num_replicas - num_logical_experts
    maxlogcnt = num_redundant_experts + 1
    log2phy: torch.Tensor = torch.full(
        (num_layers, num_logical_experts, maxlogcnt),
        -1,
        dtype=torch.int64,
        device=logcnt.device,
    )

    # Create the reverse map
    # phy2log * maxlogcnt + phyrank gives a unique index for (expert, replica_id)
    # We scatter the physical index (0..num_replicas) into this location
    log2phy.view(num_layers, -1).scatter_(
        -1,
        phy2log * maxlogcnt + phyrank,
        torch.arange(num_replicas, dtype=torch.int64,
                     device=log2phy.device).expand(num_layers, -1),
    )
    return phy2log, log2phy, logcnt