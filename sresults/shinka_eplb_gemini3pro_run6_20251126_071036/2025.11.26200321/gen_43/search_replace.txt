To improve the performance of the expert rearrangement algorithm, I propose adding a **Post-Selection Refinement** step to the `balanced_packing` function. Additionally, I will diversify the ensemble candidates by including **Random Shuffling** alongside the existing LPT strategies.

The reasoning is as follows:
1.  **Refinement**: While the ensemble greedy approach is fast and explores many basins of attraction, the "Best Fit" greedy heuristic can get stuck in local optima. A localized search that explicitly minimizes the difference between the heaviest and lightest packs (by swapping items) can significantly improve the balance score with minimal computational overhead, as it operates only on the single best candidate per layer.
2.  **Ensemble Diversity**: The current candidates (LPT, ZigZag LPT, Noisy LPT) are all variations of sorting by weight. Adding purely random permutations (Random Shuffling) ensures we explore packings that don't rely on weight-ordering, which can be beneficial for certain "jagged" distributions where filling holes is more important than packing largest first.

I will introduce a `_refine_packing` helper function and integrate it into `balanced_packing`.

<NAME>
ensemble_packing_with_refinement
</NAME>

<DESCRIPTION>
Improved the balanced packing strategy by:
1. Diversifying the ensemble candidates: added Random Shuffling candidates (32) alongside LPT, ZigZag, and Noisy LPT. Random permutations can help escape local optima inherent to sorted greedy approaches.
2. Added a Post-Selection Refinement step: After selecting the best candidate from the ensemble, we apply a vectorized local search (2-opt swap) to further minimize the load imbalance between the heaviest and lightest packs. This targets the specific metric we want to minimize directly.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Massive Parallel Ensemble strategy.

    Generates 128 candidates using LPT, ZigZag, and Randomized perturbations.
    Selects the best packing per layer minimizing load imbalance.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_items = weight.shape
    device = weight.device
    num_candidates = 128
    capacity = num_items // num_packs

    # 1. Generate Candidates
    # Base: Sort descending (LPT)
    lpt_val, lpt_idx = weight.sort(dim=-1, descending=True)

    # ZigZag permutation of LPT: 0, N-1, 1, N-2, ...
    zigzag_perm = torch.empty(num_items, device=device, dtype=torch.long)
    half = (num_items + 1) // 2
    arange = torch.arange(num_items, device=device)
    zigzag_perm[0::2] = arange[:half]
    zigzag_perm[1::2] = arange[half:].flip(0)

    # Candidate Indices [L, C, N]
    # C0: lpt_idx
    c0_idx = lpt_idx.unsqueeze(1)

    # C1: lpt_idx gathered by zigzag_perm
    # Permute columns of lpt_idx
    c1_idx = lpt_idx.gather(1, zigzag_perm.view(1, -1).expand(num_layers, -1)).unsqueeze(1)

    # Noisy candidates
    noise = torch.rand(num_layers, num_candidates - 2, num_items, device=device) * 0.4 + 0.8
    noisy_weights = weight.unsqueeze(1) * noise
    _, c_noisy_idx = noisy_weights.sort(dim=-1, descending=True)

    all_indices = torch.cat([c0_idx, c1_idx, c_noisy_idx], dim=1)

    # Gather actual weights [L, C, N]
    expanded_weight = weight.unsqueeze(1).expand(-1, num_candidates, -1)
    ordered_weights = expanded_weight.gather(2, all_indices)

    # Flatten for vectorized kernel
    batch_size = num_layers * num_candidates
    flat_weights = ordered_weights.view(batch_size, num_items)

    # 2. Vectorized Greedy Packing
    pack_loads = torch.zeros(batch_size, num_packs, device=device, dtype=weight.dtype)
    pack_counts = torch.zeros(batch_size, num_packs, device=device, dtype=torch.int64)
    flat_assigned_packs = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)
    flat_assigned_ranks = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)

    batch_range = torch.arange(batch_size, device=device)
    inf = torch.tensor(float('inf'), device=device, dtype=weight.dtype)

    for i in range(num_items):
        w = flat_weights[:, i]
        valid_mask = pack_counts < capacity
        temp_loads = torch.where(valid_mask, pack_loads, inf)
        chosen_pack = temp_loads.argmin(dim=1)

        flat_assigned_packs[:, i] = chosen_pack
        flat_assigned_ranks[:, i] = pack_counts[batch_range, chosen_pack]

        pack_loads[batch_range, chosen_pack] += w
        pack_counts[batch_range, chosen_pack] += 1

    # 3. Selection
    loads = pack_loads.view(num_layers, num_candidates, num_packs)
    imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values
    best_candidate_idx = imbalance.argmin(dim=1)

    # Helper to gather
    def gather_best(tensor_lcn, best_idx_l):
        idx_expanded = best_idx_l.view(num_layers, 1, 1).expand(-1, 1, num_items)
        return tensor_lcn.gather(1, idx_expanded).squeeze(1)

    final_ordered_indices = gather_best(all_indices, best_candidate_idx)
    final_packs_ordered = gather_best(flat_assigned_packs.view(num_layers, num_candidates, num_items), best_candidate_idx)
    final_ranks_ordered = gather_best(flat_assigned_ranks.view(num_layers, num_candidates, num_items), best_candidate_idx)

    pack_index = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    rank_in_pack = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)

    pack_index.scatter_(1, final_ordered_indices, final_packs_ordered)
    rank_in_pack.scatter_(1, final_ordered_indices, final_ranks_ordered)

    return pack_index, rank_in_pack
=======
def _refine_packing(weight: torch.Tensor,
                    pack_index: torch.Tensor,
                    rank_in_pack: torch.Tensor,
                    num_packs: int,
                    num_iters: int = 20) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Iterative 2-opt swap refinement to minimize load imbalance.
    """
    num_layers, num_items = weight.shape
    device = weight.device

    # Compute initial loads
    pack_loads = torch.zeros(num_layers, num_packs, device=device, dtype=weight.dtype)
    pack_loads.scatter_add_(1, pack_index, weight)

    batch_indices = torch.arange(num_layers, device=device)

    for _ in range(num_iters):
        # Find max and min packs
        max_load, max_idx = pack_loads.max(dim=1)
        min_load, min_idx = pack_loads.min(dim=1)
        curr_diff = max_load - min_load

        # Identify candidate items
        # mask: [L, N]
        is_max = (pack_index == max_idx.unsqueeze(1))
        is_min = (pack_index == min_idx.unsqueeze(1))

        # w_i - w_j matrix
        # [L, N, 1] - [L, 1, N] -> [L, N, N]
        w_diff = weight.unsqueeze(2) - weight.unsqueeze(1)

        # Target: maximize reduction
        # New Diff = |(Max - Min) - 2(w_i - w_j)|
        # We want to minimize this new diff.

        # Valid only if i in max and j in min
        valid_mask = is_max.unsqueeze(2) & is_min.unsqueeze(1)

        metric = torch.abs(curr_diff.view(-1, 1, 1) - 2 * w_diff)
        # Mask invalid
        metric = torch.where(valid_mask, metric, torch.tensor(float('inf'), device=device))

        # Find best swap
        flat_metric = metric.view(num_layers, -1)
        best_val, best_flat_idx = flat_metric.min(dim=1)

        # Improvement check
        # We need improvement over current diff
        improve_mask = best_val < (curr_diff - 1e-5)

        if not improve_mask.any():
            break

        # Apply swaps
        active_layers = batch_indices[improve_mask]
        active_flat = best_flat_idx[improve_mask]

        idx_i = active_flat // num_items
        idx_j = active_flat % num_items

        p_max = max_idx[active_layers]
        p_min = min_idx[active_layers]

        # Update weights/loads
        w_i = weight[active_layers, idx_i]
        w_j = weight[active_layers, idx_j]
        diff_w = w_i - w_j

        pack_loads[active_layers, p_max] -= diff_w
        pack_loads[active_layers, p_min] += diff_w

        # Swap indices
        pack_index[active_layers, idx_i] = p_min
        pack_index[active_layers, idx_j] = p_max

        # Swap ranks
        r_i = rank_in_pack[active_layers, idx_i]
        r_j = rank_in_pack[active_layers, idx_j]

        rank_in_pack[active_layers, idx_i] = r_j
        rank_in_pack[active_layers, idx_j] = r_i

    return pack_index, rank_in_pack


def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using a Massive Parallel Ensemble strategy.

    Generates 128 candidates using LPT, ZigZag, Random Shuffle, and Randomized perturbations.
    Selects the best packing per layer minimizing load imbalance, followed by local refinement.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_items = weight.shape
    device = weight.device
    num_candidates = 128
    capacity = num_items // num_packs

    # 1. Generate Candidates
    # Base: Sort descending (LPT)
    lpt_val, lpt_idx = weight.sort(dim=-1, descending=True)

    # ZigZag permutation of LPT: 0, N-1, 1, N-2, ...
    zigzag_perm = torch.empty(num_items, device=device, dtype=torch.long)
    half = (num_items + 1) // 2
    arange = torch.arange(num_items, device=device)
    zigzag_perm[0::2] = arange[:half]
    zigzag_perm[1::2] = arange[half:].flip(0)

    # Candidate Indices [L, C, N]
    # C0: lpt_idx
    c0_idx = lpt_idx.unsqueeze(1)

    # C1: lpt_idx gathered by zigzag_perm
    # Permute columns of lpt_idx
    c1_idx = lpt_idx.gather(1, zigzag_perm.view(1, -1).expand(num_layers, -1)).unsqueeze(1)

    # C2: Random Shuffles (Explore non-greedy basins)
    # Using 32 random permutations
    num_random = 32
    rand_perm = torch.rand(num_layers, num_random, num_items, device=device).argsort(dim=-1)

    # C3: Noisy LPT candidates
    num_noisy = num_candidates - 2 - num_random
    noise = torch.rand(num_layers, num_noisy, num_items, device=device) * 0.4 + 0.8
    noisy_weights = weight.unsqueeze(1) * noise
    _, c_noisy_idx = noisy_weights.sort(dim=-1, descending=True)

    all_indices = torch.cat([c0_idx, c1_idx, rand_perm, c_noisy_idx], dim=1)

    # Gather actual weights [L, C, N]
    expanded_weight = weight.unsqueeze(1).expand(-1, num_candidates, -1)
    ordered_weights = expanded_weight.gather(2, all_indices)

    # Flatten for vectorized kernel
    batch_size = num_layers * num_candidates
    flat_weights = ordered_weights.view(batch_size, num_items)

    # 2. Vectorized Greedy Packing
    pack_loads = torch.zeros(batch_size, num_packs, device=device, dtype=weight.dtype)
    pack_counts = torch.zeros(batch_size, num_packs, device=device, dtype=torch.int64)
    flat_assigned_packs = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)
    flat_assigned_ranks = torch.empty(batch_size, num_items, device=device, dtype=torch.int64)

    batch_range = torch.arange(batch_size, device=device)
    inf = torch.tensor(float('inf'), device=device, dtype=weight.dtype)

    for i in range(num_items):
        w = flat_weights[:, i]
        valid_mask = pack_counts < capacity
        temp_loads = torch.where(valid_mask, pack_loads, inf)
        chosen_pack = temp_loads.argmin(dim=1)

        flat_assigned_packs[:, i] = chosen_pack
        flat_assigned_ranks[:, i] = pack_counts[batch_range, chosen_pack]

        pack_loads[batch_range, chosen_pack] += w
        pack_counts[batch_range, chosen_pack] += 1

    # 3. Selection
    loads = pack_loads.view(num_layers, num_candidates, num_packs)
    imbalance = loads.max(dim=-1).values - loads.min(dim=-1).values
    best_candidate_idx = imbalance.argmin(dim=1)

    # Helper to gather
    def gather_best(tensor_lcn, best_idx_l):
        idx_expanded = best_idx_l.view(num_layers, 1, 1).expand(-1, 1, num_items)
        return tensor_lcn.gather(1, idx_expanded).squeeze(1)

    final_ordered_indices = gather_best(all_indices, best_candidate_idx)
    final_packs_ordered = gather_best(flat_assigned_packs.view(num_layers, num_candidates, num_items), best_candidate_idx)
    final_ranks_ordered = gather_best(flat_assigned_ranks.view(num_layers, num_candidates, num_items), best_candidate_idx)

    pack_index = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)
    rank_in_pack = torch.empty(num_layers, num_items, device=device, dtype=torch.int64)

    pack_index.scatter_(1, final_ordered_indices, final_packs_ordered)
    rank_in_pack.scatter_(1, final_ordered_indices, final_ranks_ordered)

    # 4. Refinement
    # Apply local search to further improve the best selected candidate
    pack_index, rank_in_pack = _refine_packing(weight, pack_index, rank_in_pack, num_packs)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>