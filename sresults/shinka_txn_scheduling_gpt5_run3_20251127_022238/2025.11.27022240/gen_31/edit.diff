--- a/original.py
+++ b/original.py
@@ -1,306 +1,467 @@
 # EVOLVE-BLOCK-START
 """Transaction scheduling algorithm for optimizing makespan across multiple workloads"""
 
 import time
 import random
 import sys
 import os
 
 # Add the openevolve_examples directory to the path to import txn_simulator and workloads
-# Find the repository root by looking for the openevolve_examples directory
+# Find the repository root by looking for openevolve_examples directory
 def find_repo_root(start_path):
     """Find the repository root by looking for openevolve_examples directory."""
     current = os.path.abspath(start_path)
     # Search up the directory tree
     while current != os.path.dirname(current):  # Stop at filesystem root
         candidate = os.path.join(current, 'openevolve_examples', 'txn_scheduling')
         if os.path.exists(candidate):
             return current
         current = os.path.dirname(current)
 
     # If not found by searching up, try common locations relative to known paths
     # This handles when the program is copied to a results directory
     script_dir = os.path.dirname(os.path.abspath(__file__))
     possible_roots = [
         script_dir,  # Current directory
         os.path.dirname(script_dir),  # Parent
         os.path.dirname(os.path.dirname(script_dir)),  # Grandparent
         '/home/ubuntu/ShinkaEvolve',  # Absolute path fallback for Ubuntu
         '/Users/audreycc/Documents/Work/LLMTxn/ADRS-Exps/ShinkaEvolve',  # Absolute path fallback for macOS
     ]
     for root in possible_roots:
         candidate = os.path.join(root, 'openevolve_examples', 'txn_scheduling')
         if os.path.exists(candidate):
             return root
 
     raise RuntimeError(f"Could not find openevolve_examples directory. Searched from: {start_path}")
 
 repo_root = find_repo_root(os.path.dirname(__file__))
 sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'txn_scheduling'))
 
 from txn_simulator import Workload
 from workloads import WORKLOAD_1, WORKLOAD_2, WORKLOAD_3
 
 
 def get_best_schedule(workload, num_seqs):
     """
-    Enhanced multi-start greedy with incumbent pruning, adaptive lookahead,
-    and targeted Variable Neighborhood Search (VNS) local improvement.
+    Tournament-guided, incumbent-pruned beam search with lookahead and robust VNS refinement.
 
     Args:
         workload: Workload object containing transaction data
-        num_seqs: Number of randomized restarts to attempt
+        num_seqs: Controls exploration budget (beam width and local search intensity)
 
     Returns:
         Tuple of (lowest makespan, corresponding schedule)
     """
     n = workload.num_txns
-    # Deterministic RNG per workload for stability
-    rng = random.Random(1729 + n)
-
-    best_cost = float('inf')
-    best_seq = None
-
-    # Memoization for sequence costs to avoid recomputation
+    rng = random.Random(time.time_ns() ^ (os.getpid() << 1) ^ n)
+
+    # Memoized evaluator for sequence costs to reduce simulator calls
     cost_cache = {}
 
-    def seq_cost(seq):
+    def evaluate_seq(seq):
         key = tuple(seq)
         if key in cost_cache:
             return cost_cache[key]
         c = workload.get_opt_seq_cost(seq)
         cost_cache[key] = c
         return c
 
-    # Precompute singleton costs to seed better starting points
-    singleton_costs = [(t, seq_cost([t])) for t in range(n)]
-    singleton_costs.sort(key=lambda x: x[1])
-
-    restarts = max(1, int(num_seqs))
-
-    for r in range(restarts):
-        # Diversify starts: bias to good singletons but keep randomness
-        if r < min(5, n):
-            k = min(10, n)
-            t0 = rng.choice([t for t, _ in singleton_costs[:k]])
-        else:
-            t0 = rng.randint(0, n - 1)
-
+    # Precompute singleton and pairwise costs to capture conflict structure
+    c1 = [0] * n
+    for i in range(n):
+        c1[i] = evaluate_seq([i])
+
+    M = [[0] * n for _ in range(n)]  # M[i][j] = cost([i, j])
+    for i in range(n):
+        Mi = M[i]
+        for j in range(n):
+            if i == j:
+                Mi[j] = c1[i]
+            else:
+                Mi[j] = evaluate_seq([i, j])
+
+    # Preference margins: W[i][j] < 0 suggests i before j
+    W = [[0] * n for _ in range(n)]
+    for i in range(n):
+        Wi = W[i]
+        Mi = M[i]
+        for j in range(n):
+            if i == j:
+                Wi[j] = 0
+            else:
+                Wi[j] = Mi[j] - M[j][i]
+
+    # Tournament scores: smaller => earlier
+    s = [0] * n
+    for i in range(n):
+        s[i] = sum(W[i][j] for j in range(n) if j != i)
+
+    tournament_order = list(range(n))
+    tournament_order.sort(key=lambda x: (s[x], x))
+
+    def prefer_before(a, b):
+        return M[a][b] <= M[b][a]
+
+    def tournament_bubble_pass(seq, passes=2):
+        arr = seq[:]
+        for _ in range(passes):
+            improved = False
+            for k in range(len(arr) - 1):
+                a, b = arr[k], arr[k + 1]
+                if not prefer_before(a, b):
+                    arr[k], arr[k + 1] = b, a
+                    improved = True
+            if not improved:
+                break
+        return arr
+
+    def preselect_by_tournament(prefix, remaining, k, recent_k=4):
+        if not remaining:
+            return []
+        recents = prefix[-recent_k:] if recent_k > 0 else []
+        scored = []
+        for t in remaining:
+            sc = 0
+            for x in recents:
+                sc += W[x][t]
+            scored.append((sc, t))
+        scored.sort(key=lambda z: (z[0], z[1]))
+        return [t for _, t in scored[:k]]
+
+    # Quick greedy to get incumbent for pruning
+    def quick_greedy_incumbent():
+        t0 = tournament_order[0]
         seq = [t0]
         remaining = [t for t in range(n) if t != t0]
-
-        # Greedy build: evaluate all candidates and use adaptive lookahead to break ties
-        step = 0
         while remaining:
-            # Incumbent-based pruning: if partial already not better, abort this restart
-            base_cost = seq_cost(seq)
-            if best_cost < float('inf') and base_cost >= best_cost:
-                # Abort current restart early; no completion can beat incumbent
-                seq = None
+            # immediate best extension
+            best_t = None
+            best_c = float('inf')
+            for t in remaining:
+                c = evaluate_seq(seq + [t])
+                if c < best_c:
+                    best_c = c
+                    best_t = t
+            seq.append(best_t)
+            remaining.remove(best_t)
+        seq = tournament_bubble_pass(seq, passes=2)
+        return evaluate_seq(seq), seq
+
+    incumbent_cost, incumbent_seq = quick_greedy_incumbent()
+
+    # Beam-search with tournament-guided candidate pools and limited lookahead
+    def beam_search(incumbent):
+        beam_width = max(8, min(18, int(num_seqs) + n // 20))
+        cand_per_expand = max(6, min(14, n // 7 + 6))
+        lookahead_top = 3
+        lookahead_next_k = 5
+
+        # Diverse starts: tournament-best, good singleton, randoms
+        starts = []
+        starts.append(tournament_order[0])
+        # pick one good singleton among top-10
+        topk = min(10, n)
+        good_singletons = sorted(range(n), key=lambda t: c1[t])[:topk]
+        if good_singletons:
+            starts.append(rng.choice(good_singletons))
+        # fill remaining with random distinct txns
+        remaining_candidates = [t for t in range(n) if t not in starts]
+        rng.shuffle(remaining_candidates)
+        starts.extend(remaining_candidates[:max(0, beam_width - len(starts))])
+
+        beam = []
+        used = set()
+        for t in starts:
+            seq = [t]
+            rem = frozenset(set(range(n)) - {t})
+            c = evaluate_seq(seq)
+            # prune start if it cannot beat incumbent already
+            if incumbent is not None and c >= incumbent:
+                continue
+            key = (tuple(seq), rem)
+            if key in used:
+                continue
+            used.add(key)
+            beam.append((c, seq, rem))
+
+        if not beam:
+            # fallback to tournament order
+            seq = tournament_order[:]
+            return evaluate_seq(seq), seq
+
+        for _ in range(n - 1):
+            next_beam = []
+            seen = set()
+            # Expand each partial sequence
+            for cost, seq, remaining in beam:
+                # prune partials that already exceed incumbent
+                if incumbent is not None and cost >= incumbent:
+                    continue
+
+                rem_list = list(remaining)
+                if not rem_list:
+                    next_beam.append((cost, seq, remaining))
+                    continue
+
+                # Tournament-guided preselection
+                if len(rem_list) > cand_per_expand * 2:
+                    pre = preselect_by_tournament(seq, rem_list, cand_per_expand * 2)
+                else:
+                    pre = rem_list
+
+                # Evaluate immediate cost of candidates
+                imm = []
+                for t in pre:
+                    c_im = evaluate_seq(seq + [t])
+                    # prune child immediately if exceeds incumbent
+                    if incumbent is not None and c_im >= incumbent:
+                        continue
+                    imm.append((t, c_im))
+                if not imm:
+                    continue
+                imm.sort(key=lambda x: x[1])
+
+                # Lookahead over top few
+                L = min(lookahead_top, len(imm))
+                scored_ext = []
+                for t, immediate_c in imm[:L]:
+                    next_pool_all = [x for x in rem_list if x != t]
+                    if not next_pool_all:
+                        la_cost = immediate_c
+                    else:
+                        la_pref = preselect_by_tournament(seq + [t], next_pool_all, min(lookahead_next_k, len(next_pool_all)))
+                        if not la_pref:
+                            la_pref = next_pool_all
+                        la_cost = min(evaluate_seq(seq + [t, u]) for u in la_pref)
+                    metric = min(la_cost, immediate_c)
+                    # keep a metric for ranking expansions
+                    scored_ext.append((t, metric))
+
+                # Also add a few immediate-best without lookahead to maintain diversity
+                diversity_take = min(max(2, cand_per_expand // 3), len(imm))
+                for t, c_im in imm[:diversity_take]:
+                    scored_ext.append((t, c_im))
+
+                # Deduplicate and keep best-k expansions for this parent
+                unique = {}
+                for t, m in scored_ext:
+                    if (t not in unique) or (m < unique[t]):
+                        unique[t] = m
+                items = sorted(unique.items(), key=lambda z: z[1])
+                expand_k = min(cand_per_expand, len(items))
+                for t, _metric in items[:expand_k]:
+                    new_seq = seq + [t]
+                    new_cost = evaluate_seq(new_seq)
+                    # prune again with full immediate cost
+                    if incumbent is not None and new_cost >= incumbent:
+                        continue
+                    rem_new = remaining - {t}
+                    key = (tuple(new_seq), rem_new)
+                    if key in seen:
+                        continue
+                    seen.add(key)
+                    next_beam.append((new_cost, new_seq, rem_new))
+
+            if not next_beam:
+                # Fallback: deterministic best extension for each beam item
+                for cost, seq, remaining in beam:
+                    rem_list = list(remaining)
+                    best_t = None
+                    best_c = float('inf')
+                    for t in rem_list:
+                        c = evaluate_seq(seq + [t])
+                        if c < best_c:
+                            best_c = c
+                            best_t = t
+                    if best_t is not None and (incumbent is None or best_c < incumbent):
+                        new_seq = seq + [best_t]
+                        next_beam.append((best_c, new_seq, remaining - {best_t}))
+                if not next_beam:
+                    break
+
+            # Keep top beam_width partial sequences
+            next_beam.sort(key=lambda x: x[0])
+            beam = next_beam[:beam_width]
+
+        if not beam:
+            # fallback to tournament order
+            seq = tournament_order[:]
+            return evaluate_seq(seq), seq
+
+        beam.sort(key=lambda x: x[0])
+        best_cost, best_seq, _ = beam[0]
+        return best_cost, best_seq
+
+    def local_improve(seq, base_cost=None):
+        # Robust VNS: tournament cleanup, adjacent swaps, relocations, limited swaps,
+        # targeted marginal relocations, and a light ruin-and-recreate.
+        best_seq = list(seq)
+        best_cost = evaluate_seq(best_seq) if base_cost is None else base_cost
+        nloc = len(best_seq)
+
+        # 0) Tournament-based cheap cleanup
+        cand0 = tournament_bubble_pass(best_seq, passes=2)
+        c0 = evaluate_seq(cand0)
+        if c0 < best_cost:
+            best_seq, best_cost = cand0, c0
+
+        # 1) Adjacent swap hill-climb passes
+        for _ in range(3):
+            improved = False
+            for i in range(nloc - 1):
+                cand = best_seq[:]
+                cand[i], cand[i + 1] = cand[i + 1], cand[i]
+                c = evaluate_seq(cand)
+                if c < best_cost:
+                    best_seq, best_cost = cand, c
+                    improved = True
+            if not improved:
                 break
 
-            cand_costs = []
-            base = seq
-            for t in remaining:
-                c = seq_cost(base + [t])
-                cand_costs.append((t, c))
-            cand_costs.sort(key=lambda x: x[1])
-
-            # Adaptive lookahead: slightly deeper in early steps
-            L = min(4 if step < 10 else 3, len(cand_costs))
-            top_cands = cand_costs[:L]
-
-            chosen_t = top_cands[0][0]
-            best_pair_cost = None
-            # Dynamic next-sample size: smaller when many remain
-            R = len(remaining)
-            if n <= 60:
-                base_la = 6
-            else:
-                base_la = 4
-            lookahead_samples = base_la if R <= 60 else max(3, base_la - 1)
-
-            for t, immediate_c in top_cands:
-                if R == 1:
-                    la_cost = immediate_c
-                else:
-                    next_pool = [x for x in remaining if x != t]
-                    if len(next_pool) <= lookahead_samples:
-                        sampled_next = next_pool
-                    else:
-                        sampled_next = rng.sample(next_pool, lookahead_samples)
-                    la_cost = min(seq_cost(base + [t, u]) for u in sampled_next)
-                if best_pair_cost is None or la_cost < best_pair_cost:
-                    best_pair_cost = la_cost
-                    chosen_t = t
-
-            seq.append(chosen_t)
-            remaining.remove(chosen_t)
-            step += 1
-
-        if seq is None:
-            # Pruned restart
-            continue
-
-        # Local improvement phase
-        current_cost = seq_cost(seq)
-
-        # 1) Adjacent swap hill-climb passes
-        for _ in range(2):
-            any_improve = False
-            for i in range(n - 1):
-                seq[i], seq[i + 1] = seq[i + 1], seq[i]
-                c = seq_cost(seq)
-                if c < current_cost:
-                    current_cost = c
-                    any_improve = True
-                else:
-                    # revert if no improvement
-                    seq[i], seq[i + 1] = seq[i + 1], seq[i]
-            if not any_improve:
-                break
-
-        # 1b) Limited non-adjacent 2-opt swaps (sampled)
-        swap_attempts = min(n, 60)
+        # 1b) Limited sampled 2-opt (non-adjacent swap) on conflict-heavy pairs
+        swap_attempts = min(nloc, 80)
         for _ in range(swap_attempts):
-            i, j = rng.sample(range(n), 2)
+            i, j = rng.sample(range(nloc), 2)
             if abs(i - j) <= 1:
                 continue
-            seq[i], seq[j] = seq[j], seq[i]
-            c = seq_cost(seq)
-            if c < current_cost:
-                current_cost = c
-            else:
-                # revert
-                seq[i], seq[j] = seq[j], seq[i]
-
-        # 2) Random insertion improvements with accept-if-better
-        attempts = min(150, 3 * n)
-        for _ in range(attempts):
-            i = rng.randint(0, n - 1)
-            j = rng.randint(0, n - 1)
+            cand = best_seq[:]
+            cand[i], cand[j] = cand[j], cand[i]
+            c = evaluate_seq(cand)
+            if c < best_cost:
+                best_seq, best_cost = cand, c
+
+        # 2) Sampled insertion (relocate) moves
+        relocate_attempts = min(2 * nloc, 180) + max(0, int(num_seqs // 3))
+        for _ in range(relocate_attempts):
+            i, j = rng.sample(range(nloc), 2)
             if i == j:
                 continue
-            t = seq.pop(i)
-            seq.insert(j, t)
-            c = seq_cost(seq)
-            if c < current_cost:
-                current_cost = c
-            else:
-                # revert
-                seq.pop(j)
-                seq.insert(i, t)
+            cand = best_seq[:]
+            item = cand.pop(j)
+            cand.insert(i, item)
+            c = evaluate_seq(cand)
+            if c < best_cost:
+                best_seq, best_cost = cand, c
 
         # 2b) Targeted relocate of high-marginal transactions
         # Compute prefix marginals to locate "spiky" contributors
-        prefix_costs = [0] * n
+        prefix_costs = [0] * nloc
         accum = 0
-        for i in range(n):
-            accum = seq_cost(seq[: i + 1])
+        for i in range(nloc):
+            accum = evaluate_seq(best_seq[: i + 1])
             prefix_costs[i] = accum
-        marg = [prefix_costs[0]] + [prefix_costs[i] - prefix_costs[i - 1] for i in range(1, n)]
-        # Try moving top-3 marginals to sampled positions
-        hot_positions = sorted(range(n), key=lambda idx: marg[idx], reverse=True)[:3]
+        marg = [prefix_costs[0]] + [prefix_costs[i] - prefix_costs[i - 1] for i in range(1, nloc)]
+        hot_positions = sorted(range(nloc), key=lambda idx: marg[idx], reverse=True)[:3]
         for pos in hot_positions:
-            if pos >= len(seq):
+            if pos >= len(best_seq):
                 continue
-            t = seq[pos]
-            base = seq[:pos] + seq[pos + 1:]
+            t = best_seq[pos]
+            base = best_seq[:pos] + best_seq[pos + 1:]
             best_pos_idx = None
-            best_pos_cost = current_cost
+            best_pos_cost = best_cost
             # Try ends plus a few random positions
             positions_try = {0, len(base)}
             while len(positions_try) < 6:
                 positions_try.add(rng.randint(0, len(base)))
             for j in positions_try:
                 cand = base[:j] + [t] + base[j:]
-                c = seq_cost(cand)
+                c = evaluate_seq(cand)
                 if c < best_pos_cost:
                     best_pos_cost = c
                     best_pos_idx = j
-            if best_pos_idx is not None and best_pos_cost < current_cost:
-                seq = base[:best_pos_idx] + [t] + base[best_pos_idx:]
-                current_cost = best_pos_cost
-
-        # 3) Light ruin-and-recreate with greedy reinsertion
-        if n > 12:
-            block_size = max(5, min(18, n // 6))
-            rr_tries = 1 if n >= 90 else 2
+            if best_pos_idx is not None and best_pos_cost < best_cost:
+                best_seq = base[:best_pos_idx] + [t] + base[best_pos_idx:]
+                best_cost = best_pos_cost
+
+        # 3) Ruin-and-Recreate (block removal, greedy reinsertion with sampled positions)
+        if nloc > 12:
+            block_size = max(5, min(18, nloc // 6))
+            rr_tries = 1 if nloc >= 90 else 2
             for _ in range(rr_tries):
-                start = rng.randint(0, n - block_size)
-                removed = seq[start:start + block_size]
-                base = seq[:start] + seq[start + block_size:]
+                start = rng.randint(0, nloc - block_size)
+                removed = best_seq[start:start + block_size]
+                base = best_seq[:start] + best_seq[start + block_size:]
 
                 for t in rng.sample(removed, len(removed)):
-                    # Try ends + random positions
-                    pos_candidates = {0, len(base)}
-                    while len(pos_candidates) < 6:
-                        pos_candidates.add(rng.randint(0, len(base)))
+                    # Sample positions including ends
+                    positions = {0, len(base)}
+                    sample_k = min(6, len(base) + 1)
+                    while len(positions) < sample_k:
+                        positions.add(rng.randint(0, len(base)))
                     best_pos = 0
                     best_pos_cost = float('inf')
-                    for pos in pos_candidates:
+                    for pos in positions:
                         cand = base[:pos] + [t] + base[pos:]
-                        c = seq_cost(cand)
+                        c = evaluate_seq(cand)
                         if c < best_pos_cost:
                             best_pos_cost = c
                             best_pos = pos
                     base.insert(best_pos, t)
 
-                c = seq_cost(base)
-                if c < current_cost:
-                    seq = base
-                    current_cost = c
-
-        if current_cost < best_cost:
-            best_cost = current_cost
-            best_seq = seq[:]
-
-    return best_cost, best_seq
+                c = evaluate_seq(base)
+                if c < best_cost:
+                    best_seq, best_cost = base, c
+
+        return best_cost, best_seq
+
+    # Run beam search with incumbent pruning to get high-quality schedule, then refine locally
+    beam_cost, beam_seq = beam_search(incumbent_cost)
+    # Keep the best between beam and incumbent before local improvement
+    if incumbent_cost < beam_cost:
+        base_seq = incumbent_seq
+        base_cost = incumbent_cost
+    else:
+        base_seq = beam_seq
+        base_cost = beam_cost
+
+    final_cost, final_seq = local_improve(base_seq, base_cost=base_cost)
+    return final_cost, final_seq
 
 
 def get_random_costs():
     """
     Evaluate scheduling algorithm on three different workloads.
 
     Returns:
         Tuple of (total_makespan, list_of_schedules, execution_time)
     """
     start_time = time.time()
     workload_size = 100
 
     # Workload 1: Complex mixed read/write transactions
     workload = Workload(WORKLOAD_1)
     makespan1, schedule1 = get_best_schedule(workload, 10)
     cost1 = workload.get_opt_seq_cost(schedule1)
 
     # Workload 2: Simple read-then-write pattern
     workload2 = Workload(WORKLOAD_2)
     makespan2, schedule2 = get_best_schedule(workload2, 10)
     cost2 = workload2.get_opt_seq_cost(schedule2)
 
     # Workload 3: Minimal read/write operations
     workload3 = Workload(WORKLOAD_3)
     makespan3, schedule3 = get_best_schedule(workload3, 10)
     cost3 = workload3.get_opt_seq_cost(schedule3)
 
     total_makespan = cost1 + cost2 + cost3
     schedules = [schedule1, schedule2, schedule3]
     execution_time = time.time() - start_time
 
     return total_makespan, schedules, execution_time
 
 
 # EVOLVE-BLOCK-END
 
 
 # This part remains fixed (not evolved)
 def run_scheduling():
     """Run the transaction scheduling algorithm for all workloads"""
     total_makespan, schedules, execution_time = get_random_costs()
     return total_makespan, schedules, execution_time
 
 
 if __name__ == "__main__":
     total_makespan, schedules, execution_time = run_scheduling()
     print(f"Total makespan: {total_makespan}, Execution time: {execution_time:.4f}s")
     print(f"Individual workload costs: {[workload.get_opt_seq_cost(schedule) for workload, schedule in zip([Workload(WORKLOAD_1), Workload(WORKLOAD_2), Workload(WORKLOAD_3)], schedules)]}")