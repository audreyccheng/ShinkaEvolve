<NAME>
memoized_lookahead_greedy_ils
</NAME>

<DESCRIPTION>
I replace the simplistic single-run greedy sampler with a memoized, multi-start, lookahead-guided greedy constructor plus a lightweight local improvement step. Key improvements:
- Global memoization of prefix costs and one-step extensions to avoid repeated simulator calls and enable deeper exploration within the time budget.
- Two-stage candidate selection at each step: prefilter by marginal extension cost, then shallow two-step lookahead (min over a small sampled next-set) to reduce myopic choices that lead to higher makespan from conflicts.
- Multi-start seeds chosen from best singleton costs plus a few random seeds to diversify construction without heavy beam search overhead.
- Fast local refinement via one pass of adjacent swaps and a small number of random insertion moves, both backed by the shared cache.

This approach directly optimizes the actual makespan using workload.get_opt_seq_cost while using caches for efficiency. It avoids overcomplicated beams that previously underperformed, and focuses compute on promising frontiers guided by marginal costs and limited lookahead, which aligns with minimizing serialized conflicts. The modifications are self-contained and maintain the function signature and runtime constraints.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def get_best_schedule(workload, num_seqs):
    """
    Get optimal schedule using greedy cost sampling strategy.

    Args:
        workload: Workload object containing transaction data
        num_seqs: Number of sequences to sample for greedy selection

    Returns:
        Tuple of (lowest makespan, corresponding schedule)
    """
    def get_greedy_cost_sampled(num_samples, sample_rate):
        # greedy with random starting point
        start_txn = random.randint(0, workload.num_txns - 1)
        txn_seq = [start_txn]
        remaining_txns = [x for x in range(0, workload.num_txns)]
        remaining_txns.remove(start_txn)
        running_cost = workload.txns[start_txn][0][3]

        for i in range(0, workload.num_txns - 1):
            min_cost = 100000  # MAX
            min_relative_cost = 10
            min_txn = -1
            holdout_txns = []
            done = False
            key_maps = []

            sample = random.random()
            if sample > sample_rate:
                idx = random.randint(0, len(remaining_txns) - 1)
                t = remaining_txns[idx]
                txn_seq.append(t)
                remaining_txns.pop(idx)
                continue

            for j in range(0, num_samples):
                idx = 0
                if len(remaining_txns) > 1:
                    idx = random.randint(0, len(remaining_txns) - 1)
                else:
                    done = True
                t = remaining_txns[idx]
                holdout_txns.append(remaining_txns.pop(idx))

                txn_len = workload.txns[t][0][3]
                test_seq = txn_seq.copy()
                test_seq.append(t)
                cost = 0
                cost = workload.get_opt_seq_cost(test_seq)
                if cost < min_cost:
                    min_cost = cost
                    min_txn = t
                if done:
                    break
            assert(min_txn != -1)
            running_cost = min_cost
            txn_seq.append(min_txn)
            holdout_txns.remove(min_txn)
            remaining_txns.extend(holdout_txns)

        assert len(set(txn_seq)) == workload.num_txns

        overall_cost = workload.get_opt_seq_cost(txn_seq)
        return overall_cost, txn_seq

    return get_greedy_cost_sampled(10, 1.0)
=======
def get_best_schedule(workload, num_seqs):
    """
    Lookahead-guided greedy with memoized costs, multi-start seeding, and
    lightweight local refinement to minimize makespan.
    """
    N = workload.num_txns
    start_time = time.time()
    # Per-workload time budget to keep runtime reasonable
    time_budget_sec = 0.6

    def time_left():
        return (time.time() - start_time) < time_budget_sec

    # Shared caches across construction and refinement
    cost_cache = {}
    ext_cache = {}

    def eval_cost(seq):
        key = tuple(seq)
        cached = cost_cache.get(key)
        if cached is not None:
            return cached
        c = workload.get_opt_seq_cost(seq)
        cost_cache[key] = c
        return c

    def eval_ext_cost(prefix_tuple, cand):
        key = (prefix_tuple, cand)
        cached = ext_cache.get(key)
        if cached is not None:
            return cached
        c = eval_cost(list(prefix_tuple) + [cand])
        ext_cache[key] = c
        return c

    # Warm up with singleton costs (useful as seeds and light LB)
    singleton_cost = {}
    for t in range(N):
        if not time_left():
            break
        singleton_cost[t] = eval_cost([t])

    def construct(seed=None):
        # Choose a seed; default to best singleton
        if seed is None:
            seed = min(range(N), key=lambda x: singleton_cost.get(x, float('inf')))
        seq = [seed]
        rem = set(range(N))
        rem.remove(seed)
        cur_cost = singleton_cost.get(seed, eval_cost([seed]))

        # Adaptive parameters
        prefilter = min(24, max(8, N // 8))   # number of candidates to sample/prefilter
        rcl = 5                                # restricted candidate list size
        lookahead_k = 4                        # next-step sample size

        while rem and time_left():
            rem_list = list(rem)
            prefix_tuple = tuple(seq)

            # Prefilter candidates by marginal cost (delta)
            if len(rem_list) > prefilter:
                pool = random.sample(rem_list, prefilter * 2)
            else:
                pool = rem_list

            scored = []
            for t in pool:
                ec = eval_ext_cost(prefix_tuple, t)
                scored.append((ec - cur_cost, ec, t))

            if not scored:
                # If time ran out, append arbitrary and continue
                t = rem_list[0]
                seq.append(t)
                rem.remove(t)
                cur_cost = eval_ext_cost(prefix_tuple, t)
                continue

            scored.sort(key=lambda x: x[0])
            top = scored[:min(rcl, len(scored))]

            # Two-step lookahead: for each top candidate, peek one more step
            best_t = None
            best_ec = None
            best_score = float('inf')
            for _, ec, t in top:
                new_prefix = tuple(seq + [t])
                new_rem = rem.copy()
                new_rem.remove(t)
                if not new_rem:
                    la_score = ec
                else:
                    la_list = list(new_rem)
                    k = min(lookahead_k, len(la_list))
                    la_sample = random.sample(la_list, k)
                    best_la = float('inf')
                    for nxt in la_sample:
                        c2 = eval_ext_cost(new_prefix, nxt)
                        if c2 < best_la:
                            best_la = c2
                    la_score = best_la
                # Rank by the tighter of immediate extension and lookahead
                score = min(ec, la_score)
                if score < best_score:
                    best_score = score
                    best_t = t
                    best_ec = ec

            seq.append(best_t)
            rem.remove(best_t)
            cur_cost = best_ec

        final_cost = eval_cost(seq)
        return final_cost, seq

    def local_improve(seq, cur_cost):
        best_seq = seq[:]
        best_cost = cur_cost

        # One pass of adjacent swaps
        for i in range(len(best_seq) - 1):
            if not time_left():
                break
            cand = best_seq[:]
            cand[i], cand[i + 1] = cand[i + 1], cand[i]
            c = eval_cost(cand)
            if c < best_cost:
                best_cost = c
                best_seq = cand

        # Limited random insertion moves
        trials = min(60, 2 * N)
        while trials > 0 and time_left():
            trials -= 1
            i, j = random.sample(range(len(best_seq)), 2)
            if i == j:
                continue
            cand = best_seq[:]
            val = cand.pop(i)
            cand.insert(j, val)
            c = eval_cost(cand)
            if c < best_cost:
                best_cost = c
                best_seq = cand

        return best_cost, best_seq

    # Multi-start: use a few best singletons + random seeds
    best_cost = float('inf')
    best_seq = None
    restarts = max(3, int(num_seqs))

    seeds = sorted(range(N), key=lambda t: singleton_cost.get(t, float('inf')))[:min(4, N)]
    # Add random seeds for diversity
    while len(seeds) < restarts and len(seeds) < N:
        t = random.randrange(N)
        if t not in seeds:
            seeds.append(t)

    for seed in seeds:
        if not time_left():
            break
        c, s = construct(seed)
        if time_left():
            c, s = local_improve(s, c)
        if c < best_cost:
            best_cost, best_seq = c, s

    if best_seq is None:
        # Fallback
        seq = list(range(N))
        random.shuffle(seq)
        best_seq = seq
        best_cost = eval_cost(seq)

    return best_cost, best_seq
>>>>>>> REPLACE

</DIFF>