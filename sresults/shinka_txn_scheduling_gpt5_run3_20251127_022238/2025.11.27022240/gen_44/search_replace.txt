<NAME>
child_probe_and_block_swap
</NAME>

<DESCRIPTION>
I introduce two targeted improvements to reduce makespan by tightening pruning and adding a stronger local neighborhood:

1) Child-level greedy probing during beam expansion:
   - For the top-k children of each prefix (k = params['next_k']), we greedily complete the partial sequence to a full schedule and use the resulting full cost to update the incumbent and to refine ranking. This provides much tighter bounds than singleton LBs and lookahead alone, enabling safe pruning of inferior branches and more accurate prioritization of promising prefixes. This change directly addresses conflicts by evaluating actual end-to-end makespan impact.

2) Block-swap neighborhood in local improvement:
   - In addition to adjacent swaps, relocations, and boundary repair, I add a limited block-swap move centered on the top-2 worst adjacencies (by pairwise marginal cost). Swapping two small blocks (size 3â€“6) often resolves clusters of conflicts, enabling non-local improvements that simple adjacent swaps or insertions may miss, while capping the number of evaluations to preserve runtime.

Both changes are time-bounded and reuse existing caches to stay efficient. They retain the existing structure and parameters, leveraging the next_k parameter for probing and the pair_cost surrogate for targeting block edits.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
                scored.sort(key=lambda x: (x[0], x[1]))
                top = scored[:min(branch_factor, len(scored))]
                for _delta, la_score, ec, cand in top:
                    new_seq = seq + [cand]
                    new_rem = rem.copy()
                    new_rem.remove(cand)
                    # Child LB pruning
                    if lb_singleton(ec, new_rem) >= best_full_cost:
                        continue
                    new_beam.append((ec, new_seq, new_rem, la_score))
=======
                scored.sort(key=lambda x: (x[0], x[1]))
                top = scored[:min(branch_factor, len(scored))]
                # Probe a few best children by greedily completing them to tighten incumbent
                probe_k = min(params.get('next_k', 4), len(top))
                idx_child = 0
                for _delta, la_score, ec, cand in top:
                    new_seq = seq + [cand]
                    new_rem = rem.copy()
                    new_rem.remove(cand)
                    # Child LB pruning
                    if lb_singleton(ec, new_rem) >= best_full_cost:
                        continue
                    # Greedy probe for first few children to update incumbent and refine ranking
                    if idx_child < probe_k and time_left():
                        g_cost, g_seq = greedy_finish(new_seq, new_rem, branch_k=max(6, N // 12), incumbent=best_full_cost)
                        if len(g_seq) == N and g_cost < best_full_cost:
                            best_full_cost, best_full_seq = g_cost, g_seq
                        la_score = min(la_score, g_cost)
                    idx_child += 1
                    new_beam.append((ec, new_seq, new_rem, la_score))
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
        # Attempt a couple of boundary repairs
        for _ in range(2):
            if not time_left():
                break
            c_try, s_try = boundary_repair_once(best_seq, best_cost)
            if c_try < best_cost:
                best_cost, best_seq = c_try, s_try

        # Targeted relocations with small window (reduced trials, keep diversity)
        trials = 45
        n = len(best_seq)
        while trials > 0 and time_left():
=======
        # Attempt a couple of boundary repairs
        for _ in range(2):
            if not time_left():
                break
            c_try, s_try = boundary_repair_once(best_seq, best_cost)
            if c_try < best_cost:
                best_cost, best_seq = c_try, s_try

        # Block-swap neighborhood around top-2 worst adjacencies (limited tries)
        if time_left():
            n = len(best_seq)
            if n >= 8:
                worst = []
                for i in range(n - 1):
                    if not time_left():
                        break
                    worst.append((pair_cost(best_seq[i], best_seq[i + 1]), i))
                worst.sort(key=lambda x: x[0], reverse=True)
                tries = 0
                for a in range(min(2, len(worst))):
                    if not time_left():
                        break
                    for b in range(a + 1, min(4, len(worst))):
                        if not time_left():
                            break
                        i = worst[a][1]
                        j = worst[b][1]
                        block = min(6, max(3, n // 40))
                        si = max(0, min(i - block // 2, n - block))
                        sj = max(0, min(j - block // 2, n - block))
                        # ensure non-overlap
                        if abs(si - sj) < block:
                            continue
                        cand = best_seq[:]
                        # ensure si < sj
                        if si > sj:
                            si, sj = sj, si
                        block_i = cand[si:si + block]
                        block_j = cand[sj:sj + block]
                        mid = cand[si + block:sj]
                        cand2 = cand[:si] + block_j + mid + block_i + cand[sj + block:]
                        c = eval_seq_cost(cand2)
                        tries += 1
                        if c < best_cost:
                            best_cost = c
                            best_seq = cand2
                    if tries >= 4:
                        break

        # Targeted relocations with small window (reduced trials, keep diversity)
        trials = 45
        n = len(best_seq)
        while trials > 0 and time_left():
>>>>>>> REPLACE