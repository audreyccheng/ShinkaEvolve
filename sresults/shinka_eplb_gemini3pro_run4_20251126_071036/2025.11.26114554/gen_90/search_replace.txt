To improve the performance of the expert rearrangement algorithm, I propose the following changes:

1.  **Add `bisect` import**: The optimization algorithm will use binary search to find optimal swap candidates, so the `bisect` module is required.
2.  **Hybrid Refinement Strategy**: The existing `balanced_packing` function uses a "Pool-and-Split" strategy (merging two packs and redistributing greedily). While robust, it lacks the precision of single-item swaps for fine-tuning. I propose a hybrid approach:
    *   First, attempt **Pool-and-Split** on the heaviest and lightest packs. This acts as a "large neighborhood" search to escape local optima.
    *   If Pool-and-Split fails to improve the balance, fallback to a **Pairwise Swap** search. Using binary search (`bisect`), we efficiently find the single item pair $(u, v)$ (where $u$ is from the heaviest pack and $v$ from the lightest) that minimizes the load difference.
    *   I also increase the number of restarts from 4 to 10. Since the current speed score is perfect (1.0), we can afford more computation to search for a better balance.

<NAME>
optimize_packing_imports
</NAME>

<DESCRIPTION>
Add bisect module for efficient binary search in the optimization algorithm.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
import torch
import heapq
import random
=======
import torch
import heapq
import random
import bisect
>>>>>>> REPLACE
</DIFF>

<NAME>
hybrid_refinement_strategy
</NAME>

<DESCRIPTION>
Improve `balanced_packing` by combining "Pool-and-Split" with a fine-grained "Pairwise Swap" refinement.
The "Pool-and-Split" strategy (LPT on union of Max/Min packs) is effective for large moves but can miss optimal fine-tuning.
The "Pairwise Swap" uses binary search to efficiently find the best single item exchange between Max and Min packs to further reduce the load difference.
We also increase the number of restarts to 10 to utilize the available CPU budget for better exploration.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Optimization: handle trivial case efficiently
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups,
                                  dtype=torch.int64,
                                  device=weight.device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(pack_index)
        return pack_index, rank_in_pack

    # Move computation to CPU to avoid GPU synchronization overhead for sequential logic
    weight_cpu = weight.to("cpu", dtype=torch.float32)

    # Pre-allocate result tensors on CPU
    pack_index = torch.empty((num_layers, num_groups), dtype=torch.int64, device="cpu")
    rank_in_pack = torch.empty((num_layers, num_groups), dtype=torch.int64, device="cpu")

    # Number of random restarts to escape local optima
    num_restarts = 4

    for i in range(num_layers):
        row_w = weight_cpu[i].tolist()

        # Original items (index, weight)
        original_items = sorted(enumerate(row_w), key=lambda x: x[1], reverse=True)

        best_diff = float('inf')
        best_packs = None

        for attempt in range(num_restarts):
            # 1. Initialization Strategy
            current_packs = [[] for _ in range(num_packs)]
            pack_weights = [0.0] * num_packs

            if attempt == 0:
                # Deterministic LPT
                items = original_items
            else:
                # Randomized LPT
                noise = torch.rand(num_groups).tolist()
                noisy_items = [(idx, w, w * (0.9 + 0.2 * noise[k]))
                               for k, (idx, w) in enumerate(original_items)]
                noisy_items.sort(key=lambda x: x[2], reverse=True)
                items = [(idx, w) for idx, w, _ in noisy_items]

            # Greedy Phase
            for idx, w in items:
                # Assign to the lightest pack that has space
                best_p = -1
                min_w = float('inf')
                for p in range(num_packs):
                    if len(current_packs[p]) < groups_per_pack:
                        if pack_weights[p] < min_w:
                            min_w = pack_weights[p]
                            best_p = p
                current_packs[best_p].append(idx)
                pack_weights[best_p] += w

            # 2. Refinement Phase: Pool-and-Split
            # Instead of single swaps, pick two packs (Max and Min), pool items, and re-distribute.
            for _ in range(30):
                # Identify Max and Min packs
                min_p = 0
                max_p = 0
                min_val = pack_weights[0]
                max_val = pack_weights[0]

                for p in range(1, num_packs):
                    val = pack_weights[p]
                    if val < min_val:
                        min_val = val
                        min_p = p
                    elif val > max_val:
                        max_val = val
                        max_p = p

                if max_val - min_val < 1e-6:
                    break

                # Try to improve by pairing Max Pack with Min Pack
                # Only trying Max-Min pair is usually sufficient and fast.
                p1, p2 = max_p, min_p

                # Pool items
                pooled_items = []
                for idx in current_packs[p1]:
                    pooled_items.append((idx, row_w[idx]))
                for idx in current_packs[p2]:
                    pooled_items.append((idx, row_w[idx]))

                # Sort Descending
                pooled_items.sort(key=lambda x: x[1], reverse=True)

                # Re-distribute using constrained greedy into two temp packs
                t_packs = [[], []]
                t_weights = [0.0, 0.0]

                possible = True
                for idx, w in pooled_items:
                    # Put in valid bin with min weight
                    b_best = -1
                    b_min = float('inf')
                    for b in range(2):
                        if len(t_packs[b]) < groups_per_pack:
                            if t_weights[b] < b_min:
                                b_min = t_weights[b]
                                b_best = b
                    if b_best != -1:
                        t_packs[b_best].append(idx)
                        t_weights[b_best] += w
                    else:
                        possible = False
                        break

                if not possible: break

                # Check criteria
                old_local_max = max(pack_weights[p1], pack_weights[p2])
                new_local_max = max(t_weights[0], t_weights[1])
                old_spread = abs(pack_weights[p1] - pack_weights[p2])
                new_spread = abs(t_weights[0] - t_weights[1])

                # Accept if max load reduced, or if max load unchanged but spread reduced
                if new_local_max < old_local_max - 1e-6 or \
                   (abs(new_local_max - old_local_max) < 1e-6 and new_spread < old_spread - 1e-6):
                    current_packs[p1] = t_packs[0]
                    pack_weights[p1] = t_weights[0]
                    current_packs[p2] = t_packs[1]
                    pack_weights[p2] = t_weights[1]
                else:
                    # If Pool-and-Split on Max/Min didn't help, we are likely in a local optimum
                    # reachable by this move.
                    break

            # Check global result
            curr_diff = max(pack_weights) - min(pack_weights)
            if curr_diff < best_diff:
                best_diff = curr_diff
                best_packs = [list(p) for p in current_packs]
                if best_diff < 1e-6: break

        # Fill result tensors
        for p in range(num_packs):
            for r, idx in enumerate(best_packs[p]):
                pack_index[i, idx] = p
                rank_in_pack[i, idx] = r

    return pack_index.to(weight.device), rank_in_pack.to(weight.device)
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Optimization: handle trivial case efficiently
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups,
                                  dtype=torch.int64,
                                  device=weight.device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(pack_index)
        return pack_index, rank_in_pack

    # Move computation to CPU to avoid GPU synchronization overhead for sequential logic
    weight_cpu = weight.to("cpu", dtype=torch.float32)

    # Pre-allocate result tensors on CPU
    pack_index = torch.empty((num_layers, num_groups), dtype=torch.int64, device="cpu")
    rank_in_pack = torch.empty((num_layers, num_groups), dtype=torch.int64, device="cpu")

    # Number of random restarts to escape local optima
    # Increased to 10 since speed is high and we want better balance
    num_restarts = 10

    for i in range(num_layers):
        row_w = weight_cpu[i].tolist()

        # Original items (index, weight)
        original_items = sorted(enumerate(row_w), key=lambda x: x[1], reverse=True)

        best_diff = float('inf')
        best_packs = None

        for attempt in range(num_restarts):
            # 1. Initialization Strategy
            current_packs = [[] for _ in range(num_packs)]
            pack_weights = [0.0] * num_packs

            if attempt == 0:
                # Deterministic LPT
                items = original_items
            else:
                # Randomized LPT
                noise = torch.rand(num_groups).tolist()
                noisy_items = [(idx, w, w * (0.9 + 0.2 * noise[k]))
                               for k, (idx, w) in enumerate(original_items)]
                noisy_items.sort(key=lambda x: x[2], reverse=True)
                items = [(idx, w) for idx, w, _ in noisy_items]

            # Greedy Phase
            for idx, w in items:
                # Assign to the lightest pack that has space
                best_p = -1
                min_w = float('inf')
                for p in range(num_packs):
                    if len(current_packs[p]) < groups_per_pack:
                        if pack_weights[p] < min_w:
                            min_w = pack_weights[p]
                            best_p = p
                current_packs[best_p].append(idx)
                pack_weights[best_p] += w

            # 2. Refinement Phase: Hybrid (Pool-and-Split + Swap)
            for _ in range(50):
                # Identify Max and Min packs
                min_p = 0
                max_p = 0
                min_val = pack_weights[0]
                max_val = pack_weights[0]

                for p in range(1, num_packs):
                    val = pack_weights[p]
                    if val < min_val:
                        min_val = val
                        min_p = p
                    elif val > max_val:
                        max_val = val
                        max_p = p

                diff = max_val - min_val
                if diff < 1e-6:
                    break

                improvement_found = False

                # --- Strategy A: Pool-and-Split ---
                p1, p2 = max_p, min_p

                # Pool items from Max and Min packs
                pooled_items = []
                for idx in current_packs[p1]:
                    pooled_items.append((idx, row_w[idx]))
                for idx in current_packs[p2]:
                    pooled_items.append((idx, row_w[idx]))

                # Sort Descending for LPT
                pooled_items.sort(key=lambda x: x[1], reverse=True)

                # Re-distribute using constrained greedy
                t_packs = [[], []]
                t_weights = [0.0, 0.0]
                possible = True

                for idx, w in pooled_items:
                    # Best Fit (Greedy)
                    b_best = -1
                    b_min = float('inf')
                    for b in range(2):
                        if len(t_packs[b]) < groups_per_pack:
                            if t_weights[b] < b_min:
                                b_min = t_weights[b]
                                b_best = b
                    if b_best != -1:
                        t_packs[b_best].append(idx)
                        t_weights[b_best] += w
                    else:
                        possible = False
                        break

                if possible:
                    old_local_max = max(pack_weights[p1], pack_weights[p2])
                    new_local_max = max(t_weights[0], t_weights[1])
                    old_spread = abs(pack_weights[p1] - pack_weights[p2])
                    new_spread = abs(t_weights[0] - t_weights[1])

                    # Strict improvement in local max or spread
                    if new_local_max < old_local_max - 1e-6 or \
                       (abs(new_local_max - old_local_max) < 1e-6 and new_spread < old_spread - 1e-6):
                        current_packs[p1] = t_packs[0]
                        pack_weights[p1] = t_weights[0]
                        current_packs[p2] = t_packs[1]
                        pack_weights[p2] = t_weights[1]
                        improvement_found = True

                # --- Strategy B: Single Item Swap ---
                # If Pool-and-Split failed to improve (e.g. LPT is stuck), try a precise single swap.
                if not improvement_found:
                    # We want to swap u in Max, v in Min such that w_u > w_v
                    # Target delta: (max_val - min_val) / 2
                    target = diff / 2.0

                    # Prepare items in Min pack for binary search: (weight, index_in_pack)
                    # We need to sort min pack items by weight
                    min_items = []
                    for i_v, idx_v in enumerate(current_packs[min_p]):
                        min_items.append((row_w[idx_v], i_v))
                    min_items.sort(key=lambda x: x[0])
                    min_weights = [x[0] for x in min_items]

                    best_swap = None
                    best_gap = diff # Gap from target

                    # Iterate items in Max pack
                    for i_u, idx_u in enumerate(current_packs[max_p]):
                        w_u = row_w[idx_u]
                        # We want w_u - w_v approx target  =>  w_v approx w_u - target
                        target_v = w_u - target

                        # Find insertion point
                        pos = bisect.bisect_left(min_weights, target_v)

                        # Check neighbors
                        candidates = []
                        if pos < len(min_weights): candidates.append(pos)
                        if pos > 0: candidates.append(pos - 1)

                        for c in candidates:
                            w_v = min_weights[c]
                            delta = w_u - w_v

                            # Must reduce max (delta > 0) and not flip order too much (delta < diff)
                            if 0 < delta < diff:
                                gap = abs(delta - target)
                                if gap < best_gap:
                                    best_gap = gap
                                    best_swap = (i_u, min_items[c][1], delta) # (idx_in_max, idx_in_min, delta)
                                    if gap < 1e-6: break
                        if best_swap and best_gap < 1e-6: break

                    if best_swap:
                        i_u, i_v, delta = best_swap

                        # Perform Swap
                        u = current_packs[max_p][i_u]
                        v = current_packs[min_p][i_v]

                        current_packs[max_p][i_u] = v
                        current_packs[min_p][i_v] = u
                        pack_weights[max_p] -= delta
                        pack_weights[min_p] += delta
                        improvement_found = True

                if not improvement_found:
                    break

            # Check global result
            curr_diff = max(pack_weights) - min(pack_weights)
            if curr_diff < best_diff:
                best_diff = curr_diff
                best_packs = [list(p) for p in current_packs]
                if best_diff < 1e-6: break

        # Fill result tensors
        for p in range(num_packs):
            for r, idx in enumerate(best_packs[p]):
                pack_index[i, idx] = p
                rank_in_pack[i, idx] = r

    return pack_index.to(weight.device), rank_in_pack.to(weight.device)
>>>>>>> REPLACE
</DIFF>