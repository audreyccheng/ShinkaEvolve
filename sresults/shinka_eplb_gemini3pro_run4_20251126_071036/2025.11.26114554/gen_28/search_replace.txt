think silently:
The user wants to replace code in `balanced_packing`.
The previous attempt failed because the SEARCH block was too large and possibly didn't match exactly due to formatting or hidden characters, or simply being too large for a diff match.
I should break down the changes into smaller, more targeted edits.

The overall plan is:
1.  Increase `NUM_RESTARTS` for better initial seeding.
2.  Insert a "3-Way Cyclic Swap" phase between pairwise swaps and LNS.
3.  Modify the LNS phase to try more random shuffles in addition to LPT.

Let's try to do this in one go but with a more precise SEARCH block covering the entire function body if possible, or verify carefully why it failed. The code provided in the prompt matches exactly the code I used. Maybe I should double check indentation.

Actually, the previous SEARCH block was extremely long. It's safer to replace the function internals entirely if I want to rewrite the logic flow, or replace chunks. Given the extent of changes (inserting a new phase in the middle), replacing the loop body or the whole function logic is necessary.

Let's try to replace the `balanced_packing` function body. I will copy the SEARCH block exactly from the provided code.

The previous search block started with `def balanced_packing...` and ended with `return pack_index.to(device), rank_in_pack.to(device)`.

Let's carefully verify the SEARCH block again.

Code in prompt:
```python
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Algorithm: Cyclic LNS Load Balancer (Randomized Greedy + Vectorized Swap + 3-Way LNS)

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs
    device = weight.device

    # Trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64, device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Work on CPU for efficient scalar iteration
    weight_cpu = weight.cpu()

    # Pre-allocate output tensors
    pack_index = torch.empty((num_layers, num_groups), dtype=torch.int64)
    rank_in_pack = torch.empty((num_layers, num_groups), dtype=torch.int64)

    # Tuning constants
    NUM_RESTARTS = 20
    NUM_SWAP_ITER = 15
    NUM_LNS_ITER = 20

    all_packs = list(range(num_packs))

    for i in range(num_layers):
        layer_w = weight_cpu[i]
        w_list = layer_w.tolist()

        # Base LPT sort order
        sorted_indices_base = sorted(range(num_groups), key=lambda x: w_list[x], reverse=True)

        best_max_load = float('inf')
        best_assignment = None # List[List[int]]
        best_pack_weights = None # List[float]

        # 1. Randomized Greedy Construction
        for attempt in range(NUM_RESTARTS):
            # Candidate order generation
            if attempt == 0:
                indices = sorted_indices_base
            else:
                # Add noise and resort. Noise factor 0.1 means weights vary by +/- 5%
                noise_scale = 0.1
                noisy_w = [w * (1.0 + (random.random() - 0.5) * noise_scale) for w in w_list]
                indices = sorted(range(num_groups), key=lambda x: noisy_w[x], reverse=True)

            current_assignment = [[] for _ in range(num_packs)]
            current_loads = [0.0] * num_packs
            current_counts = [0] * num_packs

            # Pack items
            for g_idx in indices:
                w = w_list[g_idx]

                # Best Fit: Find pack with min load among those with space
                best_p = -1
                min_load = float('inf')

                # Linear scan is fast enough for small num_packs (e.g. < 64)
                # Optimized logic: unroll slightly or just iterate
                for p in range(num_packs):
                    if current_counts[p] < groups_per_pack:
                        if current_loads[p] < min_load:
                            min_load = current_loads[p]
                            best_p = p

                current_assignment[best_p].append(g_idx)
                current_loads[best_p] += w
                current_counts[best_p] += 1

            max_l = max(current_loads)
            if max_l < best_max_load:
                best_max_load = max_l
                best_assignment = current_assignment
                best_pack_weights = current_loads

        # Restore best greedy state
        packs = best_assignment
        pack_weights = best_pack_weights

        # 2. Refinement: Vectorized Pairwise Swaps
        # Repeatedly optimize the heaviest pack against others
        for _ in range(NUM_SWAP_ITER):
            # Identify max pack
            max_p = -1
            max_w = -1.0
            for p in range(num_packs):
                if pack_weights[p] > max_w:
                    max_w = pack_weights[p]
                    max_p = p

            u_nodes = packs[max_p]
            w_u = layer_w[u_nodes] # Tensor indexing on CPU

            best_swap = None
            best_gain = 0.0

            # Scan other packs for improvement
            for p in range(num_packs):
                if p == max_p: continue

                diff = max_w - pack_weights[p]
                if diff < 1e-6: continue

                v_nodes = packs[p]
                w_v = layer_w[v_nodes]

                # Calculate potential swaps
                # deltas[i, j] = w_u[i] - w_v[j]
                # We want 0 < delta < diff to strictly reduce max_p and increase p without p exceeding original max_p
                # Actually, we want to minimize max(w_max - delta, w_p + delta)
                deltas = w_u.unsqueeze(1) - w_v.unsqueeze(0)
                mask = (deltas > 1e-6) & (deltas < diff)

                if not mask.any(): continue

                # Gain function: maximize delta * (diff - delta)
                # This balances minimizing the new max(u, v) and variance
                gains = deltas * (diff - deltas)
                gains = torch.where(mask, gains, -1.0)

                curr_max, curr_argmax = gains.reshape(-1).max(0)
                if curr_max.item() > best_gain:
                    best_gain = curr_max.item()
                    idx = curr_argmax.item()
                    best_swap = (max_p, p, idx // len(v_nodes), idx % len(v_nodes), deltas.reshape(-1)[idx].item())

            if best_swap:
                p1, p2, i1, i2, delta = best_swap
                # Perform swap
                val1 = packs[p1][i1]
                val2 = packs[p2][i2]
                packs[p1][i1] = val2
                packs[p2][i2] = val1
                pack_weights[p1] -= delta
                pack_weights[p2] += delta
            else:
                break

        # 3. Refinement: 3-Way Large Neighborhood Search (Ruin & Recreate)
        # Select Max, Min, and a 3rd pack to shuffle items
        if num_packs >= 3:
            for _ in range(NUM_LNS_ITER):
                # Identify key packs
                sorted_p = sorted(range(num_packs), key=pack_weights.__getitem__, reverse=True)
                p_max = sorted_p[0]
                p_min = sorted_p[-1]

                if pack_weights[p_max] - pack_weights[p_min] < 1e-6: break

                # Pick 3rd pack: Alternating between 2nd heaviest and Random
                if _ % 2 == 0:
                    p_mid = sorted_p[1]
                else:
                    candidates = [x for x in all_packs if x != p_max and x != p_min]
                    if not candidates: break
                    p_mid = random.choice(candidates)

                target_packs = [p_max, p_min, p_mid]

                # Gather items
                items = []
                for p in target_packs:
                    items.extend(packs[p])

                # Sort items LPT
                items.sort(key=lambda x: w_list[x], reverse=True)

                curr_local_max = max(pack_weights[p] for p in target_packs)
                curr_local_ss = sum(pack_weights[p]**2 for p in target_packs)

                best_sub_res = None

                # Small randomized search on subset (1 deterministic + 4 randomized)
                for sub_attempt in range(5):
                    if sub_attempt > 0:
                         # Jitter weights slightly to break ties/order
                         temp_items = sorted(items, key=lambda x: w_list[x] * (1.0 + (random.random()-0.5)*0.1), reverse=True)
                    else:
                        temp_items = items

                    sub_p = {p: [] for p in target_packs}
                    sub_w = {p: 0.0 for p in target_packs}
                    sub_c = {p: 0 for p in target_packs}

                    # Simple Greedy Loop for 3 packs
                    possible = True
                    for item in temp_items:
                        w = w_list[item]
                        # Find valid pack with min weight among targets
                        best_p = -1
                        min_val = float('inf')
                        for p in target_packs:
                            if sub_c[p] < groups_per_pack:
                                if sub_w[p] < min_val:
                                    min_val = sub_w[p]
                                    best_p = p

                        if best_p == -1:
                            possible = False; break

                        sub_p[best_p].append(item)
                        sub_w[best_p] += w
                        sub_c[best_p] += 1

                    if possible:
                        new_max = max(sub_w.values())
                        new_ss = sum(v**2 for v in sub_w.values())

                        # Accept if strictly better max, or same max with better variance
                        if new_max < curr_local_max - 1e-6:
                            curr_local_max = new_max
                            curr_local_ss = new_ss
                            best_sub_res = (sub_p, sub_w)
                        elif abs(new_max - curr_local_max) < 1e-6 and new_ss < curr_local_ss - 1e-6:
                            curr_local_ss = new_ss
                            best_sub_res = (sub_p, sub_w)

                if best_sub_res:
                    s_p, s_w = best_sub_res
                    for p in target_packs:
                        packs[p] = s_p[p]
                        pack_weights[p] = s_w[p]

        # Final write to output tensors
        for p in range(num_packs):
            for r, g_idx in enumerate(packs[p]):
                pack_index[i, g_idx] = p
                rank_in_pack[i, g_idx] = r

    return pack_index.to(device), rank_in_pack.to(device)
```

The error might have come from copy-pasting whitespace issues or hidden characters. I will be extra careful. I will reduce the context if possible, but the `balanced_packing` function is the one that needs changing.

To minimize risk, I will replace the inner loop body starting from `# Tuning constants` down to `return pack_index.to(device), rank_in_pack.to(device)`.

Actually, to implement the "3-Way Cyclic Swap", I need to add a block of code.

Here is the plan for the new code:
1. Increase `NUM_RESTARTS` to 50.
2. Inside the refinement loop, after pairwise swaps, insert a 3-way cyclic swap attempt.
   - Cycle: Max Pack (A) -> Pack B -> Pack C -> Max Pack (A).
   - Move item `a` from A to B, `b` from B to C, `c` from C to A.
   - Constraints: A is the max weight pack. We want to reduce weight of A. So `w_a` leaving A is good. `w_c` entering A must be smaller than `w_a`. `w_b` entering C must be okay for C (C is usually the min pack or random).
3. Enhance LNS to use `random.shuffle` in some attempts instead of just jittered LPT, to explore completely different packings.

Let's construct the replacement block.

```python
    # Tuning constants
    NUM_RESTARTS = 50
    NUM_SWAP_ITER = 10
    NUM_CYCLE_ITER = 10
    NUM_LNS_ITER = 30

    all_packs = list(range(num_packs))

    for i in range(num_layers):
        layer_w = weight_cpu[i]
        w_list = layer_w.tolist()

        # Base LPT sort order
        sorted_indices_base = sorted(range(num_groups), key=lambda x: w_list[x], reverse=True)

        best_max_load = float('inf')
        best_assignment = None # List[List[int]]
        best_pack_weights = None # List[float]

        # 1. Randomized Greedy Construction
        for attempt in range(NUM_RESTARTS):
            if attempt == 0:
                indices = sorted_indices_base
            else:
                # Add noise and resort
                noise_scale = 0.15
                noisy_w = [w * (1.0 + (random.random() - 0.5) * noise_scale) for w in w_list]
                indices = sorted(range(num_groups), key=lambda x: noisy_w[x], reverse=True)

            current_assignment = [[] for _ in range(num_packs)]
            current_loads = [0.0] * num_packs
            current_counts = [0] * num_packs

            # Pack items
            for g_idx in indices:
                w = w_list[g_idx]

                # Best Fit
                best_p = -1
                min_load = float('inf')

                for p in range(num_packs):
                    if current_counts[p] < groups_per_pack:
                        if current_loads[p] < min_load:
                            min_load = current_loads[p]
                            best_p = p

                current_assignment[best_p].append(g_idx)
                current_loads[best_p] += w
                current_counts[best_p] += 1

            max_l = max(current_loads)
            if max_l < best_max_load:
                best_max_load = max_l
                best_assignment = current_assignment
                best_pack_weights = current_loads

        # Restore best greedy state
        packs = best_assignment
        pack_weights = best_pack_weights

        # 2. Refinement: Pairwise Swaps + 3-Way Cyclic Swaps
        for _ in range(NUM_SWAP_ITER):
            # Identify max pack
            max_p = max(range(num_packs), key=lambda x: pack_weights[x])
            max_w = pack_weights[max_p]

            u_nodes = packs[max_p]
            w_u = layer_w[u_nodes]

            # Vectorized Pairwise Swap
            best_swap = None
            best_gain = 0.0

            for p in range(num_packs):
                if p == max_p: continue

                diff = max_w - pack_weights[p]
                if diff < 1e-6: continue

                v_nodes = packs[p]
                w_v = layer_w[v_nodes]

                deltas = w_u.unsqueeze(1) - w_v.unsqueeze(0)
                mask = (deltas > 1e-6) & (deltas < diff)

                if not mask.any(): continue

                gains = deltas * (diff - deltas)
                gains = torch.where(mask, gains, -1.0)

                curr_max, curr_argmax = gains.reshape(-1).max(0)
                if curr_max.item() > best_gain:
                    best_gain = curr_max.item()
                    idx = curr_argmax.item()
                    best_swap = (max_p, p, idx // len(v_nodes), idx % len(v_nodes), deltas.reshape(-1)[idx].item())

            if best_swap:
                p1, p2, i1, i2, delta = best_swap
                val1 = packs[p1][i1]
                val2 = packs[p2][i2]
                packs[p1][i1] = val2
                packs[p2][i2] = val1
                pack_weights[p1] -= delta
                pack_weights[p2] += delta
                continue # If swap succeeded, reiterate to check new max

            # If no pairwise swap improved max_p, try 3-Way Cyclic Swap: A -> B -> C -> A
            # A is max_p.
            # We want w_A_new < w_A. i.e. w_out_A > w_in_A (from C).
            # B receives from A. w_B_new should not exceed original Max.
            # C receives from B, gives to A.

            if num_packs < 3: break

            improved_cycle = False
            # Try a limited number of random B and C packs to save time, or iterate heaviest/lightest
            sorted_packs = sorted(range(num_packs), key=lambda x: pack_weights[x], reverse=True)
            # A is sorted_packs[0] (max_p)

            # Candidates for B and C: Try lightest packs to dump weight into, or random
            candidates = sorted_packs[-5:] # Lightest 5

            for p_b in candidates:
                if p_b == max_p: continue
                for p_c in candidates:
                    if p_c == max_p or p_c == p_b: continue

                    # Cycle: A(i) -> B, B(j) -> C, C(k) -> A
                    # Effect on Weights:
                    # A: -w_i + w_k
                    # B: +w_i - w_j
                    # C: +w_j - w_k

                    # Constraints:
                    # 1. w_k < w_i (A decreases)
                    # 2. w_B + w_i - w_j < w_A (B doesn't become new global max)
                    # 3. w_C + w_j - w_k < w_A (C doesn't become new global max)
                    # Objective: maximize decrease in A, i.e., maximize (w_i - w_k)

                    # To do this efficiently without N^3 loops:
                    # iterate i in A.
                    # iterate j in B.
                    # iterate k in C.
                    # Since N per pack is small (groups_per_pack), this is feasible.

                    nodes_a = packs[max_p]
                    nodes_b = packs[p_b]
                    nodes_c = packs[p_c]

                    w_a_vals = [w_list[x] for x in nodes_a]
                    w_b_vals = [w_list[x] for x in nodes_b]
                    w_c_vals = [w_list[x] for x in nodes_c]

                    curr_w_a = pack_weights[max_p]
                    curr_w_b = pack_weights[p_b]
                    curr_w_c = pack_weights[p_c]

                    best_cycle = None
                    best_cycle_gain = 0.0

                    for idx_a, val_a in enumerate(w_a_vals):
                        for idx_b, val_b in enumerate(w_b_vals):
                            # Check B constraint early
                            if curr_w_b + val_a - val_b >= curr_w_a - 1e-6: continue

                            for idx_c, val_c in enumerate(w_c_vals):
                                # Check A constraint
                                gain_a = val_a - val_c
                                if gain_a <= 1e-6: continue

                                # Check C constraint
                                if curr_w_c + val_b - val_c >= curr_w_a - 1e-6: continue

                                # Valid cycle. Optimize for gain_a
                                if gain_a > best_cycle_gain:
                                    best_cycle_gain = gain_a
                                    best_cycle = (idx_a, idx_b, idx_c, val_a, val_b, val_c)

                    if best_cycle:
                        ia, ib, ic, va, vb, vc = best_cycle
                        # Apply
                        item_a = packs[max_p][ia]
                        item_b = packs[p_b][ib]
                        item_c = packs[p_c][ic]

                        packs[p_b][ib] = item_a # A -> B (replacing j with i? No, insert i, remove j)
                        # Wait, list replacement logic:
                        # A has items... remove item_a, add item_c
                        # B has items... remove item_b, add item_a
                        # C has items... remove item_c, add item_b

                        packs[max_p][ia] = item_c
                        packs[p_b][ib] = item_a
                        packs[p_c][ic] = item_b

                        pack_weights[max_p] += (vc - va)
                        pack_weights[p_b] += (va - vb)
                        pack_weights[p_c] += (vb - vc)

                        improved_cycle = True
                        break # Break inner C loop
                if improved_cycle: break # Break inner B loop

            if not improved_cycle and not best_swap:
                break


        # 3. Refinement: LNS (Ruin & Recreate)
        if num_packs >= 3:
            for _ in range(NUM_LNS_ITER):
                sorted_p = sorted(range(num_packs), key=pack_weights.__getitem__, reverse=True)
                p_max = sorted_p[0]
                p_min = sorted_p[-1]

                if pack_weights[p_max] - pack_weights[p_min] < 1e-6: break

                # Pick 3rd pack
                if _ % 3 == 0: p_mid = sorted_p[1]
                elif _ % 3 == 1: p_mid = sorted_p[random.randint(1, num_packs-1)]
                else: p_mid = sorted_p[random.randint(1, num_packs-1)]

                if p_mid == p_max or p_mid == p_min:
                    candidates = [x for x in all_packs if x != p_max and x != p_min]
                    if not candidates: break
                    p_mid = random.choice(candidates)

                target_packs = [p_max, p_min, p_mid]

                items = []
                for p in target_packs:
                    items.extend(packs[p])

                best_sub_res = None

                # Try LPT and Random Shuffle
                strategies = ['lpt', 'lpt_noise', 'shuffle', 'shuffle']

                curr_local_max = max(pack_weights[p] for p in target_packs)
                curr_local_ss = sum(pack_weights[p]**2 for p in target_packs)

                for strat in strategies:
                    if strat == 'lpt':
                        temp_items = sorted(items, key=lambda x: w_list[x], reverse=True)
                    elif strat == 'lpt_noise':
                        temp_items = sorted(items, key=lambda x: w_list[x] * (1.0 + (random.random()-0.5)*0.1), reverse=True)
                    else: # shuffle
                        temp_items = list(items)
                        random.shuffle(temp_items)

                    sub_p = {p: [] for p in target_packs}
                    sub_w = {p: 0.0 for p in target_packs}
                    sub_c = {p: 0 for p in target_packs}

                    possible = True
                    for item in temp_items:
                        w = w_list[item]
                        best_p = -1
                        min_val = float('inf')
                        # Best fit
                        for p in target_packs:
                            if sub_c[p] < groups_per_pack:
                                if sub_w[p] < min_val:
                                    min_val = sub_w[p]
                                    best_p = p
                        if best_p == -1:
                            possible = False; break
                        sub_p[best_p].append(item)
                        sub_w[best_p] += w
                        sub_c[best_p] += 1

                    if possible:
                        new_max = max(sub_w.values())
                        new_ss = sum(v**2 for v in sub_w.values())

                        if new_max < curr_local_max - 1e-6:
                            curr_local_max = new_max
                            curr_local_ss = new_ss
                            best_sub_res = (sub_p, sub_w)
                        elif abs(new_max - curr_local_max) < 1e-6 and new_ss < curr_local_ss - 1e-6:
                            curr_local_ss = new_ss
                            best_sub_res = (sub_p, sub_w)

                if best_sub_res:
                    s_p, s_w = best_sub_res
                    for p in target_packs:
                        packs[p] = s_p[p]
                        pack_weights[p] = s_w[p]

        # Final write to output tensors
        for p in range(num_packs):
            for r, g_idx in enumerate(packs[p]):
                pack_index[i, g_idx] = p
                rank_in_pack[i, g_idx] = r

    return pack_index.to(device), rank_in_pack.to(device)
```

I will grab the code chunk from `# Tuning constants` in the original file to `return pack_index.to(device), rank_in_pack.to(device)` and replace it.

Original snippet starts at line 46 in the file provided in previous turn.
```python
    # Tuning constants
    NUM_RESTARTS = 20
    NUM_SWAP_ITER = 15
    NUM_LNS_ITER = 20

    all_packs = list(range(num_packs))

    for i in range(num_layers):
...
    return pack_index.to(device), rank_in_pack.to(device)
```

This seems correct and robust.

<NAME>
three_way_cycle_lns_enhanced
</NAME>

<DESCRIPTION>
1. **Three-Way Cyclic Swaps**: Added a logic phase that attempts to shift items in a circle (Max Pack -> Pack B -> Pack C -> Max Pack) to reduce the load of the Max Pack. This addresses "jammed" states where no single pairwise swap is valid.
2. **Enhanced LNS**: Modified Large Neighborhood Search to use both "jittered LPT" and "Random Shuffle" sorting strategies. LPT is good for packing big rocks, but random shuffle can sometimes find better fits for the "sand" (smaller items) when LPT gets stuck in a pattern.
3. **Increased Restarts**: Increased the initial randomized greedy restarts to 50 to better sample the search space initially.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Tuning constants
    NUM_RESTARTS = 20
    NUM_SWAP_ITER = 15
    NUM_LNS_ITER = 20

    all_packs = list(range(num_packs))

    for i in range(num_layers):
        layer_w = weight_cpu[i]
        w_list = layer_w.tolist()

        # Base LPT sort order
        sorted_indices_base = sorted(range(num_groups), key=lambda x: w_list[x], reverse=True)

        best_max_load = float('inf')
        best_assignment = None # List[List[int]]
        best_pack_weights = None # List[float]

        # 1. Randomized Greedy Construction
        for attempt in range(NUM_RESTARTS):
            # Candidate order generation
            if attempt == 0:
                indices = sorted_indices_base
            else:
                # Add noise and resort. Noise factor 0.1 means weights vary by +/- 5%
                noise_scale = 0.1
                noisy_w = [w * (1.0 + (random.random() - 0.5) * noise_scale) for w in w_list]
                indices = sorted(range(num_groups), key=lambda x: noisy_w[x], reverse=True)

            current_assignment = [[] for _ in range(num_packs)]
            current_loads = [0.0] * num_packs
            current_counts = [0] * num_packs

            # Pack items
            for g_idx in indices:
                w = w_list[g_idx]

                # Best Fit: Find pack with min load among those with space
                best_p = -1
                min_load = float('inf')

                # Linear scan is fast enough for small num_packs (e.g. < 64)
                # Optimized logic: unroll slightly or just iterate
                for p in range(num_packs):
                    if current_counts[p] < groups_per_pack:
                        if current_loads[p] < min_load:
                            min_load = current_loads[p]
                            best_p = p

                current_assignment[best_p].append(g_idx)
                current_loads[best_p] += w
                current_counts[best_p] += 1

            max_l = max(current_loads)
            if max_l < best_max_load:
                best_max_load = max_l
                best_assignment = current_assignment
                best_pack_weights = current_loads

        # Restore best greedy state
        packs = best_assignment
        pack_weights = best_pack_weights

        # 2. Refinement: Vectorized Pairwise Swaps
        # Repeatedly optimize the heaviest pack against others
        for _ in range(NUM_SWAP_ITER):
            # Identify max pack
            max_p = -1
            max_w = -1.0
            for p in range(num_packs):
                if pack_weights[p] > max_w:
                    max_w = pack_weights[p]
                    max_p = p

            u_nodes = packs[max_p]
            w_u = layer_w[u_nodes] # Tensor indexing on CPU

            best_swap = None
            best_gain = 0.0

            # Scan other packs for improvement
            for p in range(num_packs):
                if p == max_p: continue

                diff = max_w - pack_weights[p]
                if diff < 1e-6: continue

                v_nodes = packs[p]
                w_v = layer_w[v_nodes]

                # Calculate potential swaps
                # deltas[i, j] = w_u[i] - w_v[j]
                # We want 0 < delta < diff to strictly reduce max_p and increase p without p exceeding original max_p
                # Actually, we want to minimize max(w_max - delta, w_p + delta)
                deltas = w_u.unsqueeze(1) - w_v.unsqueeze(0)
                mask = (deltas > 1e-6) & (deltas < diff)

                if not mask.any(): continue

                # Gain function: maximize delta * (diff - delta)
                # This balances minimizing the new max(u, v) and variance
                gains = deltas * (diff - deltas)
                gains = torch.where(mask, gains, -1.0)

                curr_max, curr_argmax = gains.reshape(-1).max(0)
                if curr_max.item() > best_gain:
                    best_gain = curr_max.item()
                    idx = curr_argmax.item()
                    best_swap = (max_p, p, idx // len(v_nodes), idx % len(v_nodes), deltas.reshape(-1)[idx].item())

            if best_swap:
                p1, p2, i1, i2, delta = best_swap
                # Perform swap
                val1 = packs[p1][i1]
                val2 = packs[p2][i2]
                packs[p1][i1] = val2
                packs[p2][i2] = val1
                pack_weights[p1] -= delta
                pack_weights[p2] += delta
            else:
                break

        # 3. Refinement: 3-Way Large Neighborhood Search (Ruin & Recreate)
        # Select Max, Min, and a 3rd pack to shuffle items
        if num_packs >= 3:
            for _ in range(NUM_LNS_ITER):
                # Identify key packs
                sorted_p = sorted(range(num_packs), key=pack_weights.__getitem__, reverse=True)
                p_max = sorted_p[0]
                p_min = sorted_p[-1]

                if pack_weights[p_max] - pack_weights[p_min] < 1e-6: break

                # Pick 3rd pack: Alternating between 2nd heaviest and Random
                if _ % 2 == 0:
                    p_mid = sorted_p[1]
                else:
                    candidates = [x for x in all_packs if x != p_max and x != p_min]
                    if not candidates: break
                    p_mid = random.choice(candidates)

                target_packs = [p_max, p_min, p_mid]

                # Gather items
                items = []
                for p in target_packs:
                    items.extend(packs[p])

                # Sort items LPT
                items.sort(key=lambda x: w_list[x], reverse=True)

                curr_local_max = max(pack_weights[p] for p in target_packs)
                curr_local_ss = sum(pack_weights[p]**2 for p in target_packs)

                best_sub_res = None

                # Small randomized search on subset (1 deterministic + 4 randomized)
                for sub_attempt in range(5):
                    if sub_attempt > 0:
                         # Jitter weights slightly to break ties/order
                         temp_items = sorted(items, key=lambda x: w_list[x] * (1.0 + (random.random()-0.5)*0.1), reverse=True)
                    else:
                        temp_items = items

                    sub_p = {p: [] for p in target_packs}
                    sub_w = {p: 0.0 for p in target_packs}
                    sub_c = {p: 0 for p in target_packs}

                    # Simple Greedy Loop for 3 packs
                    possible = True
                    for item in temp_items:
                        w = w_list[item]
                        # Find valid pack with min weight among targets
                        best_p = -1
                        min_val = float('inf')
                        for p in target_packs:
                            if sub_c[p] < groups_per_pack:
                                if sub_w[p] < min_val:
                                    min_val = sub_w[p]
                                    best_p = p

                        if best_p == -1:
                            possible = False; break

                        sub_p[best_p].append(item)
                        sub_w[best_p] += w
                        sub_c[best_p] += 1

                    if possible:
                        new_max = max(sub_w.values())
                        new_ss = sum(v**2 for v in sub_w.values())

                        # Accept if strictly better max, or same max with better variance
                        if new_max < curr_local_max - 1e-6:
                            curr_local_max = new_max
                            curr_local_ss = new_ss
                            best_sub_res = (sub_p, sub_w)
                        elif abs(new_max - curr_local_max) < 1e-6 and new_ss < curr_local_ss - 1e-6:
                            curr_local_ss = new_ss
                            best_sub_res = (sub_p, sub_w)

                if best_sub_res:
                    s_p, s_w = best_sub_res
                    for p in target_packs:
                        packs[p] = s_p[p]
                        pack_weights[p] = s_w[p]

        # Final write to output tensors
        for p in range(num_packs):
            for r, g_idx in enumerate(packs[p]):
                pack_index[i, g_idx] = p
                rank_in_pack[i, g_idx] = r

    return pack_index.to(device), rank_in_pack.to(device)
=======
    # Tuning constants
    NUM_RESTARTS = 50
    NUM_SWAP_ITER = 10
    NUM_LNS_ITER = 30

    all_packs = list(range(num_packs))

    for i in range(num_layers):
        layer_w = weight_cpu[i]
        w_list = layer_w.tolist()

        # Base LPT sort order
        sorted_indices_base = sorted(range(num_groups), key=lambda x: w_list[x], reverse=True)

        best_max_load = float('inf')
        best_assignment = None # List[List[int]]
        best_pack_weights = None # List[float]

        # 1. Randomized Greedy Construction
        for attempt in range(NUM_RESTARTS):
            # Candidate order generation
            if attempt == 0:
                indices = sorted_indices_base
            else:
                # Add noise and resort. Noise factor 0.15 for more variance
                noise_scale = 0.15
                noisy_w = [w * (1.0 + (random.random() - 0.5) * noise_scale) for w in w_list]
                indices = sorted(range(num_groups), key=lambda x: noisy_w[x], reverse=True)

            current_assignment = [[] for _ in range(num_packs)]
            current_loads = [0.0] * num_packs
            current_counts = [0] * num_packs

            # Pack items
            for g_idx in indices:
                w = w_list[g_idx]

                # Best Fit: Find pack with min load among those with space
                best_p = -1
                min_load = float('inf')

                for p in range(num_packs):
                    if current_counts[p] < groups_per_pack:
                        if current_loads[p] < min_load:
                            min_load = current_loads[p]
                            best_p = p

                current_assignment[best_p].append(g_idx)
                current_loads[best_p] += w
                current_counts[best_p] += 1

            max_l = max(current_loads)
            if max_l < best_max_load:
                best_max_load = max_l
                best_assignment = current_assignment
                best_pack_weights = current_loads

        # Restore best greedy state
        packs = best_assignment
        pack_weights = best_pack_weights

        # 2. Refinement: Pairwise Swaps + 3-Way Cyclic Swaps
        for _ in range(NUM_SWAP_ITER):
            # Identify max pack
            max_p = max(range(num_packs), key=lambda x: pack_weights[x])
            max_w = pack_weights[max_p]

            u_nodes = packs[max_p]
            w_u = layer_w[u_nodes]

            # Vectorized Pairwise Swap
            best_swap = None
            best_gain = 0.0

            for p in range(num_packs):
                if p == max_p: continue

                diff = max_w - pack_weights[p]
                if diff < 1e-6: continue

                v_nodes = packs[p]
                w_v = layer_w[v_nodes]

                # deltas[i, j] = w_u[i] - w_v[j]
                deltas = w_u.unsqueeze(1) - w_v.unsqueeze(0)
                mask = (deltas > 1e-6) & (deltas < diff)

                if not mask.any(): continue

                # Gain: maximize delta * (diff - delta)
                gains = deltas * (diff - deltas)
                gains = torch.where(mask, gains, -1.0)

                curr_max, curr_argmax = gains.reshape(-1).max(0)
                if curr_max.item() > best_gain:
                    best_gain = curr_max.item()
                    idx = curr_argmax.item()
                    best_swap = (max_p, p, idx // len(v_nodes), idx % len(v_nodes), deltas.reshape(-1)[idx].item())

            if best_swap:
                p1, p2, i1, i2, delta = best_swap
                val1 = packs[p1][i1]
                val2 = packs[p2][i2]
                packs[p1][i1] = val2
                packs[p2][i2] = val1
                pack_weights[p1] -= delta
                pack_weights[p2] += delta
                continue # If swap succeeded, reiterate from top

            # 3-Way Cyclic Swap: A (Max) -> B -> C -> A
            if num_packs < 3: break

            improved_cycle = False
            # Sort packs by weight to pick targets (B and C)
            sorted_packs = sorted(range(num_packs), key=lambda x: pack_weights[x], reverse=True)
            # Try 5 lightest packs as candidates for B and C
            candidates = sorted_packs[-5:]

            for p_b in candidates:
                if p_b == max_p: continue
                for p_c in candidates:
                    if p_c == max_p or p_c == p_b: continue

                    nodes_a = packs[max_p]
                    nodes_b = packs[p_b]
                    nodes_c = packs[p_c]

                    w_a_vals = [w_list[x] for x in nodes_a]
                    w_b_vals = [w_list[x] for x in nodes_b]
                    w_c_vals = [w_list[x] for x in nodes_c]

                    curr_w_a = pack_weights[max_p]
                    curr_w_b = pack_weights[p_b]
                    curr_w_c = pack_weights[p_c]

                    best_cycle = None
                    best_cycle_gain = 0.0

                    # Iterate items to find best cycle
                    # Cycle: A(i) -> B, B(j) -> C, C(k) -> A
                    for idx_a, val_a in enumerate(w_a_vals):
                        for idx_b, val_b in enumerate(w_b_vals):
                            # Ensure B doesn't blow up
                            if curr_w_b + val_a - val_b >= curr_w_a - 1e-6: continue

                            for idx_c, val_c in enumerate(w_c_vals):
                                # Ensure A reduces
                                gain_a = val_a - val_c
                                if gain_a <= 1e-6: continue

                                # Ensure C doesn't blow up
                                if curr_w_c + val_b - val_c >= curr_w_a - 1e-6: continue

                                if gain_a > best_cycle_gain:
                                    best_cycle_gain = gain_a
                                    best_cycle = (idx_a, idx_b, idx_c, val_a, val_b, val_c)

                    if best_cycle:
                        ia, ib, ic, va, vb, vc = best_cycle
                        # Perform Cycle
                        item_a = packs[max_p][ia]
                        item_b = packs[p_b][ib]
                        item_c = packs[p_c][ic]

                        packs[max_p][ia] = item_c
                        packs[p_b][ib] = item_a
                        packs[p_c][ic] = item_b

                        pack_weights[max_p] += (vc - va)
                        pack_weights[p_b] += (va - vb)
                        pack_weights[p_c] += (vb - vc)

                        improved_cycle = True
                        break # Break inner C loop
                if improved_cycle: break # Break inner B loop

            if not improved_cycle:
                break

        # 3. Refinement: Enhanced LNS (Ruin & Recreate)
        if num_packs >= 3:
            for _ in range(NUM_LNS_ITER):
                sorted_p = sorted(range(num_packs), key=pack_weights.__getitem__, reverse=True)
                p_max = sorted_p[0]
                p_min = sorted_p[-1]

                if pack_weights[p_max] - pack_weights[p_min] < 1e-6: break

                # Pick 3rd pack: Mix of Top, Bottom, and Random
                if _ % 3 == 0: p_mid = sorted_p[1]
                elif _ % 3 == 1: p_mid = sorted_p[random.randint(1, num_packs-1)]
                else: p_mid = sorted_p[random.randint(1, num_packs-1)]

                if p_mid == p_max or p_mid == p_min:
                    candidates = [x for x in all_packs if x != p_max and x != p_min]
                    if not candidates: break
                    p_mid = random.choice(candidates)

                target_packs = [p_max, p_min, p_mid]

                items = []
                for p in target_packs:
                    items.extend(packs[p])

                best_sub_res = None

                # Try LPT, Jittered LPT, and Random Shuffle
                strategies = ['lpt', 'lpt_noise', 'shuffle', 'shuffle']

                curr_local_max = max(pack_weights[p] for p in target_packs)
                curr_local_ss = sum(pack_weights[p]**2 for p in target_packs)

                for strat in strategies:
                    if strat == 'lpt':
                        temp_items = sorted(items, key=lambda x: w_list[x], reverse=True)
                    elif strat == 'lpt_noise':
                        temp_items = sorted(items, key=lambda x: w_list[x] * (1.0 + (random.random()-0.5)*0.1), reverse=True)
                    else:
                        temp_items = list(items)
                        random.shuffle(temp_items)

                    sub_p = {p: [] for p in target_packs}
                    sub_w = {p: 0.0 for p in target_packs}
                    sub_c = {p: 0 for p in target_packs}

                    possible = True
                    for item in temp_items:
                        w = w_list[item]
                        best_p = -1
                        min_val = float('inf')
                        # Best fit
                        for p in target_packs:
                            if sub_c[p] < groups_per_pack:
                                if sub_w[p] < min_val:
                                    min_val = sub_w[p]
                                    best_p = p
                        if best_p == -1:
                            possible = False; break
                        sub_p[best_p].append(item)
                        sub_w[best_p] += w
                        sub_c[best_p] += 1

                    if possible:
                        new_max = max(sub_w.values())
                        new_ss = sum(v**2 for v in sub_w.values())

                        if new_max < curr_local_max - 1e-6:
                            curr_local_max = new_max
                            curr_local_ss = new_ss
                            best_sub_res = (sub_p, sub_w)
                        elif abs(new_max - curr_local_max) < 1e-6 and new_ss < curr_local_ss - 1e-6:
                            curr_local_ss = new_ss
                            best_sub_res = (sub_p, sub_w)

                if best_sub_res:
                    s_p, s_w = best_sub_res
                    for p in target_packs:
                        packs[p] = s_p[p]
                        pack_weights[p] = s_w[p]

        # Final write to output tensors
        for p in range(num_packs):
            for r, g_idx in enumerate(packs[p]):
                pack_index[i, g_idx] = p
                rank_in_pack[i, g_idx] = r

    return pack_index.to(device), rank_in_pack.to(device)
>>>>>>> REPLACE
</DIFF>