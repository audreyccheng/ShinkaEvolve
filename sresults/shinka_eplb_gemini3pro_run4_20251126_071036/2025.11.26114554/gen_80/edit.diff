--- a/original.py
+++ b/original.py
@@ -1,454 +1,452 @@
 # EVOLVE-BLOCK-START
 """
 Expert parallelism load balancer (EPLB) for vLLM.
 
 This module implements the core rearrangement algorithm.
 
 The rearrangement algorithm is adapted from
 [DeepSeek EPLB](https://github.com/deepseek-ai/eplb).
 
 Please find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example
 on how the EPLB algorithm works.
 """
 
 import torch
+import random
 
 
 def balanced_packing(weight: torch.Tensor,
                      num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
     """
     Pack n weighted objects to m packs, such that each bin contains exactly
     n/m objects and the weights of all packs are as balanced as possible.
-
-    Algorithm: Randomized Greedy + Pairwise Swap Descent + 3-Way LNS
 
     Parameters:
         weight: [X, n], the weight of each item
         num_packs: number of packs
 
     Returns:
         pack_index: [X, n], the pack index of each item
         rank_in_pack: [X, n], the rank of the item in the pack
     """
     num_layers, num_groups = weight.shape
     assert num_groups % num_packs == 0
     groups_per_pack = num_groups // num_packs
 
     device = weight.device
     if groups_per_pack == 1:
         pack_index = torch.arange(weight.size(-1),
                                   dtype=torch.int64,
                                   device=device).expand(weight.shape)
         rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
         return pack_index, rank_in_pack
 
-    # Use CPU weights
+    # Use CPU weights for efficient scalar iteration
     weight_cpu = weight.cpu()
 
-    # Outputs on CPU to avoid device synchronization during fill
+    # Pre-allocate output tensors
     pack_index = torch.empty((num_layers, num_groups), dtype=torch.int64)
     rank_in_pack = torch.empty((num_layers, num_groups), dtype=torch.int64)
 
-    # Tuning constants
-    NUM_RESTARTS = 5
+    # Cache random function
+    rand_func = random.random
 
     for i in range(num_layers):
         layer_w = weight_cpu[i]
-        layer_w_list = layer_w.tolist()
-
+        w_list = layer_w.tolist()
+
+        # --- 1. Initial Construction (Randomized Greedy) ---
+        best_assignment = None
         best_max_load = float('inf')
-        best_sum_sq = float('inf')
-        best_assignment = None
-
-        for attempt in range(NUM_RESTARTS):
-            # 1. Candidate Generation
+        best_ss = float('inf')
+
+        # Run multiple construction attempts
+        NUM_CONSTRUCT = 10
+        
+        # Base indices (sorted descending)
+        base_indices = sorted(range(num_groups), key=lambda x: w_list[x], reverse=True)
+
+        for attempt in range(NUM_CONSTRUCT):
             if attempt == 0:
-                indices = sorted(range(num_groups), key=lambda x: layer_w_list[x], reverse=True)
+                indices = base_indices
             else:
-                # Randomized LPT
-                noise = torch.rand(num_groups) * 0.15 + 0.925
-                indices = (layer_w * noise).argsort(descending=True).tolist()
-
-            # 2. Greedy Construction (LPT/Best Fit)
-            packs = [[] for _ in range(num_packs)]
-            pack_weights = [0.0] * num_packs
-            pack_counts = [0] * num_packs
-
-            for g_idx in indices:
-                w = layer_w_list[g_idx]
-                # Best Fit: Assign to valid pack with min load
+                # Randomized LPT: perturb weights
+                scale = 0.05 + 0.2 * rand_func()
+                indices = sorted(range(num_groups), 
+                                 key=lambda x: w_list[x] * (1.0 + (rand_func() - 0.5) * scale), 
+                                 reverse=True)
+
+            current_packs = [[] for _ in range(num_packs)]
+            current_weights = [0.0] * num_packs
+            current_counts = [0] * num_packs
+
+            # Greedy Best Fit
+            for idx in indices:
+                w = w_list[idx]
                 best_p = -1
                 min_w = float('inf')
-
+                
+                # Find valid pack with min weight
                 for p in range(num_packs):
-                    if pack_counts[p] < groups_per_pack:
-                        if pack_weights[p] < min_w:
-                            min_w = pack_weights[p]
+                    if current_counts[p] < groups_per_pack:
+                        if current_weights[p] < min_w:
+                            min_w = current_weights[p]
                             best_p = p
-
-                packs[best_p].append(g_idx)
-                pack_weights[best_p] += w
-                pack_counts[best_p] += 1
-
-            # 3. Refinement: Pairwise Swap + LNS
-            MAX_REFINE_STEPS = 100 if num_packs > 1 else 0
-
-            # Helper to find max/min packs
-            def get_stats(pw):
-                max_w = -1.0
-                min_w = float('inf')
-                max_p = -1
-                min_p = -1
-                for p_idx, w_val in enumerate(pw):
-                    if w_val > max_w:
-                        max_w = w_val
-                        max_p = p_idx
-                    if w_val < min_w:
-                        min_w = w_val
-                        min_p = p_idx
-                return max_p, min_p, max_w, min_w
-
-            for step in range(MAX_REFINE_STEPS):
-                max_p, min_p, max_w, min_w = get_stats(pack_weights)
-
-                if max_p == min_p or (max_w - min_w < 1e-6):
-                    break
-
-                improved_action = False
-
-                # A. Pairwise Swap Descent
-                # Try to swap item from max_p with item from any other pack p to reduce max_p's load
-                # The constraint is that the other pack p must not become heavier than max_p's *current* load (or better, the new load)
-
-                # We want: w(u) - w(v) > 0  (reduce max_p)
-                # And: pack_weights[p] + (w(u) - w(v)) < max_w  (ideally < max_w - epsilon)
-                # Or more strictly to ensure descent: pack_weights[p] + (w(u) - w(v)) < max_w - (w(u) - w(v)) ?
-                # Actually, we just need the new max of {max_p, p} to be less than old max_w.
-
-                best_swap = None
-                best_gain = 0.0 # Gain = w(u) - w(v). We maximize this subject to constraints.
-
-                # Only check max_p against others
-                u_items = packs[max_p]
-
-                for u_idx, u in enumerate(u_items):
-                    w_u = layer_w_list[u]
-
-                    for p in range(num_packs):
-                        if p == max_p: continue
-
-                        diff_p = max_w - pack_weights[p]
-                        if diff_p < 1e-6: continue # p is already essentially equal to max, can't dump weight there
-
-                        for v_idx, v in enumerate(packs[p]):
-                            w_v = layer_w_list[v]
-                            delta = w_u - w_v
-
-                            # We need delta > 0 to reduce max_p
-                            # We need pack_weights[p] + delta < max_w (strict decrease of global max if max_p was unique max)
-
-                            if delta > 1e-6 and (pack_weights[p] + delta < max_w - 1e-6):
-                                # Valid swap. Does it improve beyond best_gain?
-                                # We prefer larger delta (more reduction in max_p)
-                                if delta > best_gain:
-                                    best_gain = delta
-                                    best_swap = (p, u_idx, v_idx, delta)
-
-                if best_swap:
-                    target_p, u_idx, v_idx, delta = best_swap
-                    # Execute Swap
-                    item_u = packs[max_p][u_idx]
-                    item_v = packs[target_p][v_idx]
-
-                    packs[max_p][u_idx] = item_v
-                    packs[target_p][v_idx] = item_u
-
-                    pack_weights[max_p] -= delta
-                    pack_weights[target_p] += delta
-                    improved_action = True
-
-                # B. 3-Way LNS (Ruin & Recreate)
-                # If swaps failed to reduce max, try re-shuffling 3 packs
-                if not improved_action and num_packs >= 3:
-                     # Candidates: Max, Min, Random
-                    candidates = {max_p, min_p}
-                    # Try to find a random pack that is not max or min
-                    for _ in range(5): # retry limit
-                        r = torch.randint(0, num_packs, (1,)).item()
-                        if r not in candidates:
-                            candidates.add(r)
-                            break
-
-                    cand_list = list(candidates)
-                    if len(cand_list) < 3 and num_packs >= 3:
-                         # Could not find distinct 3rd (e.g. only 2 packs total, handled by check above)
-                         pass
-
-                    # Ruin
-                    items = []
-                    for p in cand_list:
-                        items.extend(packs[p])
-
-                    # Recreate
-                    current_sub_max = max(pack_weights[p] for p in cand_list)
-                    current_sub_ss = sum(pack_weights[p]**2 for p in cand_list)
-
-                    best_sub_res = None
-
-                    # Sort items
-                    items_sorted = sorted(items, key=lambda x: layer_w_list[x], reverse=True)
-
-                    # Few restarts for sub-problem
-                    for sub_attempt in range(15):
-                        if sub_attempt == 0:
-                            # Deterministic LPT
-                            cur_items = items_sorted
-                        else:
-                            # Perturbed
-                            noise = [1.0 + (torch.rand(1).item() - 0.5) * 0.2 for _ in items]
-                            cur_items = sorted(items, key=lambda x: layer_w_list[x] * noise[items.index(x)], reverse=True)
-
-                        # Greedy
-                        t_w = {p: 0.0 for p in cand_list}
-                        t_p = {p: [] for p in cand_list}
-                        t_c = {p: 0 for p in cand_list}
-
-                        possible = True
-                        for item in cur_items:
-                            w = layer_w_list[item]
-                            best_local = -1
-                            min_local = float('inf')
-                            for p in cand_list:
-                                if t_c[p] < groups_per_pack:
-                                    if t_w[p] < min_local:
-                                        min_local = t_w[p]
-                                        best_local = p
-
-                            if best_local == -1:
-                                possible = False; break
-                            t_p[best_local].append(item)
-                            t_w[best_local] += w
-                            t_c[best_local] += 1
-
-                        if possible:
-                            n_max = max(t_w.values())
-                            n_ss = sum(v**2 for v in t_w.values())
-
-                            if n_max < current_sub_max - 1e-6 or (abs(n_max - current_sub_max) < 1e-6 and n_ss < current_sub_ss - 1e-6):
-                                current_sub_max = n_max
-                                current_sub_ss = n_ss
-                                best_sub_res = (t_p, t_w)
-
-                    if best_sub_res:
-                        s_p, s_w = best_sub_res
-                        for p in cand_list:
-                            packs[p] = s_p[p]
-                            pack_weights[p] = s_w[p]
-                        improved_action = True
-
-                if not improved_action:
-                    break
-
-            # 4. Evaluation
-            current_max = max(pack_weights)
-            current_ss = sum(w*w for w in pack_weights)
-
-            if current_max < best_max_load - 1e-6:
-                best_max_load = current_max
-                best_sum_sq = current_ss
-                best_assignment = [list(p) for p in packs]
-            elif abs(current_max - best_max_load) < 1e-6 and current_ss < best_sum_sq - 1e-6:
-                best_sum_sq = current_ss
-                best_assignment = [list(p) for p in packs]
-
-        # 5. Final assignment for this layer
-        for p, nodes in enumerate(best_assignment):
-            for r, g_idx in enumerate(nodes):
-                pack_index[i, g_idx] = p
-                rank_in_pack[i, g_idx] = r
+                
+                current_packs[best_p].append(idx)
+                current_weights[best_p] += w
+                current_counts[best_p] += 1
+
+            # Evaluate
+            c_max = max(current_weights)
+            c_ss = sum(x*x for x in current_weights)
+
+            if c_max < best_max_load - 1e-6:
+                best_max_load = c_max
+                best_ss = c_ss
+                best_assignment = current_packs
+            elif abs(c_max - best_max_load) < 1e-6 and c_ss < best_ss - 1e-6:
+                best_ss = c_ss
+                best_assignment = current_packs
+
+        # Set best initial state
+        packs = best_assignment
+        # Recalculate weights to be precise
+        pack_weights = [sum(w_list[x] for x in p) for p in packs]
+
+        # --- 2. Refinement (Hybrid Swap + LNS) ---
+        MAX_REFINE_ITERS = 50
+        
+        for _ in range(MAX_REFINE_ITERS):
+            # Find stats
+            max_p = -1
+            min_p = -1
+            max_val = -1.0
+            min_val = float('inf')
+
+            for p_idx, val in enumerate(pack_weights):
+                if val > max_val:
+                    max_val = val
+                    max_p = p_idx
+                if val < min_val:
+                    min_val = val
+                    min_p = p_idx
+
+            if max_p == min_p or (max_val - min_val < 1e-6):
+                break
+
+            improved_action = False
+
+            # Strategy A: Pairwise Swap (First Improvement)
+            # Try to swap items between Max Pack and lighter packs (starting with Min Pack)
+            
+            # Candidates to swap with: Min Pack, and random others
+            swap_candidates = [min_p]
+            if num_packs > 2:
+                # Add random candidate
+                r = int(rand_func() * num_packs)
+                if r != max_p and r != min_p:
+                    swap_candidates.append(r)
+            
+            for t_p in swap_candidates:
+                t_val = pack_weights[t_p]
+                if max_val - t_val < 1e-6: continue
+
+                # Try simple swaps
+                # Looking for: item u in Max, item v in Target
+                # such that w_u > w_v
+                # and new max < old max (approx)
+                
+                for u_idx, u in enumerate(packs[max_p]):
+                    w_u = w_list[u]
+                    for v_idx, v in enumerate(packs[t_p]):
+                        w_v = w_list[v]
+                        delta = w_u - w_v
+
+                        if delta > 1e-6:
+                            # Verify if swap improves locally
+                            # new_max_p_w = max_val - delta
+                            # new_t_p_w = t_val + delta
+                            # condition: new_t_p_w < max_val (strictly better balance for max)
+                            
+                            if t_val + delta < max_val - 1e-6:
+                                # Execute Swap
+                                packs[max_p][u_idx] = v
+                                packs[t_p][v_idx] = u
+                                pack_weights[max_p] -= delta
+                                pack_weights[t_p] += delta
+                                improved_action = True
+                                break
+                    if improved_action: break
+                if improved_action: break
+            
+            if improved_action:
+                continue
+
+            # Strategy B: Large Neighborhood Search (LNS)
+            lns_packs = [max_p, min_p]
+            if num_packs > 2:
+                # Add a random pack to enable cyclic transfers
+                others = [p for p in range(num_packs) if p not in lns_packs]
+                if others:
+                    lns_packs.append(random.choice(others))
+            
+            current_sub_max = max(pack_weights[p] for p in lns_packs)
+            current_sub_ss = sum(pack_weights[p]**2 for p in lns_packs)
+            
+            items_sub = []
+            for p in lns_packs:
+                items_sub.extend(packs[p])
+            
+            best_sub_res = None
+            found_better_sub = False
+            
+            # Strategies for re-packing
+            strategies = []
+            # 1. Deterministic LPT
+            strategies.append(sorted(items_sub, key=lambda x: w_list[x], reverse=True))
+            
+            # 2. Randomized LPTs
+            for _ in range(10):
+                noise = 0.2 * rand_func()
+                strategies.append(sorted(items_sub, key=lambda x: w_list[x] * (1.0 + (rand_func()-0.5)*noise), reverse=True))
+
+            for item_order in strategies:
+                t_packs = {p: [] for p in lns_packs}
+                t_weights = {p: 0.0 for p in lns_packs}
+                t_cnt = {p: 0 for p in lns_packs}
+                
+                possible = True
+                for item in item_order:
+                    w = w_list[item]
+                    # Best Fit
+                    best_local = -1
+                    min_local = float('inf')
+                    for p in lns_packs:
+                        if t_cnt[p] < groups_per_pack:
+                            if t_weights[p] < min_local:
+                                min_local = t_weights[p]
+                                best_local = p
+                    
+                    if best_local == -1:
+                        possible = False; break
+                    
+                    t_packs[best_local].append(item)
+                    t_weights[best_local] += w
+                    t_cnt[best_local] += 1
+                
+                if possible:
+                    n_max = max(t_weights.values())
+                    n_ss = sum(v**2 for v in t_weights.values())
+                    
+                    if n_max < current_sub_max - 1e-6:
+                        current_sub_max = n_max
+                        current_sub_ss = n_ss
+                        best_sub_res = (t_packs, t_weights)
+                        found_better_sub = True
+                    elif abs(n_max - current_sub_max) < 1e-6 and n_ss < current_sub_ss - 1e-6:
+                        current_sub_ss = n_ss
+                        best_sub_res = (t_packs, t_weights)
+                        found_better_sub = True
+
+            if found_better_sub:
+                s_p, s_w = best_sub_res
+                for p in lns_packs:
+                    packs[p] = s_p[p]
+                    pack_weights[p] = s_w[p]
+                improved_action = True
+            
+            if not improved_action:
+                break
+
+        # 3. Final Output
+        for p_idx, p_items in enumerate(packs):
+            for r_idx, item_idx in enumerate(p_items):
+                pack_index[i, item_idx] = p_idx
+                rank_in_pack[i, item_idx] = r_idx
 
     return pack_index.to(device), rank_in_pack.to(device)
 
 
 def replicate_experts(
         weight: torch.Tensor,
         num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Replicate `num_log` experts to `num_phy` replicas, such that the maximum
     load of all replicas is minimized.
 
     Parameters:
         weight: [X, num_log]
         num_phy: total number of experts after replication
 
     Returns:
         phy2log: [X, num_phy], logical expert id of each physical expert
         rank: [X, num_phy], the replica rank
         logcnt: [X, num_log], number of replicas for each logical expert
     """
     n, num_log = weight.shape
     num_redundant = num_phy - num_log
     assert num_redundant >= 0
     device = weight.device
     phy2log = torch.arange(num_phy, dtype=torch.int64,
                            device=device).repeat(n, 1)
     rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)
     logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)
     arangen = torch.arange(n, dtype=torch.int64, device=device)
     for i in range(num_log, num_phy):
         redundant_indices = (weight / logcnt).max(dim=-1).indices
         phy2log[:, i] = redundant_indices
         rank[:, i] = logcnt[arangen, redundant_indices]
         logcnt[arangen, redundant_indices] += 1
     return phy2log, rank, logcnt
 
 
 def rebalance_experts_hierarchical(
     weight: torch.Tensor,
     num_physical_experts: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ):
     """
     Parameters:
         weight: [num_moe_layers, num_logical_experts]
         num_physical_experts: number of physical experts after replication
         num_groups: number of expert groups
         num_nodes: number of server nodes, where the intra-node network
         (e.g, NVLink) is faster
         num_gpus: number of GPUs, must be a multiple of `num_nodes`
 
     Returns:
         physical_to_logical_map: [num_moe_layers, num_physical_experts]
         logical_to_physical_map: [num_moe_layers, num_logical_experts, X]
         logical_count: [num_moe_layers, num_logical_experts]
     """
     num_layers, num_logical_experts = weight.shape
     assert num_logical_experts % num_groups == 0
     group_size = num_logical_experts // num_groups
     assert num_groups % num_nodes == 0
     groups_per_node = num_groups // num_nodes
     assert num_gpus % num_nodes == 0
     assert num_physical_experts % num_gpus == 0
     phy_experts_per_gpu = num_physical_experts // num_gpus
 
     def inverse(perm: torch.Tensor) -> torch.Tensor:
         inv = torch.empty_like(perm)
         inv.scatter_(
             1,
             perm,
             torch.arange(perm.size(1), dtype=torch.int64,
                          device=perm.device).expand(perm.shape),
         )
         return inv
 
     # Step 1: pack groups to nodes
     tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)
     group_pack_index, group_rank_in_pack = balanced_packing(
         tokens_per_group, num_nodes)
     log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *
                  group_size).unsqueeze(-1) +
                 torch.arange(group_size,
                              dtype=torch.int64,
                              device=group_pack_index.device)).flatten(-2)
     mlog2log = inverse(log2mlog)
 
     # Step 2: construct redundant experts within nodes
     # [num_layers * num_nodes, num_logical_experts // num_nodes]
     tokens_per_mlog = weight.gather(-1, mlog2log).view(
         -1, num_logical_experts // num_nodes)
     phy2mlog, phyrank, mlogcnt = replicate_experts(
         tokens_per_mlog, num_physical_experts // num_nodes)
 
     # Step 3: pack physical_experts to GPUs
     # [num_layers * num_nodes, num_physical_experts // num_nodes]
     tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
     pack_index, rank_in_pack = balanced_packing(tokens_per_phy,
                                                 num_gpus // num_nodes)
     phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
     pphy2phy = inverse(phy2pphy)
 
     pphy2mlog = phy2mlog.gather(
         -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]
     pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(
         0,
         num_logical_experts,
         num_logical_experts // num_nodes,
         device=group_pack_index.device,
     ).view(1, -1, 1)).flatten(-2)
     pphy2log = mlog2log.gather(-1, pphy2mlog)
     pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)
     logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)
     return pphy2log, pphyrank, logcnt
 
 
 def rebalance_experts(
     weight: torch.Tensor,
     num_replicas: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Entry point for expert-parallelism load balancer.
 
     Parameters:
         weight: [layers, num_logical_experts], the load statistics for all
             logical experts
         num_replicas: number of physical experts, must be a multiple of
             `num_gpus`
         num_groups: number of expert groups
         num_nodes: number of server nodes, where the intra-node network
             (e.g, NVLink) is faster
         num_gpus: number of GPUs, must be a multiple of `num_nodes`
 
     Returns:
         physical_to_logical_map: [layers, num_replicas], the expert index of
             each replica
         logical_to_physical_map: [layers, num_logical_experts, X], the replica
             indices for each expert
         expert_count: [layers, num_logical_experts], number of physical
             replicas for each logical expert
     """
     num_layers, num_logical_experts = weight.shape
     weight = weight.float().cpu()
     if num_groups % num_nodes == 0:
         # use hierarchical load-balance policy
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, num_groups, num_nodes, num_gpus)
     else:
         # use global load-balance policy
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, 1, 1, num_gpus)
     num_redundant_experts = num_replicas - num_logical_experts
     maxlogcnt = num_redundant_experts + 1
     log2phy: torch.Tensor = torch.full(
         (num_layers, num_logical_experts, maxlogcnt),
         -1,
         dtype=torch.int64,
         device=logcnt.device,
     )
     log2phy.view(num_layers, -1).scatter_(
         -1,
         phy2log * maxlogcnt + phyrank,
         torch.arange(num_replicas, dtype=torch.int64,
                      device=log2phy.device).expand(num_layers, -1),
     )
     return phy2log, log2phy, logcnt
 
 
 # EVOLVE-BLOCK-END
 
 
 # This part remains fixed (not evolved)
 def run_eplb(weight: torch.Tensor, num_replicas: int, num_groups: int,
              num_nodes: int, num_gpus: int):
     """Run the expert parallelism load balancer"""
     phy2log, log2phy, logcnt = rebalance_experts(
         weight, num_replicas, num_groups, num_nodes, num_gpus
     )
     return phy2log, log2phy, logcnt
 
 
 __all__ = ["rebalance_experts", "run_eplb"]