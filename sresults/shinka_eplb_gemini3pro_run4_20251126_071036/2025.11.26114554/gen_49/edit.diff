--- a/original.py
+++ b/original.py
@@ -1,449 +1,489 @@
 # EVOLVE-BLOCK-START
 """
 Expert parallelism load balancer (EPLB) for vLLM.
 
 This module implements the core rearrangement algorithm.
 
 The rearrangement algorithm is adapted from
 [DeepSeek EPLB](https://github.com/deepseek-ai/eplb).
 
 Please find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example
 on how the EPLB algorithm works.
 """
 
 import torch
 import random
-import math
 
 def balanced_packing(weight: torch.Tensor,
                      num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
     """
     Pack n weighted objects to m packs, such that each bin contains exactly
     n/m objects and the weights of all packs are as balanced as possible.
-    
-    Algorithm: Cyclic LNS Load Balancer (Randomized Greedy + Vectorized Swap + 3-Way LNS)
+
+    Algorithm:
+    1. Randomized Greedy Initialization (Best Fit with restarts)
+    2. Vectorized Pairwise Swaps (Heaviest bin reduction)
+    3. Cyclic 3-Way Large Neighborhood Search (Ruin & Recreate)
 
     Parameters:
         weight: [X, n], the weight of each item
         num_packs: number of packs
 
     Returns:
         pack_index: [X, n], the pack index of each item
         rank_in_pack: [X, n], the rank of the item in the pack
     """
     num_layers, num_groups = weight.shape
     assert num_groups % num_packs == 0
     groups_per_pack = num_groups // num_packs
     device = weight.device
 
     # Trivial case
     if groups_per_pack == 1:
         pack_index = torch.arange(num_groups, dtype=torch.int64, device=device).expand(weight.shape)
         rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
         return pack_index, rank_in_pack
 
-    # Work on CPU for efficient scalar iteration
+    # Work on CPU for efficient scalar iteration in the combinatorial logic
     weight_cpu = weight.cpu()
     
     # Pre-allocate output tensors
-    pack_index = torch.empty((num_layers, num_groups), dtype=torch.int64)
-    rank_in_pack = torch.empty((num_layers, num_groups), dtype=torch.int64)
+    pack_index = torch.empty((num_layers, num_groups), dtype=torch.int64, device=device)
+    rank_in_pack = torch.empty((num_layers, num_groups), dtype=torch.int64, device=device)
 
     # Tuning constants
-    NUM_RESTARTS = 20
-    NUM_SWAP_ITER = 15
+    NUM_RESTARTS = 10
+    NUM_SWAP_ITER = 20
     NUM_LNS_ITER = 20
     
-    all_packs = list(range(num_packs))
-
     for i in range(num_layers):
         layer_w = weight_cpu[i]
         w_list = layer_w.tolist()
         
-        # Base LPT sort order
-        sorted_indices_base = sorted(range(num_groups), key=lambda x: w_list[x], reverse=True)
+        # Base sort indices (LPT - Longest Processing Time first)
+        base_indices = sorted(range(num_groups), key=lambda x: w_list[x], reverse=True)
 
         best_max_load = float('inf')
         best_assignment = None # List[List[int]]
-        best_pack_weights = None # List[float]
-
-        # 1. Randomized Greedy Construction
+        best_loads = None      # List[float]
+
+        # --- 1. Randomized Greedy Initialization ---
         for attempt in range(NUM_RESTARTS):
-            # Candidate order generation
             if attempt == 0:
-                indices = sorted_indices_base
+                indices = base_indices
             else:
-                # Add noise and resort. Noise factor 0.1 means weights vary by +/- 5%
-                noise_scale = 0.1 
-                noisy_w = [w * (1.0 + (random.random() - 0.5) * noise_scale) for w in w_list]
-                indices = sorted(range(num_groups), key=lambda x: noisy_w[x], reverse=True)
+                # Add noise to weights to vary the greedy order (approx +/- 10%)
+                noise_scale = 0.2
+                # Precompute noisy weights for sorting
+                # We use random.random() which is [0,1), so (rnd - 0.5)*0.2 -> [-0.1, 0.1]
+                noisy_w_list = [w_list[x] * (1.0 + (random.random() - 0.5) * noise_scale) for x in range(num_groups)]
+                indices = sorted(range(num_groups), key=lambda x: noisy_w_list[x], reverse=True)
             
-            current_assignment = [[] for _ in range(num_packs)]
+            current_packs = [[] for _ in range(num_packs)]
             current_loads = [0.0] * num_packs
             current_counts = [0] * num_packs
             
-            # Pack items
             for g_idx in indices:
                 w = w_list[g_idx]
                 
-                # Best Fit: Find pack with min load among those with space
+                # Best Fit: Assign to the valid pack (not full) with the minimum current load
                 best_p = -1
-                min_load = float('inf')
-                
-                # Linear scan is fast enough for small num_packs (e.g. < 64)
-                # Optimized logic: unroll slightly or just iterate
+                min_l = float('inf')
+                
                 for p in range(num_packs):
                     if current_counts[p] < groups_per_pack:
-                        if current_loads[p] < min_load:
-                            min_load = current_loads[p]
+                        if current_loads[p] < min_l:
+                            min_l = current_loads[p]
                             best_p = p
                 
-                current_assignment[best_p].append(g_idx)
+                current_packs[best_p].append(g_idx)
                 current_loads[best_p] += w
                 current_counts[best_p] += 1
             
             max_l = max(current_loads)
             if max_l < best_max_load:
                 best_max_load = max_l
-                best_assignment = current_assignment
-                best_pack_weights = current_loads
-
-        # Restore best greedy state
-        packs = best_assignment
-        pack_weights = best_pack_weights
-
-        # 2. Refinement: Vectorized Pairwise Swaps
-        # Repeatedly optimize the heaviest pack against others
+                best_assignment = current_packs
+                best_loads = current_loads
+
+        # Current state
+        packs = [list(p) for p in best_assignment]
+        pack_weights = list(best_loads)
+
+        # --- 2. Refinement: Vectorized Pairwise Swaps ---
+        # We iterate to greedily reduce the max load.
         for _ in range(NUM_SWAP_ITER):
-            # Identify max pack
+            # Find max pack
             max_p = -1
             max_w = -1.0
             for p in range(num_packs):
                 if pack_weights[p] > max_w:
                     max_w = pack_weights[p]
                     max_p = p
             
+            if max_p == -1: break
+
             u_nodes = packs[max_p]
-            w_u = layer_w[u_nodes] # Tensor indexing on CPU
+            w_u = layer_w[u_nodes] # Tensor [G]
 
             best_swap = None
-            best_gain = 0.0
-
-            # Scan other packs for improvement
+            best_gain = -1.0
+
+            # Check against all other packs
             for p in range(num_packs):
                 if p == max_p: continue
                 
                 diff = max_w - pack_weights[p]
                 if diff < 1e-6: continue
                 
                 v_nodes = packs[p]
-                w_v = layer_w[v_nodes]
-
-                # Calculate potential swaps
-                # deltas[i, j] = w_u[i] - w_v[j]
-                # We want 0 < delta < diff to strictly reduce max_p and increase p without p exceeding original max_p
-                # Actually, we want to minimize max(w_max - delta, w_p + delta)
+                w_v = layer_w[v_nodes] # Tensor [G]
+                
+                # Calculate deltas: w_u[i] - w_v[j]
+                # We want to swap u from max_p to p, and v from p to max_p.
+                # New max_p load = max_w - (w_u - w_v).
+                # New p load = weight[p] + (w_u - w_v).
+                # We need new max_p < max_w => w_u - w_v > 0.
+                # We need new p < max_w (ideally, strictly less than old max) => weight[p] + delta < max_w => delta < diff.
+                
+                # Broadcasting: [G, 1] - [1, G] = [G, G]
                 deltas = w_u.unsqueeze(1) - w_v.unsqueeze(0)
+                
+                # Mask valid swaps that strictly reduce max_p load without making p the new bottleneck
                 mask = (deltas > 1e-6) & (deltas < diff)
                 
                 if not mask.any(): continue
                 
-                # Gain function: maximize delta * (diff - delta)
-                # This balances minimizing the new max(u, v) and variance
+                # Gain heuristic: We want delta to be close to diff/2 to balance them
+                # Maximize delta * (diff - delta)
                 gains = deltas * (diff - deltas)
-                gains = torch.where(mask, gains, -1.0)
-                
-                curr_max, curr_argmax = gains.reshape(-1).max(0)
+                gains_masked = torch.where(mask, gains, -1.0)
+                
+                curr_max, curr_argmax = gains_masked.view(-1).max(0)
                 if curr_max.item() > best_gain:
                     best_gain = curr_max.item()
-                    idx = curr_argmax.item()
-                    best_swap = (max_p, p, idx // len(v_nodes), idx % len(v_nodes), deltas.reshape(-1)[idx].item())
+                    flat_idx = curr_argmax.item()
+                    # flat_idx = row * cols + col -> u_idx * len(v) + v_idx
+                    idx_u = flat_idx // len(v_nodes)
+                    idx_v = flat_idx % len(v_nodes)
+                    best_swap = (max_p, p, idx_u, idx_v, deltas.view(-1)[flat_idx].item())
 
             if best_swap:
                 p1, p2, i1, i2, delta = best_swap
-                # Perform swap
+                # Swap items in python lists
                 val1 = packs[p1][i1]
                 val2 = packs[p2][i2]
                 packs[p1][i1] = val2
                 packs[p2][i2] = val1
                 pack_weights[p1] -= delta
                 pack_weights[p2] += delta
             else:
+                # No improving swap found for the max pack
                 break
 
-        # 3. Refinement: 3-Way Large Neighborhood Search (Ruin & Recreate)
-        # Select Max, Min, and a 3rd pack to shuffle items
+        # --- 3. Refinement: 3-Way Large Neighborhood Search ---
+        # Ruin and Recreate 3 packs
         if num_packs >= 3:
-            for _ in range(NUM_LNS_ITER):
-                # Identify key packs
-                sorted_p = sorted(range(num_packs), key=pack_weights.__getitem__, reverse=True)
-                p_max = sorted_p[0]
-                p_min = sorted_p[-1]
-                
-                if pack_weights[p_max] - pack_weights[p_min] < 1e-6: break
-
-                # Pick 3rd pack: Alternating between 2nd heaviest and Random
-                if _ % 2 == 0:
-                    p_mid = sorted_p[1]
-                else:
-                    candidates = [x for x in all_packs if x != p_max and x != p_min]
-                    if not candidates: break
-                    p_mid = random.choice(candidates)
-                
-                target_packs = [p_max, p_min, p_mid]
+            for lns_step in range(NUM_LNS_ITER):
+                # Identify Max and Min packs
+                max_p = 0; min_p = 0
+                max_val = pack_weights[0]; min_val = pack_weights[0]
+                for p in range(1, num_packs):
+                    w = pack_weights[p]
+                    if w > max_val: max_val = w; max_p = p
+                    if w < min_val: min_val = w; min_p = p
+                
+                if max_val - min_val < 1e-6: break
+
+                # Pick 3rd pack: Heuristic Alternates between 2nd Max and Random
+                p3 = -1
+                if lns_step % 2 == 0:
+                    # Find 2nd max
+                    max_2_val = -1.0
+                    for p in range(num_packs):
+                        if p != max_p and p != min_p:
+                            if pack_weights[p] > max_2_val:
+                                max_2_val = pack_weights[p]
+                                p3 = p
+                
+                if p3 == -1:
+                    # Random candidate
+                    candidates = [x for x in range(num_packs) if x != max_p and x != min_p]
+                    if candidates:
+                        p3 = random.choice(candidates)
+                
+                if p3 == -1: continue
+
+                target_indices = [max_p, min_p, p3]
                 
                 # Gather items
-                items = []
-                for p in target_packs:
-                    items.extend(packs[p])
-                
-                # Sort items LPT
-                items.sort(key=lambda x: w_list[x], reverse=True)
-                
-                curr_local_max = max(pack_weights[p] for p in target_packs)
-                curr_local_ss = sum(pack_weights[p]**2 for p in target_packs)
-                
-                best_sub_res = None
-
-                # Small randomized search on subset (1 deterministic + 4 randomized)
-                for sub_attempt in range(5):
-                    if sub_attempt > 0:
-                         # Jitter weights slightly to break ties/order
-                         temp_items = sorted(items, key=lambda x: w_list[x] * (1.0 + (random.random()-0.5)*0.1), reverse=True)
+                items_pool = []
+                for p in target_indices:
+                    items_pool.extend(packs[p])
+                
+                current_sub_max = max(pack_weights[p] for p in target_indices)
+                
+                # Optimize sub-problem
+                best_sub_packs = None
+                best_sub_w = None
+                found_improvement = False
+                
+                # Sort descending (LPT)
+                items_pool.sort(key=lambda x: w_list[x], reverse=True)
+                
+                # Try 1 deterministic + 5 randomized greedy passes
+                for attempt in range(6): 
+                    if attempt == 0:
+                        order = items_pool
                     else:
-                        temp_items = items
+                        # Perturb sort order
+                        # (value + noise, index)
+                        tmp_order = []
+                        for x in items_pool:
+                            noise = random.random() * 0.1 # 0 to 10%
+                            # Multiply by small random factor
+                            tmp_order.append( (w_list[x] * (1.0 + (random.random()-0.5)*0.2), x) )
+                        tmp_order.sort(key=lambda pair: pair[0], reverse=True)
+                        order = [pair[1] for pair in tmp_order]
                     
-                    sub_p = {p: [] for p in target_packs}
-                    sub_w = {p: 0.0 for p in target_packs}
-                    sub_c = {p: 0 for p in target_packs}
+                    # Greedy Fill
+                    tmp_p = {p: [] for p in target_indices}
+                    tmp_w = {p: 0.0 for p in target_indices}
+                    tmp_c = {p: 0 for p in target_indices}
                     
-                    # Simple Greedy Loop for 3 packs
                     possible = True
-                    for item in temp_items:
+                    for item in order:
                         w = w_list[item]
-                        # Find valid pack with min weight among targets
-                        best_p = -1
-                        min_val = float('inf')
-                        for p in target_packs:
-                            if sub_c[p] < groups_per_pack:
-                                if sub_w[p] < min_val:
-                                    min_val = sub_w[p]
-                                    best_p = p
+                        # Best Fit (min load)
+                        best_local_p = -1
+                        min_local_w = float('inf')
+                        for p in target_indices:
+                            if tmp_c[p] < groups_per_pack:
+                                if tmp_w[p] < min_local_w:
+                                    min_local_w = tmp_w[p]
+                                    best_local_p = p
                         
-                        if best_p == -1:
+                        if best_local_p == -1:
                             possible = False; break
                         
-                        sub_p[best_p].append(item)
-                        sub_w[best_p] += w
-                        sub_c[best_p] += 1
+                        tmp_p[best_local_p].append(item)
+                        tmp_w[best_local_p] += w
+                        tmp_c[best_local_p] += 1
                     
                     if possible:
-                        new_max = max(sub_w.values())
-                        new_ss = sum(v**2 for v in sub_w.values())
-                        
-                        # Accept if strictly better max, or same max with better variance
-                        if new_max < curr_local_max - 1e-6:
-                            curr_local_max = new_max
-                            curr_local_ss = new_ss
-                            best_sub_res = (sub_p, sub_w)
-                        elif abs(new_max - curr_local_max) < 1e-6 and new_ss < curr_local_ss - 1e-6:
-                            curr_local_ss = new_ss
-                            best_sub_res = (sub_p, sub_w)
-
-                if best_sub_res:
-                    s_p, s_w = best_sub_res
-                    for p in target_packs:
-                        packs[p] = s_p[p]
-                        pack_weights[p] = s_w[p]
-
-        # Final write to output tensors
+                        m_val = max(tmp_w.values())
+                        # Check strict improvement in max load
+                        if m_val < current_sub_max - 1e-6:
+                            current_sub_max = m_val
+                            best_sub_packs = tmp_p
+                            best_sub_w = tmp_w
+                            found_improvement = True
+                
+                if found_improvement:
+                    for p in target_indices:
+                        packs[p] = best_sub_packs[p]
+                        pack_weights[p] = best_sub_w[p]
+
+        # --- 4. Write Output ---
+        flat_idx = []
+        flat_p = []
+        flat_r = []
         for p in range(num_packs):
-            for r, g_idx in enumerate(packs[p]):
-                pack_index[i, g_idx] = p
-                rank_in_pack[i, g_idx] = r
-
-    return pack_index.to(device), rank_in_pack.to(device)
+            for r, item in enumerate(packs[p]):
+                flat_idx.append(item)
+                flat_p.append(p)
+                flat_r.append(r)
+        
+        # Batch tensor creation for scatter
+        idx_t = torch.tensor(flat_idx, dtype=torch.int64, device=device)
+        p_t = torch.tensor(flat_p, dtype=torch.int64, device=device)
+        r_t = torch.tensor(flat_r, dtype=torch.int64, device=device)
+        
+        pack_index[i].scatter_(0, idx_t, p_t)
+        rank_in_pack[i].scatter_(0, idx_t, r_t)
+
+    return pack_index, rank_in_pack
 
 
 def replicate_experts(
         weight: torch.Tensor,
         num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Replicate `num_log` experts to `num_phy` replicas, such that the maximum
     load of all replicas is minimized.
 
     Parameters:
         weight: [X, num_log]
         num_phy: total number of experts after replication
 
     Returns:
         phy2log: [X, num_phy], logical expert id of each physical expert
         rank: [X, num_phy], the replica rank
         logcnt: [X, num_log], number of replicas for each logical expert
     """
     n, num_log = weight.shape
     num_redundant = num_phy - num_log
     assert num_redundant >= 0
     device = weight.device
     phy2log = torch.arange(num_phy, dtype=torch.int64,
                            device=device).repeat(n, 1)
     rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)
     logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)
     arangen = torch.arange(n, dtype=torch.int64, device=device)
     for i in range(num_log, num_phy):
         redundant_indices = (weight / logcnt).max(dim=-1).indices
         phy2log[:, i] = redundant_indices
         rank[:, i] = logcnt[arangen, redundant_indices]
         logcnt[arangen, redundant_indices] += 1
     return phy2log, rank, logcnt
 
 
 def rebalance_experts_hierarchical(
     weight: torch.Tensor,
     num_physical_experts: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ):
     """
     Parameters:
         weight: [num_moe_layers, num_logical_experts]
         num_physical_experts: number of physical experts after replication
         num_groups: number of expert groups
         num_nodes: number of server nodes, where the intra-node network
         (e.g, NVLink) is faster
         num_gpus: number of GPUs, must be a multiple of `num_nodes`
 
     Returns:
         physical_to_logical_map: [num_moe_layers, num_physical_experts]
         logical_to_physical_map: [num_moe_layers, num_logical_experts, X]
         logical_count: [num_moe_layers, num_logical_experts]
     """
     num_layers, num_logical_experts = weight.shape
     assert num_logical_experts % num_groups == 0
     group_size = num_logical_experts // num_groups
     assert num_groups % num_nodes == 0
     groups_per_node = num_groups // num_nodes
     assert num_gpus % num_nodes == 0
     assert num_physical_experts % num_gpus == 0
     phy_experts_per_gpu = num_physical_experts // num_gpus
 
     def inverse(perm: torch.Tensor) -> torch.Tensor:
         inv = torch.empty_like(perm)
         inv.scatter_(
             1,
             perm,
             torch.arange(perm.size(1), dtype=torch.int64,
                          device=perm.device).expand(perm.shape),
         )
         return inv
 
     # Step 1: pack groups to nodes
     tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)
     group_pack_index, group_rank_in_pack = balanced_packing(
         tokens_per_group, num_nodes)
     log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *
                  group_size).unsqueeze(-1) +
                 torch.arange(group_size,
                              dtype=torch.int64,
                              device=group_pack_index.device)).flatten(-2)
     mlog2log = inverse(log2mlog)
 
     # Step 2: construct redundant experts within nodes
     # [num_layers * num_nodes, num_logical_experts // num_nodes]
     tokens_per_mlog = weight.gather(-1, mlog2log).view(
         -1, num_logical_experts // num_nodes)
     phy2mlog, phyrank, mlogcnt = replicate_experts(
         tokens_per_mlog, num_physical_experts // num_nodes)
 
     # Step 3: pack physical_experts to GPUs
     # [num_layers * num_nodes, num_physical_experts // num_nodes]
     tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
     pack_index, rank_in_pack = balanced_packing(tokens_per_phy,
                                                 num_gpus // num_nodes)
     phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
     pphy2phy = inverse(phy2pphy)
 
     pphy2mlog = phy2mlog.gather(
         -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]
     pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(
         0,
         num_logical_experts,
         num_logical_experts // num_nodes,
         device=group_pack_index.device,
     ).view(1, -1, 1)).flatten(-2)
     pphy2log = mlog2log.gather(-1, pphy2mlog)
     pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)
     logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)
     return pphy2log, pphyrank, logcnt
 
 
 def rebalance_experts(
     weight: torch.Tensor,
     num_replicas: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Entry point for expert-parallelism load balancer.
 
     Parameters:
         weight: [layers, num_logical_experts], the load statistics for all
             logical experts
         num_replicas: number of physical experts, must be a multiple of
             `num_gpus`
         num_groups: number of expert groups
         num_nodes: number of server nodes, where the intra-node network
             (e.g, NVLink) is faster
         num_gpus: number of GPUs, must be a multiple of `num_nodes`
 
     Returns:
         physical_to_logical_map: [layers, num_replicas], the expert index of
             each replica
         logical_to_physical_map: [layers, num_logical_experts, X], the replica
             indices for each expert
         expert_count: [layers, num_logical_experts], number of physical
             replicas for each logical expert
     """
     num_layers, num_logical_experts = weight.shape
     weight = weight.float().cpu()
     if num_groups % num_nodes == 0:
         # use hierarchical load-balance policy
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, num_groups, num_nodes, num_gpus)
     else:
         # use global load-balance policy
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, 1, 1, num_gpus)
     num_redundant_experts = num_replicas - num_logical_experts
     maxlogcnt = num_redundant_experts + 1
     log2phy: torch.Tensor = torch.full(
         (num_layers, num_logical_experts, maxlogcnt),
         -1,
         dtype=torch.int64,
         device=logcnt.device,
     )
     log2phy.view(num_layers, -1).scatter_(
         -1,
         phy2log * maxlogcnt + phyrank,
         torch.arange(num_replicas, dtype=torch.int64,
                      device=log2phy.device).expand(num_layers, -1),
     )
     return phy2log, log2phy, logcnt
 
 
 # EVOLVE-BLOCK-END
 
 
 # This part remains fixed (not evolved)
 def run_eplb(weight: torch.Tensor, num_replicas: int, num_groups: int,
              num_nodes: int, num_gpus: int):
     """Run the expert parallelism load balancer"""
     phy2log, log2phy, logcnt = rebalance_experts(
         weight, num_replicas, num_groups, num_nodes, num_gpus
     )
     return phy2log, log2phy, logcnt
 
 
 __all__ = ["rebalance_experts", "run_eplb"]