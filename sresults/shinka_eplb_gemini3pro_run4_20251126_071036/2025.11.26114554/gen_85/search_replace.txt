<NAME>
randomized_lns_refinement
</NAME>

<DESCRIPTION>
This edit completely revamps the `balanced_packing` function to improve load balancing, leveraging the available CPU time budget.

1.  **Imports**: Adds `import random` to facilitate randomized algorithms.
2.  **Enhanced Initialization**: Increases the number of candidate solutions generated during the initialization phase from a fixed few to 20. It uses one deterministic LPT pass and 19 randomized LPT passes (with weight perturbation) to better explore the initial solution space.
3.  **Robust LNS Sub-solver**: The Large Neighborhood Search (LNS) logic has been significantly upgraded.
    -   **Dynamic Scope**: It selects a subset of packs including the heaviest, lightest, and up to 2 random additional packs, creating a larger search space for "ruin and recreate" moves.
    -   **Randomized Sub-solver**: Instead of a single deterministic re-packing of the subset, it now runs multiple randomized greedy attempts (10 restarts) with perturbed sorting orders. This helps the algorithm escape local optima where simple LPT sorting fails to find a valid swap.
    -   **Optimization Objective**: It explicitly minimizes Max Load first, and Sum of Squared Loads second (variance proxy), ensuring strictly better or more balanced local moves.
4.  **Vectorized Swaps**: Retains and cleans up the efficient vectorized pairwise swap logic to quickly reduce peak loads.

These changes aim to break the balancedness score plateau (~0.31) by performing deeper combinatorial searches.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
import torch
=======
import torch
import random
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs
    device = weight.device

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Operations on CPU are generally faster for this sequential/iterative logic
    weight_cpu = weight.cpu()

    pack_index = torch.empty(weight.shape, dtype=torch.int64, device=device)
    rank_in_pack = torch.empty(weight.shape, dtype=torch.int64, device=device)

    # Pre-allocate helper tensors for scatter
    flat_packs = torch.arange(num_packs, device=device).unsqueeze(1).expand(-1, groups_per_pack).reshape(-1)
    flat_ranks = torch.arange(groups_per_pack, device=device).unsqueeze(0).expand(num_packs, -1).reshape(-1)

    for i in range(num_layers):
        # Extract weights for this layer as a list for fast Python iteration
        layer_weights_t = weight_cpu[i]
        layer_weights = layer_weights_t.tolist()

        # Candidate generation:
        # We try:
        # 1. Deterministic LPT (sort descending)
        # 2. Randomized LPT (multiple attempts)

        candidates = []

        # 1. Deterministic
        indices_det = layer_weights_t.sort(descending=True).indices.tolist()
        candidates.append(indices_det)

        # 2. Perturbed (only if we have enough groups for it to matter)
        if groups_per_pack > 1:
            for attempt in range(4):
                # Add small random noise to explore different greedy decisions
                noise = torch.rand(num_groups) * 0.1 + 0.95 # +/- 5%
                indices_rand = (layer_weights_t * noise).sort(descending=True).indices.tolist()
                candidates.append(indices_rand)

        best_assignment = None
        best_pack_weights = None
        min_max_load = float('inf')

        # Evaluate candidates with fast Python Greedy
        for indices in candidates:
            # Re-init packs using lists for speed
            current_pack_contents = [[] for _ in range(num_packs)]
            current_pack_weights = [0.0] * num_packs
            current_pack_cnt = [0] * num_packs

            # Pack
            for idx in indices:
                w = layer_weights[idx]

                # Find best pack: valid (not full) and min weight
                best_p = -1
                min_w = float('inf')

                for p in range(num_packs):
                    if current_pack_cnt[p] < groups_per_pack:
                        pw = current_pack_weights[p]
                        if pw < min_w:
                            min_w = pw
                            best_p = p

                # Assign
                current_pack_contents[best_p].append(idx)
                current_pack_weights[best_p] += w
                current_pack_cnt[best_p] += 1

            # Metric: minimize max load
            max_load = max(current_pack_weights)
            if max_load < min_max_load:
                min_max_load = max_load
                best_assignment = current_pack_contents
                best_pack_weights = current_pack_weights

        # Convert best assignment to tensor for vectorized refinement
        # [num_packs, groups_per_pack]
        pack_assignment = torch.tensor(best_assignment, dtype=torch.int64)
        pack_weights = torch.tensor(best_pack_weights, dtype=torch.float32)

        # Global Refinement Loop
        # Alternates between Vectorized Pairwise Swaps and Large Neighborhood Search (LNS)

        NUM_GLOBAL_ITER = 3

        for global_iter in range(NUM_GLOBAL_ITER):

            # Phase 1: Vectorized Local Search (Pairwise Swaps)
            # Efficiently checks swaps between max pack and others

            improved_swap = False
            for _ in range(20): # Limit iterations per phase
                max_pack = torch.argmax(pack_weights).item()
                max_w = pack_weights[max_pack].item()

                # Items in max pack: [G]
                u_indices = pack_assignment[max_pack]
                w_u = layer_weights_t[u_indices].view(1, groups_per_pack, 1) # [1, G, 1]

                # Items in all packs: [M, G]
                w_v = layer_weights_t[pack_assignment].view(num_packs, 1, groups_per_pack) # [M, 1, G]

                # Compute deltas: w_u - w_v
                deltas = w_u - w_v # [M, G, G]

                # Diff with max pack
                diffs = (max_w - pack_weights).view(num_packs, 1, 1) # [M, 1, 1]

                # Gain metric: delta * (diff - delta)
                # We want 0 < delta < diff

                mask = (deltas > 1e-6) & (deltas < diffs)

                if not mask.any():
                    break

                gains = deltas * (diffs - deltas)
                gains = torch.where(mask, gains, -1.0)

                best_flat = torch.argmax(gains).item()
                max_gain = gains.view(-1)[best_flat].item()

                if max_gain < 0:
                    break

                # Perform Swap
                best_p = best_flat // (groups_per_pack * groups_per_pack)
                rem = best_flat % (groups_per_pack * groups_per_pack)
                best_u_idx = rem // groups_per_pack
                best_v_idx = rem % groups_per_pack

                u_val = pack_assignment[max_pack, best_u_idx].item()
                v_val = pack_assignment[best_p, best_v_idx].item()

                pack_assignment[max_pack, best_u_idx] = v_val
                pack_assignment[best_p, best_v_idx] = u_val

                d_val = deltas[best_p, best_u_idx, best_v_idx].item()
                pack_weights[max_pack] -= d_val
                pack_weights[best_p] += d_val
                improved_swap = True

            # Phase 2: Large Neighborhood Search (LNS)
            # Ruin and Recreate 3 packs (Max, Min, Random)
            # This helps to perform 3-way exchanges or more complex reshuffling

            improved_lns = False
            if num_packs >= 3:
                # Number of LNS attempts
                for _ in range(15):
                    max_p = torch.argmax(pack_weights).item()
                    min_p = torch.argmin(pack_weights).item()

                    if max_p == min_p:
                        break

                    # Pick 3rd pack random
                    rand_p = torch.randint(0, num_packs, (1,)).item()
                    while rand_p == max_p or rand_p == min_p:
                        rand_p = torch.randint(0, num_packs, (1,)).item()

                    indices_list = [max_p, min_p, rand_p]

                    # Collect items
                    items = []
                    for p in indices_list:
                         items.extend(pack_assignment[p].tolist())

                    # Sort items LPT
                    items.sort(key=lambda x: layer_weights[x], reverse=True)

                    # Repack Greedy into these 3
                    temp_packs = {p: [] for p in indices_list}
                    temp_weights = {p: 0.0 for p in indices_list}
                    temp_counts = {p: 0 for p in indices_list}

                    possible = True
                    for item_idx in items:
                        w = layer_weights[item_idx]
                        best_local_p = -1
                        min_local_w = float('inf')
                        found = False

                        # Greedy Best Fit on subset
                        for p in indices_list:
                             if temp_counts[p] < groups_per_pack:
                                 if temp_weights[p] < min_local_w:
                                     min_local_w = temp_weights[p]
                                     best_local_p = p
                                     found = True

                        if not found:
                            possible = False
                            break

                        temp_packs[best_local_p].append(item_idx)
                        temp_weights[best_local_p] += w
                        temp_counts[best_local_p] += 1

                    if possible:
                        old_max = max(pack_weights[p].item() for p in indices_list)
                        new_max = max(temp_weights.values())

                        old_ss = sum(pack_weights[p].item()**2 for p in indices_list)
                        new_ss = sum(w**2 for w in temp_weights.values())

                        # Accept if improves max load or variance
                        if new_max < old_max - 1e-6 or (abs(new_max - old_max) < 1e-6 and new_ss < old_ss - 1e-6):
                            for p in indices_list:
                                pack_assignment[p] = torch.tensor(temp_packs[p], dtype=torch.int64)
                                pack_weights[p] = temp_weights[p]
                            improved_lns = True

            if not improved_swap and not improved_lns:
                break

        # Final write to output tensors
        # pack_assignment contains expert indices in packed order
        flat_experts = pack_assignment.view(-1).to(device)
        pack_index[i].scatter_(0, flat_experts, flat_packs)
        rank_in_pack[i].scatter_(0, flat_experts, flat_ranks)

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs
    device = weight.device

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Operations on CPU are generally faster for this sequential/iterative logic
    weight_cpu = weight.cpu()

    pack_index = torch.empty(weight.shape, dtype=torch.int64, device=device)
    rank_in_pack = torch.empty(weight.shape, dtype=torch.int64, device=device)

    # Pre-allocate helper tensors for scatter
    flat_packs = torch.arange(num_packs, device=device).unsqueeze(1).expand(-1, groups_per_pack).reshape(-1)
    flat_ranks = torch.arange(groups_per_pack, device=device).unsqueeze(0).expand(num_packs, -1).reshape(-1)

    for i in range(num_layers):
        layer_weights_t = weight_cpu[i]
        layer_weights = layer_weights_t.tolist()

        # --- 1. Initialization (Diversified Greedy) ---
        candidates = []
        # Deterministic LPT
        candidates.append(sorted(range(num_groups), key=lambda x: layer_weights[x], reverse=True))

        # Randomized LPT
        # Increase candidates to find better starting points
        for _ in range(19):
            noise = torch.rand(num_groups) * 0.2 + 0.9 # [0.9, 1.1]
            # Use tensor operations for fast noise application and sorting
            indices_rand = (layer_weights_t * noise).argsort(descending=True).tolist()
            candidates.append(indices_rand)

        best_assignment = None
        best_pack_weights = None
        min_max_load = float('inf')
        min_ss = float('inf')

        # Evaluate candidates
        for indices in candidates:
            curr_packs = [[] for _ in range(num_packs)]
            curr_weights = [0.0] * num_packs
            curr_cnt = [0] * num_packs

            for idx in indices:
                w = layer_weights[idx]

                # Greedy Best Fit
                best_p = -1
                min_w = float('inf')

                for p in range(num_packs):
                    if curr_cnt[p] < groups_per_pack:
                        if curr_weights[p] < min_w:
                            min_w = curr_weights[p]
                            best_p = p

                curr_packs[best_p].append(idx)
                curr_weights[best_p] += w
                curr_cnt[best_p] += 1

            m_load = max(curr_weights)
            ss_load = sum(w*w for w in curr_weights)

            if m_load < min_max_load - 1e-6:
                min_max_load = m_load
                min_ss = ss_load
                best_assignment = curr_packs
                best_pack_weights = curr_weights
            elif abs(m_load - min_max_load) < 1e-6 and ss_load < min_ss - 1e-6:
                min_ss = ss_load
                best_assignment = curr_packs
                best_pack_weights = curr_weights

        # To Tensor
        pack_assignment = torch.tensor(best_assignment, dtype=torch.int64)
        pack_weights = torch.tensor(best_pack_weights, dtype=torch.float32)

        # --- 2. Refinement Loop ---
        MAX_GLOBAL_ITERS = 50

        for _ in range(MAX_GLOBAL_ITERS):
            improved = False

            # Phase A: Vectorized Swaps (Max Pack vs Others)
            max_p_idx = torch.argmax(pack_weights).item()
            max_w = pack_weights[max_p_idx].item()

            w_u = layer_weights_t[pack_assignment[max_p_idx]].view(1, groups_per_pack, 1)
            w_v = layer_weights_t[pack_assignment].view(num_packs, 1, groups_per_pack)

            deltas = w_u - w_v
            diffs = (max_w - pack_weights).view(num_packs, 1, 1)

            # We want swap if 0 < delta < diff (strict reduction of max_w and p_target_w doesn't exceed old max_w)