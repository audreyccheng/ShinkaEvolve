--- a/original.py
+++ b/original.py
@@ -1,454 +1,505 @@
 # EVOLVE-BLOCK-START
 """
 Expert parallelism load balancer (EPLB) for vLLM.
 
 This module implements the core rearrangement algorithm.
 
 The rearrangement algorithm is adapted from
 [DeepSeek EPLB](https://github.com/deepseek-ai/eplb).
 
 Please find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example
 on how the EPLB algorithm works.
 """
 
 import torch
-
+import random
+import math
 
 def balanced_packing(weight: torch.Tensor,
                      num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
     """
     Pack n weighted objects to m packs, such that each bin contains exactly
     n/m objects and the weights of all packs are as balanced as possible.
 
-    Algorithm: Randomized Greedy + Pairwise Swap Descent + 3-Way LNS
+    Algorithm:
+    1. Randomized Greedy Initialization (Multiple Restarts)
+    2. Iterative Pairwise Rebalancing (Targeting Max Load)
+    3. Three-way Large Neighborhood Search (Breaking Local Optima)
 
     Parameters:
         weight: [X, n], the weight of each item
         num_packs: number of packs
 
     Returns:
         pack_index: [X, n], the pack index of each item
         rank_in_pack: [X, n], the rank of the item in the pack
     """
     num_layers, num_groups = weight.shape
     assert num_groups % num_packs == 0
     groups_per_pack = num_groups // num_packs
 
     device = weight.device
     if groups_per_pack == 1:
         pack_index = torch.arange(weight.size(-1),
                                   dtype=torch.int64,
                                   device=device).expand(weight.shape)
         rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
         return pack_index, rank_in_pack
 
-    # Use CPU weights
+    # Work on CPU for scalar speed
     weight_cpu = weight.cpu()
-
-    # Outputs on CPU to avoid device synchronization during fill
+    
+    # Pre-allocate output tensors
     pack_index = torch.empty((num_layers, num_groups), dtype=torch.int64)
     rank_in_pack = torch.empty((num_layers, num_groups), dtype=torch.int64)
 
-    # Tuning constants
-    NUM_RESTARTS = 5
+    # Number of candidates for initialization
+    INIT_RESTARTS = 10
+    
+    # Limit refinement iterations
+    MAX_REFINE_ITERS = 50
 
     for i in range(num_layers):
-        layer_w = weight_cpu[i]
-        layer_w_list = layer_w.tolist()
-
+        layer_w = weight_cpu[i].tolist()
+        
+        # --- Phase 1: Initialization ---
+        # Generate multiple valid packings using Randomized Greedy and pick the best.
+        
+        best_packs = None
+        best_pack_weights = None
         best_max_load = float('inf')
-        best_sum_sq = float('inf')
-        best_assignment = None
-
-        for attempt in range(NUM_RESTARTS):
-            # 1. Candidate Generation
+        best_ss = float('inf')
+
+        # Create a list of item indices
+        all_indices = list(range(num_groups))
+
+        for attempt in range(INIT_RESTARTS):
+            # Sort order: 1st determininstic LPT, others randomized
             if attempt == 0:
-                indices = sorted(range(num_groups), key=lambda x: layer_w_list[x], reverse=True)
+                # Deterministic LPT
+                indices = sorted(all_indices, key=lambda x: layer_w[x], reverse=True)
             else:
                 # Randomized LPT
-                noise = torch.rand(num_groups) * 0.15 + 0.925
-                indices = (layer_w * noise).argsort(descending=True).tolist()
-
-            # 2. Greedy Construction (LPT/Best Fit)
-            packs = [[] for _ in range(num_packs)]
-            pack_weights = [0.0] * num_packs
-            pack_counts = [0] * num_packs
-
-            for g_idx in indices:
-                w = layer_w_list[g_idx]
-                # Best Fit: Assign to valid pack with min load
+                # Perturb weights: w * (1 + uniform(-0.1, 0.1))
+                # Using a list comprehension for speed
+                indices = sorted(all_indices, 
+                                 key=lambda x: layer_w[x] * (0.9 + 0.2 * random.random()), 
+                                 reverse=True)
+            
+            # Greedy Fill
+            current_packs = [[] for _ in range(num_packs)]
+            current_weights = [0.0] * num_packs
+            current_counts = [0] * num_packs
+            
+            # Use simple scan is fast for small num_packs (<64)
+            for idx in indices:
+                w = layer_w[idx]
+                
+                # Find best valid bin
                 best_p = -1
-                min_w = float('inf')
-
+                min_load = float('inf')
+                
                 for p in range(num_packs):
-                    if pack_counts[p] < groups_per_pack:
-                        if pack_weights[p] < min_w:
-                            min_w = pack_weights[p]
+                    if current_counts[p] < groups_per_pack:
+                        if current_weights[p] < min_load:
+                            min_load = current_weights[p]
                             best_p = p
-
-                packs[best_p].append(g_idx)
-                pack_weights[best_p] += w
-                pack_counts[best_p] += 1
-
-            # 3. Refinement: Pairwise Swap + LNS
-            MAX_REFINE_STEPS = 100 if num_packs > 1 else 0
-
-            # Helper to find max/min packs
-            def get_stats(pw):
-                max_w = -1.0
-                min_w = float('inf')
-                max_p = -1
-                min_p = -1
-                for p_idx, w_val in enumerate(pw):
-                    if w_val > max_w:
-                        max_w = w_val
-                        max_p = p_idx
-                    if w_val < min_w:
-                        min_w = w_val
-                        min_p = p_idx
-                return max_p, min_p, max_w, min_w
-
-            for step in range(MAX_REFINE_STEPS):
-                max_p, min_p, max_w, min_w = get_stats(pack_weights)
-
-                if max_p == min_p or (max_w - min_w < 1e-6):
-                    break
-
-                improved_action = False
-
-                # A. Pairwise Swap Descent
-                # Try to swap item from max_p with item from any other pack p to reduce max_p's load
-                # The constraint is that the other pack p must not become heavier than max_p's *current* load (or better, the new load)
-
-                # We want: w(u) - w(v) > 0  (reduce max_p)
-                # And: pack_weights[p] + (w(u) - w(v)) < max_w  (ideally < max_w - epsilon)
-                # Or more strictly to ensure descent: pack_weights[p] + (w(u) - w(v)) < max_w - (w(u) - w(v)) ?
-                # Actually, we just need the new max of {max_p, p} to be less than old max_w.
-
-                best_swap = None
-                best_gain = 0.0 # Gain = w(u) - w(v). We maximize this subject to constraints.
-
-                # Only check max_p against others
-                u_items = packs[max_p]
-
-                for u_idx, u in enumerate(u_items):
-                    w_u = layer_w_list[u]
-
-                    for p in range(num_packs):
-                        if p == max_p: continue
-
-                        diff_p = max_w - pack_weights[p]
-                        if diff_p < 1e-6: continue # p is already essentially equal to max, can't dump weight there
-
-                        for v_idx, v in enumerate(packs[p]):
-                            w_v = layer_w_list[v]
-                            delta = w_u - w_v
-
-                            # We need delta > 0 to reduce max_p
-                            # We need pack_weights[p] + delta < max_w (strict decrease of global max if max_p was unique max)
-
-                            if delta > 1e-6 and (pack_weights[p] + delta < max_w - 1e-6):
-                                # Valid swap. Does it improve beyond best_gain?
-                                # We prefer larger delta (more reduction in max_p)
-                                if delta > best_gain:
-                                    best_gain = delta
-                                    best_swap = (p, u_idx, v_idx, delta)
-
-                if best_swap:
-                    target_p, u_idx, v_idx, delta = best_swap
-                    # Execute Swap
-                    item_u = packs[max_p][u_idx]
-                    item_v = packs[target_p][v_idx]
-
-                    packs[max_p][u_idx] = item_v
-                    packs[target_p][v_idx] = item_u
-
-                    pack_weights[max_p] -= delta
-                    pack_weights[target_p] += delta
-                    improved_action = True
-
-                # B. 3-Way LNS (Ruin & Recreate)
-                # If swaps failed to reduce max, try re-shuffling 3 packs
-                if not improved_action and num_packs >= 3:
-                     # Candidates: Max, Min, Random
-                    candidates = {max_p, min_p}
-                    # Try to find a random pack that is not max or min
-                    for _ in range(5): # retry limit
-                        r = torch.randint(0, num_packs, (1,)).item()
-                        if r not in candidates:
-                            candidates.add(r)
-                            break
-
-                    cand_list = list(candidates)
-                    if len(cand_list) < 3 and num_packs >= 3:
-                         # Could not find distinct 3rd (e.g. only 2 packs total, handled by check above)
-                         pass
-
-                    # Ruin
-                    items = []
-                    for p in cand_list:
-                        items.extend(packs[p])
-
-                    # Recreate
-                    current_sub_max = max(pack_weights[p] for p in cand_list)
-                    current_sub_ss = sum(pack_weights[p]**2 for p in cand_list)
-
-                    best_sub_res = None
-
-                    # Sort items
-                    items_sorted = sorted(items, key=lambda x: layer_w_list[x], reverse=True)
-
-                    # Few restarts for sub-problem
-                    for sub_attempt in range(15):
-                        if sub_attempt == 0:
-                            # Deterministic LPT
-                            cur_items = items_sorted
+                
+                current_packs[best_p].append(idx)
+                current_weights[best_p] += w
+                current_counts[best_p] += 1
+            
+            # Evaluate
+            c_max = max(current_weights)
+            # Optimization: only compute SS if max is tied or better
+            if c_max <= best_max_load + 1e-6:
+                c_ss = sum(w*w for w in current_weights)
+                if c_max < best_max_load - 1e-6 or c_ss < best_ss - 1e-6:
+                    best_max_load = c_max
+                    best_ss = c_ss
+                    best_packs = current_packs
+                    best_pack_weights = current_weights
+
+        # Load best init
+        packs = best_packs
+        pack_weights = best_pack_weights
+        
+        # --- Phase 2: Refinement ---
+        
+        for _ in range(MAX_REFINE_ITERS):
+            # Find pack with Max Load
+            max_p = -1
+            max_val = -1.0
+            min_p = -1
+            min_val = float('inf')
+            
+            for p, w in enumerate(pack_weights):
+                if w > max_val:
+                    max_val = w
+                    max_p = p
+                if w < min_val:
+                    min_val = w
+                    min_p = p
+            
+            if max_p == min_p or (max_val - min_val < 1e-6):
+                break
+                
+            improved = False
+            
+            # Strategy A: Pairwise Rebalancing with 'max_p'
+            # We try to rebalance 'max_p' with 'min_p' first, then others if needed.
+            # To be thorough, we can try rebalancing max_p with a few lightest packs.
+            
+            # Sort packs by weight ascending to pair max_p with lightest ones
+            # Create list of (weight, index)
+            sorted_packs = sorted(((pack_weights[p], p) for p in range(num_packs)), key=lambda x: x[0])
+            
+            # Try top k lightest packs
+            k_scan = min(len(sorted_packs), 5)
+            
+            for _, other_p in sorted_packs[:k_scan]:
+                if other_p == max_p: continue
+                
+                # Combine items
+                items_pooled = packs[max_p] + packs[other_p]
+                
+                # Try to partition items_pooled into 2 sets of size groups_per_pack
+                # such that max(sum(set1), sum(set2)) is minimized.
+                # Heuristic: Randomized Greedy on this subset
+                
+                sub_best_max = max(pack_weights[max_p], pack_weights[other_p])
+                sub_best_assign = None
+                sub_best_weights = None
+                
+                # LPT Sort for the pool
+                items_pooled_sorted = sorted(items_pooled, key=lambda x: layer_w[x], reverse=True)
+                
+                # Run a few randomized greedy passes for the sub-problem
+                for attempt in range(5): # 1 det, 4 rand
+                    if attempt > 0:
+                        # Perturb
+                        items_order = sorted(items_pooled, key=lambda x: layer_w[x] * (0.9 + 0.2*random.random()), reverse=True)
+                    else:
+                        items_order = items_pooled_sorted
+                        
+                    # Fill 2 bins
+                    b1, b2 = [], []
+                    w1, w2 = 0.0, 0.0
+                    c1, c2 = 0, 0
+                    
+                    possible_sub = True
+                    for item in items_order:
+                        w_item = layer_w[item]
+                        # Put in lighter valid bin
+                        if c1 < groups_per_pack and (c2 == groups_per_pack or w1 <= w2):
+                            b1.append(item)
+                            w1 += w_item
+                            c1 += 1
+                        elif c2 < groups_per_pack:
+                            b2.append(item)
+                            w2 += w_item
+                            c2 += 1
                         else:
-                            # Perturbed
-                            noise = [1.0 + (torch.rand(1).item() - 0.5) * 0.2 for _ in items]
-                            cur_items = sorted(items, key=lambda x: layer_w_list[x] * noise[items.index(x)], reverse=True)
-
-                        # Greedy
-                        t_w = {p: 0.0 for p in cand_list}
-                        t_p = {p: [] for p in cand_list}
-                        t_c = {p: 0 for p in cand_list}
-
-                        possible = True
-                        for item in cur_items:
-                            w = layer_w_list[item]
-                            best_local = -1
-                            min_local = float('inf')
-                            for p in cand_list:
-                                if t_c[p] < groups_per_pack:
-                                    if t_w[p] < min_local:
-                                        min_local = t_w[p]
-                                        best_local = p
-
-                            if best_local == -1:
-                                possible = False; break
-                            t_p[best_local].append(item)
-                            t_w[best_local] += w
-                            t_c[best_local] += 1
-
-                        if possible:
-                            n_max = max(t_w.values())
-                            n_ss = sum(v**2 for v in t_w.values())
-
-                            if n_max < current_sub_max - 1e-6 or (abs(n_max - current_sub_max) < 1e-6 and n_ss < current_sub_ss - 1e-6):
-                                current_sub_max = n_max
-                                current_sub_ss = n_ss
-                                best_sub_res = (t_p, t_w)
-
-                    if best_sub_res:
-                        s_p, s_w = best_sub_res
-                        for p in cand_list:
-                            packs[p] = s_p[p]
-                            pack_weights[p] = s_w[p]
-                        improved_action = True
-
-                if not improved_action:
-                    break
-
-            # 4. Evaluation
-            current_max = max(pack_weights)
-            current_ss = sum(w*w for w in pack_weights)
-
-            if current_max < best_max_load - 1e-6:
-                best_max_load = current_max
-                best_sum_sq = current_ss
-                best_assignment = [list(p) for p in packs]
-            elif abs(current_max - best_max_load) < 1e-6 and current_ss < best_sum_sq - 1e-6:
-                best_sum_sq = current_ss
-                best_assignment = [list(p) for p in packs]
-
-        # 5. Final assignment for this layer
-        for p, nodes in enumerate(best_assignment):
-            for r, g_idx in enumerate(nodes):
-                pack_index[i, g_idx] = p
-                rank_in_pack[i, g_idx] = r
+                            possible_sub = False; break
+                            
+                    if possible_sub:
+                        c_sub_max = max(w1, w2)
+                        # We accept if we reduce the max load of the pair (strictly)
+                        # or if max is same but variance improves significantly (proxy: abs diff)
+                        # Primary goal: reduce max_val (which is max(w_max, w_other))
+                        
+                        if c_sub_max < sub_best_max - 1e-6:
+                            sub_best_max = c_sub_max
+                            sub_best_assign = (b1, b2)
+                            sub_best_weights = (w1, w2)
+                        elif abs(c_sub_max - sub_best_max) < 1e-6:
+                            # Tie-break with diff
+                            curr_diff = abs(w1 - w2)
+                            best_diff = float('inf')
+                            if sub_best_weights:
+                                best_diff = abs(sub_best_weights[0] - sub_best_weights[1])
+                            else:
+                                best_diff = abs(pack_weights[max_p] - pack_weights[other_p])
+                            
+                            if curr_diff < best_diff - 1e-6:
+                                sub_best_max = c_sub_max
+                                sub_best_assign = (b1, b2)
+                                sub_best_weights = (w1, w2)
+
+                if sub_best_assign:
+                    # Apply move
+                    packs[max_p] = sub_best_assign[0]
+                    packs[other_p] = sub_best_assign[1]
+                    pack_weights[max_p] = sub_best_weights[0]
+                    pack_weights[other_p] = sub_best_weights[1]
+                    improved = True
+                    break # Break inner loop to re-evaluate max_p
+            
+            if improved:
+                continue
+                
+            # Strategy B: 3-Way Shuffle (LNS)
+            # If pairwise didn't work, try mixing max, min, and a random one.
+            if num_packs >= 3:
+                # Candidates
+                candidates = [max_p, min_p]
+                # Pick 1 random distinct
+                for _ in range(10):
+                    r = random.randint(0, num_packs-1)
+                    if r != max_p and r != min_p:
+                        candidates.append(r)
+                        break
+                
+                if len(candidates) == 3:
+                    items_pooled = []
+                    for p in candidates: items_pooled.extend(packs[p])
+                    
+                    old_sub_max = max(pack_weights[p] for p in candidates)
+                    old_sub_ss = sum(pack_weights[p]**2 for p in candidates)
+                    
+                    best_res = None
+                    
+                    # Sort
+                    items_pooled.sort(key=lambda x: layer_w[x], reverse=True)
+                    
+                    for attempt in range(10): # Quick randomized search
+                        if attempt > 0:
+                            order = sorted(items_pooled, key=lambda x: layer_w[x] * (0.9 + 0.2*random.random()), reverse=True)
+                        else:
+                            order = items_pooled
+                        
+                        t_packs = {p: [] for p in candidates}
+                        t_weights = {p: 0.0 for p in candidates}
+                        t_counts = {p: 0 for p in candidates}
+                        
+                        possible_3 = True
+                        for item in order:
+                            w = layer_w[item]
+                            best_b = -1
+                            min_b_w = float('inf')
+                            
+                            for b in candidates:
+                                if t_counts[b] < groups_per_pack:
+                                    if t_weights[b] < min_b_w:
+                                        min_b_w = t_weights[b]
+                                        best_b = b
+                            
+                            if best_b == -1: possible_3=False; break
+                            
+                            t_packs[best_b].append(item)
+                            t_weights[best_b] += w
+                            t_counts[best_b] += 1
+                        
+                        if possible_3:
+                            nm = max(t_weights.values())
+                            nss = sum(v*v for v in t_weights.values())
+                            
+                            if nm < old_sub_max - 1e-6:
+                                old_sub_max = nm
+                                old_sub_ss = nss
+                                best_res = (t_packs, t_weights)
+                            elif abs(nm - old_sub_max) < 1e-6 and nss < old_sub_ss - 1e-6:
+                                old_sub_ss = nss
+                                best_res = (t_packs, t_weights)
+                    
+                    if best_res:
+                        new_ps, new_ws = best_res
+                        for p in candidates:
+                            packs[p] = new_ps[p]
+                            pack_weights[p] = new_ws[p]
+                        improved = True
+            
+            if not improved:
+                break
+
+        # Record result
+        for p in range(num_packs):
+            for r, item_idx in enumerate(packs[p]):
+                pack_index[i, item_idx] = p
+                rank_in_pack[i, item_idx] = r
 
     return pack_index.to(device), rank_in_pack.to(device)
 
 
 def replicate_experts(
         weight: torch.Tensor,
         num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Replicate `num_log` experts to `num_phy` replicas, such that the maximum
     load of all replicas is minimized.
 
     Parameters:
         weight: [X, num_log]
         num_phy: total number of experts after replication
 
     Returns:
         phy2log: [X, num_phy], logical expert id of each physical expert
         rank: [X, num_phy], the replica rank
         logcnt: [X, num_log], number of replicas for each logical expert
     """
     n, num_log = weight.shape
     num_redundant = num_phy - num_log
     assert num_redundant >= 0
     device = weight.device
     phy2log = torch.arange(num_phy, dtype=torch.int64,
                            device=device).repeat(n, 1)
     rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)
     logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)
     arangen = torch.arange(n, dtype=torch.int64, device=device)
     for i in range(num_log, num_phy):
         redundant_indices = (weight / logcnt).max(dim=-1).indices
         phy2log[:, i] = redundant_indices
         rank[:, i] = logcnt[arangen, redundant_indices]
         logcnt[arangen, redundant_indices] += 1
     return phy2log, rank, logcnt
 
 
 def rebalance_experts_hierarchical(
     weight: torch.Tensor,
     num_physical_experts: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ):
     """
     Parameters:
         weight: [num_moe_layers, num_logical_experts]
         num_physical_experts: number of physical experts after replication
         num_groups: number of expert groups
         num_nodes: number of server nodes, where the intra-node network
         (e.g, NVLink) is faster
         num_gpus: number of GPUs, must be a multiple of `num_nodes`
 
     Returns:
         physical_to_logical_map: [num_moe_layers, num_physical_experts]
         logical_to_physical_map: [num_moe_layers, num_logical_experts, X]
         logical_count: [num_moe_layers, num_logical_experts]
     """
     num_layers, num_logical_experts = weight.shape
     assert num_logical_experts % num_groups == 0
     group_size = num_logical_experts // num_groups
     assert num_groups % num_nodes == 0
     groups_per_node = num_groups // num_nodes
     assert num_gpus % num_nodes == 0
     assert num_physical_experts % num_gpus == 0
     phy_experts_per_gpu = num_physical_experts // num_gpus
 
     def inverse(perm: torch.Tensor) -> torch.Tensor:
         inv = torch.empty_like(perm)
         inv.scatter_(
             1,
             perm,
             torch.arange(perm.size(1), dtype=torch.int64,
                          device=perm.device).expand(perm.shape),
         )
         return inv
 
     # Step 1: pack groups to nodes
     tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)
     group_pack_index, group_rank_in_pack = balanced_packing(
         tokens_per_group, num_nodes)
     log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *
                  group_size).unsqueeze(-1) +
                 torch.arange(group_size,
                              dtype=torch.int64,
                              device=group_pack_index.device)).flatten(-2)
     mlog2log = inverse(log2mlog)
 
     # Step 2: construct redundant experts within nodes
     # [num_layers * num_nodes, num_logical_experts // num_nodes]
     tokens_per_mlog = weight.gather(-1, mlog2log).view(
         -1, num_logical_experts // num_nodes)
     phy2mlog, phyrank, mlogcnt = replicate_experts(
         tokens_per_mlog, num_physical_experts // num_nodes)
 
     # Step 3: pack physical_experts to GPUs
     # [num_layers * num_nodes, num_physical_experts // num_nodes]
     tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
     pack_index, rank_in_pack = balanced_packing(tokens_per_phy,
                                                 num_gpus // num_nodes)
     phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
     pphy2phy = inverse(phy2pphy)
 
     pphy2mlog = phy2mlog.gather(
         -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]
     pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(
         0,
         num_logical_experts,
         num_logical_experts // num_nodes,
         device=group_pack_index.device,
     ).view(1, -1, 1)).flatten(-2)
     pphy2log = mlog2log.gather(-1, pphy2mlog)
     pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)
     logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)
     return pphy2log, pphyrank, logcnt
 
 
 def rebalance_experts(
     weight: torch.Tensor,
     num_replicas: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Entry point for expert-parallelism load balancer.
 
     Parameters:
         weight: [layers, num_logical_experts], the load statistics for all
             logical experts
         num_replicas: number of physical experts, must be a multiple of
             `num_gpus`
         num_groups: number of expert groups
         num_nodes: number of server nodes, where the intra-node network
             (e.g, NVLink) is faster
         num_gpus: number of GPUs, must be a multiple of `num_nodes`
 
     Returns:
         physical_to_logical_map: [layers, num_replicas], the expert index of
             each replica
         logical_to_physical_map: [layers, num_logical_experts, X], the replica
             indices for each expert
         expert_count: [layers, num_logical_experts], number of physical
             replicas for each logical expert
     """
     num_layers, num_logical_experts = weight.shape
     weight = weight.float().cpu()
     if num_groups % num_nodes == 0:
         # use hierarchical load-balance policy
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, num_groups, num_nodes, num_gpus)
     else:
         # use global load-balance policy
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, 1, 1, num_gpus)
     num_redundant_experts = num_replicas - num_logical_experts
     maxlogcnt = num_redundant_experts + 1
     log2phy: torch.Tensor = torch.full(
         (num_layers, num_logical_experts, maxlogcnt),
         -1,
         dtype=torch.int64,
         device=logcnt.device,
     )
     log2phy.view(num_layers, -1).scatter_(
         -1,
         phy2log * maxlogcnt + phyrank,
         torch.arange(num_replicas, dtype=torch.int64,
                      device=log2phy.device).expand(num_layers, -1),
     )
     return phy2log, log2phy, logcnt
 
 
 # EVOLVE-BLOCK-END
 
 
 # This part remains fixed (not evolved)
 def run_eplb(weight: torch.Tensor, num_replicas: int, num_groups: int,
              num_nodes: int, num_gpus: int):
     """Run the expert parallelism load balancer"""
     phy2log, log2phy, logcnt = rebalance_experts(
         weight, num_replicas, num_groups, num_nodes, num_gpus
     )
     return phy2log, log2phy, logcnt
 
 
 __all__ = ["rebalance_experts", "run_eplb"]