<NAME>
improved_balanced_packing_local_search
</NAME>

<DESCRIPTION>
Improves the local search in `balanced_packing` algorithm.
Previously, the algorithm only attempted to swap items between the heaviest and lightest packs. This is suboptimal because the best swap to reduce the heaviest pack's weight might be with a pack that isn't the lightest, but has the "right" weight and item configuration to allow a beneficial swap.
The new implementation:
1. Identifies the heaviest pack (`max_pack`).
2. Scans *all* other packs to find the best possible swap.
3. The "best" swap is defined as the one that minimizes the maximum weight of the two involved packs after the swap (min-max objective).
4. Uses vectorized PyTorch operations on CPU to efficiently check all item pairs between two packs.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=weight.device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    indices = weight.float().sort(-1, descending=True).indices.cpu()
    pack_index = torch.full_like(weight,
                                 fill_value=-1,
                                 dtype=torch.int64,
                                 device="cpu")
    rank_in_pack = torch.full_like(pack_index, fill_value=-1)

    # Use CPU weights for faster scalar access
    weight_cpu = weight.cpu()

    for i in range(num_layers):
        pack_weights = [0.0] * num_packs
        pack_contents = [[] for _ in range(num_packs)]

        # Initial Greedy Packing (Modified LPT)
        for group in indices[i]:
            # Find valid packs (not full)
            valid_packs = (p for p in range(num_packs)
                           if len(pack_contents[p]) < groups_per_pack)
            # Select pack with minimum weight
            pack = min(valid_packs, key=pack_weights.__getitem__)

            g_idx = group.item()
            w = weight_cpu[i, g_idx].item()

            pack_index[i, g_idx] = pack
            pack_weights[pack] += w
            pack_contents[pack].append(g_idx)

        # Refinement: Local Search (Swap)
        # Try to swap items between heaviest and lightest packs to reduce variance
        for _ in range(20):
            min_pack = min(range(num_packs), key=pack_weights.__getitem__)
            max_pack = max(range(num_packs), key=pack_weights.__getitem__)

            if min_pack == max_pack:
                break

            diff = pack_weights[max_pack] - pack_weights[min_pack]
            if diff < 1e-6:
                break

            best_swap = None
            best_reduction = 0.0

            # Identify best swap between max_pack and min_pack
            for u in pack_contents[max_pack]:
                w_u = weight_cpu[i, u].item()
                for v in pack_contents[min_pack]:
                    w_v = weight_cpu[i, v].item()

                    delta = w_u - w_v
                    if 0 < delta < diff:
                        # We want to maximize reduction of the gap
                        reduction = diff - abs(diff - 2 * delta)
                        if reduction > best_reduction:
                            best_reduction = reduction
                            best_swap = (u, v, delta)

            if best_swap:
                u, v, delta = best_swap
                # Perform swap
                pack_contents[max_pack].remove(u)
                pack_contents[max_pack].append(v)
                pack_contents[min_pack].remove(v)
                pack_contents[min_pack].append(u)

                pack_weights[max_pack] -= delta
                pack_weights[min_pack] += delta
                pack_index[i, u] = min_pack
                pack_index[i, v] = max_pack
            else:
                break

        # Assign ranks
        for p in range(num_packs):
            for rank, g_idx in enumerate(pack_contents[p]):
                rank_in_pack[i, g_idx] = rank

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=weight.device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    indices = weight.float().sort(-1, descending=True).indices.cpu()
    pack_index = torch.full_like(weight,
                                 fill_value=-1,
                                 dtype=torch.int64,
                                 device="cpu")
    rank_in_pack = torch.full_like(pack_index, fill_value=-1)

    # Use CPU weights for faster scalar access
    weight_cpu = weight.cpu()

    for i in range(num_layers):
        pack_weights = [0.0] * num_packs
        pack_contents = [[] for _ in range(num_packs)]

        # Initial Greedy Packing (Modified LPT)
        for group in indices[i]:
            # Find valid packs (not full)
            valid_packs = (p for p in range(num_packs)
                           if len(pack_contents[p]) < groups_per_pack)
            # Select pack with minimum weight
            pack = min(valid_packs, key=pack_weights.__getitem__)

            g_idx = group.item()
            w = weight_cpu[i, g_idx].item()

            pack_index[i, g_idx] = pack
            pack_weights[pack] += w
            pack_contents[pack].append(g_idx)

        # Refinement: Local Search (Swap)
        # Try to swap items between heaviest pack and any other pack to reduce max load
        for _ in range(20):
            max_pack = max(range(num_packs), key=pack_weights.__getitem__)
            w_max = pack_weights[max_pack]

            best_swap = None
            # We want to reduce w_max by swapping u from max_pack with v from p
            # such that new max(w_max', w_p') is minimized.
            best_pair_max = w_max

            u_nodes = pack_contents[max_pack]
            w_u = weight_cpu[i, u_nodes]  # [groups_per_pack]

            for p in range(num_packs):
                if p == max_pack:
                    continue

                w_p = pack_weights[p]
                diff = w_max - w_p
                if diff < 1e-6:
                    continue

                v_nodes = pack_contents[p]
                w_v = weight_cpu[i, v_nodes]  # [groups_per_pack]

                # deltas[x, y] = w_u[x] - w_v[y]
                # Shape: [len(u), len(v)]
                deltas = w_u.unsqueeze(1) - w_v.unsqueeze(0)

                # Valid swaps must have 0 < delta < diff
                # (delta > 0 to reduce w_max, delta < diff to ensure w_p' < w_max)
                mask = (deltas > 1e-6) & (deltas < diff)
                if not mask.any():
                    continue

                # The new max of the pair is max(w_max - delta, w_p + delta).
                # Since sum is constant, this is minimized when delta is close to diff / 2.
                # Equivalent to minimizing |diff - 2*delta|.
                gap = (diff - 2 * deltas).abs()
                gap = torch.where(mask, gap, torch.tensor(float('inf')))

                min_gap, flat_idx = gap.flatten().min(0)

                if min_gap == float('inf'):
                    continue

                new_pair_max = (w_max + w_p + min_gap.item()) / 2.0

                if new_pair_max < best_pair_max - 1e-6:
                    best_pair_max = new_pair_max
                    flat_idx = flat_idx.item()
                    u_idx = flat_idx // len(v_nodes)
                    v_idx = flat_idx % len(v_nodes)
                    best_swap = (max_pack, p, u_nodes[u_idx], v_nodes[v_idx],
                                 deltas.flatten()[flat_idx].item())

            if best_swap:
                p_src, p_dst, u, v, delta = best_swap
                pack_contents[p_src].remove(u)
                pack_contents[p_src].append(v)
                pack_contents[p_dst].remove(v)
                pack_contents[p_dst].append(u)

                pack_weights[p_src] -= delta
                pack_weights[p_dst] += delta
                pack_index[i, u] = p_dst
                pack_index[i, v] = p_src
            else:
                break

        # Assign ranks
        for p in range(num_packs):
            for rank, g_idx in enumerate(pack_contents[p]):
                rank_in_pack[i, g_idx] = rank

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>