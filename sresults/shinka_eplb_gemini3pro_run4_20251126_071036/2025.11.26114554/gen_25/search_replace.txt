<NAME>
iterative_refinement_lns
</NAME>

<DESCRIPTION>
Improve load balancing by enhancing the expert packing algorithm.
1. Increase the number of randomized restarts in the initialization phase to better explore the solution space.
2. Introduce an iterative refinement strategy that interleaves vectorized 1-opt swaps with a Large Neighborhood Search (LNS) step. The LNS step extracts the heaviest, lightest, and a random third pack, pools their items, and re-packs them using a constrained greedy heuristic. This allows for complex multi-item moves (equivalent to 3-way or k-way swaps) that can escape local optima where simple pairwise swaps fail.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        # 2. Perturbed (only if we have enough groups for it to matter)
        if groups_per_pack > 2:
            # Add small random noise to explore different greedy decisions
            noise = torch.rand(num_groups) * 0.05 + 1.0 # 0-5% noise
            indices_rand = (layer_weights_t * noise).sort(descending=True).indices.tolist()
            candidates.append(indices_rand)

        best_assignment = None
=======
        # 2. Perturbed (only if we have enough groups for it to matter)
        if groups_per_pack > 1:
            # Add small random noise to explore different greedy decisions
            # Generate multiple candidates to improve diversity
            for _ in range(4):
                noise = torch.rand(num_groups) * 0.1 + 0.95 # +/- 5% noise
                indices_rand = (layer_weights_t * noise).sort(descending=True).indices.tolist()
                candidates.append(indices_rand)

        best_assignment = None
>>>>>>> REPLACE
<<<<<<< SEARCH
        # Convert best assignment to tensor for vectorized refinement
        # [num_packs, groups_per_pack]
        pack_assignment = torch.tensor(best_assignment, dtype=torch.int64)
        pack_weights = torch.tensor(best_pack_weights, dtype=torch.float32)

        # Refinement: Vectorized Local Search
        # Uses broadcasting to evaluate swaps between the heaviest pack and all others simultaneously

        for _ in range(50):
            max_pack = torch.argmax(pack_weights).item()
            max_w = pack_weights[max_pack].item()

            # Items in max pack: [G]
            u_indices = pack_assignment[max_pack]
            w_u = layer_weights_t[u_indices].view(1, groups_per_pack, 1) # [1, G, 1]

            # Items in all packs: [M, G]
            w_v = layer_weights_t[pack_assignment].view(num_packs, 1, groups_per_pack) # [M, 1, G]

            # Compute deltas: w_u - w_v
            deltas = w_u - w_v # [M, G, G]

            # Diff with max pack
            diffs = (max_w - pack_weights).view(num_packs, 1, 1) # [M, 1, 1]

            # We want to swap such that max_load is reduced.
            # New max load of the pair will be approx max(w_max - delta, w_p + delta).
            # This is minimized when delta is close to diff/2.
            # Gain metric: delta * (diff - delta) is parabolic peaking at delta = diff/2.

            mask = (deltas > 1e-6) & (deltas < diffs)

            if not mask.any():
                break

            gains = deltas * (diffs - deltas)
            gains = torch.where(mask, gains, -1.0)

            best_flat = torch.argmax(gains).item()
            max_gain = gains.view(-1)[best_flat].item()

            if max_gain < 0:
                break

            # Perform Swap
            # Decode indices from flat index [M * G * G]
            best_p = best_flat // (groups_per_pack * groups_per_pack)
            rem = best_flat % (groups_per_pack * groups_per_pack)
            best_u_idx = rem // groups_per_pack
            best_v_idx = rem % groups_per_pack

            # Swap in assignment tensor
            u_val = pack_assignment[max_pack, best_u_idx].item()
            v_val = pack_assignment[best_p, best_v_idx].item()

            pack_assignment[max_pack, best_u_idx] = v_val
            pack_assignment[best_p, best_v_idx] = u_val

            # Update weights
            d_val = deltas[best_p, best_u_idx, best_v_idx].item()
            pack_weights[max_pack] -= d_val
            pack_weights[best_p] += d_val
=======
        # Convert best assignment to tensor for vectorized refinement
        # [num_packs, groups_per_pack]
        pack_assignment = torch.tensor(best_assignment, dtype=torch.int64)
        pack_weights = torch.tensor(best_pack_weights, dtype=torch.float32)

        # Iterative Refinement: Mix of Vectorized Swaps and LNS
        for outer_loop in range(3):
            # Phase 1: Vectorized Local Search (Steepest Descent 1-Swap)
            # Uses broadcasting to evaluate swaps between the heaviest pack and all others simultaneously
            improved_swap = False
            for _ in range(20):
                max_pack = torch.argmax(pack_weights).item()
                max_w = pack_weights[max_pack].item()

                # Items in max pack: [G]
                u_indices = pack_assignment[max_pack]
                w_u = layer_weights_t[u_indices].view(1, groups_per_pack, 1) # [1, G, 1]

                # Items in all packs: [M, G]
                w_v = layer_weights_t[pack_assignment].view(num_packs, 1, groups_per_pack) # [M, 1, G]

                # Compute deltas: w_u - w_v
                deltas = w_u - w_v # [M, G, G]

                # Diff with max pack
                diffs = (max_w - pack_weights).view(num_packs, 1, 1) # [M, 1, 1]

                # We want to swap such that max_load is reduced.
                mask = (deltas > 1e-6) & (deltas < diffs)

                if not mask.any():
                    break

                gains = deltas * (diffs - deltas)
                gains = torch.where(mask, gains, -1.0)

                best_flat = torch.argmax(gains).item()
                max_gain = gains.view(-1)[best_flat].item()

                if max_gain < 0:
                    break

                # Perform Swap
                best_p = best_flat // (groups_per_pack * groups_per_pack)
                rem = best_flat % (groups_per_pack * groups_per_pack)
                best_u_idx = rem // groups_per_pack
                best_v_idx = rem % groups_per_pack

                u_val = pack_assignment[max_pack, best_u_idx].item()
                v_val = pack_assignment[best_p, best_v_idx].item()

                pack_assignment[max_pack, best_u_idx] = v_val
                pack_assignment[best_p, best_v_idx] = u_val

                d_val = deltas[best_p, best_u_idx, best_v_idx].item()
                pack_weights[max_pack] -= d_val
                pack_weights[best_p] += d_val
                improved_swap = True

            # Phase 2: LNS (3-Pack Repacking)
            # Pick heaviest, lightest, and one random pack to reshuffle
            # This helps break out of local optima where 1-swaps fail
            max_p = torch.argmax(pack_weights).item()
            min_p = torch.argmin(pack_weights).item()

            possible_others = [p for p in range(num_packs) if p != max_p and p != min_p]
            if possible_others:
                # Deterministic rotation based on loop index to cover different combos
                other_p = possible_others[outer_loop % len(possible_others)]
                lns_packs = [max_p, min_p, other_p]
            elif max_p != min_p:
                lns_packs = [max_p, min_p]
            else:
                break

            # Extract items
            items_flat = pack_assignment[lns_packs].view(-1).tolist()
            weights_flat = [layer_weights[idx] for idx in items_flat]

            # Sort for LPT
            item_pairs = sorted(zip(weights_flat, items_flat), key=lambda x: x[0], reverse=True)

            # Greedy repack
            new_bins = [[] for _ in lns_packs]
            new_bin_weights = [0.0] * len(lns_packs)

            for w, idx in item_pairs:
                # Find best bin (min weight and not full)
                best_b = -1
                min_bw = float('inf')
                for b in range(len(lns_packs)):
                    if len(new_bins[b]) < groups_per_pack:
                        if new_bin_weights[b] < min_bw:
                            min_bw = new_bin_weights[b]
                            best_b = b
                new_bins[best_b].append(idx)
                new_bin_weights[best_b] += w

            # Check for improvement (minimized max load within this subset)
            old_local_max = max(pack_weights[p].item() for p in lns_packs)
            new_local_max = max(new_bin_weights)

            # Accept if max load reduced, or same max load but better variance (sum of squares)
            if new_local_max < old_local_max - 1e-6:
                accept = True
            elif abs(new_local_max - old_local_max) < 1e-6:
                old_ss = sum(pack_weights[p].item()**2 for p in lns_packs)
                new_ss = sum(w**2 for w in new_bin_weights)
                accept = new_ss < old_ss - 1e-6
            else:
                accept = False

            if accept:
                for b_i, p_idx in enumerate(lns_packs):
                    pack_assignment[p_idx] = torch.tensor(new_bins[b_i], dtype=torch.int64)
                    pack_weights[p_idx] = new_bin_weights[b_i]

            if not improved_swap and not accept:
                break
>>>>>>> REPLACE
</DIFF>