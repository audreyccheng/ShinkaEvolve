<NAME>
improved_lns_with_optimal_swap
</NAME>

<DESCRIPTION>
This edit improves the `balanced_packing` function by refining the Large Neighborhood Search (LNS) strategy. Specifically:
1. It introduces a deterministic O(N^2) pairwise swap check between the heaviest and lightest packs to optimally solve 1-swaps. This overcomes local optima where randomized greedy might fail.
2. It enhances the LNS repacking step by using multiple randomized greedy attempts for the chosen subset of packs (Heaviest, Lightest, Random), rather than a single attempt.
3. It uses Python lists for weights to avoid Tensor access overhead in the inner loops.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs
    device = weight.device

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Operations on CPU are generally faster for this sequential/iterative logic
    weight_cpu = weight.cpu()

    # Pre-allocate output tensors
    pack_index = torch.empty(weight.shape, dtype=torch.int64, device=device)
    rank_in_pack = torch.empty(weight.shape, dtype=torch.int64, device=device)

    # Number of restarts for randomized greedy
    NUM_RESTARTS = 20

    for i in range(num_layers):
        layer_weights = weight_cpu[i]

        # Base sort indices (LPT)
        base_indices = layer_weights.argsort(descending=True).tolist()

        best_assignment = None
        best_max_load = float('inf')

        for attempt in range(NUM_RESTARTS):
            # 1. Randomized Candidate Generation
            if attempt == 0:
                indices = base_indices
            else:
                # Add noise to weights to perturb sort order
                # 5% noise is usually sufficient to swap similar items
                noise = torch.rand(num_groups) * 0.05
                indices = (layer_weights * (1 + noise)).argsort(descending=True).tolist()

            # 2. Greedy Construction (LPT / Best Fit)
            # pack_contents: list of lists storing item indices
            pack_contents = [[] for _ in range(num_packs)]
            pack_weights = [0.0] * num_packs
            pack_sizes = [0] * num_packs

            for idx in indices:
                w = layer_weights[idx].item()

                # Find lightest valid pack
                best_p = -1
                min_w = float('inf')

                # Simple linear scan is fast for small num_packs
                for p in range(num_packs):
                    if pack_sizes[p] < groups_per_pack:
                        if pack_weights[p] < min_w:
                            min_w = pack_weights[p]
                            best_p = p

                pack_contents[best_p].append(idx)
                pack_weights[best_p] += w
                pack_sizes[best_p] += 1

            # 3. Refinement: Large Neighborhood Search (LNS)
            # Iteratively improve by repacking subsets of bins

            MAX_LNS_STEPS = 50
            consecutive_no_improve = 0

            for step in range(MAX_LNS_STEPS):
                # Sort packs by weight
                sorted_packs = sorted(range(num_packs), key=lambda x: pack_weights[x], reverse=True)
                max_p = sorted_packs[0]
                min_p = sorted_packs[-1]

                # If balanced enough, stop
                if pack_weights[max_p] - pack_weights[min_p] < 1e-6:
                    break

                # Select candidates for LNS: Heaviest, Lightest, and potentially a random one
                candidates = [max_p, min_p]
                if num_packs > 2:
                    # Pick a random pack that is not max or min to facilitate 3-way/4-way cyclic shifts
                    # Use a simple pseudo-random approach based on step
                    candidates.append(sorted_packs[(step + 1) % (num_packs - 1) + 1])

                # Collect items from candidates
                items_to_repack = []
                for p in candidates:
                    items_to_repack.extend(pack_contents[p])

                # Sort descending (LPT)
                items_to_repack.sort(key=lambda x: layer_weights[x].item(), reverse=True)

                # Repack into these bins using Bounded Best Fit
                new_loads = {p: 0.0 for p in candidates}
                new_contents = {p: [] for p in candidates}
                possible = True

                for item_idx in items_to_repack:
                    w = layer_weights[item_idx].item()
                    # Assign to candidate bin with min current load that has space
                    best_local_p = -1
                    min_local_w = float('inf')
                    for p in candidates:
                        if len(new_contents[p]) < groups_per_pack:
                            if new_loads[p] < min_local_w:
                                min_local_w = new_loads[p]
                                best_local_p = p

                    if best_local_p == -1:
                        possible = False
                        break

                    new_contents[best_local_p].append(item_idx)
                    new_loads[best_local_p] += w

                if possible:
                    # Check improvement criteria
                    old_local_max = max(pack_weights[p] for p in candidates)
                    new_local_max = max(new_loads.values())

                    # Accept if max load decreases OR max load is same but variance decreases (better balance)
                    # Sum of squares is a good proxy for variance
                    old_ss = sum(pack_weights[p]**2 for p in candidates)
                    new_ss = sum(val**2 for val in new_loads.values())

                    if new_local_max < old_local_max - 1e-6 or (abs(new_local_max - old_local_max) < 1e-6 and new_ss < old_ss - 1e-6):
                        # Apply Update
                        for p in candidates:
                            pack_contents[p] = new_contents[p]
                            pack_weights[p] = new_loads[p]
                            pack_sizes[p] = groups_per_pack # Should be full
                        consecutive_no_improve = 0
                    else:
                        consecutive_no_improve += 1
                else:
                    consecutive_no_improve += 1

                if consecutive_no_improve >= 5:
                    break

            # 4. Check global best
            current_max = max(pack_weights)
            if current_max < best_max_load - 1e-6:
                best_max_load = current_max
                best_assignment = [list(p) for p in pack_contents]

        # 5. Write best assignment to output tensors
        # Flatten for scatter
        flat_indices = []
        flat_packs = []
        flat_ranks = []

        for p, items in enumerate(best_assignment):
            for r, idx in enumerate(items):
                flat_indices.append(idx)
                flat_packs.append(p)
                flat_ranks.append(r)

        idx_tensor = torch.tensor(flat_indices, dtype=torch.int64, device=device)
        pack_tensor = torch.tensor(flat_packs, dtype=torch.int64, device=device)
        rank_tensor = torch.tensor(flat_ranks, dtype=torch.int64, device=device)

        pack_index[i].scatter_(0, idx_tensor, pack_tensor)
        rank_in_pack[i].scatter_(0, idx_tensor, rank_tensor)

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs
    device = weight.device

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Operations on CPU are generally faster for this sequential/iterative logic
    weight_cpu = weight.cpu()

    # Pre-allocate output tensors
    pack_index = torch.empty(weight.shape, dtype=torch.int64, device=device)
    rank_in_pack = torch.empty(weight.shape, dtype=torch.int64, device=device)

    # Number of restarts for randomized greedy
    NUM_RESTARTS = 10

    import random

    for i in range(num_layers):
        layer_weights = weight_cpu[i]
        layer_weights_list = layer_weights.tolist()

        # Base sort indices (LPT)
        base_indices = layer_weights.argsort(descending=True).tolist()

        best_assignment = None
        best_max_load = float('inf')
        best_ss = float('inf')

        for attempt in range(NUM_RESTARTS):
            # 1. Randomized Candidate Generation
            if attempt == 0:
                indices = base_indices
            else:
                # Add noise to weights to perturb sort order
                noise = torch.rand(num_groups) * 0.1
                indices = (layer_weights * (1 + noise)).argsort(descending=True).tolist()

            # 2. Greedy Construction (LPT / Best Fit)
            pack_contents = [[] for _ in range(num_packs)]
            pack_weights = [0.0] * num_packs
            pack_sizes = [0] * num_packs

            for idx in indices:
                w = layer_weights_list[idx]

                # Find lightest valid pack
                best_p = -1
                min_w = float('inf')

                for p in range(num_packs):
                    if pack_sizes[p] < groups_per_pack:
                        if pack_weights[p] < min_w:
                            min_w = pack_weights[p]
                            best_p = p

                pack_contents[best_p].append(idx)
                pack_weights[best_p] += w
                pack_sizes[best_p] += 1

            # 3. Refinement: Large Neighborhood Search (LNS)
            MAX_LNS_STEPS = 50
            consecutive_no_improve = 0

            # Initial sort of packs
            sorted_packs_idx = sorted(range(num_packs), key=lambda x: pack_weights[x], reverse=True)
            current_max_p = sorted_packs_idx[0]
            current_min_p = sorted_packs_idx[-1]

            for step in range(MAX_LNS_STEPS):
                # Check termination
                if pack_weights[current_max_p] - pack_weights[current_min_p] < 1e-6:
                    break

                improved_iter = False

                # Strategy A: Optimal Pairwise Swap (Max <-> Min)
                # Try to swap one item from max pack with one from min pack
                # to strictly reduce the max load of the two
                max_w_val = pack_weights[current_max_p]
                min_w_val = pack_weights[current_min_p]

                best_swap_gain = -1.0
                best_u_idx = -1
                best_v_idx = -1

                # Check all pairs
                for u_i, u_idx in enumerate(pack_contents[current_max_p]):
                    w_u = layer_weights_list[u_idx]
                    for v_i, v_idx in enumerate(pack_contents[current_min_p]):
                        w_v = layer_weights_list[v_idx]

                        delta = w_u - w_v
                        # We want to move positive weight from max to min

                        new_w_max = max_w_val - delta
                        new_w_min = min_w_val + delta

                        if new_w_max < max_w_val - 1e-6 and new_w_min < max_w_val - 1e-6:
                            # Improvement in max load
                            gain = max_w_val - max(new_w_max, new_w_min)
                            if gain > best_swap_gain:
                                best_swap_gain = gain
                                best_u_idx = u_i
                                best_v_idx = v_i

                if best_swap_gain > 0:
                    # Perform Swap
                    u = pack_contents[current_max_p][best_u_idx]
                    v = pack_contents[current_min_p][best_v_idx]

                    pack_contents[current_max_p][best_u_idx] = v
                    pack_contents[current_min_p][best_v_idx] = u

                    w_u = layer_weights_list[u]
                    w_v = layer_weights_list[v]

                    pack_weights[current_max_p] -= (w_u - w_v)
                    pack_weights[current_min_p] += (w_u - w_v)

                    improved_iter = True
                    consecutive_no_improve = 0
                else:
                    # Strategy B: 3-Way Randomized Repacking
                    candidates = [current_max_p, current_min_p]
                    if num_packs > 2:
                        # Pick a random pack
                        p3 = random.randint(0, num_packs - 1)
                        while p3 in candidates:
                            p3 = random.randint(0, num_packs - 1)
                        candidates.append(p3)

                    items_to_repack = []
                    for p in candidates:
                        items_to_repack.extend(pack_contents[p])

                    # Try multiple randomized greedy packings on this subset
                    best_sub_load = max(pack_weights[p] for p in candidates)
                    best_sub_ss = sum(pack_weights[p]**2 for p in candidates)
                    best_sub_packs = None
                    best_sub_weights = None
                    found_sub_better = False

                    # Sort data for repacking
                    item_data = [(layer_weights_list[x], x) for x in items_to_repack]
                    item_data.sort(key=lambda x: x[0], reverse=True)

                    for sub_attempt in range(5):
                        if sub_attempt == 0:
                            current_items = item_data
                        else:
                            # Perturb
                            current_items = sorted(item_data, key=lambda x: x[0] * (0.9 + 0.2*random.random()), reverse=True)

                        temp_contents = {p: [] for p in candidates}
                        temp_weights = {p: 0.0 for p in candidates}
                        temp_counts = {p: 0 for p in candidates}

                        possible = True
                        for w, idx in current_items:
                            # Greedy Best Fit on subset
                            best_b = -1
                            min_local = float('inf')
                            for p in candidates:
                                if temp_counts[p] < groups_per_pack:
                                    if temp_weights[p] < min_local:
                                        min_local = temp_weights[p]
                                        best_b = p
                            if best_b == -1:
                                possible = False
                                break
                            temp_contents[best_b].append(idx)
                            temp_weights[best_b] += w
                            temp_counts[best_b] += 1

                        if possible:
                            new_max = max(temp_weights.values())
                            new_ss = sum(v**2 for v in temp_weights.values())

                            if new_max < best_sub_load - 1e-6:
                                best_sub_load = new_max
                                best_sub_ss = new_ss
                                best_sub_packs = temp_contents
                                best_sub_weights = temp_weights
                                found_sub_better = True
                            elif abs(new_max - best_sub_load) < 1e-6 and new_ss < best_sub_ss - 1e-6:
                                best_sub_ss = new_ss
                                best_sub_packs = temp_contents
                                best_sub_weights = temp_weights
                                found_sub_better = True

                    if found_sub_better:
                        for p in candidates:
                            pack_contents[p] = best_sub_packs[p]
                            pack_weights[p] = best_sub_weights[p]
                        improved_iter = True
                        consecutive_no_improve = 0
                    else:
                        consecutive_no_improve += 1

                # Update Max/Min pointers
                current_max_p = -1
                current_min_p = -1
                max_w_val = -1.0
                min_w_val = float('inf')
                for p_idx, pw in enumerate(pack_weights):
                    if pw > max_w_val:
                        max_w_val = pw
                        current_max_p = p_idx
                    if pw < min_w_val:
                        min_w_val = pw
                        current_min_p = p_idx

                if not improved_iter:
                     if consecutive_no_improve >= 5:
                         break

            # 4. Check global best
            current_max = max(pack_weights)
            current_ss = sum(w**2 for w in pack_weights)

            if current_max < best_max_load - 1e-6:
                best_max_load = current_max
                best_ss = current_ss
                best_assignment = [list(p) for p in pack_contents]
            elif abs(current_max - best_max_load) < 1e-6:
                if current_ss < best_ss - 1e-6:
                    best_ss = current_ss
                    best_assignment = [list(p) for p in pack_contents]

        # 5. Write best assignment to output tensors
        # Flatten for scatter
        flat_indices = []
        flat_packs = []
        flat_ranks = []

        for p, items in enumerate(best_assignment):
            for r, idx in enumerate(items):
                flat_indices.append(idx)
                flat_packs.append(p)
                flat_ranks.append(r)

        idx_tensor = torch.tensor(flat_indices, dtype=torch.int64, device=device)
        pack_tensor = torch.tensor(flat_packs, dtype=torch.int64, device=device)
        rank_tensor = torch.tensor(flat_ranks, dtype=torch.int64, device=device)

        pack_index[i].scatter_(0, idx_tensor, pack_tensor)
        rank_in_pack[i].scatter_(0, idx_tensor, rank_tensor)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>