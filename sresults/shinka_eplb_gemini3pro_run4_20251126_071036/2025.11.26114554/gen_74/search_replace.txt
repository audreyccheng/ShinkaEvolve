<NAME>
pooled_repartitioning
</NAME>

<DESCRIPTION>
I replace the pairwise item swap strategy in `balanced_packing` with a "pooled 2-pack repartitioning" strategy.

The previous refinement phase relied on swapping a single item from the heaviest pack with a single item from a lighter pack. This often fails when optimal balancing requires multi-item exchanges (e.g., swapping one large item for two smaller ones) or when single swaps cannot reduce the max load without violating the pack size constraint.

The new algorithm works as follows:
1.  **Selection**: In each refinement step, identify the heaviest pack ($P_{max}$) and iterate through other packs ($P_{other}$), prioritizing lighter ones.
2.  **Pooling**: Combine all items from $P_{max}$ and $P_{other}$ into a temporary pool.
3.  **Repartitioning**: Use a randomized greedy heuristic (Longest Processing Time with noise) to partition this pool back into two valid packs. The goal is to minimize the maximum weight of the two new packs.
4.  **Acceptance**: If the new partition results in a lower maximum weight than the original $P_{max}$, accept the change.

To afford the computational cost of this more intensive refinement, I reduce the number of outer loop restarts from 4 to 2. This effectively shifts the compute budget from global random exploration to local optimization power, which is typically more effective for balancing problems where the "shape" of the solution is constrained by item granularity.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Handle trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups,
                                  dtype=torch.int64,
                                  device=weight.device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(pack_index)
        return pack_index, rank_in_pack

    # CPU processing for scalar efficiency
    weight_cpu = weight.to("cpu", dtype=torch.float32)
    pack_index = torch.empty((num_layers, num_groups), dtype=torch.int64, device="cpu")
    rank_in_pack = torch.empty((num_layers, num_groups), dtype=torch.int64, device="cpu")

    # Hyperparameters
    num_restarts = 4

    for i in range(num_layers):
        row_weight = weight_cpu[i]

        best_diff = float('inf')
        best_packs = None

        # Original indices sorted by weight descending
        # We use a stable sort or just standard sort
        # items: list of (index, weight)
        original_items = []
        for idx in range(num_groups):
            original_items.append((idx, row_weight[idx].item()))

        # Sort descending
        original_items.sort(key=lambda x: x[1], reverse=True)

        for attempt in range(num_restarts):
            # 1. Initialization
            current_packs = [[] for _ in range(num_packs)]
            pack_weights = [0.0] * num_packs

            if attempt == 0:
                # Deterministic Snake/ZigZag Packing
                # Distributes high/low weights evenly
                for k, (idx, w) in enumerate(original_items):
                    # Zig-zag logic
                    # 0, 1, 2, ..., M-1, M-1, ..., 2, 1, 0
                    cycle_pos = k % (2 * num_packs)
                    if cycle_pos < num_packs:
                        p = cycle_pos
                    else:
                        p = (2 * num_packs - 1) - cycle_pos

                    current_packs[p].append((idx, w))
                    pack_weights[p] += w
            else:
                # Randomized Greedy
                # Shuffle items slightly by adding noise to sorting key
                # Re-sort based on noisy weights
                noise = torch.rand(num_groups) * 0.1 + 0.95 # 0.95 to 1.05
                noisy_items = []
                for k, (idx, w) in enumerate(original_items):
                    noisy_items.append((idx, w, w * noise[k].item()))
                noisy_items.sort(key=lambda x: x[2], reverse=True)

                # Simple Greedy: Min-Load
                # Just fill in order
                for idx, w, _ in noisy_items:
                    # Find pack with min weight that has capacity
                    best_p = -1
                    min_w = float('inf')
                    for p in range(num_packs):
                        if len(current_packs[p]) < groups_per_pack:
                            if pack_weights[p] < min_w:
                                min_w = pack_weights[p]
                                best_p = p
                    current_packs[best_p].append((idx, w))
                    pack_weights[best_p] += w

            # 2. Refinement Loop
            # We want to minimize the Makespan (Max Load).
            # Strategy: Repeatedly take the Heaviest Pack and try to offload to lighter packs.

            # Optimization: keep packs somewhat sorted or just identify Max/Min
            for _ in range(25): # Iterations
                # Find Max Pack
                max_p = 0
                max_w = pack_weights[0]
                min_p = 0
                min_w = pack_weights[0]

                for p in range(1, num_packs):
                    w = pack_weights[p]
                    if w > max_w:
                        max_w = w
                        max_p = p
                    if w < min_w:
                        min_w = w
                        min_p = p

                diff_global = max_w - min_w
                if diff_global < 1e-6:
                    break

                # If we've improved beyond best, save immediately (optional, but we do it at end of restart)

                # Attempt to swap items from max_p with items from other packs
                # We prioritize swapping with the lightest packs to maximize 'headroom'
                # But any pack p_other where we can swap (u, v) such that:
                # new_max < current_max and new_other < current_max is valid.

                # Sort other packs by weight ascending (lightest first)
                other_packs = sorted([p for p in range(num_packs) if p != max_p],
                                     key=pack_weights.__getitem__)

                swap_executed = False

                for p_other in other_packs:
                    w_other = pack_weights[p_other]
                    # Allowed gap: we can transfer at most (max_w - w_other)
                    # Ideally we transfer (max_w - w_other) / 2

                    gap = max_w - w_other
                    if gap < 1e-6: continue

                    target = gap / 2.0

                    best_swap = None
                    best_dev = float('inf') # Deviation from target

                    # Search for u in max_p, v in p_other
                    # delta = w_u - w_v
                    # We want delta > 0 and delta < gap (strictly, to ensure max_p decreases and p_other doesn't exceed old max_p)
                    # Actually, to guarantee max_p reduces, we just need delta > 0.
                    # But we must ensure p_other + delta < max_p.

                    p_max_items = current_packs[max_p]
                    p_other_items = current_packs[p_other]

                    for i_u, (idx_u, w_u) in enumerate(p_max_items):
                        for i_v, (idx_v, w_v) in enumerate(p_other_items):
                            delta = w_u - w_v
                            if delta > 0 and delta < gap:
                                # This swap is valid (reduces max, doesn't create new max >= old max)
                                # How good is it? Closer to target is better for balance.
                                dev = abs(delta - target)
                                if dev < best_dev:
                                    best_dev = dev
                                    best_swap = (i_u, i_v, delta, idx_u, w_u, idx_v, w_v)
                                    if dev < 1e-5: break # Perfect
                        if best_swap and best_dev < 1e-5: break

                    if best_swap:
                        i_u, i_v, delta, idx_u, w_u, idx_v, w_v = best_swap

                        # Execute Swap
                        current_packs[max_p][i_u] = (idx_v, w_v)
                        current_packs[p_other][i_v] = (idx_u, w_u)
                        pack_weights[max_p] -= delta
                        pack_weights[p_other] += delta

                        swap_executed = True
                        break # Break inner loop to re-evaluate Max Pack

                if not swap_executed:
                    # If we couldn't swap with any pack to reduce Max,
                    # we are in a local optimum w.r.t 1-item swaps from Max.
                    # Stop this restart.
                    break

            # Evaluate this restart
            curr_max = max(pack_weights)
            curr_min = min(pack_weights)
            curr_diff = curr_max - curr_min

            if curr_diff < best_diff:
                best_diff = curr_diff
                best_packs = [list(p) for p in current_packs] # Deep copy
                if best_diff < 1e-6:
                    break

        # Write result
        for p in range(num_packs):
            for r, (idx, _) in enumerate(best_packs[p]):
                pack_index[i, idx] = p
                rank_in_pack[i, idx] = r

    return pack_index.to(weight.device), rank_in_pack.to(weight.device)
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Handle trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups,
                                  dtype=torch.int64,
                                  device=weight.device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(pack_index)
        return pack_index, rank_in_pack

    # CPU processing for scalar efficiency
    weight_cpu = weight.to("cpu", dtype=torch.float32)
    pack_index = torch.empty((num_layers, num_groups), dtype=torch.int64, device="cpu")
    rank_in_pack = torch.empty((num_layers, num_groups), dtype=torch.int64, device="cpu")

    num_restarts = 2

    for i in range(num_layers):
        row_weight = weight_cpu[i]

        best_diff = float('inf')
        best_packs = None

        original_items = []
        for idx in range(num_groups):
            original_items.append((idx, row_weight[idx].item()))

        # Sort descending
        original_items.sort(key=lambda x: x[1], reverse=True)

        for attempt in range(num_restarts):
            # 1. Initialization
            current_packs = [[] for _ in range(num_packs)]
            pack_weights = [0.0] * num_packs

            if attempt == 0:
                # Deterministic ZigZag
                for k, (idx, w) in enumerate(original_items):
                    cycle_pos = k % (2 * num_packs)
                    p = cycle_pos if cycle_pos < num_packs else (2 * num_packs - 1) - cycle_pos
                    current_packs[p].append((idx, w))
                    pack_weights[p] += w
            else:
                # Randomized Greedy
                noise = torch.rand(num_groups) * 0.15 + 0.925
                noisy_items = []
                for k, (idx, w) in enumerate(original_items):
                    noisy_items.append((idx, w, w * noise[k].item()))
                noisy_items.sort(key=lambda x: x[2], reverse=True)

                for idx, w, _ in noisy_items:
                    best_p = -1
                    min_w = float('inf')
                    for p in range(num_packs):
                        if len(current_packs[p]) < groups_per_pack:
                            if pack_weights[p] < min_w:
                                min_w = pack_weights[p]
                                best_p = p
                    current_packs[best_p].append((idx, w))
                    pack_weights[best_p] += w

            # 2. Refinement: Pooled 2-Pack Repartitioning
            # Repeatedly find the Max pack and try to re-partition it with a lighter pack
            for _ in range(20):
                max_p = max(range(num_packs), key=pack_weights.__getitem__)
                max_w = pack_weights[max_p]

                # Sort other packs by weight ascending (try lightest first)
                other_packs = sorted([p for p in range(num_packs) if p != max_p],
                                     key=pack_weights.__getitem__)

                improvement_found = False

                for p_other in other_packs:
                    w_other = pack_weights[p_other]
                    if max_w - w_other < 1e-6:
                        continue # No meaningful gap

                    # Pool items
                    pool = current_packs[max_p] + current_packs[p_other]

                    # Try to partition pool into 2 sets of size groups_per_pack
                    # Goal: Minimize max(sum(A), sum(B))

                    # Heuristic: LPT with Multi-Start
                    best_local_max = max_w # Start with current situation
                    best_A = None
                    best_B = None

                    # Sub-trials for the 2-partition problem
                    sub_trials = 5
                    for sub in range(sub_trials):
                        if sub == 0:
                             # Pure LPT
                             sorted_pool = sorted(pool, key=lambda x: x[1], reverse=True)
                        else:
                             # Randomized LPT
                             noise_pool = [(x, x[1] * random.uniform(0.9, 1.1)) for x in pool]
                             sorted_pool = [x[0] for x in sorted(noise_pool, key=lambda y: y[1], reverse=True)]

                        binA = []
                        wA = 0.0
                        binB = []
                        wB = 0.0

                        for item in sorted_pool:
                            w = item[1]
                            # Greedy placement with capacity check
                            if len(binA) < groups_per_pack and len(binB) < groups_per_pack:
                                if wA <= wB:
                                    binA.append(item)
                                    wA += w
                                else:
                                    binB.append(item)
                                    wB += w
                            elif len(binA) < groups_per_pack:
                                binA.append(item)
                                wA += w
                            else:
                                binB.append(item)
                                wB += w

                        current_local_max = max(wA, wB)
                        if current_local_max < best_local_max - 1e-6:
                            best_local_max = current_local_max
                            best_A = binA
                            best_B = binB

                    if best_A is not None:
                         # Apply improvement
                         current_packs[max_p] = best_A
                         current_packs[p_other] = best_B
                         pack_weights[max_p] = sum(x[1] for x in best_A)
                         pack_weights[p_other] = sum(x[1] for x in best_B)
                         improvement_found = True
                         break

                if not improvement_found:
                    break

            # Evaluate this restart
            curr_max = max(pack_weights)
            curr_min = min(pack_weights)
            curr_diff = curr_max - curr_min

            if curr_diff < best_diff:
                best_diff = curr_diff
                best_packs = [list(p) for p in current_packs] # Deep copy
                if best_diff < 1e-6:
                    break

        # Write result
        for p in range(num_packs):
            for r, (idx, _) in enumerate(best_packs[p]):
                pack_index[i, idx] = p
                rank_in_pack[i, idx] = r

    return pack_index.to(weight.device), rank_in_pack.to(weight.device)
>>>>>>> REPLACE
</DIFF>