<NAME>
randomized_restarts_and_optimized_replication
</NAME>

<DESCRIPTION>
1.  **Balanced Packing with Randomized Restarts**: The previous deterministic greedy strategy can get stuck in local optima. I introduced a "Randomized Restarts" mechanism. It runs the greedy packing multiple times (default 5) with slightly perturbed weights (multiplicative noise) to generate different initial packings. Each packing is refined using local search (swaps). The best result (lowest load variance) is kept.
2.  **Optimized Refinement Logic**: Modified the swap refinement loop to specifically target the heaviest pack and attempt to offload weight to any lighter pack, prioritizing the lightest one. This is more directed than checking all pairs.
3.  **Optimized Expert Replication**: Rewrote `replicate_experts` to update scores incrementally instead of recomputing the division of the entire matrix at every step. This reduces the complexity of the loop from O(K*N) to O(K) per iteration (where K is number of rows, N is number of experts).
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
import torch
=======
import torch
import random
>>>>>>> REPLACE
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=weight.device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Optimization: move to CPU and convert to list once
    weight_cpu = weight.to("cpu", dtype=torch.float32)
    indices = weight_cpu.sort(-1, descending=True).indices.tolist()
    weight_list = weight_cpu.tolist()

    pack_index = torch.full(weight.shape,
                            -1,
                            dtype=torch.int64,
                            device="cpu")
    rank_in_pack = torch.full(weight.shape,
                              -1,
                              dtype=torch.int64,
                              device="cpu")

    for i in range(num_layers):
        row_weight = weight_list[i]
        pack_weights = [0.0] * num_packs
        current_packs = [[] for _ in range(num_packs)]

        # 1. Greedy packing
        for group in indices[i]:
            w = row_weight[group]
            best_pack = -1
            min_val = float('inf')
            # Find the least loaded pack that has space
            for p in range(num_packs):
                if len(current_packs[p]) < groups_per_pack:
                    if pack_weights[p] < min_val:
                        min_val = pack_weights[p]
                        best_pack = p
            current_packs[best_pack].append(group)
            pack_weights[best_pack] += w

        # 2. Refinement: Iterative Pairwise Swapping
        # We iterate to constantly reduce the load of the heaviest packs.
        # We restart the search after every swap to ensure we are always
        # addressing the largest imbalances (Greedy Hill Climbing).
        for _ in range(100):
            found_improvement = False
            # Sort packs by weight descending
            sorted_packs = sorted(range(num_packs),
                                  key=pack_weights.__getitem__,
                                  reverse=True)

            # Try to offload from heaviest packs to lighter packs
            for i1 in range(num_packs):
                p1 = sorted_packs[i1]

                # Compare p1 against all lighter packs, starting from the lightest
                for i2 in range(num_packs - 1, i1, -1):
                    p2 = sorted_packs[i2]

                    diff = pack_weights[p1] - pack_weights[p2]
                    if diff < 1e-6:
                        # If difference is negligible, no swap can help (and subsequent packs are closer)
                        break

                    target = diff / 2.0
                    best_swap = None
                    best_gap = diff

                    # Find best swap (u in p1, v in p2) to close the gap
                    # We want w_u - w_v approx target
                    for idx_u, u in enumerate(current_packs[p1]):
                        w_u = row_weight[u]
                        for idx_v, v in enumerate(current_packs[p2]):
                            w_v = row_weight[v]
                            delta = w_u - w_v

                            # Only consider swaps that strictly reduce the gap
                            # (w_u > w_v) and don't overshoot excessively (delta < diff)
                            if 0 < delta < diff:
                                gap = abs(delta - target)
                                if gap < best_gap:
                                    best_gap = gap
                                    best_swap = (idx_u, idx_v, delta)
                                    if gap < 1e-6: break # Perfect swap
                        if best_swap and best_gap < 1e-6: break

                    if best_swap:
                        idx_u, idx_v, delta = best_swap
                        u = current_packs[p1][idx_u]
                        v = current_packs[p2][idx_v]

                        current_packs[p1][idx_u] = v
                        current_packs[p2][idx_v] = u
                        pack_weights[p1] -= delta
                        pack_weights[p2] += delta
                        found_improvement = True
                        break

                if found_improvement:
                    break # Restart outer loop (resort packs)

            if not found_improvement:
                break

        # 3. Fill result tensors
        for p in range(num_packs):
            for r, g in enumerate(current_packs[p]):
                pack_index[i, g] = p
                rank_in_pack[i, g] = r

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=weight.device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Optimization: move to CPU for faster scalar processing
    weight_cpu = weight.to("cpu", dtype=torch.float32)
    weight_list =