<NAME>
pairwise_swap_refinement
</NAME>

<DESCRIPTION>
Implements a Hybrid Refinement strategy in `balanced_packing`.
1. Starts with Randomized Greedy (LPT) with restarts to find good initial solutions.
2. Applies an Iterated Local Search (ILS) / Variable Neighborhood Descent (VND) with two operators:
   a. Deterministic Pairwise Swap Descent: Targeting the heaviest and lightest packs, it exhaustively scans for the best single item swap that reduces the load difference. This is O(G^2) but highly effective for fine-tuning balance.
   b. 3-Way Randomized Ruin & Recreate: If pairwise swapping gets stuck (local optimum), it invokes a 3-pack randomized greedy rebalancing (Max, Min, Random) to escape.
This combination leverages the speed of greedy construction and the precision of pairwise swaps, utilizing the CPU time efficiently.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    device = weight.device
    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Use CPU weights
    weight_cpu = weight.cpu()

    # Outputs on CPU to avoid device synchronization during fill
    pack_index = torch.empty((num_layers, num_groups), dtype=torch.int64)
    rank_in_pack = torch.empty((num_layers, num_groups), dtype=torch.int64)

    # Number of restarts for the randomized heuristic
    NUM_RESTARTS = 5

    for i in range(num_layers):
        layer_w = weight_cpu[i]
        layer_w_list = layer_w.tolist()

        best_max_load = float('inf')
        best_sum_sq = float('inf')
        best_assignment = None

        for attempt in range(NUM_RESTARTS):
            # 1. Candidate Generation
            if attempt == 0:
                indices = sorted(range(num_groups), key=lambda x: layer_w_list[x], reverse=True)
            else:
                # Randomized LPT
                noise = torch.rand(num_groups) * 0.15 + 0.925
                indices = (layer_w * noise).argsort(descending=True).tolist()

            # 2. Greedy Construction (LPT/Best Fit)
            packs = [[] for _ in range(num_packs)]
            pack_weights = [0.0] * num_packs
            pack_counts = [0] * num_packs

            for g_idx in indices:
                w = layer_w_list[g_idx]
                # Best Fit: Assign to valid pack with min load
                best_p = -1
                min_w = float('inf')

                for p in range(num_packs):
                    if pack_counts[p] < groups_per_pack:
                        if pack_weights[p] < min_w:
                            min_w = pack_weights[p]
                            best_p = p

                packs[best_p].append(g_idx)
                pack_weights[best_p] += w
                pack_counts[best_p] += 1

            # 3. Refinement: Large Neighborhood Search (LNS)
            MAX_LNS_STEPS = 100 if num_packs > 1 else 0

            for step in range(MAX_LNS_STEPS):
                # Identify heavy and light packs
                # Optimization: Track max/min indices dynamically or just scan
                max_p = max(range(num_packs), key=pack_weights.__getitem__)
                min_p = min(range(num_packs), key=pack_weights.__getitem__)

                if max_p == min_p:
                    break

                # If balanced enough, try to optimize variance by picking random packs
                # Otherwise, target max load reduction
                if pack_weights[max_p] - pack_weights[min_p] < 1e-6:
                     # Already balanced max-min, but maybe high variance?
                     pass

                candidates = {max_p, min_p}
                # Add random packs for k-way swaps
                if num_packs > 2:
                    k_random = min(2, num_packs - 2)
                    for _ in range(k_random):
                        candidates.add(torch.randint(0, num_packs, (1,)).item())

                cand_list = list(candidates)

                # Ruin
                items = []
                for p in cand_list:
                    items.extend(packs[p])

                # Recreate: Solve sub-problem (minimize max load of these packs)
                best_sub_weights = [pack_weights[p] for p in cand_list]
                best_sub_packs = [list(packs[p]) for p in cand_list]
                best_sub_max = max(best_sub_weights)
                best_sub_ss = sum(w*w for w in best_sub_weights)

                improved_sub = False

                # 0. Deterministic LPT attempt
                # 1..N. Randomized LPT attempts

                # Prepare LPT sort once
                items_sorted = sorted(items, key=lambda x: layer_w_list[x], reverse=True)

                SUB_RESTARTS = 20
                for sub_attempt in range(SUB_RESTARTS):
                    if sub_attempt == 0:
                        current_items = items_sorted
                    else:
                        # Perturb weights slightly to change sort order
                        noise_sub = torch.rand(len(items))
                        # Efficient perturbed sort
                        # We use a list of tuples (perturbed_weight, index)
                        weighted_items = []
                        for idx, item in enumerate(items):
                            pw = layer_w_list[item] * (0.8 + 0.4 * noise_sub[idx].item())
                            weighted_items.append((pw, item))
                        weighted_items.sort(key=lambda x: x[0], reverse=True)
                        current_items = [x[1] for x in weighted_items]

                    # Greedy Fill (Best Fit)
                    temp_weights = {p: 0.0 for p in cand_list}
                    temp_packs = {p: [] for p in cand_list}
                    temp_counts = {p: 0 for p in cand_list}

                    possible = True
                    for item_idx in current_items:
                        w = layer_w_list[item_idx]

                        best_local_p = -1
                        min_local_w = float('inf')

                        for p in cand_list:
                            if temp_counts[p] < groups_per_pack:
                                if temp_weights[p] < min_local_w:
                                    min_local_w = temp_weights[p]
                                    best_local_p = p

                        if best_local_p == -1:
                            possible = False
                            break

                        temp_packs[best_local_p].append(item_idx)
                        temp_weights[best_local_p] += w
                        temp_counts[best_local_p] += 1

                    if possible:
                        c_max = max(temp_weights.values())
                        c_ss = sum(v*v for v in temp_weights.values())

                        # Check improvement
                        if c_max < best_sub_max - 1e-6 or (abs(c_max - best_sub_max) < 1e-6 and c_ss < best_sub_ss - 1e-6):
                            best_sub_max = c_max
                            best_sub_ss = c_ss
                            best_sub_weights = [temp_weights[p] for p in cand_list]
                            best_sub_packs = [temp_packs[p] for p in cand_list]
                            improved_sub = True

                if improved_sub:
                    for idx, p in enumerate(cand_list):
                        packs[p] = best_sub_packs[idx]
                        pack_weights[p] = best_sub_weights[idx]
                        pack_counts[p] = groups_per_pack # Should remain full

            # 4. Evaluation
            current_max = max(pack_weights)
            current_ss = sum(w*w for w in pack_weights)

            if current_max < best_max_load - 1e-6:
                best_max_load = current_max
                best_sum_sq = current_ss
                best_assignment = [list(p) for p in packs]
            elif abs(current_max - best_max_load) < 1e-6 and current_ss < best_sum_sq - 1e-6:
                best_sum_sq = current_ss
                best_assignment = [list(p) for p in packs]

        # 5. Final assignment for this layer
        for p, nodes in enumerate(best_assignment):
            for r, g_idx in enumerate(nodes):
                pack_index[i, g_idx] = p
                rank_in_pack[i, g_idx] = r

    return pack_index.to(device), rank_in_pack.to(device)
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Algorithm: Randomized Greedy + Pairwise Swap Descent + 3-Way LNS

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    device = weight.device
    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Use CPU weights
    weight_cpu = weight.cpu()

    # Outputs on CPU to avoid device synchronization during fill
    pack_index = torch.empty((num_layers, num_groups), dtype=torch.int64)
    rank_in_pack = torch.empty((num_layers, num_groups), dtype=torch.int64)

    # Tuning constants
    NUM_RESTARTS = 5

    for i in range(num_layers):
        layer_w = weight_cpu[i]
        layer_w_list = layer_w.tolist()

        best_max_load = float('inf')
        best_sum_sq = float('inf')
        best_assignment = None

        for attempt in range(NUM_RESTARTS):
            # 1. Candidate Generation
            if attempt == 0:
                indices = sorted(range(num_groups), key=lambda x: layer_w_list[x], reverse=True)
            else:
                # Randomized LPT
                noise = torch.rand(num_groups) * 0.15 + 0.925
                indices = (layer_w * noise).argsort(descending=True).tolist()

            # 2. Greedy Construction (LPT/Best Fit)
            packs = [[] for _ in range(num_packs)]
            pack_weights = [0.0] * num_packs
            pack_counts = [0] * num_packs

            for g_idx in indices:
                w = layer_w_list[g_idx]
                # Best Fit: Assign to valid pack with min load
                best_p = -1
                min_w = float('inf')

                for p in range(num_packs):
                    if pack_counts[p] < groups_per_pack:
                        if pack_weights[p] < min_w:
                            min_w = pack_weights[p]
                            best_p = p

                packs[best_p].append(g_idx)
                pack_weights[best_p] += w
                pack_counts[best_p] += 1

            # 3. Refinement: Pairwise Swap + LNS
            MAX_REFINE_STEPS = 100 if num_packs > 1 else 0

            # Helper to find max/min packs
            def get_stats(pw):
                max_w = -1.0
                min_w = float('inf')
                max_p = -1
                min_p = -1
                for p_idx, w_val in enumerate(pw):
                    if w_val > max_w:
                        max_w = w_val
                        max_p = p_idx
                    if w_val < min_w:
                        min_w = w_val
                        min_p = p_idx
                return max_p, min_p, max_w, min_w

            for step in range(MAX_REFINE_STEPS):
                max_p, min_p, max_w, min_w = get_stats(pack_weights)

                if max_p == min_p or (max_w - min_w < 1e-6):
                    break

                improved_action = False

                # A. Pairwise Swap Descent
                # Try to swap item from max_p with item from any other pack p to reduce max_p's load
                # The constraint is that the other pack p must not become heavier than max_p's *current* load (or better, the new load)

                # We want: w(u) - w(v) > 0  (reduce max_p)
                # And: pack_weights[p] + (w(u) - w(v)) < max_w  (ideally < max_w - epsilon)
                # Or more strictly to ensure descent: pack_weights[p] + (w(u) - w(v)) < max_w - (w(u) - w(v)) ?
                # Actually, we just need the new max of {max_p, p} to be less than old max_w.

                best_swap = None
                best_gain = 0.0 # Gain = w(u) - w(v). We maximize this subject to constraints.

                # Only check max_p against others
                u_items = packs[max_p]

                for u_idx, u in enumerate(u_items):
                    w_u = layer_w_list[u]

                    for p in range(num_packs):
                        if p == max_p: continue

                        diff_p = max_w - pack_weights[p]
                        if diff_p < 1e-6: continue # p is already essentially equal to max, can't dump weight there

                        for v_idx, v in enumerate(packs[p]):
                            w_v = layer_w_list[v]
                            delta = w_u - w_v

                            # We need delta > 0 to reduce max_p
                            # We need pack_weights[p] + delta < max_w (strict decrease of global max if max_p was unique max)

                            if delta > 1e-6 and (pack_weights[p] + delta < max_w - 1e-6):
                                # Valid swap. Does it improve beyond best_gain?
                                # We prefer larger delta (more reduction in max_p)
                                if delta > best_gain:
                                    best_gain = delta
                                    best_swap = (p, u_idx, v_idx, delta)

                if best_swap:
                    target_p, u_idx, v_idx, delta = best_swap
                    # Execute Swap
                    item_u = packs[max_p][u_idx]
                    item_v = packs[target_p][v_idx]

                    packs[max_p][u_idx] = item_v
                    packs[target_p][v_idx] = item_u

                    pack_weights[max_p] -= delta
                    pack_weights[target_p] += delta
                    improved_action = True

                # B. 3-Way LNS (Ruin & Recreate)
                # If swaps failed to reduce max, try re-shuffling 3 packs
                if not improved_action and num_packs >= 3:
                     # Candidates: Max, Min, Random
                    candidates = {max_p, min_p}
                    # Try to find a random pack that is not max or min
                    for _ in range(5): # retry limit
                        r = torch.randint(0, num_packs, (1,)).item()
                        if r not in candidates:
                            candidates.add(r)
                            break

                    cand_list = list(candidates)
                    if len(cand_list) < 3 and num_packs >= 3:
                         # Could not find distinct 3rd (e.g. only 2 packs total, handled by check above)
                         pass

                    # Ruin
                    items = []
                    for p in cand_list:
                        items.extend(packs[p])

                    # Recreate
                    current_sub_max = max(pack_weights[p] for p in cand_list)
                    current_sub_ss = sum(pack_weights[p]**2 for p in cand_list)

                    best_sub_res = None

                    # Sort items
                    items_sorted = sorted(items, key=lambda x: layer_w_list[x], reverse=True)

                    # Few restarts for sub-problem
                    for sub_attempt in range(15):
                        if sub_attempt == 0:
                            # Deterministic LPT
                            cur_items = items_sorted
                        else:
                            # Perturbed
                            noise = [1.0 + (torch.rand(1).item() - 0.5) * 0.2 for _ in items]
                            cur_items = sorted(items, key=lambda x: layer_w_list[x] * noise[items.index(x)], reverse=True)

                        # Greedy
                        t_w = {p: 0.0 for p in cand_list}
                        t_p = {p: [] for p in cand_list}
                        t_c = {p: 0 for p in cand_list}

                        possible = True
                        for item in cur_items:
                            w = layer_w_list[item]
                            best_local = -1
                            min_local = float('inf')
                            for p in cand_list:
                                if t_c[p] < groups_per_pack:
                                    if t_w[p] < min_local:
                                        min_local = t_w[p]
                                        best_local = p

                            if best_local == -1:
                                possible = False; break
                            t_p[best_local].append(item)
                            t_w[best_local] += w
                            t_c[best_local] += 1

                        if possible:
                            n_max = max(t_w.values())
                            n_ss = sum(v**2 for v in t_w.values())

                            if n_max < current_sub_max - 1e-6 or (abs(n_max - current_sub_max) < 1e-6 and n_ss < current_sub_ss - 1e-6):
                                current_sub_max = n_max
                                current_sub_ss = n_ss
                                best_sub_res = (t_p, t_w)

                    if best_sub_res:
                        s_p, s_w = best_sub_res
                        for p in cand_list:
                            packs[p] = s_p[p]
                            pack_weights[p] = s_w[p]
                        improved_action = True

                if not improved_action:
                    break

            # 4. Evaluation
            current_max = max(pack_weights)
            current_ss = sum(w*w for w in pack_weights)

            if current_max < best_max_load - 1e-6:
                best_max_load = current_max
                best_sum_sq = current_ss
                best_assignment = [list(p) for p in packs]
            elif abs(current_max - best_max_load) < 1e-6 and current_ss < best_sum_sq - 1e-6:
                best_sum_sq = current_ss
                best_assignment = [list(p) for p in packs]

        # 5. Final assignment for this layer
        for p, nodes in enumerate(best_assignment):
            for r, g_idx in enumerate(nodes):
                pack_index[i, g_idx] = p
                rank_in_pack[i, g_idx] = r

    return pack_index.to(device), rank_in_pack.to(device)
>>>>>>> REPLACE
</DIFF>