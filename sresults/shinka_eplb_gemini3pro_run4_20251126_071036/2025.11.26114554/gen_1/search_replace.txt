<NAME>
refine_packing
</NAME>

<DESCRIPTION>
The current `balanced_packing` function uses a greedy Longest Processing Time (LPT) strategy to distribute weighted items into packs. While efficient, this approach often leaves room for improvement in load balancing, as indicated by the low `balancedness_score`.

This modification introduces a refinement step after the initial greedy packing. It uses a local search algorithm that iteratively identifies the heaviest and lightest packs and attempts to swap a pair of items between them to reduce the weight difference (load variance). By performing these swaps, the algorithm achieves a tighter packing and better load balance. The number of iterations is limited to ensure the algorithm remains fast, preserving the `speed_score`. The use of CPU-based scalar operations (`.item()`) ensures the refinement loop is efficient even for Python code.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    indices = weight.float().sort(-1, descending=True).indices.cpu()
    pack_index = torch.full_like(weight,
                                 fill_value=-1,
                                 dtype=torch.int64,
                                 device="cpu")
    rank_in_pack = torch.full_like(pack_index, fill_value=-1)
    for i in range(num_layers):
        pack_weights = [0] * num_packs
        pack_items = [0] * num_packs
        for group in indices[i]:
            pack = min(
                (i
                 for i in range(num_packs) if pack_items[i] < groups_per_pack),
                key=pack_weights.__getitem__,
            )
            assert pack_items[pack] < groups_per_pack
            pack_index[i, group] = pack
            rank_in_pack[i, group] = pack_items[pack]
            pack_weights[pack] += weight[i, group]
            pack_items[pack] += 1
    return pack_index, rank_in_pack
=======
    indices = weight.float().sort(-1, descending=True).indices.cpu()
    pack_index = torch.full_like(weight,
                                 fill_value=-1,
                                 dtype=torch.int64,
                                 device="cpu")
    rank_in_pack = torch.full_like(pack_index, fill_value=-1)

    # Use CPU weights for faster scalar access
    weight_cpu = weight.cpu()

    for i in range(num_layers):
        pack_weights = [0.0] * num_packs
        pack_contents = [[] for _ in range(num_packs)]

        # Initial Greedy Packing (Modified LPT)
        for group in indices[i]:
            # Find valid packs (not full)
            valid_packs = (p for p in range(num_packs)
                           if len(pack_contents[p]) < groups_per_pack)
            # Select pack with minimum weight
            pack = min(valid_packs, key=pack_weights.__getitem__)

            g_idx = group.item()
            w = weight_cpu[i, g_idx].item()

            pack_index[i, g_idx] = pack
            pack_weights[pack] += w
            pack_contents[pack].append(g_idx)

        # Refinement: Local Search (Swap)
        # Try to swap items between heaviest and lightest packs to reduce variance
        for _ in range(20):
            min_pack = min(range(num_packs), key=pack_weights.__getitem__)
            max_pack = max(range(num_packs), key=pack_weights.__getitem__)

            if min_pack == max_pack:
                break

            diff = pack_weights[max_pack] - pack_weights[min_pack]
            if diff < 1e-6:
                break

            best_swap = None
            best_reduction = 0.0

            # Identify best swap between max_pack and min_pack
            for u in pack_contents[max_pack]:
                w_u = weight_cpu[i, u].item()
                for v in pack_contents[min_pack]:
                    w_v = weight_cpu[i, v].item()

                    delta = w_u - w_v
                    if 0 < delta < diff:
                        # We want to maximize reduction of the gap
                        reduction = diff - abs(diff - 2 * delta)
                        if reduction > best_reduction:
                            best_reduction = reduction
                            best_swap = (u, v, delta)

            if best_swap:
                u, v, delta = best_swap
                # Perform swap
                pack_contents[max_pack].remove(u)
                pack_contents[max_pack].append(v)
                pack_contents[min_pack].remove(v)
                pack_contents[min_pack].append(u)

                pack_weights[max_pack] -= delta
                pack_weights[min_pack] += delta
                pack_index[i, u] = min_pack
                pack_index[i, v] = max_pack
            else:
                break

        # Assign ranks
        for p in range(num_packs):
            for rank, g_idx in enumerate(pack_contents[p]):
                rank_in_pack[i, g_idx] = rank

    return pack_index, rank_in_pack
>>>>>>> REPLACE

</DIFF>