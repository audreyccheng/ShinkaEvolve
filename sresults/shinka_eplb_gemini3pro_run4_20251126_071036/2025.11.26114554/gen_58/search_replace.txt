<NAME>
hybrid_beam_ring_packing
</NAME>

<DESCRIPTION>
Replaces the packing algorithm with a hybrid approach:
1.  **Beam Search Initialization**: Builds a solid initial packing structure using a beam search (width 4) for the first 30 items, minimizing sum of squares.
2.  **Pairwise Perturbed LPT Refinement**: Replaces the random shuffle splitting with a robust "Perturbed LPT" greedy partitioner. This tries 8 variants (1 deterministic, 7 randomized) to optimally re-balance any two packs (targeting Max-Min and Max-Random pairs).
3.  **3-Way Ring Swaps**: Introduces a targeted $O(G^3)$ search for cyclic exchanges ($P_{max} \to P_{mid} \to P_{min} \to P_{max}$) to resolve deadlocks where pairwise swaps fail. This specifically targets reducing the Max load by moving a heavy item to a buffer pack which then passes a lighter item to the Min pack.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Algorithm: Beam Search Initialization + Pairwise Cyclic Repartitioning

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs
    device = weight.device

    # Trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64, device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # CPU for scalar logic
    weight_cpu = weight.cpu()

    # Pre-allocate output on CPU
    pack_index = torch.empty((num_layers, num_groups), dtype=torch.int64)
    rank_in_pack = torch.empty((num_layers, num_groups), dtype=torch.int64)

    for i in range(num_layers):
        w_tensor = weight_cpu[i]
        w_list = w_tensor.tolist()

        # Sort items descending (LPT)
        sorted_indices = sorted(range(num_groups), key=lambda x: w_list[x], reverse=True)

        # --- Phase 1: Beam Search Initialization ---
        # We use beam search for the heaviest items to establish a good structure.
        # Deep search for the first few items prevents irreversible greedy mistakes.
        BEAM_WIDTH = 4
        BEAM_DEPTH = min(num_groups, 24)

        # State: (score, loads_tuple, counts_tuple, path_tuple)
        # score: Sum of squared loads (proxy for variance)
        # path: Tuple of pack indices assigned to the first k items

        beam = [(0.0, tuple([0.0]*num_packs), tuple([0]*num_packs), ())]

        for k in range(BEAM_DEPTH):
            item_idx = sorted_indices[k]
            w = w_list[item_idx]
            new_beam = []

            for score, loads, counts, path in beam:
                seen_states = set()

                for p in range(num_packs):
                    if counts[p] < groups_per_pack:
                        # Symmetry breaking: if multiple packs have identical load/count,
                        # they are equivalent. Only try the first one encountered.
                        # Rounding handles float precision issues.
                        state_sig = (round(loads[p], 5), counts[p])
                        if state_sig in seen_states:
                            continue
                        seen_states.add(state_sig)

                        # Update state
                        new_loads = list(loads)
                        new_loads[p] += w
                        new_counts = list(counts)
                        new_counts[p] += 1

                        # Incremental Score Update: min sum(load^2)
                        # New term: (L+w)^2 = L^2 + 2Lw + w^2.
                        # Delta = 2Lw + w^2.
                        delta_score = 2 * loads[p] * w + w * w
                        new_score = score + delta_score

                        new_beam.append((new_score, tuple(new_loads), tuple(new_counts), path + (p,)))

            # Prune
            if len(new_beam) > BEAM_WIDTH:
                beam = heapq.nsmallest(BEAM_WIDTH, new_beam, key=lambda x: x[0])
            else:
                beam = new_beam

        # Reconstruct assignment from best beam state
        best_node = beam[0]
        _, current_loads, current_counts, path = best_node

        packs = [[] for _ in range(num_packs)]
        pack_weights = list(current_loads)
        pack_counts = list(current_counts)

        for k in range(BEAM_DEPTH):
            idx = sorted_indices[k]
            p = path[k]
            packs[p].append(idx)

        # --- Phase 2: Greedy Finish ---
        # Fill remaining items using Best Fit (Least Loaded Valid Bin)
        for k in range(BEAM_DEPTH, num_groups):
            idx = sorted_indices[k]
            w = w_list[idx]

            best_p = -1
            min_load = float('inf')

            for p in range(num_packs):
                if pack_counts[p] < groups_per_pack:
                    if pack_weights[p] < min_load:
                        min_load = pack_weights[p]
                        best_p = p

            packs[best_p].append(idx)
            pack_weights[best_p] += w
            pack_counts[best_p] += 1

        # --- Phase 3: Pairwise Cyclic Repartitioning ---
        # Iteratively pick heaviest and lightest packs and re-partition their pooled items.
        # This acts as a powerful local search that can move multiple items at once.

        MAX_REFINE_ITER = 20
        for _ in range(MAX_REFINE_ITER):
            # Identify extremes
            max_p = -1; max_w = -1.0
            min_p = -1; min_w = float('inf')

            for p in range(num_packs):
                pw = pack_weights[p]
                if pw > max_w: max_w = pw; max_p = p
                if pw < min_w: min_w = pw; min_p = p

            if max_w - min_w < 1e-6:
                break

            # Candidate Pairs: (Max, Min) and optionally (Max, Random) to escape local optima
            pairs = [(max_p, min_p)]
            if num_packs > 2:
                # Try to offload from Max to a random node (not Max/Min)
                # This facilitates cyclic transfers: Max -> Random -> Min
                rand_p = random.randint(0, num_packs - 1)
                if rand_p != max_p and rand_p != min_p:
                    pairs.append((max_p, rand_p))

            improved_any = False
            for p1, p2 in pairs:
                items = packs[p1] + packs[p2]
                current_diff = abs(pack_weights[p1] - pack_weights[p2])

                # We need to split `items` into two sets of equal size `groups_per_pack`
                # Heuristic: Randomized Greedy Number Partitioning

                sub_items = sorted(items, key=lambda x: w_list[x], reverse=True)

                best_split = None
                best_split_diff = current_diff

                # 5 attempts: 1 deterministic, 4 randomized
                for attempt in range(5):
                    # Prepare item order
                    if attempt == 0:
                        order = sub_items
                    else:
                        # Shuffle order for variety
                        order = list(sub_items)
                        random.shuffle(order)

                    l1, l2 = 0.0, 0.0
                    c1, c2 = 0, 0
                    ass1, ass2 = [], []

                    for idx in order:
                        w = w_list[idx]
                        # Put in lighter valid bin
                        if c1 < groups_per_pack and (c2 == groups_per_pack or l1 < l2):
                            ass1.append(idx)
                            l1 += w
                            c1 += 1
                        else:
                            ass2.append(idx)
                            l2 += w
                            c2 += 1

                    diff = abs(l1 - l2)
                    if diff < best_split_diff - 1e-6:
                        best_split_diff = diff
                        best_split = (ass1, ass2, l1, l2)
                        if diff < 1e-6: break

                if best_split:
                    ass1, ass2, l1, l2 = best_split
                    packs[p1] = ass1
                    packs[p2] = ass2
                    pack_weights[p1] = l1
                    pack_weights[p2] = l2
                    improved_any = True

            if not improved_any:
                # If we couldn't improve Max/Min or Max/Random, we are likely at a good optimum
                pass

        # Fill output tensors
        for p in range(num_packs):
            for r, idx in enumerate(packs[p]):
                pack_index[i, idx] = p
                rank_in_pack[i, idx] = r

    return pack_index.to(device), rank_in_pack.to(device)
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Algorithm: Beam Search Initialization + Perturbed LPT Refinement + Ring Swaps

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs
    device = weight.device

    # Trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64, device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # CPU for scalar logic
    weight_cpu = weight.cpu()

    # Pre-allocate output on CPU
    pack_index = torch.empty((num_layers, num_groups), dtype=torch.int64)
    rank_in_pack = torch.empty((num_layers, num_groups), dtype=torch.int64)

    for i in range(num_layers):
        w_tensor = weight_cpu[i]
        w_list = w_tensor.tolist()

        sorted_indices = sorted(range(num_groups), key=lambda x: w_list[x], reverse=True)

        # --- Phase 1: Beam Search Initialization ---
        BEAM_WIDTH = 4
        BEAM_DEPTH = min(num_groups, 30)

        # State: (score, loads_tuple, counts_tuple, path_tuple)
        beam = [(0.0, tuple([0.0]*num_packs), tuple([0]*num_packs), ())]

        for k in range(BEAM_DEPTH):
            item_idx = sorted_indices[k]
            w = w_list[item_idx]
            new_beam = []

            for score, loads, counts, path in beam:
                seen_states = set()

                for p in range(num_packs):
                    if counts[p] < groups_per_pack:
                        state_sig = (round(loads[p], 5), counts[p])
                        if state_sig in seen_states: continue
                        seen_states.add(state_sig)

                        nl = list(loads)
                        nl[p] += w
                        nc = list(counts)
                        nc[p] += 1

                        # Incremental Score Update: min sum(load^2)
                        delta_score = 2 * loads[p] * w + w * w

                        new_beam.append((score + delta_score, tuple(nl), tuple(nc), path + (p,)))

            if len(new_beam) > BEAM_WIDTH:
                beam = heapq.nsmallest(BEAM_WIDTH, new_beam, key=lambda x: x[0])
            else:
                beam = new_beam

        best_node = beam[0]
        _, current_loads, current_counts, path = best_node
        packs = [[] for _ in range(num_packs)]
        pack_weights = list(current_loads)
        pack_counts = list(current_counts)
        for k in range(BEAM_DEPTH):
            packs[path[k]].append(sorted_indices[k])

        # --- Phase 2: Greedy Finish ---
        for k in range(BEAM_DEPTH, num_groups):
            idx = sorted_indices[k]
            w = w_list[idx]
            best_p = -1
            min_load = float('inf')
            for p in range(num_packs):
                if pack_counts[p] < groups_per_pack:
                    if pack_weights[p] < min_load:
                        min_load = pack_weights[p]
                        best_p = p
            packs[best_p].append(idx)
            pack_weights[best_p] += w
            pack_counts[best_p] += 1

        # --- Phase 3: Iterative Refinement ---
        MAX_REFINE_ITER = 20

        for iter_ in range(MAX_REFINE_ITER):
            max_p = max(range(num_packs), key=pack_weights.__getitem__)
            min_p = min(range(num_packs), key=pack_weights.__getitem__)

            if pack_weights[max_p] - pack_weights[min_p] < 1e-6:
                break

            improved = False

            # Strategy A: Pairwise Optimization (Max-Min and Max-Random)
            pairs = [(max_p, min_p)]
            if num_packs > 2:
                candidates = [p for p in range(num_packs) if p != max_p and p != min_p]
                if candidates:
                    pairs.append((max_p, random.choice(candidates)))

            for p1, p2 in pairs:
                items = packs[p1] + packs[p2]
                best_sub_diff = abs(pack_weights[p1] - pack_weights[p2])
                best_sub_assign = None

                # 8 attempts: 1 deterministic LPT, 7 Perturbed LPT
                # Perturbed LPT is superior to random shuffle for packing
                for attempt in range(8):
                    if attempt == 0:
                        order = sorted(items, key=lambda x: w_list[x], reverse=True)
                    else:
                        # Perturb weights for sorting only
                        noise = [random.uniform(0.9, 1.1) for _ in items]
                        order = sorted(items, key=lambda x: w_list[x] * noise[items.index(x)], reverse=True)

                    l1, l2 = 0.0, 0.0
                    c1, c2 = 0, 0
                    ass1, ass2 = [], []

                    for item in order:
                        w = w_list[item]
                        # Greedy balance sums: put in lighter bin unless full
                        if c1 < groups_per_pack and (c2 == groups_per_pack or l1 < l2):
                            ass1.append(item)
                            l1 += w
                            c1 += 1
                        else:
                            ass2.append(item)
                            l2 += w
                            c2 += 1

                    diff = abs(l1 - l2)
                    if diff < best_sub_diff - 1e-6:
                        best_sub_diff = diff
                        best_sub_assign = (ass1, ass2, l1, l2)

                if best_sub_assign:
                    ass1, ass2, l1, l2 = best_sub_assign
                    packs[p1] = ass1
                    packs[p2] = ass2
                    pack_weights[p1] = l1
                    pack_weights[p2] = l2
                    improved = True

            if improved:
                continue

            # Strategy B: 3-Way Ring Swap (Max -> Mid -> Min -> Max)
            if num_packs < 3: continue

            mids = [p for p in range(num_packs) if p != max_p and p != min_p]
            mids.sort(key=pack_weights.__getitem__) # Try lightest mids first

            found_ring = False
            for mid_p in mids[:2]: # Limit to 2 best candidates
                # Filter candidates for speed
                u_cands = sorted(packs[max_p], key=lambda x: w_list[x], reverse=True)
                w_cands = sorted(packs[min_p], key=lambda x: w_list[x]) # Smallest w out of min? No, we want Min to gain.
                # Actually, Min loses w, gains v. We want v > w.
                # So we want w to be small, to easily find v > w.
                v_cands = sorted(packs[mid_p], key=lambda x: w_list[x], reverse=True) # Big v to help Min most

                limit_chk = 0
                for u in u_cands:
                    wu = w_list[u]
                    for w in w_cands:
                        ww = w_list[w]
                        if wu <= ww: continue # Max must decrease

                        gain_max = wu - ww

                        # Need v in Mid s.t. Mid doesn't explode
                        # Mid new = Mid + wu - wv < Max_old - gain_max (ideally)
                        # wv > Mid + wu - (Max_old - gain_max)
                        min_wv_req = pack_weights[mid_p] + wu - pack_weights[max_p] + 1e-6

                        for v in v_cands:
                            wv = w_list[v]
                            if wv <= ww: break # Min must increase (v > w) - optional but efficient

                            # Check system constraint
                            # New loads
                            n_max = pack_weights[max_p] - wu + ww
                            n_mid = pack_weights[mid_p] + wu - wv
                            n_min = pack_weights[min_p] + wv - ww

                            sys_max = max(n_max, n_mid, n_min)

                            if sys_max < pack_weights[max_p] - 1e-6:
                                packs[max_p].remove(u); packs[max_p].append(w)
                                packs[mid_p].remove(v); packs[mid_p].append(u)
                                packs[min_p].remove(w); packs[min_p].append(v)
                                pack_weights[max_p] = n_max
                                pack_weights[mid_p] = n_mid
                                pack_weights[min_p] = n_min
                                found_ring = True
                                break

                        if found_ring: break
                        limit_chk += 1
                        if limit_chk > 40: break
                    if found_ring: break
                if found_ring: break

        # Fill output tensors
        for p in range(num_packs):
            for r, idx in enumerate(packs[p]):
                pack_index[i, idx] = p
                rank_in_pack[i, idx] = r

    return pack_index.to(device), rank_in_pack.to(device)
>>>>>>> REPLACE
</DIFF>