<NAME>
randomized_greedy_restarts
</NAME>

<DESCRIPTION>
Implement Randomized Greedy with Multiple Restarts in the `balanced_packing` function.
Instead of a single deterministic LPT pass, the algorithm now performs multiple packing attempts (default 10).
The first attempt uses standard LPT. Subsequent attempts add multiplicative random noise to the weights before sorting, creating diverse greedy initializations.
Each initialization is followed by a refinement phase that swaps items between the most and least loaded packs.
The best solution (lowest load variance/spread) among all restarts is returned.
This approach helps escape local optima in the NP-hard partitioning problem, improving load balance at the cost of slightly more CPU compute (which is affordable given the high speed score).
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=weight.device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    indices = weight.float().sort(-1, descending=True).indices.cpu()
    pack_index = torch.full_like(weight,
                                 fill_value=-1,
                                 dtype=torch.int64,
                                 device="cpu")
    rank_in_pack = torch.full_like(pack_index, fill_value=-1)

    # Optimization: Move weights to CPU to avoid slow element-wise GPU access
    weight_cpu = weight.to("cpu", dtype=torch.float32)

    for i in range(num_layers):
        pack_weights = [0.0] * num_packs
        current_packs = [[] for _ in range(num_packs)]
        row_weight = weight_cpu[i]

        # 1. Greedy packing
        for group in indices[i].tolist():
            w = row_weight[group].item()
            best_pack = -1
            min_val = float('inf')
            # Find the least loaded pack that has space
            for p in range(num_packs):
                if len(current_packs[p]) < groups_per_pack:
                    if pack_weights[p] < min_val:
                        min_val = pack_weights[p]
                        best_pack = p
            current_packs[best_pack].append(group)
            pack_weights[best_pack] += w

        # 2. Refinement: Swap items between max-load and min-load packs
        for _ in range(20):
            min_p = min(range(num_packs), key=pack_weights.__getitem__)
            max_p = max(range(num_packs), key=pack_weights.__getitem__)

            if min_p == max_p:
                break

            diff = pack_weights[max_p] - pack_weights[min_p]
            if diff < 1e-6:
                break

            best_swap = None
            target = diff / 2.0
            found_swap = False

            # Try to swap item u from max_p with item v from min_p
            # We look for a swap that reduces the diff (i.e., w_u > w_v)
            for idx_u, u in enumerate(current_packs[max_p]):
                w_u = row_weight[u].item()
                for idx_v, v in enumerate(current_packs[min_p]):
                    w_v = row_weight[v].item()

                    delta = w_u - w_v
                    if 0 < delta < diff:
                        gap = abs(delta - target)
                        if best_swap is None or gap < best_swap[0]:
                            best_swap = (gap, idx_u, idx_v, delta)
                            found_swap = True

            if found_swap:
                _, idx_u, idx_v, delta = best_swap
                u = current_packs[max_p][idx_u]
                v = current_packs[min_p][idx_v]

                current_packs[max_p][idx_u] = v
                current_packs[min_p][idx_v] = u
                pack_weights[max_p] -= delta
                pack_weights[min_p] += delta
            else:
                break

        # 3. Fill result tensors
        for p in range(num_packs):
            for r, g in enumerate(current_packs[p]):
                pack_index[i, g] = p
                rank_in_pack[i, g] = r

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=weight.device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Optimization: Move weights to CPU to avoid slow element-wise GPU access
    weight_cpu = weight.to("cpu", dtype=torch.float32)

    # Pre-allocate output tensors
    pack_index = torch.empty((num_layers, num_groups),
                             dtype=torch.int64,
                             device="cpu")
    rank_in_pack = torch.empty((num_layers, num_groups),
                               dtype=torch.int64,
                               device="cpu")

    num_restarts = 10  # Number of attempts with randomization

    for i in range(num_layers):
        row_weight = weight_cpu[i]
        # Precompute list for fast access
        row_weight_list = row_weight.tolist()

        best_diff = float('inf')
        best_packs = None

        # Base indices: sorted by weight descending
        base_indices = torch.argsort(row_weight, descending=True)

        for attempt in range(num_restarts):
            # 1. Randomized Greedy Initialization
            if attempt == 0:
                # First attempt: Pure LPT (Longest Processing Time first)
                indices = base_indices.tolist()
            else:
                # Subsequent attempts: Randomized order
                # Add noise to weights only for sorting purposes
                noise = torch.rand(num_groups, device="cpu") * 0.2 + 0.9
                indices = torch.argsort(row_weight * noise,
                                        descending=True).tolist()

            current_packs = [[] for _ in range(num_packs)]
            pack_weights = [0.0] * num_packs

            # Greedy assignment based on (possibly randomized) sort order
            for group_idx in indices:
                w = row_weight_list[group_idx]

                # Find lightest valid pack
                best_p = -1
                min_val = float('inf')

                # Small optimization: if num_packs is small, linear scan is fast enough
                for p in range(num_packs):
                    if len(current_packs[p]) < groups_per_pack:
                        if pack_weights[p] < min_val:
                            min_val = pack_weights[p]
                            best_p = p

                current_packs[best_p].append(group_idx)
                pack_weights[best_p] += w

            # 2. Refinement Phase (Local Search)
            # Try to swap items to improve balance
            for _ in range(20):
                min_p = min(range(num_packs), key=pack_weights.__getitem__)
                max_p = max(range(num_packs), key=pack_weights.__getitem__)

                diff = pack_weights[max_p] - pack_weights[min_p]
                if diff < 1e-6:
                    break

                target_diff = diff / 2.0
                best_swap = None

                # Search for best swap between max and min packs
                # We want (w_u - w_v) â‰ˆ diff/2
                for idx_u, u in enumerate(current_packs[max_p]):
                    w_u = row_weight_list[u]
                    for idx_v, v in enumerate(current_packs[min_p]):
                        w_v = row_weight_list[v]
                        delta = w_u - w_v
                        if 0 < delta < diff:
                            gap = abs(delta - target_diff)
                            if best_swap is None or gap < best_swap[0]:
                                best_swap = (gap, idx_u, idx_v, delta)
                                if gap < 1e-4: break
                    if best_swap and best_swap[0] < 1e-4: break

                if best_swap:
                    _, idx_u, idx_v, delta = best_swap
                    u = current_packs[max_p][idx_u]
                    v = current_packs[min_p][idx_v]

                    current_packs[max_p][idx_u] = v
                    current_packs[min_p][idx_v] = u
                    pack_weights[max_p] -= delta
                    pack_weights[min_p] += delta
                else:
                    break

            # Check if this attempt is better
            current_max = max(pack_weights)
            current_min = min(pack_weights)
            current_diff = current_max - current_min

            if current_diff < best_diff:
                best_diff = current_diff
                best_packs = current_packs
                if best_diff < 1e-6:
                    break  # Perfect balance found

        # 3. Fill result tensors with best found configuration
        for p in range(num_packs):
            for r, g in enumerate(best_packs[p]):
                pack_index[i, g] = p
                rank_in_pack[i, g] = r

    return pack_index, rank_in_pack
>>>>>>> REPLACE

</DIFF>