<NAME>
lns_repacking
</NAME>

<DESCRIPTION>
Replace the 2-way Vectorized Swap refinement with a more powerful Large Neighborhood Search (LNS) using "Ruin and Recreate".
Instead of swapping pairs, this method selects a subset of packs (Heaviest, Lightest, and Random), combines their items, and re-packs them using a randomized greedy heuristic. This effectively performs K-way swaps and can escape local optima that pairwise swaps cannot (e.g., 3-way cyclic dependencies).
Also reduces `NUM_RESTARTS` to 5 to allocate compute budget to the LNS steps.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Number of restarts for the randomized heuristic
    NUM_RESTARTS = 10

    for i in range(num_layers):
        layer_w = weight_cpu[i]

        best_max_load = float('inf')
        best_assignment = None

        for attempt in range(NUM_RESTARTS):
            # 1. Candidate Generation
            if attempt == 0:
                # Deterministic LPT
                indices = layer_w.argsort(descending=True).tolist()
            else:
                # Randomized LPT (Perturbed weights)
                noise = torch.rand(num_groups) * 0.2 + 0.9 # 0.9 to 1.1
                indices = (layer_w * noise).argsort(descending=True).tolist()

            # 2. Greedy Construction
            packs = [[] for _ in range(num_packs)]
            pack_weights = [0.0] * num_packs

            for g_idx in indices:
                w = layer_w[g_idx].item()
                # Best fit among non-full packs (min current load)
                best_p = -1
                min_w = float('inf')
                for p in range(num_packs):
                    if len(packs[p]) < groups_per_pack:
                        if pack_weights[p] < min_w:
                            min_w = pack_weights[p]
                            best_p = p

                packs[best_p].append(g_idx)
                pack_weights[best_p] += w

            # 3. Refinement (Local Search)
            for _ in range(20):
                # Identify max load pack
                max_p = -1
                max_w = -1.0
                for p in range(num_packs):
                    if pack_weights[p] > max_w:
                        max_w = pack_weights[p]
                        max_p = p

                best_swap = None
                best_gain = 0.0

                # Tensorize items in max pack for fast vectorized comp
                u_nodes = packs[max_p]
                w_u = layer_w[u_nodes]

                for p in range(num_packs):
                    if p == max_p:
                        continue

                    diff = max_w - pack_weights[p]
                    if diff < 1e-6:
                        continue

                    v_nodes = packs[p]
                    w_v = layer_w[v_nodes]

                    # Deltas matrix: w_u[i] - w_v[j]
                    deltas = w_u.unsqueeze(1) - w_v.unsqueeze(0)

                    # Valid swaps: 0 < delta < diff
                    mask = (deltas > 1e-6) & (deltas < diff)
                    if not mask.any():
                        continue

                    # Gain metric: maximize delta * (diff - delta)
                    gains = deltas * (diff - deltas)
                    gains = torch.where(mask, gains, -1.0)

                    curr_max_gain, curr_idx = gains.flatten().max(0)
                    curr_max_gain = curr_max_gain.item()

                    if curr_max_gain > best_gain:
                        best_gain = curr_max_gain
                        idx = curr_idx.item()
                        u_idx = idx // len(v_nodes)
                        v_idx = idx % len(v_nodes)
                        best_swap = (max_p, p, u_idx, v_idx, deltas.flatten()[idx].item())

                if best_swap:
                    p1, p2, u_i, v_i, delta = best_swap
                    # Execute swap
                    val_u = packs[p1][u_i]
                    val_v = packs[p2][v_i]
                    packs[p1][u_i] = val_v
                    packs[p2][v_i] = val_u

                    pack_weights[p1] -= delta
                    pack_weights[p2] += delta
                else:
                    break

            # 4. Evaluation
            current_max = max(pack_weights)
            if current_max < best_max_load - 1e-6:
                best_max_load = current_max
                best_assignment = [list(p) for p in packs]

        # 5. Final assignment for this layer
=======
    # Number of restarts for the randomized heuristic
    NUM_RESTARTS = 5

    for i in range(num_layers):
        layer_w = weight_cpu[i]
        layer_w_list = layer_w.tolist()

        best_max_load = float('inf')
        best_sum_sq = float('inf')
        best_assignment = None

        for attempt in range(NUM_RESTARTS):
            # 1. Candidate Generation
            if attempt == 0:
                indices = sorted(range(num_groups), key=lambda x: layer_w_list[x], reverse=True)
            else:
                # Randomized LPT
                noise = torch.rand(num_groups) * 0.15 + 0.925
                indices = (layer_w * noise).argsort(descending=True).tolist()

            # 2. Greedy Construction (LPT/Best Fit)
            packs = [[] for _ in range(num_packs)]
            pack_weights = [0.0] * num_packs
            pack_counts = [0] * num_packs

            for g_idx in indices:
                w = layer_w_list[g_idx]
                # Best Fit: Assign to valid pack with min load
                best_p = -1
                min_w = float('inf')

                for p in range(num_packs):
                    if pack_counts[p] < groups_per_pack:
                        if pack_weights[p] < min_w:
                            min_w = pack_weights[p]
                            best_p = p

                packs[best_p].append(g_idx)
                pack_weights[best_p] += w
                pack_counts[best_p] += 1

            # 3. Refinement: Large Neighborhood Search (LNS)
            MAX_LNS_STEPS = 100 if num_packs > 1 else 0

            for step in range(MAX_LNS_STEPS):
                # Identify heavy and light packs
                # Optimization: Track max/min indices dynamically or just scan
                max_p = max(range(num_packs), key=pack_weights.__getitem__)
                min_p = min(range(num_packs), key=pack_weights.__getitem__)

                if max_p == min_p:
                    break

                # If balanced enough, try to optimize variance by picking random packs
                # Otherwise, target max load reduction
                if pack_weights[max_p] - pack_weights[min_p] < 1e-6:
                     # Already balanced max-min, but maybe high variance?
                     pass

                candidates = {max_p, min_p}
                # Add random packs for k-way swaps
                if num_packs > 2:
                    k_random = min(2, num_packs - 2)
                    for _ in range(k_random):
                        candidates.add(torch.randint(0, num_packs, (1,)).item())

                cand_list = list(candidates)

                # Ruin
                items = []
                for p in cand_list:
                    items.extend(packs[p])

                # Recreate: Solve sub-problem (minimize max load of these packs)
                best_sub_weights = [pack_weights[p] for p in cand_list]
                best_sub_packs = [list(packs[p]) for p in cand_list]
                best_sub_max = max(best_sub_weights)
                best_sub_ss = sum(w*w for w in best_sub_weights)

                improved_sub = False

                # 0. Deterministic LPT attempt
                # 1..N. Randomized LPT attempts

                # Prepare LPT sort once
                items_sorted = sorted(items, key=lambda x: layer_w_list[x], reverse=True)

                SUB_RESTARTS = 20
                for sub_attempt in range(SUB_RESTARTS):
                    if sub_attempt == 0:
                        current_items = items_sorted
                    else:
                        # Perturb weights slightly to change sort order
                        noise_sub = torch.rand(len(items))
                        # Efficient perturbed sort
                        # We use a list of tuples (perturbed_weight, index)
                        weighted_items = []
                        for idx, item in enumerate(items):
                            pw = layer_w_list[item] * (0.8 + 0.4 * noise_sub[idx].item())
                            weighted_items.append((pw, item))
                        weighted_items.sort(key=lambda x: x[0], reverse=True)
                        current_items = [x[1] for x in weighted_items]

                    # Greedy Fill (Best Fit)
                    temp_weights = {p: 0.0 for p in cand_list}
                    temp_packs = {p: [] for p in cand_list}
                    temp_counts = {p: 0 for p in cand_list}

                    possible = True
                    for item_idx in current_items:
                        w = layer_w_list[item_idx]

                        best_local_p = -1
                        min_local_w = float('inf')

                        for p in cand_list:
                            if temp_counts[p] < groups_per_pack:
                                if temp_weights[p] < min_local_w:
                                    min_local_w = temp_weights[p]
                                    best_local_p = p

                        if best_local_p == -1:
                            possible = False
                            break

                        temp_packs[best_local_p].append(item_idx)
                        temp_weights[best_local_p] += w
                        temp_counts[best_local_p] += 1

                    if possible:
                        c_max = max(temp_weights.values())
                        c_ss = sum(v*v for v in temp_weights.values())

                        # Check improvement
                        if c_max < best_sub_max - 1e-6 or (abs(c_max - best_sub_max) < 1e-6 and c_ss < best_sub_ss - 1e-6):
                            best_sub_max = c_max
                            best_sub_ss = c_ss
                            best_sub_weights = [temp_weights[p] for p in cand_list]
                            best_sub_packs = [temp_packs[p] for p in cand_list]
                            improved_sub = True

                if improved_sub:
                    for idx, p in enumerate(cand_list):
                        packs[p] = best_sub_packs[idx]
                        pack_weights[p] = best_sub_weights[idx]
                        pack_counts[p] = groups_per_pack # Should remain full

            # 4. Evaluation
            current_max = max(pack_weights)
            current_ss = sum(w*w for w in pack_weights)

            if current_max < best_max_load - 1e-6:
                best_max_load = current_max
                best_sum_sq = current_ss
                best_assignment = [list(p) for p in packs]
            elif abs(current_max - best_max_load) < 1e-6 and current_ss < best_sum_sq - 1e-6:
                best_sum_sq = current_ss
                best_assignment = [list(p) for p in packs]

        # 5. Final assignment for this layer
>>>>>>> REPLACE
</DIFF>