--- a/original.py
+++ b/original.py
@@ -1,502 +1,484 @@
 # EVOLVE-BLOCK-START
 """
 Expert parallelism load balancer (EPLB) for vLLM.
 
 This module implements the core rearrangement algorithm.
 
 The rearrangement algorithm is adapted from
 [DeepSeek EPLB](https://github.com/deepseek-ai/eplb).
 
 Please find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example
 on how the EPLB algorithm works.
 """
 
 import torch
 import random
 import math
 
 def balanced_packing(weight: torch.Tensor,
                      num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
     """
     Pack n weighted objects to m packs, such that each bin contains exactly
     n/m objects and the weights of all packs are as balanced as possible.
 
     Algorithm: Cyclic LNS Load Balancer (Randomized Greedy + Vectorized Swap + 3-Way LNS)
 
     Parameters:
         weight: [X, n], the weight of each item
         num_packs: number of packs
 
     Returns:
         pack_index: [X, n], the pack index of each item
         rank_in_pack: [X, n], the rank of the item in the pack
     """
     num_layers, num_groups = weight.shape
     assert num_groups % num_packs == 0
     groups_per_pack = num_groups // num_packs
     device = weight.device
 
     # Trivial case
     if groups_per_pack == 1:
         pack_index = torch.arange(num_groups, dtype=torch.int64, device=device).expand(weight.shape)
         rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
         return pack_index, rank_in_pack
 
     # Work on CPU for efficient scalar iteration
     weight_cpu = weight.cpu()
 
     # Pre-allocate output tensors
     pack_index = torch.empty((num_layers, num_groups), dtype=torch.int64)
     rank_in_pack = torch.empty((num_layers, num_groups), dtype=torch.int64)
 
     # Tuning constants
     NUM_RESTARTS = 50
     NUM_SWAP_ITER = 10
     NUM_LNS_ITER = 30
 
     all_packs = list(range(num_packs))
 
     for i in range(num_layers):
         layer_w = weight_cpu[i]
         w_list = layer_w.tolist()
 
         # Base LPT sort order
         sorted_indices_base = sorted(range(num_groups), key=lambda x: w_list[x], reverse=True)
 
         best_max_load = float('inf')
         best_assignment = None # List[List[int]]
         best_pack_weights = None # List[float]
 
         # 1. Randomized Greedy Construction
         for attempt in range(NUM_RESTARTS):
             # Candidate order generation
             if attempt == 0:
                 indices = sorted_indices_base
             else:
                 # Add noise and resort. Noise factor 0.15 for more variance
                 noise_scale = 0.15
                 noisy_w = [w * (1.0 + (random.random() - 0.5) * noise_scale) for w in w_list]
                 indices = sorted(range(num_groups), key=lambda x: noisy_w[x], reverse=True)
 
             current_assignment = [[] for _ in range(num_packs)]
             current_loads = [0.0] * num_packs
             current_counts = [0] * num_packs
 
             # Pack items
             for g_idx in indices:
                 w = w_list[g_idx]
 
                 # Best Fit: Find pack with min load among those with space
                 best_p = -1
                 min_load = float('inf')
 
                 for p in range(num_packs):
                     if current_counts[p] < groups_per_pack:
                         if current_loads[p] < min_load:
                             min_load = current_loads[p]
                             best_p = p
 
                 current_assignment[best_p].append(g_idx)
                 current_loads[best_p] += w
                 current_counts[best_p] += 1
 
             max_l = max(current_loads)
             if max_l < best_max_load:
                 best_max_load = max_l
                 best_assignment = current_assignment
                 best_pack_weights = current_loads
 
         # Restore best greedy state
         packs = best_assignment
         pack_weights = best_pack_weights
 
-        # 2. Refinement: Pairwise Swaps + 3-Way Cyclic Swaps
-        for _ in range(NUM_SWAP_ITER):
-            # Identify max pack
-            max_p = max(range(num_packs), key=lambda x: pack_weights[x])
-            max_w = pack_weights[max_p]
-
-            u_nodes = packs[max_p]
-            w_u = layer_w[u_nodes]
-
-            # Vectorized Pairwise Swap
-            best_swap = None
-            best_gain = 0.0
-
-            for p in range(num_packs):
-                if p == max_p: continue
-
-                diff = max_w - pack_weights[p]
-                if diff < 1e-6: continue
-
-                v_nodes = packs[p]
-                w_v = layer_w[v_nodes]
-
-                # deltas[i, j] = w_u[i] - w_v[j]
-                deltas = w_u.unsqueeze(1) - w_v.unsqueeze(0)
-                mask = (deltas > 1e-6) & (deltas < diff)
-
-                if not mask.any(): continue
-
-                # Gain: maximize delta * (diff - delta)
-                gains = deltas * (diff - deltas)
-                gains = torch.where(mask, gains, -1.0)
-
-                curr_max, curr_argmax = gains.reshape(-1).max(0)
-                if curr_max.item() > best_gain:
-                    best_gain = curr_max.item()
-                    idx = curr_argmax.item()
-                    best_swap = (max_p, p, idx // len(v_nodes), idx % len(v_nodes), deltas.reshape(-1)[idx].item())
-
-            if best_swap:
-                p1, p2, i1, i2, delta = best_swap
-                val1 = packs[p1][i1]
-                val2 = packs[p2][i2]
-                packs[p1][i1] = val2
-                packs[p2][i2] = val1
-                pack_weights[p1] -= delta
-                pack_weights[p2] += delta
-                continue # If swap succeeded, reiterate from top
-
-            # 3-Way Cyclic Swap: A (Max) -> B -> C -> A
-            if num_packs < 3: break
-
-            improved_cycle = False
-            # Sort packs by weight to pick targets (B and C)
-            sorted_packs = sorted(range(num_packs), key=lambda x: pack_weights[x], reverse=True)
-            # Try 5 lightest packs as candidates for B and C
-            candidates = sorted_packs[-5:]
-
-            for p_b in candidates:
-                if p_b == max_p: continue
-                for p_c in candidates:
-                    if p_c == max_p or p_c == p_b: continue
-
-                    nodes_a = packs[max_p]
-                    nodes_b = packs[p_b]
-                    nodes_c = packs[p_c]
-
-                    w_a_vals = [w_list[x] for x in nodes_a]
-                    w_b_vals = [w_list[x] for x in nodes_b]
-                    w_c_vals = [w_list[x] for x in nodes_c]
-
-                    curr_w_a = pack_weights[max_p]
-                    curr_w_b = pack_weights[p_b]
-                    curr_w_c = pack_weights[p_c]
-
-                    best_cycle = None
-                    best_cycle_gain = 0.0
-
-                    # Iterate items to find best cycle
-                    # Cycle: A(i) -> B, B(j) -> C, C(k) -> A
-                    for idx_a, val_a in enumerate(w_a_vals):
-                        for idx_b, val_b in enumerate(w_b_vals):
-                            # Ensure B doesn't blow up
-                            if curr_w_b + val_a - val_b >= curr_w_a - 1e-6: continue
-
-                            for idx_c, val_c in enumerate(w_c_vals):
-                                # Ensure A reduces
-                                gain_a = val_a - val_c
-                                if gain_a <= 1e-6: continue
-
-                                # Ensure C doesn't blow up
-                                if curr_w_c + val_b - val_c >= curr_w_a - 1e-6: continue
-
-                                if gain_a > best_cycle_gain:
-                                    best_cycle_gain = gain_a
-                                    best_cycle = (idx_a, idx_b, idx_c, val_a, val_b, val_c)
-
-                    if best_cycle:
-                        ia, ib, ic, va, vb, vc = best_cycle
-                        # Perform Cycle
-                        item_a = packs[max_p][ia]
-                        item_b = packs[p_b][ib]
-                        item_c = packs[p_c][ic]
-
-                        packs[max_p][ia] = item_c
-                        packs[p_b][ib] = item_a
-                        packs[p_c][ic] = item_b
-
-                        pack_weights[max_p] += (vc - va)
-                        pack_weights[p_b] += (va - vb)
-                        pack_weights[p_c] += (vb - vc)
-
-                        improved_cycle = True
-                        break # Break inner C loop
-                if improved_cycle: break # Break inner B loop
-
-            if not improved_cycle:
+        # 2. Refinement: Pairwise Rebalancing (Heavier Pack vs Others)
+        # We try to rebalance the heaviest pack with the lightest pack (and random others).
+        # We pool items from both and re-partition them using randomized greedy.
+
+        NUM_PAIRWISE_ITER = 30
+        for _ in range(NUM_PAIRWISE_ITER):
+            # Identify packs sorted by weight
+            sorted_by_w = sorted(range(num_packs), key=lambda x: pack_weights[x])
+            max_p = sorted_by_w[-1]
+            min_p = sorted_by_w[0]
+
+            if pack_weights[max_p] - pack_weights[min_p] < 1e-6:
+                break
+
+            # Candidates to pair with max_p: Min pack, and maybe a random one
+            candidates = [min_p]
+            if num_packs > 2:
+                # Add a few random candidates to help escape local optima
+                for _ in range(2):
+                    r = random.randint(0, num_packs - 1)
+                    if r != max_p and r not in candidates:
+                        candidates.append(r)
+
+            improved_any = False
+            for other_p in candidates:
+                # Current max of the pair
+                old_pair_max = max(pack_weights[max_p], pack_weights[other_p])
+                old_pair_ss = pack_weights[max_p]**2 + pack_weights[other_p]**2
+
+                items = packs[max_p] + packs[other_p]
+
+                # Try to re-partition items into two sets of size groups_per_pack
+                best_split = None
+                best_split_max = old_pair_max
+                best_split_ss = old_pair_ss
+
+                # Strategies: Deterministic LPT, then Randomized LPTs
+                # Pre-sort items LPT
+                items_desc = sorted(items, key=lambda x: w_list[x], reverse=True)
+
+                # Small sub-problem restarts
+                SUB_RESTARTS = 20
+
+                for attempt in range(SUB_RESTARTS):
+                    if attempt == 0:
+                        current_items = items_desc
+                    else:
+                        # Perturbed weights
+                        noise = [random.uniform(0.85, 1.15) for _ in range(len(items))]
+                        # Sort by perturbed weight
+                        current_items = sorted(items, key=lambda x: w_list[x] * noise[items.index(x)], reverse=True)
+
+                    # Greedy Fill for 2 bins
+                    bin_a = []
+                    bin_b = []
+                    w_a = 0.0
+                    w_b = 0.0
+
+                    for item in current_items:
+                        w = w_list[item]
+                        # Put in lighter bin if space allows
+                        can_a = len(bin_a) < groups_per_pack
+                        can_b = len(bin_b) < groups_per_pack
+
+                        if can_a and can_b:
+                            if w_a < w_b:
+                                bin_a.append(item); w_a += w
+                            else:
+                                bin_b.append(item); w_b += w
+                        elif can_a:
+                            bin_a.append(item); w_a += w
+                        elif can_b:
+                            bin_b.append(item); w_b += w
+                        else:
+                            # Should not happen if sizes match
+                            pass
+
+                    cur_max = max(w_a, w_b)
+                    cur_ss = w_a**2 + w_b**2
+
+                    if cur_max < best_split_max - 1e-6:
+                        best_split_max = cur_max
+                        best_split_ss = cur_ss
+                        best_split = (bin_a, bin_b, w_a, w_b)
+                    elif abs(cur_max - best_split_max) < 1e-6 and cur_ss < best_split_ss - 1e-6:
+                        best_split_max = cur_max
+                        best_split_ss = cur_ss
+                        best_split = (bin_a, bin_b, w_a, w_b)
+
+                if best_split:
+                    packs[max_p] = best_split[0]
+                    packs[other_p] = best_split[1]
+                    pack_weights[max_p] = best_split[2]
+                    pack_weights[other_p] = best_split[3]
+                    improved_any = True
+                    break # Restart outer loop to identify new max
+
+            if not improved_any:
                 break
 
         # 3. Refinement: Enhanced LNS (Ruin & Recreate)
         if num_packs >= 3:
             for _ in range(NUM_LNS_ITER):
                 sorted_p = sorted(range(num_packs), key=pack_weights.__getitem__, reverse=True)
                 p_max = sorted_p[0]
                 p_min = sorted_p[-1]
 
                 if pack_weights[p_max] - pack_weights[p_min] < 1e-6: break
 
                 # Pick 3rd pack: Mix of Top, Bottom, and Random
                 if _ % 3 == 0: p_mid = sorted_p[1]
                 elif _ % 3 == 1: p_mid = sorted_p[random.randint(1, num_packs-1)]
                 else: p_mid = sorted_p[random.randint(1, num_packs-1)]
 
                 if p_mid == p_max or p_mid == p_min:
                     candidates = [x for x in all_packs if x != p_max and x != p_min]
                     if not candidates: break
                     p_mid = random.choice(candidates)
 
                 target_packs = [p_max, p_min, p_mid]
 
                 items = []
                 for p in target_packs:
                     items.extend(packs[p])
 
                 best_sub_res = None
 
                 # Try LPT, Jittered LPT, and Random Shuffle
                 strategies = ['lpt', 'lpt_noise', 'shuffle', 'shuffle']
 
                 curr_local_max = max(pack_weights[p] for p in target_packs)
                 curr_local_ss = sum(pack_weights[p]**2 for p in target_packs)
 
                 for strat in strategies:
                     if strat == 'lpt':
                         temp_items = sorted(items, key=lambda x: w_list[x], reverse=True)
                     elif strat == 'lpt_noise':
                         temp_items = sorted(items, key=lambda x: w_list[x] * (1.0 + (random.random()-0.5)*0.1), reverse=True)
                     else:
                         temp_items = list(items)
                         random.shuffle(temp_items)
 
                     sub_p = {p: [] for p in target_packs}
                     sub_w = {p: 0.0 for p in target_packs}
                     sub_c = {p: 0 for p in target_packs}
 
                     possible = True
                     for item in temp_items:
                         w = w_list[item]
                         best_p = -1
                         min_val = float('inf')
                         # Best fit
                         for p in target_packs:
                             if sub_c[p] < groups_per_pack:
                                 if sub_w[p] < min_val:
                                     min_val = sub_w[p]
                                     best_p = p
                         if best_p == -1:
                             possible = False; break
                         sub_p[best_p].append(item)
                         sub_w[best_p] += w
                         sub_c[best_p] += 1
 
                     if possible:
                         new_max = max(sub_w.values())
                         new_ss = sum(v**2 for v in sub_w.values())
 
                         if new_max < curr_local_max - 1e-6:
                             curr_local_max = new_max
                             curr_local_ss = new_ss
                             best_sub_res = (sub_p, sub_w)
                         elif abs(new_max - curr_local_max) < 1e-6 and new_ss < curr_local_ss - 1e-6:
                             curr_local_ss = new_ss
                             best_sub_res = (sub_p, sub_w)
 
                 if best_sub_res:
                     s_p, s_w = best_sub_res
                     for p in target_packs:
                         packs[p] = s_p[p]
                         pack_weights[p] = s_w[p]
 
         # Final write to output tensors
         for p in range(num_packs):
             for r, g_idx in enumerate(packs[p]):
                 pack_index[i, g_idx] = p
                 rank_in_pack[i, g_idx] = r
 
     return pack_index.to(device), rank_in_pack.to(device)
 
 
 def replicate_experts(
         weight: torch.Tensor,
         num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Replicate `num_log` experts to `num_phy` replicas, such that the maximum
     load of all replicas is minimized.
 
     Parameters:
         weight: [X, num_log]
         num_phy: total number of experts after replication
 
     Returns:
         phy2log: [X, num_phy], logical expert id of each physical expert
         rank: [X, num_phy], the replica rank
         logcnt: [X, num_log], number of replicas for each logical expert
     """
     n, num_log = weight.shape
     num_redundant = num_phy - num_log
     assert num_redundant >= 0
     device = weight.device
     phy2log = torch.arange(num_phy, dtype=torch.int64,
                            device=device).repeat(n, 1)
     rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)
     logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)
     arangen = torch.arange(n, dtype=torch.int64, device=device)
     for i in range(num_log, num_phy):
         redundant_indices = (weight / logcnt).max(dim=-1).indices
         phy2log[:, i] = redundant_indices
         rank[:, i] = logcnt[arangen, redundant_indices]
         logcnt[arangen, redundant_indices] += 1
     return phy2log, rank, logcnt
 
 
 def rebalance_experts_hierarchical(
     weight: torch.Tensor,
     num_physical_experts: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ):
     """
     Parameters:
         weight: [num_moe_layers, num_logical_experts]
         num_physical_experts: number of physical experts after replication
         num_groups: number of expert groups
         num_nodes: number of server nodes, where the intra-node network
         (e.g, NVLink) is faster
         num_gpus: number of GPUs, must be a multiple of `num_nodes`
 
     Returns:
         physical_to_logical_map: [num_moe_layers, num_physical_experts]
         logical_to_physical_map: [num_moe_layers, num_logical_experts, X]
         logical_count: [num_moe_layers, num_logical_experts]
     """
     num_layers, num_logical_experts = weight.shape
     assert num_logical_experts % num_groups == 0
     group_size = num_logical_experts // num_groups
     assert num_groups % num_nodes == 0
     groups_per_node = num_groups // num_nodes
     assert num_gpus % num_nodes == 0
     assert num_physical_experts % num_gpus == 0
     phy_experts_per_gpu = num_physical_experts // num_gpus
 
     def inverse(perm: torch.Tensor) -> torch.Tensor:
         inv = torch.empty_like(perm)
         inv.scatter_(
             1,
             perm,
             torch.arange(perm.size(1), dtype=torch.int64,
                          device=perm.device).expand(perm.shape),
         )
         return inv
 
     # Step 1: pack groups to nodes
     tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)
     group_pack_index, group_rank_in_pack = balanced_packing(
         tokens_per_group, num_nodes)
     log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *
                  group_size).unsqueeze(-1) +
                 torch.arange(group_size,
                              dtype=torch.int64,
                              device=group_pack_index.device)).flatten(-2)
     mlog2log = inverse(log2mlog)
 
     # Step 2: construct redundant experts within nodes
     # [num_layers * num_nodes, num_logical_experts // num_nodes]
     tokens_per_mlog = weight.gather(-1, mlog2log).view(
         -1, num_logical_experts // num_nodes)
     phy2mlog, phyrank, mlogcnt = replicate_experts(
         tokens_per_mlog, num_physical_experts // num_nodes)
 
     # Step 3: pack physical_experts to GPUs
     # [num_layers * num_nodes, num_physical_experts // num_nodes]
     tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
     pack_index, rank_in_pack = balanced_packing(tokens_per_phy,
                                                 num_gpus // num_nodes)
     phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
     pphy2phy = inverse(phy2pphy)
 
     pphy2mlog = phy2mlog.gather(
         -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]
     pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(
         0,
         num_logical_experts,
         num_logical_experts // num_nodes,
         device=group_pack_index.device,
     ).view(1, -1, 1)).flatten(-2)
     pphy2log = mlog2log.gather(-1, pphy2mlog)
     pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)
     logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)
     return pphy2log, pphyrank, logcnt
 
 
 def rebalance_experts(
     weight: torch.Tensor,
     num_replicas: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Entry point for expert-parallelism load balancer.
 
     Parameters:
         weight: [layers, num_logical_experts], the load statistics for all
             logical experts
         num_replicas: number of physical experts, must be a multiple of
             `num_gpus`
         num_groups: number of expert groups
         num_nodes: number of server nodes, where the intra-node network
             (e.g, NVLink) is faster
         num_gpus: number of GPUs, must be a multiple of `num_nodes`
 
     Returns:
         physical_to_logical_map: [layers, num_replicas], the expert index of
             each replica
         logical_to_physical_map: [layers, num_logical_experts, X], the replica
             indices for each expert
         expert_count: [layers, num_logical_experts], number of physical
             replicas for each logical expert
     """
     num_layers, num_logical_experts = weight.shape
     weight = weight.float().cpu()
     if num_groups % num_nodes == 0:
         # use hierarchical load-balance policy
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, num_groups, num_nodes, num_gpus)
     else:
         # use global load-balance policy
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, 1, 1, num_gpus)
     num_redundant_experts = num_replicas - num_logical_experts
     maxlogcnt = num_redundant_experts + 1
     log2phy: torch.Tensor = torch.full(
         (num_layers, num_logical_experts, maxlogcnt),
         -1,
         dtype=torch.int64,
         device=logcnt.device,
     )
     log2phy.view(num_layers, -1).scatter_(
         -1,
         phy2log * maxlogcnt + phyrank,
         torch.arange(num_replicas, dtype=torch.int64,
                      device=log2phy.device).expand(num_layers, -1),
     )
     return phy2log, log2phy, logcnt
 
 
 # EVOLVE-BLOCK-END
 
 
 # This part remains fixed (not evolved)
 def run_eplb(weight: torch.Tensor, num_replicas: int, num_groups: int,
              num_nodes: int, num_gpus: int):
     """Run the expert parallelism load balancer"""
     phy2log, log2phy, logcnt = rebalance_experts(
         weight, num_replicas, num_groups, num_nodes, num_gpus
     )
     return phy2log, log2phy, logcnt
 
 
 __all__ = ["rebalance_experts", "run_eplb"]