<NAME>
parameter_diversity_and_lahc_tuning
</NAME>

<DESCRIPTION>
1.  **Diversified Greedy Phase**: Instead of using fixed hyperparameters (`alpha=0.05`, `threshold=0.90`) for all greedy candidates, we now randomize them for each candidate. `alpha` varies (0.0 to 0.15) to explore different trade-offs between immediate cost minimization and packing large transactions ("Big Rocks"). `threshold` varies (0.85 to 0.98) to change the exclusivity of the Big Rock pool. This generates a more diverse set of initial candidates for the refinement phases.
2.  **Increased Greedy Sampling**: Increased base sample size from 12 to 16 to improve the quality of greedy decisions.
3.  **Enhanced LAHC Operators**: Increased the maximum block size for Block Shift mutation from 4 to 6 to allow moving larger dependency chains.
4.  **Marathon Tuning**: Increased the LAHC history length from 50 to 100 for the final Marathon phase to allow the algorithm to escape larger local optima during the extended search.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Algorithm Hyperparameters
    SAMPLE_SIZE = 12
    BIG_ROCK_THRESHOLD = 0.90
    WEIGHT_ALPHA = 0.05  # Allows sacrificing cost slightly to pack large items early

    candidates = []

    # --- Phase 1: Adaptive Greedy Construction ---
    for _ in range(num_seqs):
        remaining = set(range(workload.num_txns))

        # Random start transaction to diversify the greedy construction
        start_txn = random.choice(list(remaining))
        current_seq = [start_txn]
        remaining.remove(start_txn)

        while remaining:
            # Dynamic "Big Rocks": What is the max length remaining?
            rem_lens = [txn_lens[t] for t in remaining]
            max_rem_len = max(rem_lens) if rem_lens else 0
            threshold = max_rem_len * BIG_ROCK_THRESHOLD

            # Pool 1: Big Rocks (Longest remaining transactions)
            big_rocks = [t for t in remaining if txn_lens[t] >= threshold]

            pool = list(big_rocks)

            # Pool 2: Random fill to maintain diversity if pool is small
            needed = SAMPLE_SIZE - len(pool)
            if needed > 0:
                others = [t for t in remaining if t not in big_rocks]
                if len(others) > needed:
                    pool.extend(random.sample(others, needed))
                else:
                    pool.extend(others)

            # Deduplicate
            pool = list(set(pool))

            best_cand = -1
            best_score = float('inf')

            # Evaluate Pool with Weighted Scoring
            for t in pool:
                # Simulator call (expensive)
                cost = workload.get_opt_seq_cost(current_seq + [t])

                # Score: Cost minus a bonus for transaction length
                # This encourages scheduling long transactions earlier even if they fit imperfectly
                score = cost - (WEIGHT_ALPHA * txn_lens[t])

                if score < best_score:
                    best_score = score
                    best_cand = t

            current_seq.append(best_cand)
            remaining.remove(best_cand)
=======
    # Algorithm Hyperparameters
    # We now randomize these per sequence to generate diverse starting points
    BASE_SAMPLE_SIZE = 16

    candidates = []

    # --- Phase 1: Diversified Adaptive Greedy Construction ---
    for _ in range(num_seqs):
        # Randomize heuristics for this candidate to explore different strategies
        # Alpha: Balance between minimizing cost (0) and packing large items (high)
        # Threshold: How exclusive is the "Big Rock" club
        current_alpha = random.uniform(0.0, 0.15)
        current_threshold_ratio = random.uniform(0.85, 0.98)

        remaining = set(range(workload.num_txns))

        # Random start transaction to diversify the greedy construction
        start_txn = random.choice(list(remaining))
        current_seq = [start_txn]
        remaining.remove(start_txn)

        while remaining:
            # Dynamic "Big Rocks": What is the max length remaining?
            rem_lens = [txn_lens[t] for t in remaining]
            max_rem_len = max(rem_lens) if rem_lens else 0
            threshold = max_rem_len * current_threshold_ratio

            # Pool 1: Big Rocks (Longest remaining transactions)
            big_rocks = [t for t in remaining if txn_lens[t] >= threshold]

            pool = list(big_rocks)

            # Pool 2: Random fill to maintain diversity if pool is small
            needed = BASE_SAMPLE_SIZE - len(pool)
            if needed > 0:
                others = [t for t in remaining if t not in big_rocks]
                if len(others) > needed:
                    pool.extend(random.sample(others, needed))
                else:
                    pool.extend(others)

            # Deduplicate
            pool = list(set(pool))

            best_cand = -1
            best_score = float('inf')

            # Evaluate Pool with Weighted Scoring
            for t in pool:
                # Simulator call (expensive)
                cost = workload.get_opt_seq_cost(current_seq + [t])

                # Score: Cost minus a bonus for transaction length
                # This encourages scheduling long transactions earlier even if they fit imperfectly
                score = cost - (current_alpha * txn_lens[t])

                if score < best_score:
                    best_score = score
                    best_cand = t

            current_seq.append(best_cand)
            remaining.remove(best_cand)
>>>>>>> REPLACE
<<<<<<< SEARCH
            elif op < 0.8:
                # Block Shift - 30%
                # Moves a small block of transactions together
                if slen < 5: continue
                bsize = random.randint(2, 4)
                f = random.randint(0, slen-bsize)
                block = neigh_s[f:f+bsize]
=======
            elif op < 0.8:
                # Block Shift - 30%
                # Moves a small block of transactions together.
                # Slightly increased block size range to move larger dependency chains.
                if slen < 5: continue
                bsize = random.randint(2, 6)
                f = random.randint(0, slen-bsize)
                block = neigh_s[f:f+bsize]
>>>>>>> REPLACE
<<<<<<< SEARCH
    MARATHON_ITERS = 2500
    final_cost, final_seq = run_lahc(champion_seq, champion_cost, MARATHON_ITERS)

    return final_cost, final_seq
=======
    MARATHON_ITERS = 2500
    # Use a longer history for the marathon to escape deeper local optima
    final_cost, final_seq = run_lahc(champion_seq, champion_cost, MARATHON_ITERS, history_len=100)

    return final_cost, final_seq
>>>>>>> REPLACE
</DIFF>