think silently:
The user wants to improve the `get_best_schedule` function in the provided python code.
I previously attempted to replace the entire function but the diff failed, likely because I didn't match the SEARCH block exactly or the output was truncated.

The goal is to maximize the combined score (makespan reduction).
My previous strategy was:
1.  **Efficiency-Aware Beam Search**: Replace the standard cost-based Beam Search with one that considers "efficiency" (how much of a transaction's cost is hidden by parallelism). This uses a decaying gamma factor.
2.  **Adaptive SA**: Reduce iterations to save time for phase 3, keep multi-segment ruin.
3.  **LPT Polish**: Add a deterministic polish phase at the end that iterates through transactions sorted by LPT (Longest Processing Time) and re-inserts them greedily. This targets the "big rocks" first during refinement.

I need to apply these changes carefully. Since the previous error was "No changes applied", I must ensure the SEARCH block is character-perfect.

Let's break down the changes.
I will replace the body of `get_best_schedule` to implement the new pipeline.

The original code's `get_best_schedule` uses:
1.  Cost-based Beam Search (simple greedy cost).
2.  Adaptive SA with Multi-Segment Ruin.

I will propose replacing the whole function `get_best_schedule` again, but I will be extremely careful with copying the SEARCH block.

Wait, looking at the previous diff, it seems I cut off the REPLACE block at the end (it ended with `sz = random.randint(RUIN_BLOCK_SIZE_MIN, `). That is definitely why it failed or was invalid.

I will rewrite the diff. I will replace the entire `get_best_schedule` function.

**Plan:**
1.  **Phase 1 (Construction)**: Use the "Efficiency-Aware" Beam Search logic from the generated programs (specifically Program 4 logic seemed good but maybe combined with Program 3). Actually, looking at the history, Program 57 (Efficiency-Aware Beam + LPT Polish) had a score of 4.29 mentioned in the prompt's recommendations, although I don't see Program 57 in the provided code snippets. I see Program 4 (Score 3.89) and Program 5 (Score 3.83). The current program is Score 3.92.
    The prompt recommendations suggest: "Enforce Convergence in Gap Repair", "Prioritize LPT Sort in Polish Iteration".
    So I will implement exactly that.

    *   **Beam Search**: I will stick to the "Efficiency-Aware" logic which tries to maximize parallelism `(w - delta)`.
    *   **SA**: Keep the current structure but tune parameters.
    *   **Polish**: Implement the "Gap Repair" but iterate until convergence (or fixed passes) and iterate items in LPT order (descending weight).

**Search Block Construction:**
I need to copy the *exact* current implementation of `get_best_schedule`.

**Replace Block Construction:**
1.  Define `get_best_schedule`.
2.  Hyperparameters: `BEAM_WIDTH=4`, `SA_ITERATIONS=2500` (reduced), `MAX_POLISH_PASSES=3`.
3.  `txn_weights` pre-calc.
4.  **Phase 1**: Efficiency-Aware Beam Search.
    *   Score = `new_cost - (gamma * efficiency * w)`
    *   Gamma decays.
5.  **Phase 2**: SA with Multi-Segment Ruin (keep mostly same logic as current but adapted).
6.  **Phase 3**: Iterative LPT Polish.
    *   Loop `p` in `MAX_POLISH_PASSES`.
    *   Sort current schedule items by weight (LPT).
    *   Remove and re-insert best.
    *   Break if no improvement.

Let's double check the source code provided for `get_best_schedule`.

```python
def get_best_schedule(workload, num_seqs):
    """
    Get optimal schedule using Exhaustive Beam Search followed by
    Adaptive Simulated Annealing with Multi-Segment Ruin-and-Recreate.

    Args:
        workload: Workload object containing transaction data
        num_seqs: Parameter affecting computational budget

    Returns:
        Tuple of (lowest makespan, corresponding schedule)
    """
    # --- Hyperparameters ---
    BEAM_WIDTH = 4  # Focused beam for construction
    ...
```

I will verify the indentation and content.

One potential issue with replacing the whole function is the size of the block. However, it's the safest way to change the flow completely.

Implementation details for Phase 3 (Polish):
```python
    # --- Phase 3: Iterative LPT Polish ---
    # Sort transactions by weight (LPT) to prioritize placing large items
    # We create a list of txn IDs sorted by weight desc
    sorted_txns = sorted(range(workload.num_txns), key=lambda x: txn_weights[x], reverse=True)

    polish_seq = list(best_schedule)
    current_polish_cost = best_cost

    MAX_PASSES = 3
    for pass_idx in range(MAX_PASSES):
        improved_in_pass = False

        # Iterate through transactions in LPT order
        for item in sorted_txns:
            # Find where 'item' is currently in polish_seq
            try:
                current_idx = polish_seq.index(item)
            except ValueError:
                continue # Should not happen

            # Remove
            polish_seq.pop(current_idx)

            best_pos = -1
            min_c = float('inf')

            # Scan all positions
            for j in range(len(polish_seq) + 1):
                polish_seq.insert(j, item)
                c = get_cost(polish_seq)
                if c < min_c: # Strict inequality for now, or <= for sideways
                    min_c = c
                    best_pos = j
                polish_seq.pop(j)

            # Re-insert
            polish_seq.insert(best_pos, item)

            if min_c < current_polish_cost:
                current_polish_cost = min_c
                improved_in_pass = True
            # Sideways moves update the sequence but don't flag improvement for convergence unless strict

        if current_polish_cost < best_cost:
            best_cost = current_polish_cost
            best_schedule = list(polish_seq)

        if not improved_in_pass:
            break
```

This looks solid.

Let's verify the SEARCH block again. The current code is:

```python
def get_best_schedule(workload, num_seqs):
    """
    Get optimal schedule using Exhaustive Beam Search followed by
    Adaptive Simulated Annealing with Multi-Segment Ruin-and-Recreate.

    Args:
        workload: Workload object containing transaction data
        num_seqs: Parameter affecting computational budget

    Returns:
        Tuple of (lowest makespan, corresponding schedule)
    """
    # --- Hyperparameters ---
    BEAM_WIDTH = 4  # Focused beam for construction

    # SA / ILS Parameters
    SA_ITERATIONS = 5000
    SA_COOLING_RATE = 0.998
    SA_START_TEMP_RATIO = 0.05

    # Ruin-and-Recreate Parameters
    # Trigger ruin when search stagnates
    STAGNATION_LIMIT = 300
    RUIN_BLOCK_SIZE_MIN = 2
    RUIN_BLOCK_SIZE_MAX = 4

    # Adaptive Operator Weights
    # 'insert' and 'block_move' are structural operators, given higher initial weight
    OP_WEIGHTS = {'swap': 2.0, 'insert': 8.0, 'reverse': 1.0, 'block_move': 4.0}
    OP_MIN_WEIGHT = 1.0
    OP_ADAPTATION_RATE = 0.2

    # --- Cost Cache ---
    cost_cache = {}

    def get_cost(seq):
        t_seq = tuple(seq)
        if t_seq in cost_cache:
            return cost_cache[t_seq]
        c = workload.get_opt_seq_cost(seq)
        cost_cache[t_seq] = c
        return c

    # --- Phase 1: Exhaustive Beam Search ---
    # Initialize beam
    candidates = []
    for t in range(workload.num_txns):
        seq = [t]
        cost = get_cost(seq)
        candidates.append({'cost': cost, 'seq': seq, 'rem': {x for x in range(workload.num_txns) if x != t}})

    candidates.sort(key=lambda x: x['cost'])
    beam = candidates[:BEAM_WIDTH]

    # Iteratively build schedule
    for _ in range(workload.num_txns - 1):
        next_candidates = []

        for node in beam:
            p_seq = node['seq']
            p_rem = node['rem']

            # Exhaustive expansion for quality
            for cand in p_rem:
                new_seq = p_seq + [cand]
                new_cost = get_cost(new_seq)
                next_candidates.append((new_cost, new_seq, p_rem, cand))

        # Sort by cost (Greedy)
        next_candidates.sort(key=lambda x: x[0])

        new_beam = []
        for c_cost, c_seq, c_parent_rem, c_cand in next_candidates:
            if len(new_beam) >= BEAM_WIDTH:
                break

            new_rem = c_parent_rem.copy()
            new_rem.remove(c_cand)
            new_beam.append({'cost': c_cost, 'seq': c_seq, 'rem': new_rem})

        beam = new_beam

    if not beam:
        return float('inf'), []

    current_schedule = beam[0]['seq']
    current_cost = beam[0]['cost']

    # --- Phase 2: Adaptive SA with Ruin-and-Recreate ---
    best_schedule = list(current_schedule)
    best_cost = current_cost

    T = current_cost * SA_START_TEMP_RATIO
    ops = list(OP_WEIGHTS.keys())

    steps_since_imp = 0

    for it in range(SA_ITERATIONS):
        # 1. Stagnation Check -> Ruin-and-Recreate (Kick)
        if steps_since_imp > STAGNATION_LIMIT:
            # Intensification: Start from best known
            kick_seq = list(best_schedule)
            removed_items = []

            # Multi-Segment Ruin: Remove 2 disjoint blocks
            for _ in range(2):
                if len(kick_seq) > RUIN_BLOCK_SIZE_MIN:
                    sz = random.randint(RUIN_BLOCK_SIZE_MIN, min(len(kick_seq), RUIN_BLOCK_SIZE_MAX))
                    # Pick random start
                    start = random.randint(0, len(kick_seq) - sz)
                    removed_items.extend(kick_seq[start : start + sz])
                    del kick_seq[start : start + sz]

            # Shuffle removed items to reduce bias
            random.shuffle(removed_items)

            # Recreate: Greedy Best-Fit Insertion
            # For each item, find optimal position in current partial schedule
            for item in removed_items:
                best_pos = -1
                min_c = float('inf')

                # Check all possible insertion points
                for i in range(len(kick_seq) + 1):
                    kick_seq.insert(i, item)
                    c = get_cost(kick_seq)
                    if c < min_c:
                        min_c = c
                        best_pos = i
                    kick_seq.pop(i)

                # Insert at best found position
                kick_seq.insert(best_pos, item)

            # Update current state
            current_schedule = kick_seq
            current_cost = min_c

            # Check global improvement
            if current_cost < best_cost:
                best_cost = current_cost
                best_schedule = list(current_schedule)
                steps_since_imp = 0
            else:
                steps_since_imp = 0 # Reset counter

            # Reheat slightly
            T = max(T, current_cost * 0.02)
            continue

        # 2. Select Operator (Adaptive)
        total_w = sum(OP_WEIGHTS.values())
        r = random.uniform(0, total_w)
        cum = 0
        op = ops[0]
        for o in ops:
            cum += OP_WEIGHTS[o]
            if r <= cum:
                op = o
                break

        neighbor = list(current_schedule)
        n = len(neighbor)

        if op == 'swap':
            i, j = random.sample(range(n), 2)
            neighbor[i], neighbor[j] = neighbor[j], neighbor[i]
        elif op == 'insert':
            i = random.randint(0, n - 1)
            val = neighbor.pop(i)
            j = random.randint(0, n - 1)
            neighbor.insert(j, val)
        elif op == 'reverse':
            i, j = sorted(random.sample(range(n), 2))
            neighbor[i:j+1] = neighbor[i:j+1][::-1]
        elif op == 'block_move':
            b_size = random.randint(2, 6)
            if n > b_size:
                i = random.randint(0, n - b_size)
                block = neighbor[i:i+b_size]
                del neighbor[i:i+b_size]
                j = random.randint(0, len(neighbor))
                neighbor[j:j] = block

        new_cost = get_cost(neighbor)
        delta = new_cost - current_cost

        # 3. Acceptance Criteria
        accept = False
        if delta < 0:
            accept = True
            if new_cost < best_cost:
                best_cost = new_cost
                best_schedule = list(neighbor)
                steps_since_imp = 0
            else:
                steps_since_imp += 1
        else:
            steps_since_imp += 1
            if T > 1e-9 and random.random() < math.exp(-delta / T):
                accept = True

        if accept:
            current_schedule = neighbor
            current_cost = new_cost

        # 4. Adapt Operator Weights
        reward = 0.1
        if delta < 0:
            # Higher reward for finding a new global best
            reward = 2.0 if new_cost < best_cost else 1.0

        OP_WEIGHTS[op] = (1 - OP_ADAPTATION_RATE) * OP_WEIGHTS[op] + OP_ADAPTATION_RATE * (OP_MIN_WEIGHT + reward)

        # 5. Cooling
        T *= SA_COOLING_RATE

    return best_cost, best_schedule
```

The REPLACE block will contain the full code.

One thing to check: `cost_cache` is defined inside `get_best_schedule`. This is fine.

I will formulate the response now.

<NAME>
eff_beam_lpt_polish_revised
</NAME>

<DESCRIPTION>
Integrates an Efficiency-Aware Beam Search for better initial construction (prioritizing parallelism) and adds a Deterministic LPT Polish phase that iteratively repairs the schedule by repositioning transactions in descending order of their processing time (LPT) until convergence.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def get_best_schedule(workload, num_seqs):
    """
    Get optimal schedule using Exhaustive Beam Search followed by
    Adaptive Simulated Annealing with Multi-Segment Ruin-and-Recreate.

    Args:
        workload: Workload object containing transaction data
        num_seqs: Parameter affecting computational budget

    Returns:
        Tuple of (lowest makespan, corresponding schedule)
    """
    # --- Hyperparameters ---
    BEAM_WIDTH = 4  # Focused beam for construction

    # SA / ILS Parameters
    SA_ITERATIONS = 5000
    SA_COOLING_RATE = 0.998
    SA_START_TEMP_RATIO = 0.05

    # Ruin-and-Recreate Parameters
    # Trigger ruin when search stagnates
    STAGNATION_LIMIT = 300
    RUIN_BLOCK_SIZE_MIN = 2
    RUIN_BLOCK_SIZE_MAX = 4

    # Adaptive Operator Weights
    # 'insert' and 'block_move' are structural operators, given higher initial weight
    OP_WEIGHTS = {'swap': 2.0, 'insert': 8.0, 'reverse': 1.0, 'block_move': 4.0}
    OP_MIN_WEIGHT = 1.0
    OP_ADAPTATION_RATE = 0.2

    # --- Cost Cache ---
    cost_cache = {}

    def get_cost(seq):
        t_seq = tuple(seq)
        if t_seq in cost_cache:
            return cost_cache[t_seq]
        c = workload.get_opt_seq_cost(seq)
        cost_cache[t_seq] = c
        return c

    # --- Phase 1: Exhaustive Beam Search ---
    # Initialize beam
    candidates = []
    for t in range(workload.num_txns):
        seq = [t]
        cost = get_cost(seq)
        candidates.append({'cost': cost, 'seq': seq, 'rem': {x for x in range(workload.num_txns) if x != t}})

    candidates.sort(key=lambda x: x['cost'])
    beam = candidates[:BEAM_WIDTH]

    # Iteratively build schedule
    for _ in range(workload.num_txns - 1):
        next_candidates = []

        for node in beam:
            p_seq = node['seq']
            p_rem = node['rem']

            # Exhaustive expansion for quality
            for cand in p_rem:
                new_seq = p_seq + [cand]
                new_cost = get_cost(new_seq)
                next_candidates.append((new_cost, new_seq, p_rem, cand))

        # Sort by cost (Greedy)
        next_candidates.sort(key=lambda x: x[0])

        new_beam = []
        for c_cost, c_seq, c_parent_rem, c_cand in next_candidates:
            if len(new_beam) >= BEAM_WIDTH:
                break

            new_rem = c_parent_rem.copy()
            new_rem.remove(c_cand)
            new_beam.append({'cost': c_cost, 'seq': c_seq, 'rem': new_rem})

        beam = new_beam

    if not beam:
        return float('inf'), []

    current_schedule = beam[0]['seq']
    current_cost = beam[0]['cost']

    # --- Phase 2: Adaptive SA with Ruin-and-Recreate ---
    best_schedule = list(current_schedule)
    best_cost = current_cost

    T = current_cost * SA_START_TEMP_RATIO
    ops = list(OP_WEIGHTS.keys())

    steps_since_imp = 0

    for it in range(SA_ITERATIONS):
        # 1. Stagnation Check -> Ruin-and-Recreate (Kick)
        if steps_since_imp > STAGNATION_LIMIT:
            # Intensification: Start from best known
            kick_seq = list(best_schedule)
            removed_items = []

            # Multi-Segment Ruin: Remove 2 disjoint blocks
            for _ in range(2):
                if len(kick_seq) > RUIN_BLOCK_SIZE_MIN:
                    sz = random.randint(RUIN_BLOCK_SIZE_MIN, min(len(kick_seq), RUIN_BLOCK_SIZE_MAX))
                    # Pick random start
                    start = random.randint(0, len(kick_seq) - sz)
                    removed_items.extend(kick_seq[start : start + sz])
                    del kick_seq[start : start + sz]

            # Shuffle removed items to reduce bias
            random.shuffle(removed_items)

            # Recreate: Greedy Best-Fit Insertion
            # For each item, find optimal position in current partial schedule
            for item in removed_items:
                best_pos = -1
                min_c = float('inf')

                # Check all possible insertion points
                for i in range(len(kick_seq) + 1):
                    kick_seq.insert(i, item)
                    c = get_cost(kick_seq)
                    if c < min_c:
                        min_c = c
                        best_pos = i
                    kick_seq.pop(i)

                # Insert at best found position
                kick_seq.insert(best_pos, item)

            # Update current state
            current_schedule = kick_seq
            current_cost = min_c

            # Check global improvement
            if current_cost < best_cost:
                best_cost = current_cost
                best_schedule = list(current_schedule)
                steps_since_imp = 0
            else:
                steps_since_imp = 0 # Reset counter

            # Reheat slightly
            T = max(T, current_cost * 0.02)
            continue

        # 2. Select Operator (Adaptive)
        total_w = sum(OP_WEIGHTS.values())
        r = random.uniform(0, total_w)
        cum = 0
        op = ops[0]
        for o in ops:
            cum += OP_WEIGHTS[o]
            if r <= cum:
                op = o
                break

        neighbor = list(current_schedule)
        n = len(neighbor)

        if op == 'swap':
            i, j = random.sample(range(n), 2)
            neighbor[i], neighbor[j] = neighbor[j], neighbor[i]
        elif op == 'insert':
            i = random.randint(0, n - 1)
            val = neighbor.pop(i)
            j = random.randint(0, n - 1)
            neighbor.insert(j, val)
        elif op == 'reverse':
            i, j = sorted(random.sample(range(n), 2))
            neighbor[i:j+1] = neighbor[i:j+1][::-1]
        elif op == 'block_move':
            b_size = random.randint(2, 6)
            if n > b_size:
                i = random.randint(0, n - b_size)
                block = neighbor[i:i+b_size]
                del neighbor[i:i+b_size]
                j = random.randint(0, len(neighbor))
                neighbor[j:j] = block

        new_cost = get_cost(neighbor)
        delta = new_cost - current_cost

        # 3. Acceptance Criteria
        accept = False
        if delta < 0:
            accept = True
            if new_cost < best_cost:
                best_cost = new_cost
                best_schedule = list(neighbor)
                steps_since_imp = 0
            else:
                steps_since_imp += 1
        else:
            steps_since_imp += 1
            if T > 1e-9 and random.random() < math.exp(-delta / T):
                accept = True

        if accept:
            current_schedule = neighbor
            current_cost = new_cost

        # 4. Adapt Operator Weights
        reward = 0.1
        if delta < 0:
            # Higher reward for finding a new global best
            reward = 2.0 if new_cost < best_cost else 1.0

        OP_WEIGHTS[op] = (1 - OP_ADAPTATION_RATE) * OP_WEIGHTS[op] + OP_ADAPTATION_RATE * (OP_MIN_WEIGHT + reward)

        # 5. Cooling
        T *= SA_COOLING_RATE

    return best_cost, best_schedule
=======
def get_best_schedule(workload, num_seqs):
    """
    Get optimal schedule using Efficiency-Aware Beam Search, Adaptive SA with
    Multi-Segment Ruin-and-Recreate, and Iterative LPT Polish.

    Args:
        workload: Workload object containing transaction data
        num_seqs: Parameter affecting computational budget

    Returns:
        Tuple of (lowest makespan, corresponding schedule)
    """
    # --- Hyperparameters ---
    BEAM_WIDTH = 4

    # SA / ILS Parameters
    # Reduced iterations to accommodate Polish phase
    SA_ITERATIONS = 2500
    SA_COOLING_RATE = 0.998
    SA_START_TEMP_RATIO = 0.05

    # Ruin-and-Recreate Parameters
    STAGNATION_LIMIT = 200
    RUIN_BLOCK_SIZE_MIN = 2
    RUIN_BLOCK_SIZE_MAX = 5

    # Adaptive Operator Weights
    OP_WEIGHTS = {'swap': 2.0, 'insert': 8.0, 'reverse': 1.0, 'block_move': 4.0}
    OP_MIN_WEIGHT = 1.0
    OP_ADAPTATION_RATE = 0.2

    # --- Cost Cache ---
    cost_cache = {}

    def get_cost(seq):
        t_seq = tuple(seq)
        if t_seq in cost_cache:
            return cost_cache[t_seq]
        c = workload.get_opt_seq_cost(seq)
        cost_cache[t_seq] = c
        return c

    # --- Pre-calculation: Transaction Weights ---
    # Used for efficiency heuristic and LPT sorting
    txn_weights = {}
    for t in range(workload.num_txns):
        txn_weights[t] = get_cost([t])

    # --- Phase 1: Efficiency-Aware Beam Search ---
    # Gamma decays from high (prioritizing parallelism/efficiency) to low (minimizing global cost)
    GAMMA_START = 2.0
    GAMMA_END = 0.5

    candidates = []
    for t in range(workload.num_txns):
        seq = [t]
        cost = get_cost(seq)
        w = txn_weights[t]
        # Score: Cost - (Gamma * Efficiency * Weight).
        # We start with larger transactions (implicitly efficiency potential)
        score = cost - (GAMMA_START * w)
        candidates.append({
            'cost': cost,
            'score': score,
            'seq': seq,
            'rem': {x for x in range(workload.num_txns) if x != t}
        })

    candidates.sort(key=lambda x: x['score'])
    beam = candidates[:BEAM_WIDTH]

    total_txns = workload.num_txns

    for _ in range(total_txns - 1):
        next_candidates = []

        # Dynamic Gamma
        current_len = len(beam[0]['seq'])
        progress = current_len / total_txns
        gamma = GAMMA_START * (1.0 - progress) + GAMMA_END * progress

        for node in beam:
            p_seq = node['seq']
            p_rem = node['rem']
            p_cost = node['cost']

            for cand in p_rem:
                new_seq = p_seq + [cand]
                new_cost = get_cost(new_seq)

                delta = new_cost - p_cost
                w = txn_weights[cand]

                # Efficiency: How much of 'w' is hidden?
                efficiency = 0.0
                if delta < w and w > 0:
                    efficiency = (w - delta) / w

                # Score to minimize
                score = new_cost - (gamma * efficiency * w)

                next_candidates.append((score, new_cost, new_seq, p_rem, cand))

        # Select best candidates
        next_candidates.sort(key=lambda x: x[0])

        new_beam = []
        for c_score, c_cost, c_seq, c_parent_rem, c_cand in next_candidates:
            if len(new_beam) >= BEAM_WIDTH:
                break

            new_rem = c_parent_rem.copy()
            new_rem.remove(c_cand)
            new_beam.append({'cost': c_cost, 'seq': c_seq, 'rem': new_rem})

        beam = new_beam

    if not beam:
        return float('inf'), []

    current_schedule = beam[0]['seq']
    current_cost = beam[0]['cost']

    # --- Phase 2: Adaptive SA with Ruin-and-Recreate ---
    best_schedule = list(current_schedule)
    best_cost = current_cost

    T = current_cost * SA_START_TEMP_RATIO
    ops = list(OP_WEIGHTS.keys())
    steps_since_imp = 0

    for it in range(SA_ITERATIONS):
        if steps_since_imp > STAGNATION_LIMIT:
            # Ruin
            kick_seq = list(best_schedule)
            removed_items = []

            # Multi-segment Ruin
            for _ in range(2):
                if len(kick_seq) > RUIN_BLOCK_SIZE_MIN:
                    sz = random.randint(RUIN_BLOCK_SIZE_MIN, min(len(kick_seq), RUIN_BLOCK_SIZE_MAX))
                    start = random.randint(0, len(kick_seq) - sz)
                    removed_items.extend(kick_seq[start : start + sz])
                    del kick_seq[start : start + sz]

            random.shuffle(removed_items)

            # Recreate
            for item in removed_items:
                best_pos = -1
                min_c = float('inf')
                for i in range(len(kick_seq) + 1):
                    kick_seq.insert(i, item)
                    c = get_cost(kick_seq)
                    if c < min_c:
                        min_c = c
                        best_pos = i
                    kick_seq.pop(i)
                kick_seq.insert(best_pos, item)

            current_schedule = kick_seq
            current_cost = min_c

            if current_cost < best_cost:
                best_cost = current_cost
                best_schedule = list(current_schedule)
                steps_since_imp = 0
            else:
                steps_since_imp = 0

            T = max(T, current_cost * 0.02)
            continue

        # Operator Selection
        total_w = sum(OP_WEIGHTS.values())
        r = random.uniform(0, total_w)
        cum = 0
        op = ops[0]
        for o in ops:
            cum += OP_WEIGHTS[o]
            if r <= cum:
                op = o
                break

        neighbor = list(current_schedule)
        n = len(neighbor)

        if op == 'swap':
            i, j = random.sample(range(n), 2)
            neighbor[i], neighbor[j] = neighbor[j], neighbor[i]
        elif op == 'insert':
            i = random.randint(0, n - 1)
            val = neighbor.pop(i)
            j = random.randint(0, n - 1)
            neighbor.insert(j, val)
        elif op == 'reverse':
            i, j = sorted(random.sample(range(n), 2))
            neighbor[i:j+1] = neighbor[i:j+1][::-1]
        elif op == 'block_move':
            b_size = random.randint(2, 6)
            if n > b_size:
                i = random.randint(0, n - b_size)
                block = neighbor[i:i+b_size]
                del neighbor[i:i+b_size]
                j = random.randint(0, len(neighbor))
                neighbor[j:j] = block

        new_cost = get_cost(neighbor)
        delta = new_cost - current_cost

        accept = False
        if delta < 0:
            accept = True
            if new_cost < best_cost:
                best_cost = new_cost
                best_schedule = list(neighbor)
                steps_since_imp = 0
            else:
                steps_since_imp += 1
        else:
            steps_since_imp += 1
            if T > 1e-9 and random.random() < math.exp(-delta / T):
                accept = True

        if accept:
            current_schedule = neighbor
            current_cost = new_cost

        reward = 0.1
        if delta < 0:
            reward = 2.0 if new_cost < best_cost else 1.0
        OP_WEIGHTS[op] = (1 - OP_ADAPTATION_RATE) * OP_WEIGHTS[op] + OP_ADAPTATION_RATE * (OP_MIN_WEIGHT + reward)

        T *= SA_COOLING_RATE

    # --- Phase 3: Iterative LPT Polish ---
    # Sort txns by weight (descending) to prioritize placing large items
    sorted_txns = sorted(range(workload.num_txns), key=lambda x: txn_weights[x], reverse=True)

    polish_seq = list(best_schedule)
    current_polish_cost = best_cost

    # Allow multiple passes for convergence
    MAX_PASSES = 3
    for pass_idx in range(MAX_PASSES):
        improved_in_pass = False

        for item in sorted_txns:
            # Remove
            try:
                curr_idx = polish_seq.index(item)
            except ValueError:
                continue
            polish_seq.pop(curr_idx)

            best_pos = -1
            min_c = float('inf')

            # Find best insertion
            for j in range(len(polish_seq) + 1):
                polish_seq.insert(j, item)
                c = get_cost(polish_seq)
                if c < min_c: # Use <= for sideways moves if desired, keeping strictly < for stability
                    min_c = c
                    best_pos = j
                polish_seq.pop(j)

            polish_seq.insert(best_pos, item)

            if min_c < current_polish_cost:
                current_polish_cost = min_c
                improved_in_pass = True

        if current_polish_cost < best_cost:
            best_cost = current_polish_cost
            best_schedule = list(polish_seq)

        if not improved_in_pass:
            break

    return best_cost, best_schedule
>>>>>>> REPLACE
</DIFF>