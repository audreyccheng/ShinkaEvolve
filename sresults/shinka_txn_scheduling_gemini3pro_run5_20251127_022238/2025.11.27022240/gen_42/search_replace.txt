<NAME>
density_aware_beam_and_scatter_ruin
</NAME>

<DESCRIPTION>
1.  **Density-Aware Beam Search**: Introduced a scoring function `cost - (0.001 * weight)` for the Beam Search. This heuristic breaks ties (or near-ties) in makespan cost by preferring schedules that pack "heavier" (longer duration) transactions, thereby increasing work density and potentially leaving smaller gaps for later.
2.  **Multi-Mode Ruin**: Replaced the single-mode Block Ruin with a Multi-Mode Ruin strategy that randomly selects between "Block Ruin" (removing a contiguous sequence) and "Scatter Ruin" (removing random dependencies globally). This helps the algorithm escape local optima caused by both clustering and ordering constraints.
3.  **Faster Stagnation Trigger**: Lowered the stagnation limit from 500 to 150 iterations to trigger the intensification (Ruin-and-Recreate) phase more frequently, improving exploration efficiency within the fixed iteration budget.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # --- Cost Cache ---
    cost_cache = {}

    def get_cost(seq):
        t_seq = tuple(seq)
        if t_seq in cost_cache:
            return cost_cache[t_seq]
        c = workload.get_opt_seq_cost(seq)
        cost_cache[t_seq] = c
        return c

    # --- Phase 1: Exhaustive Beam Search Construction ---
    # Initialize beam with all possible start transactions
    candidates = []
    for t in range(workload.num_txns):
        seq = [t]
        cost = get_cost(seq)
        candidates.append((cost, seq, [x for x in range(workload.num_txns) if x != t]))

    candidates.sort(key=lambda x: x[0])
    beam = candidates[:BEAM_WIDTH]

    # Iteratively build schedule
    for _ in range(workload.num_txns - 1):
        next_candidates = []

        for b_cost, b_seq, b_rem in beam:
            # Exhaustive expansion: check ALL remaining transactions.
            # This is expensive (O(N^2 * Width)) but critical for high quality structure.
            for cand in b_rem:
                new_seq = b_seq + [cand]
                # Optimistically we might want to prune here, but strict cost
                # evaluation is safer for dependency heavy workloads.
                new_cost = get_cost(new_seq)
                next_candidates.append((new_cost, new_seq, b_rem, cand))

        # Select best global candidates
        next_candidates.sort(key=lambda x: x[0])

        new_beam = []
        for c_cost, c_seq, c_parent_rem, c_cand in next_candidates:
            if len(new_beam) >= BEAM_WIDTH:
                break

            new_rem = list(c_parent_rem)
            new_rem.remove(c_cand)
            new_beam.append((c_cost, c_seq, new_rem))

        beam = new_beam

    if not beam:
        return float('inf'), []

    current_schedule = beam[0][1]
    current_cost = beam[0][0]

    # --- Phase 2: Adaptive SA with Ruin-and-Recreate ---
    best_schedule = list(current_schedule)
    best_cost = current_cost

    T = current_cost * SA_START_TEMP_RATIO
    ops = list(OP_WEIGHTS.keys())

    steps_since_improvement = 0

    for it in range(SA_ITERATIONS):
        # 1. Stagnation Check -> Ruin-and-Recreate (Kick)
        if steps_since_improvement > STAGNATION_LIMIT:
            # Ruin: Remove random block
            kick_seq = list(best_schedule) # Kick from best found (Intensification around best)
            n = len(kick_seq)

            # Select block
            b_size = random.randint(KICK_BLOCK_SIZE_MIN, KICK_BLOCK_SIZE_MAX)
            if n > b_size:
                start = random.randint(0, n - b_size)
                removed_block = kick_seq[start : start + b_size]
                del kick_seq[start : start + b_size]

                # Recreate: Greedy Best-Fit Insertion
                # For each removed item, find optimal position in current partial schedule
                for item in removed_block:
                    best_pos = -1
                    min_c = float('inf')

                    # Try all positions
                    for i in range(len(kick_seq) + 1):
                        kick_seq.insert(i, item)
                        c = get_cost(kick_seq)
                        if c < min_c:
                            min_c = c
                            best_pos = i
                        kick_seq.pop(i)

                    # Insert at best position
                    kick_seq.insert(best_pos, item)

                current_schedule = kick_seq
                current_cost = min_c # min_c is cost of last insertion

                # Check global best
                if current_cost < best_cost:
                    best_cost = current_cost
                    best_schedule = list(current_schedule)
                    steps_since_improvement = 0
                else:
                    steps_since_improvement = 0 # Reset stagnation counter anyway

                # Reset Temperature to allow settling from new state
                T = current_cost * 0.02
                continue
=======
    # --- Cost Cache ---
    cost_cache = {}

    def get_cost(seq):
        t_seq = tuple(seq)
        if t_seq in cost_cache:
            return cost_cache[t_seq]
        c = workload.get_opt_seq_cost(seq)
        cost_cache[t_seq] = c
        return c

    # --- Pre-calculation: Transaction Weights ---
    # Used for tie-breaking in Beam Search (prefer packing heavier txns for free)
    txn_weights = {}
    for t in range(workload.num_txns):
        txn_weights[t] = get_cost([t])

    # --- Phase 1: Density-Aware Beam Search ---
    # Initialize beam
    candidates = []
    for t in range(workload.num_txns):
        seq = [t]
        cost = get_cost(seq)
        # Score: minimize cost, then maximize weight (subtracted).
        # Factor 0.001 ensures it only breaks ties or near-ties.
        score = cost - (0.001 * txn_weights[t])
        candidates.append({
            'cost': cost,
            'score': score,
            'seq': seq,
            'rem': [x for x in range(workload.num_txns) if x != t]
        })

    candidates.sort(key=lambda x: x['score'])
    beam = candidates[:BEAM_WIDTH]

    # Iteratively build schedule
    for _ in range(workload.num_txns - 1):
        next_candidates = []

        for node in beam:
            b_seq = node['seq']
            b_rem = node['rem']

            for cand in b_rem:
                new_seq = b_seq + [cand]
                new_cost = get_cost(new_seq)

                # Density-Aware Scoring
                # If new_cost is same as old, we packed 'cand' for free.
                # We prefer packing larger 'cand'.
                score = new_cost - (0.001 * txn_weights[cand])

                next_candidates.append((score, new_cost, new_seq, b_rem, cand))

        # Select best global candidates by score
        next_candidates.sort(key=lambda x: x[0])

        new_beam = []
        for c_score, c_cost, c_seq, c_parent_rem, c_cand in next_candidates:
            if len(new_beam) >= BEAM_WIDTH:
                break

            new_rem = list(c_parent_rem)
            new_rem.remove(c_cand)
            new_beam.append({'cost': c_cost, 'seq': c_seq, 'rem': new_rem})

        beam = new_beam

    if not beam:
        return float('inf'), []

    current_schedule = beam[0]['seq']
    current_cost = beam[0]['cost']

    # --- Phase 2: Adaptive SA with Multi-Mode Ruin-and-Recreate ---
    best_schedule = list(current_schedule)
    best_cost = current_cost

    T = current_cost * SA_START_TEMP_RATIO
    ops = list(OP_WEIGHTS.keys())

    steps_since_improvement = 0

    for it in range(SA_ITERATIONS):
        # 1. Stagnation Check -> Ruin-and-Recreate (Kick)
        # Lower threshold to trigger intensification more often
        if steps_since_improvement > 150:
            # Multi-Mode Ruin
            kick_seq = list(best_schedule)
            n = len(kick_seq)
            removed_items = []

            mode = random.random()
            if mode < 0.5:
                # Mode A: Block Ruin (removes contiguity)
                if n > KICK_BLOCK_SIZE_MIN:
                    b_size = random.randint(KICK_BLOCK_SIZE_MIN, KICK_BLOCK_SIZE_MAX)
                    start = random.randint(0, n - b_size)
                    removed_items = kick_seq[start : start + b_size]
                    del kick_seq[start : start + b_size]
            else:
                # Mode B: Scatter Ruin (removes dependencies globally)
                # Pick K random indices to remove
                if n > KICK_BLOCK_SIZE_MIN:
                    num_to_remove = random.randint(KICK_BLOCK_SIZE_MIN, KICK_BLOCK_SIZE_MAX)
                    # Sample indices
                    indices = sorted(random.sample(range(n), num_to_remove), reverse=True)
                    for idx in indices:
                        removed_items.append(kick_seq.pop(idx))

            # Recreate: Greedy Best-Fit Insertion
            # Shuffle removed items to avoid insertion bias
            random.shuffle(removed_items)

            for item in removed_items:
                best_pos = -1
                min_c = float('inf')

                # Check all valid positions
                for i in range(len(kick_seq) + 1):
                    kick_seq.insert(i, item)
                    c = get_cost(kick_seq)
                    if c < min_c:
                        min_c = c
                        best_pos = i
                    kick_seq.pop(i)

                kick_seq.insert(best_pos, item)

            current_schedule = kick_seq
            current_cost = min_c

            # Check global best
            if current_cost < best_cost:
                best_cost = current_cost
                best_schedule = list(current_schedule)
                steps_since_improvement = 0
            else:
                # Reset anyway to explore the new basin
                steps_since_improvement = 0

            # Reheat Temperature
            T = max(T, current_cost * 0.05)
            continue
>>>>>>> REPLACE
</DIFF>