<NAME>
hybrid_scheduling_adaptive_gamma
</NAME>

<DESCRIPTION>
I am updating the scheduling algorithm to a hybrid approach that combines an improved Beam Search with an Iterated Local Search (ILS) refinement phase.

1.  **Adaptive Gamma & Zero-Cost Bonus:** In the Beam Search construction, I'm implementing an adaptive `gamma` that decays from 1.5 to 1.0 as the schedule fills up. This favors packing "heavy" work early (to maximize parallelism) while shifting to "best-fit" logic later to minimize tail latency. I also added a significant score bonus for transactions that can be added with zero cost increase (perfect parallelism).
2.  **Multi-Segment Ruin & Recreate:** I am re-introducing the ILS phase which was removed in the current version. Specifically, I'm using a multi-segment ruin operator that removes two disjoint blocks of transactions instead of one. This helps shuffle dependencies better than single-block moves.
3.  **Refinement:** After the ruin phase, a greedy best-fit reconstruction repairs the schedule, followed by a short simulated annealing burst to polish local optima.

This addresses the regression in score (3.50 vs 3.98) by restoring and enhancing the refinement strategies found in top-performing prior generations.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
import time
import random
import sys
import os
=======
import time
import random
import math
import sys
import os
>>>>>>> REPLACE
<<<<<<< SEARCH
def get_best_schedule(workload, num_seqs):
    """
    Get optimal schedule using Beam Search with Work-Density scoring.

    The algorithm uses a beam search where candidates are evaluated not just by
    total makespan, but by a density metric (Makespan - Gamma * Work_Done).
    This encourages the scheduler to tackle 'expensive' (long) transactions early,
    avoiding the "Shortest Processing Time" bias of pure greedy makespan minimization.

    Args:
        workload: Workload object containing transaction data
        num_seqs: Hint for computational budget (used to scale beam width)

    Returns:
        Tuple of (lowest makespan, corresponding schedule)
    """
    num_txns = workload.num_txns

    # 1. Metric Precomputation
    # Get raw costs (lengths) for density calculation
    raw_costs = {}
    for i in range(num_txns):
        try:
            # txns[i] -> (info_tuple, ops...)
            # info_tuple -> (id, type, ?, cost)
            raw_costs[i] = workload.txns[i][0][3]
        except (IndexError, TypeError, AttributeError):
            raw_costs[i] = 1.0

    # Sorted indices for LPT (Longest Processing Time) heuristics
    lpt_indices = sorted(raw_costs.keys(), key=lambda k: raw_costs[k], reverse=True)

    # 2. Search Parameters
    # Beam Width: Number of active schedules to track
    # Scaled slightly based on input hint, but pinned higher for quality
    BEAM_WIDTH = max(16, int(num_seqs * 2))

    # Expansion Params
    HEURISTIC_COUNT = 10  # Always check top 10 longest remaining
    RANDOM_COUNT = 6      # Check 6 random others

    # Scoring Param: Higher gamma prefers packing work early
    GAMMA = 1.3

    # 3. Initialization
    beam = []

    # Seed with diverse starts: Top LPTs + Randoms
    seeds = set(lpt_indices[:BEAM_WIDTH])
    seeds.update(random.sample(range(num_txns), min(num_txns, BEAM_WIDTH)))

    for t in seeds:
        seq = [t]
        cost = workload.get_opt_seq_cost(seq)
        raw_work = raw_costs[t]

        # Priority: Lower is better
        # We subtract work done to prioritize "heavy" partial schedules
        score = cost - (GAMMA * raw_work)

        rem = set(range(num_txns))
        rem.remove(t)

        beam.append({
            'seq': seq,
            'cost': cost,
            'score': score,
            'raw_work': raw_work,
            'rem': rem
        })

    # Initial prune
    beam.sort(key=lambda x: x['score'])
    beam = beam[:BEAM_WIDTH]

    # 4. Beam Search Loop
    for _ in range(num_txns - 1):
        candidates = []

        for item in beam:
            curr_rem = item['rem']
            if not curr_rem:
                continue

            # Selection Strategy
            to_try = set()

            # A) Heuristic LPT
            added = 0
            for t in lpt_indices:
                if t in curr_rem:
                    to_try.add(t)
                    added += 1
                    if added >= HEURISTIC_COUNT:
                        break

            # B) Random Sampling
            needed = RANDOM_COUNT
            curr_rem_list = list(curr_rem)

            if len(curr_rem_list) <= (len(to_try) + needed):
                 to_try.update(curr_rem_list)
            else:
                 # Efficiently sample without converting set to list again if possible
                 # Just sample from list and check existence
                 samples = random.sample(curr_rem_list, min(len(curr_rem_list), needed * 2))
                 for s in samples:
                     if len(to_try) >= (HEURISTIC_COUNT + needed):
                         break
                     to_try.add(s)

            # Evaluation
            base_seq = item['seq']
            base_work = item['raw_work']

            for t in to_try:
                new_seq = base_seq + [t]

                # Simulation Cost (Makespan)
                new_cost = workload.get_opt_seq_cost(new_seq)

                new_work = base_work + raw_costs[t]
                new_score = new_cost - (GAMMA * new_work)

                new_rem = curr_rem.copy()
                new_rem.remove(t)

                candidates.append({
                    'seq': new_seq,
                    'cost': new_cost, # True objective
                    'score': new_score, # Guidance objective
                    'raw_work': new_work,
                    'rem': new_rem
                })

        if not candidates:
            break

        # Pruning with Diversity
        # Sort by guided score
        candidates.sort(key=lambda x: x['score'])

        # Keep best K deterministically
        k_best = int(BEAM_WIDTH * 0.6)
        next_beam = candidates[:k_best]

        # Fill rest from a larger pool to maintain diversity
        remaining_slots = BEAM_WIDTH - len(next_beam)
        if remaining_slots > 0:
            pool = candidates[k_best : k_best * 5] # Look deeper
            if len(pool) <= remaining_slots:
                next_beam.extend(pool)
            else:
                next_beam.extend(random.sample(pool, remaining_slots))

        beam = next_beam

    # 5. Final Selection
    # Sort by actual Makespan (cost), not the guided score
    beam.sort(key=lambda x: x['cost'])
    best_result = beam[0]

    return best_result['cost'], best_result['seq']
=======
def get_best_schedule(workload, num_seqs):
    """
    Hybrid Scheduling Algorithm: Adaptive Beam Search + Multi-Segment Ruin & Recreate.

    1. Constructive Phase:
       - Beam Search with Adaptive Gamma (1.5 -> 1.0) based on progress.
       - "Zero-Cost Bonus" for perfect parallelism candidates.

    2. Refinement Phase (ILS):
       - Multi-segment Ruin: Removes disjoint blocks to allow shuffling dependencies.
       - Recreate: Greedy Best-Fit insertion.
       - Local Search: Simulated Annealing for local polish.
    """
    num_txns = workload.num_txns

    # --- Precomputation ---
    txn_lengths = {}
    for i in range(num_txns):
        try:
            txn_lengths[i] = workload.txns[i][0][3]
        except (IndexError, TypeError, AttributeError):
            txn_lengths[i] = 1.0

    lpt_indices = sorted(txn_lengths.keys(), key=lambda k: txn_lengths[k], reverse=True)

    # --- 1. Constructive Phase: Adaptive Beam Search ---
    BEAM_WIDTH = max(8, int(num_seqs * 1.5))

    # Initial seeds
    seeds = set(lpt_indices[:BEAM_WIDTH])
    if len(seeds) < BEAM_WIDTH:
        rem = BEAM_WIDTH - len(seeds)
        pool = [x for x in range(num_txns) if x not in seeds]
        seeds.update(random.sample(pool, min(len(pool), rem)))

    beam = []
    # Initial Gamma
    gamma = 1.5

    for t in seeds:
        seq = [t]
        cost = workload.get_opt_seq_cost(seq)
        work = txn_lengths[t]
        score = cost - (gamma * work)
        rem = set(range(num_txns))
        rem.remove(t)
        beam.append({'seq': seq, 'cost': cost, 'score': score, 'work': work, 'rem': rem})

    beam.sort(key=lambda x: x['score'])
    beam = beam[:BEAM_WIDTH]

    for step in range(num_txns - 1):
        candidates = []

        # Adaptive Gamma Decay: 1.5 -> 1.0
        # step goes from 0 to num_txns-2. Progress roughly 0 to 1.
        progress = step / max(1, num_txns - 1)
        gamma = 1.5 - (0.5 * progress)

        for parent in beam:
            if not parent['rem']: continue

            # Smart Candidate Selection
            to_eval = set()
            rem_list = list(parent['rem'])

            # Top LPT
            added = 0
            for t in lpt_indices:
                if t in parent['rem']:
                    to_eval.add(t)
                    added += 1
                    if added >= 5: break

            # Random
            needed = 4
            if len(rem_list) > len(to_eval):
                pool = [x for x in rem_list if x not in to_eval]
                to_eval.update(random.sample(pool, min(len(pool), needed)))

            base_cost = parent['cost']

            for t in to_eval:
                new_seq = parent['seq'] + [t]
                new_cost = workload.get_opt_seq_cost(new_seq)
                new_work = parent['work'] + txn_lengths[t]

                # Base score
                new_score = new_cost - (gamma * new_work)

                # Zero-Cost Bonus: If cost doesn't increase, reward heavily
                if new_cost <= base_cost + 0.0001:
                    new_score -= (txn_lengths[t] * 3.0)

                new_rem = parent['rem'].copy()
                new_rem.remove(t)

                candidates.append({
                    'seq': new_seq,
                    'cost': new_cost,
                    'score': new_score,
                    'work': new_work,
                    'rem': new_rem
                })

        if not candidates: break

        candidates.sort(key=lambda x: x['score'])
        beam = candidates[:BEAM_WIDTH]

    beam.sort(key=lambda x: x['cost'])
    best_candidate = beam[0]

    current_schedule = best_candidate['seq']
    current_cost = best_candidate['cost']

    # --- 2. Refinement Phase: Multi-Segment Ruin & Recreate ---

    best_schedule = list(current_schedule)
    best_cost = current_cost

    # Only run refinement if workload is large enough
    if num_txns > 5:
        ILS_CYCLES = 4
        SA_STEPS = 50

        for cycle in range(ILS_CYCLES):
            work_seq = list(current_schedule)

            # Multi-Segment Ruin: Remove 2 separate small blocks
            removed_txns = []

            # Block 1
            if len(work_seq) > 2:
                bs1 = random.randint(1, max(2, int(num_txns * 0.1)))
                idx1 = random.randint(0, len(work_seq) - bs1)
                removed_txns.extend(work_seq[idx1 : idx1 + bs1])
                del work_seq[idx1 : idx1 + bs1]

            # Block 2
            if len(work_seq) > 2:
                bs2 = random.randint(1, max(2, int(num_txns * 0.1)))
                idx2 = random.randint(0, len(work_seq) - bs2)
                removed_txns.extend(work_seq[idx2 : idx2 + bs2])
                del work_seq[idx2 : idx2 + bs2]

            # Shuffle for re-insertion
            random.shuffle(removed_txns)

            # Recreate: Best-Fit
            for txn in removed_txns:
                best_pos = -1
                best_incr = float('inf')

                for pos in range(len(work_seq) + 1):
                    work_seq.insert(pos, txn)
                    c = workload.get_opt_seq_cost(work_seq)
                    if c < best_incr:
                        best_incr = c
                        best_pos = pos
                    del work_seq[pos]

                work_seq.insert(best_pos, txn)

            current_schedule = work_seq
            current_cost = best_incr

            if current_cost < best_cost:
                best_cost = current_cost
                best_schedule = list(current_schedule)
            else:
                # Restart chance from global best
                if random.random() < 0.2:
                     current_schedule = list(best_schedule)
                     current_cost = best_cost

            # Local Search (SA)
            temp = max(0.5, current_cost * 0.01)
            for _ in range(SA_STEPS):
                neighbor = list(current_schedule)
                op = random.random()
                if op < 0.5: # Swap
                    i, j = random.sample(range(len(neighbor)), 2)
                    neighbor[i], neighbor[j] = neighbor[j], neighbor[i]
                else: # Insert
                    i = random.randint(0, len(neighbor)-1)
                    val = neighbor.pop(i)
                    j = random.randint(0, len(neighbor))
                    neighbor.insert(j, val)

                n_cost = workload.get_opt_seq_cost(neighbor)
                delta = n_cost - current_cost

                if delta < 0 or (temp > 1e-4 and random.random() < math.exp(-delta/temp)):
                    current_schedule = neighbor
                    current_cost = n_cost
                    if current_cost < best_cost:
                        best_cost = current_cost
                        best_schedule = list(current_schedule)
                temp *= 0.9

    return best_cost, best_schedule
>>>>>>> REPLACE
</DIFF>