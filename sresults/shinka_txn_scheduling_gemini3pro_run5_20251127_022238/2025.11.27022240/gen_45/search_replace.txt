<NAME>
soft_efficiency_bonus_and_deterministic_ils
</NAME>

<DESCRIPTION>
1. Constructive Phase: Implements a "Soft Efficiency Bonus" in the Beam Search. Instead of a hard threshold for zero-cost insertions, it calculates an efficiency score `(len - delta_cost)/len` and rewards candidates with high efficiency (>40%) using a quadratic curve. This captures parallel moves that are "good enough" but not perfect. It also adjusts the Gamma decay to start higher (1.6) for stronger initial parallelism.

2. Refinement Phase: Replaces the stochastic Variable Neighborhood Descent (VND) with a Deterministic Iterative Improvement strategy. Instead of testing random swaps/shifts, it systematically attempts to move transactions to their optimal position within the schedule (Systematic Best-Insertion). To manage computational cost, it uses a strided scan for large schedules and limits total evaluations. This ensures that when we try to fix a transaction, we find its best possible slot rather than hoping a random move lands there.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Construction Loop
    for step in range(num_txns - 1):
        # Adaptive Gamma Decay: 1.5 -> 1.0
        # Slowly reduce bias for heavy items, increase importance of fit
        progress = (step + 1) / num_txns
        current_gamma = 1.5 - (0.5 * progress)

        candidates = []
        for parent in beam:
            rem = list(parent['rem'])
            if not rem: continue

            # Smart Candidate Selection
            to_eval = set()

            # 1. Top LPT candidates
            added_lpt = 0
            for t in lpt_indices:
                if t in parent['rem']:
                    to_eval.add(t)
                    added_lpt += 1
                    if added_lpt >= 5: break

            # 2. Random candidates for diversity
            if len(rem) > len(to_eval):
                pool = [x for x in rem if x not in to_eval]
                count = min(len(pool), 5)
                to_eval.update(random.sample(pool, count))

            base_cost = parent['cost']

            for t in to_eval:
                new_seq = parent['seq'] + [t]
                new_cost = workload.get_opt_seq_cost(new_seq)
                new_work = parent['work'] + txn_lengths[t]

                # Base Score: Cost - Gamma * Work
                new_score = new_cost - (current_gamma * new_work)

                # Zero-Cost Bonus (Gap Filling/Parallelism)
                # If adding the txn doesn't increase cost (or increases very little),
                # reward it heavily to prioritize this perfect fit.
                if new_cost <= base_cost + 0.0001:
                    new_score -= (txn_lengths[t] * 2.5)

                new_rem = parent['rem'].copy()
                new_rem.remove(t)

                candidates.append({
                    'seq': new_seq,
                    'cost': new_cost,
                    'work': new_work,
                    'score': new_score,
                    'rem': new_rem
                })
=======
    # Construction Loop
    for step in range(num_txns - 1):
        # Adaptive Gamma Decay: 1.6 -> 1.0
        # Start higher to force parallelism early, decay to packing
        progress = (step + 1) / num_txns
        current_gamma = 1.6 - (0.6 * progress)

        candidates = []
        for parent in beam:
            rem = list(parent['rem'])
            if not rem: continue

            # Smart Candidate Selection
            to_eval = set()

            # 1. Top LPT candidates
            added_lpt = 0
            for t in lpt_indices:
                if t in parent['rem']:
                    to_eval.add(t)
                    added_lpt += 1
                    if added_lpt >= 5: break

            # 2. Random candidates for diversity
            if len(rem) > len(to_eval):
                pool = [x for x in rem if x not in to_eval]
                count = min(len(pool), 5)
                to_eval.update(random.sample(pool, count))

            base_cost = parent['cost']

            for t in to_eval:
                new_seq = parent['seq'] + [t]
                new_cost = workload.get_opt_seq_cost(new_seq)
                new_work = parent['work'] + txn_lengths[t]

                # Base Score
                new_score = new_cost - (current_gamma * new_work)

                # "Soft" Efficiency Bonus
                # Reward high parallelism efficiency even if not perfect zero-cost.
                delta_cost = new_cost - base_cost
                t_len = txn_lengths[t]

                if t_len > 1e-6:
                    # Efficiency = 1.0 means perfectly parallel (delta_cost = 0)
                    # Efficiency = 0.0 means sequential (delta_cost = t_len)
                    efficiency = (t_len - delta_cost) / t_len

                    if efficiency > 0.4: # Reward anything better than 60% overlap
                         # Bonus scales quadratically with efficiency to prefer near-perfect fits
                        bonus = t_len * 3.0 * (efficiency ** 2)
                        new_score -= bonus

                new_rem = parent['rem'].copy()
                new_rem.remove(t)

                candidates.append({
                    'seq': new_seq,
                    'cost': new_cost,
                    'work': new_work,
                    'score': new_score,
                    'rem': new_rem
                })
>>>>>>> REPLACE
<<<<<<< SEARCH
        # C. Variable Neighborhood Descent (VND)
        # Cycle through neighborhoods. If improvement found, restart cycle.
        # This replaces generic SA with structured moves.

        improved = True
        vnd_evals = 0
        MAX_VND_EVALS = 400 # Budget per ILS cycle

        while improved and vnd_evals < MAX_VND_EVALS:
            improved = False

            for nh in neighborhoods:
                # Try a batch of random moves in this neighborhood
                # This approximates exploring the neighborhood
                batch_size = 30

                best_neighbor = None
                best_neighbor_cost = current_cost
                found_better_in_batch = False

                for _ in range(batch_size):
                    cand = list(current_schedule)

                    if nh == 'shift':
                        i = random.randint(0, len(cand)-1)
                        val = cand.pop(i)
                        j = random.randint(0, len(cand))
                        cand.insert(j, val)
                    elif nh == 'swap':
                        i, j = random.sample(range(len(cand)), 2)
                        cand[i], cand[j] = cand[j], cand[i]
                    elif nh == 'reverse':
                        if len(cand) < 4: continue
                        bs = random.randint(3, 8)
                        si = random.randint(0, len(cand) - bs)
                        cand[si:si+bs] = reversed(cand[si:si+bs])

                    c = workload.get_opt_seq_cost(cand)
                    vnd_evals += 1

                    if c < best_neighbor_cost:
                        best_neighbor_cost = c
                        best_neighbor = cand
                        found_better_in_batch = True

                # If the batch produced a better solution than current, accept it
                if found_better_in_batch:
                    current_schedule = best_neighbor
                    current_cost = best_neighbor_cost
                    if current_cost < best_cost:
                        best_cost = current_cost
                        best_schedule = list(current_schedule)
                    improved = True
                    # Restart neighborhood cycle on improvement (VND logic)
                    break

                if vnd_evals >= MAX_VND_EVALS: break
=======
        # C. Deterministic Iterative Improvement (Systematic Best-Insertion)
        # Instead of random moves, iterate through transactions and greedily
        # move them to their optimal position in the schedule.
        # This is computationally heavier per pass but much more effective.

        improved_pass = True
        evals_count = 0
        MAX_EVALS = 450 # Budget for local search per ILS cycle

        # Limit scanning if schedule is very long to avoid O(N^2) explosion
        seq_len = len(current_schedule)
        stride = 1
        if seq_len > 60: stride = 3 # Sample positions for long schedules

        while improved_pass and evals_count < MAX_EVALS:
            improved_pass = False

            # Create processing order (randomized to avoid bias)
            indices = list(range(seq_len))
            random.shuffle(indices)

            for i in indices:
                # Current transaction to optimize
                txn = current_schedule[i]

                # Remove txn
                temp_seq = list(current_schedule)
                del temp_seq[i]

                best_pos = -1
                best_val = float('inf')

                # Check valid insertion points
                # Use strided scan to save budget, but always check boundaries
                check_positions = list(range(0, len(temp_seq) + 1, stride))
                if len(temp_seq) not in check_positions:
                    check_positions.append(len(temp_seq))

                found_better = False

                for pos in check_positions:
                    temp_seq.insert(pos, txn)
                    c = workload.get_opt_seq_cost(temp_seq)
                    evals_count += 1

                    if c < best_val:
                        best_val = c
                        best_pos = pos

                    del temp_seq[pos]

                    if evals_count >= MAX_EVALS: break

                if best_val < current_cost:
                    # Found an improvement
                    temp_seq.insert(best_pos, txn)
                    current_schedule = temp_seq
                    current_cost = best_val
                    improved_pass = True
                    if current_cost < best_cost:
                        best_cost = current_cost
                        best_schedule = list(current_schedule)

                if evals_count >= MAX_EVALS: break
>>>>>>> REPLACE

</DIFF>