<NAME>
randomized_restarts_packing
</NAME>

<DESCRIPTION>
I am modifying `balanced_packing` to use Vectorized Randomized Restarts. The previous implementation utilized a single deterministic LPT pass followed by refinement. The new implementation runs 64 parallel greedy packing instances (1 deterministic + 63 with perturbed sorting weights) to explore a wider solution space. It selects the best initialization (minimum load imbalance) and then proceeds with the existing powerful "Mini-Greedy Pairwise Partitioning" refinement on the best candidate. This approach aims to break out of local optima and achieve better balancedness scores while leveraging the available compute budget (Speed Score 1.0).
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses LPT (Longest Processing Time) greedy initialization followed by
    iterative pairwise re-partitioning of the heaviest and lightest packs.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # --- Step 1: Initial Greedy Packing (LPT) ---
    # Sort weights descending for LPT heuristic
    sorted_weight, sorted_indices = weight.float().sort(dim=-1, descending=True)

    pack_weights = torch.zeros(num_layers, num_packs, device=device)
    pack_counts = torch.zeros(num_layers,
                              num_packs,
                              dtype=torch.int64,
                              device=device)

    # Storage for the assignment in terms of sorted item indices
    pack_assignment = torch.zeros(num_layers,
                                  num_groups,
                                  dtype=torch.int64,
                                  device=device)
    rank_assignment = torch.zeros(num_layers,
                                  num_groups,
                                  dtype=torch.int64,
                                  device=device)

    layer_indices = torch.arange(num_layers, device=device)
    inf_tensor = torch.tensor(float('inf'), device=device)

    # Vectorized greedy assignment
    for i in range(num_groups):
        w = sorted_weight[:, i]

        # Identify valid packs
        valid_mask = pack_counts < groups_per_pack

        # Choose pack with min weight among valid ones
        temp_weights = torch.where(valid_mask, pack_weights, inf_tensor)
        chosen_pack = torch.argmin(temp_weights, dim=1)

        # Update state
        # Use scatter_add_ logic or direct indexing. Direct indexing is cleaner given layer_indices.
        pack_weights[layer_indices, chosen_pack] += w
        rank_assignment[:, i] = pack_counts[layer_indices, chosen_pack]
        pack_counts[layer_indices, chosen_pack] += 1
        pack_assignment[:, i] = chosen_pack

    # --- Step 2: Iterative Refinement (Pairwise Re-partitioning) ---
    # Construct explicit pack contents map: [L, M, K] -> sorted_item_idx
    pack_contents = torch.zeros(num_layers,
                                num_packs,
                                groups_per_pack,
                                dtype=torch.int64,
                                device=device)

    # We populate pack_contents using the assignment from Step 1
    # pack_assignment[l, i] = p, rank_assignment[l, i] = r
    # We want pack_contents[l, p, r] = i

    flat_l = layer_indices.unsqueeze(1).expand(-1, num_groups).flatten()
    flat_p = pack_assignment.flatten()
    flat_r = rank_assignment.flatten()
    flat_i = torch.arange(num_groups, device=device).unsqueeze(0).expand(num_layers, -1).flatten()

    # Use index_put_ to scatter sorted item indices into the pack structure
    pack_contents.index_put_((flat_l, flat_p, flat_r), flat_i)

    # Refinement loop
    num_iters = 20
    for _ in range(num_iters):
        # Compute current weights from contents (stability)
        # Gather weights using the sorted item indices stored in pack_contents
        current_item_weights = sorted_weight.gather(
            1, pack_contents.view(num_layers, -1)).view(num_layers, num_packs,
                                                        groups_per_pack)
        current_pack_weights = current_item_weights.sum(dim=2)

        # Identify imbalance
        max_val, max_pack = current_pack_weights.max(dim=1)
        min_val, min_pack = current_pack_weights.min(dim=1)

        diff = max_val - min_val
        if diff.max() < 1e-4:
            break

        # Gather items from max and min packs
        # indices: [L, K]
        idx_max = max_pack.view(-1, 1).expand(-1, groups_per_pack)
        idx_min = min_pack.view(-1, 1).expand(-1, groups_per_pack)

        items_max = pack_contents.gather(1, idx_max.unsqueeze(1)).squeeze(1)
        items_min = pack_contents.gather(1, idx_min.unsqueeze(1)).squeeze(1)

        w_max = sorted_weight.gather(1, items_max)
        w_min = sorted_weight.gather(1, items_min)

        # Combine and Sort
        combined_items = torch.cat([items_max, items_min], dim=1)
        combined_weights = torch.cat([w_max, w_min], dim=1)

        # Sort combined items by weight descending
        c_w_sorted, sort_idx = combined_weights.sort(dim=1, descending=True)
        c_items_sorted = combined_items.gather(1, sort_idx)

        # Mini-Greedy Re-partitioning into 2 buckets
        n_combined = groups_per_pack * 2
        b_weights = torch.zeros(num_layers, 2, device=device)
        b_counts = torch.zeros(num_layers, 2, dtype=torch.int64, device=device)

        # Destination buffers [L, 2*K]
        destinations = torch.empty(num_layers, n_combined, dtype=torch.int64, device=device)
        ranks = torch.empty(num_layers, n_combined, dtype=torch.int64, device=device)

        # Greedy loop for 2 bins
        for k in range(n_combined):
            w_item = c_w_sorted[:, k]

            can_fit_0 = b_counts[:, 0] < groups_per_pack
            can_fit_1 = b_counts[:, 1] < groups_per_pack
            prefer_0 = b_weights[:, 0] <= b_weights[:, 1]

            # If both fit, pick lighter. Else pick available.
            choose_0 = (can_fit_0 & can_fit_1 & prefer_0) | (can_fit_0 & (~can_fit_1))

            # Update weights
            w_add_0 = torch.where(choose_0, w_item, torch.tensor(0.0, device=device))
            w_add_1 = torch.where(~choose_0, w_item, torch.tensor(0.0, device=device))
            b_weights[:, 0] += w_add_0
            b_weights[:, 1] += w_add_1

            # Record assignment
            destinations[:, k] = torch.where(choose_0, 0, 1)
            ranks[:, k] = torch.where(choose_0, b_counts[:, 0], b_counts[:, 1])

            # Update counts
            b_counts[:, 0] += choose_0.long()
            b_counts[:, 1] += (~choose_0).long()

        # Scatter results back to pack_contents
        # Construct update buffer [L, 2, K]
        # We need to map the decisions back to the original pack indices (max_pack and min_pack)

        # destinations is 0 or 1. 0 maps to max_pack, 1 maps to min_pack (or vice versa, order doesn't matter as long as consistent)
        # Actually, let's map 0 -> max_pack, 1 -> min_pack

        # temp_buffer[l, 0, r] stores the item assigned to bin 0 at rank r
        # We need to scatter c_items_sorted into temp_buffer based on destinations and ranks

        temp_buffer = torch.zeros(num_layers, 2, groups_per_pack, dtype=torch.int64, device=device)

        # Flatten for scatter
        flat_dest = destinations.flatten() # [L*2K]
        flat_rank = ranks.flatten()        # [L*2K]
        flat_layer = layer_indices.unsqueeze(1).expand(-1, n_combined).flatten()

        # Indices into temp_buffer [L, 2, K]
        # We calculate linear index for scatter
        # dim 0: layer, dim 1: bin (0/1), dim 2: rank
        linear_idx = flat_layer * (2 * groups_per_pack) + flat_dest * groups_per_pack + flat_rank

        temp_buffer.view(-1).scatter_(0, linear_idx, c_items_sorted.flatten())

        # Update global state: Put bin 0 back to max_pack, bin 1 back to min_pack
        # We use scatter_ on pack_contents [L, P, K]
        # idx_max is [L, K], idx_min is [L, K]. These are pack indices (P dimension)

        # Update max_pack entries
        pack_contents.scatter_(1, idx_max.unsqueeze(1), temp_buffer[:, 0, :].unsqueeze(1))
        # Update min_pack entries
        pack_contents.scatter_(1, idx_min.unsqueeze(1), temp_buffer[:, 1, :].unsqueeze(1))

    # --- Step 3: Reconstruct Output ---
    # Map back from sorted_indices to original structure

    # Grid for pack indices and ranks
    grid_p = torch.arange(num_packs, device=device).view(1, num_packs, 1).expand(num_layers, -1, groups_per_pack)
    grid_r = torch.arange(groups_per_pack, device=device).view(1, 1, groups_per_pack).expand(num_layers, num_packs, -1)

    pack_sorted = torch.zeros(num_layers, num_groups, dtype=torch.int64, device=device)
    rank_sorted = torch.zeros(num_layers, num_groups, dtype=torch.int64, device=device)

    # Scatter using pack_contents as indices (these are the sorted item indices)
    # pack_contents[l, p, r] = k.  We want pack_sorted[l, k] = p

    flat_indices = pack_contents.view(num_layers, -1)

    pack_sorted.scatter_(1, flat_indices, grid_p.reshape(num_layers, -1))
    rank_sorted.scatter_(1, flat_indices, grid_r.reshape(num_layers, -1))

    # Finally map to original unsorted order
    pack_index = torch.empty_like(pack_sorted)
    rank_in_pack = torch.empty_like(rank_sorted)

    pack_index.scatter_(1, sorted_indices, pack_sorted)
    rank_in_pack.scatter_(1, sorted_indices, rank_sorted)

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses Randomized Restarts with Parallel Greedy Assignment followed by
    Pairwise Re-partitioning Refinement on the best candidate.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # --- Step 1: Randomized Greedy Initialization ---
    # We use 64 restarts to explore the solution space.
    num_restarts = 64

    # Expand weights: [L, R, G]
    w_expanded = weight.unsqueeze(1).expand(-1, num_restarts, -1)

    # Add noise for sorting (Randomized LPT)
    # Restart 0 is deterministic (noise=0), others have 5% noise
    noise = torch.rand_like(w_expanded) * 0.05
    noise[:, 0, :] = 0.0

    # Sort indices based on noisy weights
    w_noisy = w_expanded * (1.0 + noise)
    # Sort descending
    _, sorted_indices = w_noisy.sort(dim=-1, descending=True)

    # Gather actual weights [L, R, G] in the sorted order
    w_sorted = w_expanded.gather(2, sorted_indices)

    # Flatten batch dimensions for vectorized greedy packing
    # B = L * R
    B = num_layers * num_restarts
    w_flat = w_sorted.view(B, num_groups)

    # Allocations for greedy state
    pack_weights = torch.zeros(B, num_packs, device=device)
    pack_counts = torch.zeros(B, num_packs, dtype=torch.int64, device=device)

    # Result holders (in sorted order of each restart)
    pack_assign_sorted = torch.zeros(B, num_groups, dtype=torch.int64, device=device)
    rank_assign_sorted = torch.zeros(B, num_groups, dtype=torch.int64, device=device)

    range_B = torch.arange(B, device=device)
    inf_tensor = torch.tensor(float('inf'), device=device)

    # Vectorized Greedy Packing
    for i in range(num_groups):
        item_w = w_flat[:, i]

        # Identify valid packs (count < limit)
        valid_mask = pack_counts < groups_per_pack

        # Select pack with minimum weight among valid packs
        curr_costs = torch.where(valid_mask, pack_weights, inf_tensor)
        chosen_pack = curr_costs.argmin(dim=1)

        # Update State
        pack_weights[range_B, chosen_pack] += item_w
        rank_assign_sorted[:, i] = pack_counts[range_B, chosen_pack]
        pack_counts[range_B, chosen_pack] += 1
        pack_assign_sorted[:, i] = chosen_pack

    # --- Step 2: Select Best Restart per Layer ---
    # Metric: (Max - Min) weight in packs
    diffs = pack_weights.max(dim=1).values - pack_weights.min(dim=1).values
    diffs = diffs.view(num_layers, num_restarts)
    best_restart_idx = diffs.argmin(dim=1) # [L]

    # Calculate flat indices of the best restarts
    best_b_indices = torch.arange(num_layers, device=device) * num_restarts + best_restart_idx

    # Gather best assignments and original indices
    # [L, G]
    best_pack_assign = pack_assign_sorted[best_b_indices]
    best_rank_assign = rank_assign_sorted[best_b_indices]
    best_orig_indices = sorted_indices.view(B, num_groups)[best_b_indices]

    # --- Step 3: Pairwise Re-partitioning Refinement ---
    # We refine the best candidate from the randomized search.

    # Construct pack_contents: [L, P, K] -> Original Item Index
    pack_contents = torch.zeros(num_layers, num_packs, groups_per_pack, dtype=torch.int64, device=device)

    flat_l = torch.arange(num_layers, device=device).unsqueeze(1).expand(-1, num_groups).flatten()
    flat_p = best_pack_assign.flatten()
    flat_r = best_rank_assign.flatten()
    flat_item_idx = best_orig_indices.flatten()

    pack_contents.index_put_((flat_l, flat_p, flat_r), flat_item_idx)

    layer_indices = torch.arange(num_layers, device=device)

    for _ in range(20):
        # Gather weights from original `weight` tensor
        # pack_contents holds original item indices.
        flat_idx = pack_contents.view(num_layers, -1)
        current_w = weight.gather(1, flat_idx).view(num_layers, num_packs, groups_per_pack)

        pack_sums = current_w.sum(dim=2)
        max_val, max_pack = pack_sums.max(dim=1)
        min_val, min_pack = pack_sums.min(dim=1)

        if (max_val - min_val).max() < 1e-4:
            break

        # Indices for max/min packs
        idx_max = max_pack.view(-1, 1).expand(-1, groups_per_pack)
        idx_min = min_pack.view(-1, 1).expand(-1, groups_per_pack)

        # Get Original Item Indices in max/min packs
        items_max = pack_contents.gather(1, idx_max.unsqueeze(1)).squeeze(1)
        items_min = pack_contents.gather(1, idx_min.unsqueeze(1)).squeeze(1)

        # Get Weights
        w_max = weight.gather(1, items_max)
        w_min = weight.gather(1, items_min)

        # Combine
        combined_items = torch.cat([items_max, items_min], dim=1)
        combined_weights = torch.cat([w_max, w_min], dim=1)

        # Sort combined items descending (Mini-Greedy Prep)
        c_w_sorted, sort_idx = combined_weights.sort(dim=1, descending=True)
        c_items_sorted = combined_items.gather(1, sort_idx)

        # Mini-Greedy Partitioning (Partition 2*K items into 2 packs of size K)
        n_combined = groups_per_pack * 2
        b_weights = torch.zeros(num_layers, 2, device=device)
        b_counts = torch.zeros(num_layers, 2, dtype=torch.int64, device=device)

        destinations = torch.empty(num_layers, n_combined, dtype=torch.int64, device=device)
        ranks = torch.empty(num_layers, n_combined, dtype=torch.int64, device=device)

        for k in range(n_combined):
            w_item = c_w_sorted[:, k]

            can_fit_0 = b_counts[:, 0] < groups_per_pack
            can_fit_1 = b_counts[:, 1] < groups_per_pack
            prefer_0 = b_weights[:, 0] <= b_weights[:, 1]

            choose_0 = (can_fit_0 & can_fit_1 & prefer_0) | (can_fit_0 & (~can_fit_1))

            # Update weights (safe 0.0 addition)
            w_add_0 = torch.where(choose_0, w_item, torch.zeros_like(w_item))
            w_add_1 = torch.where(~choose_0, w_item, torch.zeros_like(w_item))
            b_weights[:, 0] += w_add_0
            b_weights[:, 1] += w_add_1

            destinations[:, k] = torch.where(choose_0, 0, 1)
            ranks[:, k] = torch.where(choose_0, b_counts[:, 0], b_counts[:, 1])

            b_counts[:, 0] += choose_0.long()
            b_counts[:, 1] += (~choose_0).long()

        # Scatter results back into pack_contents
        temp_buffer = torch.zeros(num_layers, 2, groups_per_pack, dtype=torch.int64, device=device)

        flat_dest = destinations.flatten()
        flat_rank = ranks.flatten()
        flat_layer = layer_indices.unsqueeze(1).expand(-1, n_combined).flatten()

        # linear_idx into temp_buffer [L, 2, K]
        linear_idx = flat_layer * (2 * groups_per_pack) + flat_dest * groups_per_pack + flat_rank
        temp_buffer.view(-1).scatter_(0, linear_idx, c_items_sorted.flatten())

        # Update packs: map bin 0 -> max_pack, bin 1 -> min_pack
        pack_contents.scatter_(1, idx_max.unsqueeze(1), temp_buffer[:, 0, :].unsqueeze(1))
        pack_contents.scatter_(1, idx_min.unsqueeze(1), temp_buffer[:, 1, :].unsqueeze(1))

    # --- Step 4: Reconstruct Final Output ---
    pack_index = torch.empty(num_layers, num_groups, dtype=torch.int64, device=device)
    rank_in_pack = torch.empty(num_layers, num_groups, dtype=torch.int64, device=device)

    grid_p = torch.arange(num_packs, device=device).view(1, num_packs, 1).expand(num_layers, -1, groups_per_pack)
    grid_r = torch.arange(groups_per_pack, device=device).view(1, 1, groups_per_pack).expand(num_layers, num_packs, -1)

    # pack_contents[l, p, r] = original_item_id
    flat_indices = pack_contents.view(num_layers, -1)

    # Scatter pack ID and rank to the position given by original_item_id
    pack_index.scatter_(1, flat_indices, grid_p.reshape(num_layers, -1))
    rank_in_pack.scatter_(1, flat_indices, grid_r.reshape(num_layers, -1))

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>