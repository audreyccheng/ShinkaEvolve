<NAME>
vectorized_local_search_refinement
</NAME>

<DESCRIPTION>
Add a vectorized local search refinement step to the `balanced_packing` function. After the initial greedy assignment (LPT), this refinement iteratively swaps items between the most loaded and least loaded packs to reduce load imbalance. This implementation maintains high efficiency by vectorizing operations across all layers simultaneously and using advanced indexing to perform swaps in parallel.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Sort weights descending to apply LPT (Longest Processing Time) heuristic
    sorted_weight, sorted_indices = weight.float().sort(dim=-1, descending=True)

    # Track state for all layers in parallel
    pack_weights = torch.zeros(num_layers, num_packs, device=device)
    pack_counts = torch.zeros(num_layers,
                              num_packs,
                              dtype=torch.int64,
                              device=device)

    # Temporary storage for results in sorted order
    pack_index_sorted = torch.empty(num_layers,
                                    num_groups,
                                    dtype=torch.int64,
                                    device=device)
    rank_in_pack_sorted = torch.empty(num_layers,
                                      num_groups,
                                      dtype=torch.int64,
                                      device=device)

    # Vectorized greedy assignment
    # We assign items (columns in sorted_weight) one by one
    for i in range(num_groups):
        w = sorted_weight[:, i]  # [num_layers]

        # Identify valid packs (not full)
        valid_mask = pack_counts < groups_per_pack  # [num_layers, num_packs]

        # Find pack with min weight among valid packs
        # We add a large penalty to invalid packs to exclude them from argmin
        current_weights_with_penalty = pack_weights.clone()
        current_weights_with_penalty[~valid_mask] = float('inf')

        chosen_pack = torch.argmin(current_weights_with_penalty,
                                   dim=1)  # [num_layers]

        # Update pack weights and counts
        # scatter_add_ requires index to have same number of dimensions
        # But here we can use advanced indexing since we want to update one element per row
        row_indices = torch.arange(num_layers, device=device)

        pack_weights[row_indices, chosen_pack] += w

        # Store rank (current count before increment)
        rank_in_pack_sorted[:, i] = pack_counts[row_indices, chosen_pack]

        # Increment count
        pack_counts[row_indices, chosen_pack] += 1

        # Store pack assignment
        pack_index_sorted[:, i] = chosen_pack

    # Scatter results back to original order
    pack_index = torch.empty_like(pack_index_sorted)
    rank_in_pack = torch.empty_like(rank_in_pack_sorted)

    pack_index.scatter_(1, sorted_indices, pack_index_sorted)
    rank_in_pack.scatter_(1, sorted_indices, rank_in_pack_sorted)

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Sort weights descending to apply LPT (Longest Processing Time) heuristic
    sorted_weight, sorted_indices = weight.float().sort(dim=-1, descending=True)

    # Track state for all layers in parallel
    pack_weights = torch.zeros(num_layers, num_packs, device=device)
    pack_counts = torch.zeros(num_layers,
                              num_packs,
                              dtype=torch.int64,
                              device=device)

    # Temporary storage for results in sorted order
    pack_index_sorted = torch.empty(num_layers,
                                    num_groups,
                                    dtype=torch.int64,
                                    device=device)
    rank_in_pack_sorted = torch.empty(num_layers,
                                      num_groups,
                                      dtype=torch.int64,
                                      device=device)

    row_indices = torch.arange(num_layers, device=device)

    # Vectorized greedy assignment
    # We assign items (columns in sorted_weight) one by one
    for i in range(num_groups):
        w = sorted_weight[:, i]  # [num_layers]

        # Identify valid packs (not full)
        valid_mask = pack_counts < groups_per_pack  # [num_layers, num_packs]

        # Find pack with min weight among valid packs
        # We add a large penalty to invalid packs to exclude them from argmin
        current_weights_with_penalty = pack_weights.clone()
        current_weights_with_penalty[~valid_mask] = float('inf')

        chosen_pack = torch.argmin(current_weights_with_penalty,
                                   dim=1)  # [num_layers]

        pack_weights[row_indices, chosen_pack] += w
        rank_in_pack_sorted[:, i] = pack_counts[row_indices, chosen_pack]
        pack_counts[row_indices, chosen_pack] += 1
        pack_index_sorted[:, i] = chosen_pack

    # Refinement Step: Pairwise swap between heaviest and lightest packs
    # Construct pack_contents to facilitate swapping
    pack_contents = torch.zeros(num_layers,
                                num_packs,
                                groups_per_pack,
                                device=device)
    batch_indices_flat = row_indices.unsqueeze(1).expand(-1,
                                                         num_groups).flatten()
    pack_indices_flat = pack_index_sorted.flatten()
    rank_indices_flat = rank_in_pack_sorted.flatten()

    pack_contents.index_put_(
        (batch_indices_flat, pack_indices_flat, rank_indices_flat),
        sorted_weight.flatten())

    # Track original sorted indices (0..N-1) to reconstruct assignment later
    pack_item_ids = torch.zeros(num_layers,
                                num_packs,
                                groups_per_pack,
                                dtype=torch.int64,
                                device=device)
    item_ids = torch.arange(num_groups, device=device).expand(num_layers, -1)
    pack_item_ids.index_put_(
        (batch_indices_flat, pack_indices_flat, rank_indices_flat),
        item_ids.flatten())

    for _ in range(20):  # 20 iterations of refinement
        # Recompute pack weights from contents to avoid drift (and it's fast enough)
        pack_weights = pack_contents.sum(dim=-1)

        max_val, max_pack = pack_weights.max(dim=1)
        min_val, min_pack = pack_weights.min(dim=1)
        diff = max_val - min_val

        # Mask for layers where diff is significant
        active_mask = diff > 1e-4
        if not active_mask.any():
            break

        # Gather items from max and min packs
        # [L, G]
        items_max = pack_contents[row_indices, max_pack]
        items_min = pack_contents[row_indices, min_pack]

        # Compute potential reduction for all pairs
        # delta = w_max - w_min. We want delta > 0 and 2*delta < 2*diff (approx)
        delta = items_max.unsqueeze(2) - items_min.unsqueeze(1)  # [L, G, G]

        # Improvement = diff - |diff - 2*delta|
        # maximize improvement
        change = (diff.view(-1, 1, 1) - 2 * delta).abs()
        improvement = diff.view(-1, 1, 1) - change

        best_imp_flat, best_idx_flat = improvement.view(num_layers, -1).max(dim=1)

        # Determine which layers to swap
        do_swap = (best_imp_flat > 1e-6) & active_mask

        if not do_swap.any():
            break

        # Perform swaps
        swap_indices = best_idx_flat[do_swap]
        layer_indices = row_indices[do_swap]

        idx_max = swap_indices // groups_per_pack
        idx_min = swap_indices % groups_per_pack

        p_max = max_pack[do_swap]
        p_min = min_pack[do_swap]

        # Swap values
        val_max = pack_contents[layer_indices, p_max, idx_max]
        val_min = pack_contents[layer_indices, p_min, idx_min]

        pack_contents[layer_indices, p_max, idx_max] = val_min
        pack_contents[layer_indices, p_min, idx_min] = val_max

        # Swap IDs
        id_max = pack_item_ids[layer_indices, p_max, idx_max]
        id_min = pack_item_ids[layer_indices, p_min, idx_min]
        pack_item_ids[layer_indices, p_max, idx_max] = id_min
        pack_item_ids[layer_indices, p_min, idx_min] = id_max

    # Reconstruct assignment from pack_item_ids
    # pack_item_ids[l, p, g] = k  => pack_index_sorted[l, k] = p, rank[l, k] = g
    flat_ids = pack_item_ids.view(num_layers, -1)
    flat_packs = torch.arange(num_packs, device=device).view(1, -1, 1).expand(
        num_layers, -1, groups_per_pack).reshape(num_layers, -1)
    flat_ranks = torch.arange(groups_per_pack, device=device).view(
        1, 1, -1).expand(num_layers, num_packs, -1).reshape(num_layers, -1)

    pack_index_sorted.scatter_(1, flat_ids, flat_packs)
    rank_in_pack_sorted.scatter_(1, flat_ids, flat_ranks)

    # Scatter results back to original order
    pack_index = torch.empty_like(pack_index_sorted)
    rank_in_pack = torch.empty_like(rank_in_pack_sorted)

    pack_index.scatter_(1, sorted_indices, pack_index_sorted)
    rank_in_pack.scatter_(1, sorted_indices, rank_in_pack_sorted)

    return pack_index, rank_in_pack
>>>>>>> REPLACE

</DIFF>