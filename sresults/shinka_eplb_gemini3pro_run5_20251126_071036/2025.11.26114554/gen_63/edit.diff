--- a/original.py
+++ b/original.py
@@ -1,328 +1,432 @@
 # EVOLVE-BLOCK-START
 """
 Expert parallelism load balancer (EPLB) for vLLM.
 
 This module implements the core rearrangement algorithm.
 
 The rearrangement algorithm is adapted from
 [DeepSeek EPLB](https://github.com/deepseek-ai/eplb).
 
 Please find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example
 on how the EPLB algorithm works.
 """
 
 import torch
 
 
+def _refine_variance(weights: torch.Tensor,
+                     pack_indices: torch.Tensor,
+                     pack_weights: torch.Tensor,
+                     num_packs: int,
+                     groups_per_pack: int,
+                     max_iters: int = 20) -> None:
+    """
+    Refines packing by minimizing load variance (L2 norm) via all-pairs swapping.
+    Operates in-place.
+    """
+    B = weights.shape[0]
+    device = weights.device
+    N = num_packs * groups_per_pack
+    
+    # Pre-compute pack id for each slot [0,0,..,1,1,..]
+    # [N]
+    slot_to_pid = torch.arange(num_packs, device=device).view(-1, 1).expand(-1, groups_per_pack).reshape(-1)
+    # Mask for same-pack swaps
+    same_pack = slot_to_pid.unsqueeze(0) == slot_to_pid.unsqueeze(1) # [N, N]
+    
+    batch_idx = torch.arange(B, device=device)
+    
+    for _ in range(max_iters):
+        # Flatten structure
+        flat_indices = pack_indices.view(B, N)
+        
+        # Gather weights: [B, N]
+        w = weights.gather(1, flat_indices)
+        
+        # Gather pack loads for each item slot: [B, N]
+        l = pack_weights.gather(1, slot_to_pid.expand(B, -1))
+        
+        # Compute D = w[u] - w[v] (weight moved from u to v)
+        # We want to swap item at u with item at v
+        w_diff = w.unsqueeze(2) - w.unsqueeze(1) # [B, N, N]
+        l_diff = l.unsqueeze(1) - l.unsqueeze(2) # [B, N, N] (L(v) - L(u))
+        
+        # Change in sum of squares = 2 * d * (L_v - L_u + d)
+        # We want to minimize this.
+        delta_cost = 2 * w_diff * (l_diff + w_diff)
+        
+        # Apply mask (prevent same pack swaps and self swaps)
+        delta_cost.masked_fill_(same_pack, float('inf'))
+        
+        # Find minimum change
+        flat_cost = delta_cost.view(B, -1)
+        min_val, flat_idx = torch.min(flat_cost, dim=1)
+        
+        # Threshold for improvement
+        active = min_val < -1e-4
+        if not active.any():
+            break
+            
+        # Execute swaps
+        active_batches = batch_idx[active]
+        idx_pair = flat_idx[active]
+        
+        u = idx_pair // N
+        v = idx_pair % N
+        
+        # Swap indices
+        item_u = flat_indices[active_batches, u]
+        item_v = flat_indices[active_batches, v]
+        
+        flat_indices[active_batches, u] = item_v
+        flat_indices[active_batches, v] = item_u
+        
+        # Update loads
+        pid_u = slot_to_pid[u]
+        pid_v = slot_to_pid[v]
+        
+        d_val = w_diff[active_batches, u, v]
+        
+        pack_weights[active_batches, pid_u] -= d_val
+        pack_weights[active_batches, pid_v] += d_val
+
+
 def _refine_minmax(weights: torch.Tensor,
                    pack_indices: torch.Tensor,
                    pack_weights: torch.Tensor,
-                   max_iters: int = 50) -> torch.Tensor:
-    """
-    Refines packing by swapping items from the heaviest pack to minimize max load.
-    Vectorized over the batch dimension (num_layers).
-    """
-    L, M, G = pack_indices.shape
-    batch_idx = torch.arange(L, device=weights.device)
+                   max_iters: int = 50) -> None:
+    """
+    Refines packing by reducing Max Load (L_inf norm).
+    Targeted swaps from Max Pack to others.
+    """
+    B, M, G = pack_indices.shape
+    device = weights.device
+    N = M * G
+    batch_idx = torch.arange(B, device=device)
+    
+    # Precompute slot grid
+    slot_to_pid = torch.arange(M, device=device).view(-1, 1).expand(-1, G).reshape(-1)
 
     for _ in range(max_iters):
-        # 1. Identify the max load pack
-        max_load, max_pid = torch.max(pack_weights, dim=1)
-
-        # 2. Gather weights
-        max_pack_items = pack_indices[batch_idx, max_pid] # [L, G]
-        w_max_items = weights.gather(1, max_pack_items)   # [L, G]
-        w_all_items = weights.gather(1, pack_indices.view(L, -1)).view(L, M, G)
-
-        # 3. Deltas: w_u (max) - w_v (other)
-        deltas = w_max_items.unsqueeze(1).unsqueeze(2) - w_all_items.unsqueeze(3)
-
-        # 4. Prospective loads
-        # New Max = Max - delta, New Other = Other + delta
-        new_max_load = max_load.view(L, 1, 1, 1) - deltas
-        new_other_load = pack_weights.view(L, M, 1, 1) + deltas
-
-        metrics = torch.max(new_max_load, new_other_load)
-
-        # Mask self-swaps
-        mask = (torch.arange(M, device=weights.device).unsqueeze(0) == max_pid.unsqueeze(1))
-        mask = mask.view(L, M, 1, 1).expand(-1, -1, G, G)
-        metrics.masked_fill_(mask, float('inf'))
-
-        # 5. Best swap
-        flat_metrics = metrics.view(L, -1)
-        best_metric, best_idx = torch.min(flat_metrics, dim=1)
-
-        improve_mask = best_metric < (max_load - 1e-5)
-        if not improve_mask.any():
+        # Identify max pack
+        max_load, max_pid = torch.max(pack_weights, dim=1) # [B]
+        
+        # Gather items in max pack: [B, G]
+        gather_idx = max_pid.view(B, 1, 1).expand(-1, 1, G)
+        items_max = pack_indices.gather(1, gather_idx).squeeze(1) # [B, G]
+        
+        w_max = weights.gather(1, items_max) # [B, G]
+        
+        # Gather all items for comparison: [B, N]
+        flat_indices = pack_indices.view(B, N)
+        w_all = weights.gather(1, flat_indices)
+        
+        # Deltas: w_max[i] - w_all[j]
+        # [B, G, 1] - [B, 1, N] -> [B, G, N]
+        deltas = w_max.unsqueeze(2) - w_all.unsqueeze(1)
+        
+        # Prospective Loads
+        # New Max = Max - delta
+        # New Other = Other + delta
+        l_all = pack_weights.gather(1, slot_to_pid.expand(B, -1)) # [B, N]
+        
+        new_max = max_load.view(B, 1, 1) - deltas
+        new_other = l_all.unsqueeze(1) + deltas
+        
+        # Objective: max(new_max, new_other)
+        obj = torch.max(new_max, new_other)
+        
+        # Mask where j is in max_pid
+        is_max = slot_to_pid.unsqueeze(0) == max_pid.unsqueeze(1) # [B, N]
+        mask = is_max.unsqueeze(1).expand(-1, G, -1)
+        obj.masked_fill_(mask, float('inf'))
+        
+        # Find best
+        flat_obj = obj.view(B, -1)
+        best_val, best_idx_flat = torch.min(flat_obj, dim=1)
+        
+        # Check improvement
+        improve = best_val < (max_load - 1e-5)
+        if not improve.any():
             break
-
-        active_indices = batch_idx[improve_mask]
-        if len(active_indices) == 0:
-            break
-
-        # 6. Execute swap
-        idx_flat = best_idx[improve_mask]
-        p_other = idx_flat // (G * G)
-        rem = idx_flat % (G * G)
-        g_other = rem // G
-        g_max = rem % G
-
-        p_max_active = max_pid[active_indices]
-
-        item_max = pack_indices[active_indices, p_max_active, g_max]
-        item_other = pack_indices[active_indices, p_other, g_other]
-
-        pack_indices[active_indices, p_max_active, g_max] = item_other
-        pack_indices[active_indices, p_other, g_other] = item_max
-
-        w_max_val = weights[active_indices, item_max]
-        w_other_val = weights[active_indices, item_other]
-        d_val = w_max_val - w_other_val
-
-        pack_weights[active_indices, p_max_active] -= d_val
-        pack_weights[active_indices, p_other] += d_val
-
-    return pack_indices
+            
+        active = batch_idx[improve]
+        if len(active) == 0: break
+            
+        # Decode
+        idx_flat = best_idx_flat[improve]
+        i = idx_flat // N
+        j = idx_flat % N
+        
+        pid_max_active = max_pid[active]
+        pid_other_active = slot_to_pid[j]
+        
+        d_val = deltas[active, i, j]
+        
+        # Update weights
+        pack_weights[active, pid_max_active] -= d_val
+        pack_weights[active, pid_other_active] += d_val
+        
+        # Update indices
+        flat_offset_i = pid_max_active * G + i
+        
+        val_i = flat_indices[active, flat_offset_i]
+        val_j = flat_indices[active, j]
+        
+        flat_indices[active, flat_offset_i] = val_j
+        flat_indices[active, j] = val_i
+
 
 def balanced_packing(weight: torch.Tensor,
                      num_packs: int,
-                     num_attempts: int = 5) -> tuple[torch.Tensor, torch.Tensor]:
-    """
-    Pack n weighted objects to m packs using Iterated Greedy with Load-Based Re-weighting.
-    This effectively helps the greedy heuristic avoid local optima by forcing
-    problematic items (those in heavy packs) to be scheduled earlier.
+                     num_candidates: int = 64) -> tuple[torch.Tensor, torch.Tensor]:
+    """
+    Pack n weighted objects to m packs using Parallel Randomized Greedy + Hybrid Refinement.
     """
     num_layers, num_groups = weight.shape
     device = weight.device
-
-    if num_packs == num_groups:
-        pack_index = torch.arange(num_groups, device=device).expand(num_layers, -1)
-        rank_in_pack = torch.zeros_like(pack_index)
+    assert num_groups % num_packs == 0
+    groups_per_pack = num_groups // num_packs
+
+    if groups_per_pack == 1:
+        pack_index = torch.arange(num_groups, dtype=torch.int64,
+                                  device=device).expand(num_layers, -1)
+        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
         return pack_index, rank_in_pack
 
-    groups_per_pack = num_groups // num_packs
-
-    # State tracking
-    best_pack_index = torch.zeros_like(weight, dtype=torch.int64)
-    best_rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
-    best_max_loads = torch.full((num_layers,), float('inf'), device=device)
-
-    # Virtual weights start as copy of real weights
-    virtual_weight = weight.clone().float()
-
-    # Pre-computed grids for scatter
-    p_ids = torch.arange(num_packs, device=device).view(1, -1, 1).expand(num_layers, -1, groups_per_pack)
-    r_ids = torch.arange(groups_per_pack, device=device).view(1, 1, -1).expand(num_layers, num_packs, -1)
-
-    # Constant ranges
-    batch_idx = torch.arange(num_layers, device=device)
-    inf_tensor = torch.tensor(float('inf'), device=device)
-
-    for attempt in range(num_attempts):
-        # 1. Greedy Init with Virtual Weights Order
-        # Sort based on virtual weights
-        sorted_indices = torch.argsort(virtual_weight, dim=1, descending=True)
-        # Gather real weights for load calculation
-        sorted_real_weights = weight.gather(1, sorted_indices)
-
-        pack_weights = torch.zeros(num_layers, num_packs, device=device, dtype=weight.dtype)
-        pack_counts = torch.zeros(num_layers, num_packs, device=device, dtype=torch.int64)
-        pack_assignment = torch.zeros(num_layers, num_packs, groups_per_pack, dtype=torch.int64, device=device)
-
-        for i in range(num_groups):
-            w_real = sorted_real_weights[:, i]
-            idx_original = sorted_indices[:, i]
-
-            # Mask full packs
-            is_full = (pack_counts >= groups_per_pack)
-            candidate_weights = torch.where(is_full, inf_tensor, pack_weights)
-
-            best_pack = torch.argmin(candidate_weights, dim=1)
-
-            slots = pack_counts[batch_idx, best_pack]
-            pack_assignment[batch_idx, best_pack, slots] = idx_original
-            pack_weights[batch_idx, best_pack] += w_real
-            pack_counts[batch_idx, best_pack] += 1
-
-        # 2. Refinement using REAL weights
-        pack_assignment = _refine_minmax(weight, pack_assignment, pack_weights, max_iters=50)
-
-        # 3. Check improvement
-        current_max_loads, max_pids = torch.max(pack_weights, dim=1)
-        improved_mask = current_max_loads < best_max_loads
-
-        if improved_mask.any():
-            # Update best assignment for improved layers
-            flat_assignment = pack_assignment.view(num_layers, -1)
-            flat_p = p_ids.reshape(num_layers, -1)
-            flat_r = r_ids.reshape(num_layers, -1)
-
-            curr_pidx = torch.empty_like(best_pack_index)
-            curr_rank = torch.empty_like(best_rank_in_pack)
-            curr_pidx.scatter_(1, flat_assignment, flat_p)
-            curr_rank.scatter_(1, flat_assignment, flat_r)
-
-            best_max_loads = torch.where(improved_mask, current_max_loads, best_max_loads)
-
-            mask_exp = improved_mask.unsqueeze(1).expand_as(best_pack_index)
-            best_pack_index = torch.where(mask_exp, curr_pidx, best_pack_index)
-            best_rank_in_pack = torch.where(mask_exp, curr_rank, best_rank_in_pack)
-
-        # 4. Re-weighting for next iteration
-        if attempt < num_attempts - 1:
-            # Identify items in the max pack
-            max_pack_items = pack_assignment[batch_idx, max_pids] # [L, G]
-
-            # Multiplicative update: increase weight by 5%
-            # We construct a multiplier tensor [L, N]
-            multiplier = torch.ones_like(virtual_weight)
-            # Create update values [L, G]
-            updates = torch.full_like(max_pack_items, 1.05, dtype=virtual_weight.dtype)
-            multiplier.scatter_(1, max_pack_items, updates)
-
-            virtual_weight = virtual_weight * multiplier
-
-    return best_pack_index, best_rank_in_pack
+    # Expand for candidates
+    # Total Batch Size B = num_layers * num_candidates
+    B = num_layers * num_candidates
+    
+    # [L, 1, G] -> [L, K, G] -> [B, G]
+    weights_expanded = weight.unsqueeze(1).expand(-1, num_candidates, -1).reshape(B, num_groups)
+    
+    # Generate Sort Keys with Noise
+    # Multiplicative noise 1.0 + U(0, alpha)
+    noise = torch.rand(B, num_groups, device=device) * 0.15 # up to 15% noise
+    # Force first candidate of each layer to have 0 noise (Pure Greedy LPT)
+    noise.view(num_layers, num_candidates, num_groups)[:, 0, :] = 0.0
+    
+    sort_keys = weights_expanded * (1.0 + noise)
+    
+    # Sort
+    sorted_res = torch.sort(sort_keys, dim=1, descending=True)
+    sorted_indices = sorted_res.indices
+    sorted_weights = torch.gather(weights_expanded, 1, sorted_indices)
+    
+    # 1. Vectorized Greedy Packing
+    pack_weights = torch.zeros(B, num_packs, device=device, dtype=weight.dtype)
+    pack_counts = torch.zeros(B, num_packs, device=device, dtype=torch.int64)
+    pack_indices = torch.zeros(B, num_packs, groups_per_pack, device=device, dtype=torch.int64)
+    
+    batch_idx = torch.arange(B, device=device)
+    inf_val = torch.tensor(float('inf'), device=device)
+    
+    for i in range(num_groups):
+        w = sorted_weights[:, i]
+        idx = sorted_indices[:, i]
+        
+        is_full = pack_counts >= groups_per_pack
+        cand_weights = torch.where(is_full, inf_val, pack_weights)
+        best_pack = torch.argmin(cand_weights, dim=1)
+        
+        slots = pack_counts[batch_idx, best_pack]
+        pack_indices[batch_idx, best_pack, slots] = idx
+        pack_weights[batch_idx, best_pack] += w
+        pack_counts[batch_idx, best_pack] += 1
+        
+    # 2. Hybrid Refinement
+    # Step A: Variance Reduction (Global smoothing)
+    _refine_variance(weights_expanded, pack_indices, pack_weights, num_packs, groups_per_pack)
+    
+    # Step B: Min-Max Reduction (Peak shaving)
+    _refine_minmax(weights_expanded, pack_indices, pack_weights)
+    
+    # 3. Select Best Candidate
+    pw_reshaped = pack_weights.view(num_layers, num_candidates, num_packs)
+    max_loads = torch.max(pw_reshaped, dim=2).values # [L, K]
+    best_k = torch.argmin(max_loads, dim=1) # [L]
+    
+    # Extract Best Assignments
+    pi_reshaped = pack_indices.view(num_layers, num_candidates, num_packs, groups_per_pack)
+    best_assignment = pi_reshaped[torch.arange(num_layers, device=device), best_k]
+    
+    # 4. Construct Output Maps
+    flat_assignment = best_assignment.view(num_layers, -1)
+    
+    p_ids = torch.arange(num_packs, device=device).unsqueeze(1).expand(-1, groups_per_pack).reshape(1, -1).expand(num_layers, -1)
+    r_ids = torch.arange(groups_per_pack, device=device).unsqueeze(0).expand(num_packs, -1).reshape(1, -1).expand(num_layers, -1)
+    
+    pack_index = torch.empty_like(weight, dtype=torch.int64)
+    rank_in_pack = torch.empty_like(weight, dtype=torch.int64)
+    
+    pack_index.scatter_(1, flat_assignment, p_ids)
+    rank_in_pack.scatter_(1, flat_assignment, r_ids)
+
+    return pack_index, rank_in_pack
 
 
 def replicate_experts(
         weight: torch.Tensor,
         num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
-    Replicate experts to `num_phy` replicas.
+    Replicate experts to `num_phy` replicas, minimizing the maximum load.
+    Uses a vectorized greedy approach.
     """
     n, num_log = weight.shape
     device = weight.device
-
+    
+    # Initialize: Each expert gets at least 1 replica
     logcnt = torch.ones((n, num_log), dtype=torch.int64, device=device)
-
-    # Pre-allocate output buffers
-    phy2log = torch.empty((n, num_phy), dtype=torch.int64, device=device)
-    rank = torch.empty((n, num_phy), dtype=torch.int64, device=device)
-
-    # Initial fill
-    # First num_log physical experts map 1:1 to logical experts
-    phy2log[:, :num_log] = torch.arange(num_log, device=device).expand(n, -1)
-    rank[:, :num_log] = 0
-
-    rows = torch.arange(n, device=device)
+    
+    # Greedily assign remaining replicas
     for _ in range(num_log, num_phy):
         scores = weight / logcnt
-        best_expert = torch.argmax(scores, dim=1) # [n]
-        logcnt[rows, best_expert] += 1
-
-    # Reconstruct the mapping from counts
+        indices = torch.argmax(scores, dim=-1)
+        rows = torch.arange(n, device=device)
+        logcnt[rows, indices] += 1
+
+    # Reconstruction of the mapping tables from counts
+    phy2log = torch.zeros((n, num_phy), dtype=torch.int64, device=device)
+    rank = torch.zeros((n, num_phy), dtype=torch.int64, device=device)
+
+    # Reconstruct mappings row by row
     for i in range(n):
-        c = logcnt[i]
-        # Expand logical IDs
-        phy2log[i] = torch.repeat_interleave(torch.arange(num_log, device=device), c)
-
-        # Calculate ranks
+        counts = logcnt[i]
+        l_ids = torch.repeat_interleave(
+            torch.arange(num_log, device=device), counts
+        )
+        phy2log[i] = l_ids
+        
         curr = 0
         for idx in range(num_log):
-             cnt = c[idx].item()
-             rank[i, curr:curr+cnt] = torch.arange(cnt, device=device)
-             curr += cnt
-
+            c = counts[idx].item()
+            rank[i, curr:curr+c] = torch.arange(c, device=device)
+            curr += c
+            
     return phy2log, rank, logcnt
 
 
 def rebalance_experts_hierarchical(
     weight: torch.Tensor,
     num_physical_experts: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ):
+    """
+    Hierarchical rebalancing: Groups->Nodes, then Replicas->GPUs.
+    """
     num_layers, num_logical_experts = weight.shape
     assert num_logical_experts % num_groups == 0
     group_size = num_logical_experts // num_groups
     assert num_groups % num_nodes == 0
     groups_per_node = num_groups // num_nodes
     assert num_gpus % num_nodes == 0
     assert num_physical_experts % num_gpus == 0
     phy_experts_per_gpu = num_physical_experts // num_gpus
 
     def inverse(perm: torch.Tensor) -> torch.Tensor:
         inv = torch.empty_like(perm)
-        inv.scatter_(1, perm, torch.arange(perm.size(1), device=perm.device).expand(perm.shape))
+        inv.scatter_(
+            1,
+            perm,
+            torch.arange(perm.size(1), dtype=torch.int64,
+                         device=perm.device).expand(perm.shape),
+        )
         return inv
 
-    # Step 1: Groups -> Nodes
+    # Step 1: pack groups to nodes
     tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)
-    group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)
-
-    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +
-                torch.arange(group_size, device=weight.device)).flatten(-2)
+    
+    group_pack_index, group_rank_in_pack = balanced_packing(
+        tokens_per_group, num_nodes)
+        
+    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *
+                 group_size).unsqueeze(-1) +
+                torch.arange(group_size,
+                             dtype=torch.int64,
+                             device=group_pack_index.device)).flatten(-2)
     mlog2log = inverse(log2mlog)
 
-    # Step 2: Replicas -> Nodes
-    tokens_per_mlog = weight.gather(-1, mlog2log).view(-1, num_logical_experts // num_nodes)
-    phy2mlog, phyrank, mlogcnt = replicate_experts(tokens_per_mlog, num_physical_experts // num_nodes)
-
-    # Step 3: Replicas -> GPUs
+    # Step 2: construct redundant experts within nodes
+    tokens_per_mlog = weight.gather(-1, mlog2log).view(
+        -1, num_logical_experts // num_nodes)
+    phy2mlog, phyrank, mlogcnt = replicate_experts(
+        tokens_per_mlog, num_physical_experts // num_nodes)
+
+    # Step 3: pack physical_experts to GPUs
     tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
-    pack_index, rank_in_pack = balanced_packing(tokens_per_phy, num_gpus // num_nodes)
-
+    
+    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,
+                                                num_gpus // num_nodes)
+                                                
     phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
     pphy2phy = inverse(phy2pphy)
 
-    pphy2mlog = phy2mlog.gather(-1, pphy2phy)
-
-    # Adjust offsets
-    node_offsets = torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=weight.device)
-    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + node_offsets.view(1, -1, 1)).flatten(-2)
-
+    pphy2mlog = phy2mlog.gather(
+        -1, pphy2phy)
+    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(
+        0,
+        num_logical_experts,
+        num_logical_experts // num_nodes,
+        device=group_pack_index.device,
+    ).view(1, -1, 1)).flatten(-2)
     pphy2log = mlog2log.gather(-1, pphy2mlog)
     pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)
     logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)
-
     return pphy2log, pphyrank, logcnt
 
 
 def rebalance_experts(
     weight: torch.Tensor,
     num_replicas: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Entry point for expert-parallelism load balancer.
     """
     num_layers, num_logical_experts = weight.shape
-    weight = weight.float().cpu() # Ensure CPU
-
+    weight = weight.float().cpu()
+    
+    # Dispatch policy
     if num_groups % num_nodes == 0:
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, num_groups, num_nodes, num_gpus)
     else:
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, 1, 1, num_gpus)
-
+            
     num_redundant_experts = num_replicas - num_logical_experts
     maxlogcnt = num_redundant_experts + 1
-
-    log2phy = torch.full((num_layers, num_logical_experts, maxlogcnt), -1, dtype=torch.int64, device=logcnt.device)
-
+    
+    log2phy: torch.Tensor = torch.full(
+        (num_layers, num_logical_experts, maxlogcnt),
+        -1,
+        dtype=torch.int64,
+        device=logcnt.device,
+    )
+    
     log2phy.view(num_layers, -1).scatter_(
         -1,
         phy2log * maxlogcnt + phyrank,
-        torch.arange(num_replicas, dtype=torch.int64, device=log2phy.device).expand(num_layers, -1),
+        torch.arange(num_replicas, dtype=torch.int64,
+                     device=log2phy.device).expand(num_layers, -1),
     )
     return phy2log, log2phy, logcnt
 # EVOLVE-BLOCK-END
 
 
 # This part remains fixed (not evolved)
 def run_eplb(weight: torch.Tensor, num_replicas: int, num_groups: int,
              num_nodes: int, num_gpus: int):
     """Run the expert parallelism load balancer"""
     phy2log, log2phy, logcnt = rebalance_experts(
         weight, num_replicas, num_groups, num_nodes, num_gpus
     )
     return phy2log, log2phy, logcnt
 
 
 __all__ = ["rebalance_experts", "run_eplb"]