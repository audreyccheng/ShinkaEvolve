The user wants to improve the performance of the EPLB (Expert Parallelism Load Balancer) algorithm, specifically maximizing the combined score which involves `balancedness_score` and `speed_score`.
The current implementation uses `balanced_packing` with "Snake Sort" initialization and "Batched Refinement". The balancedness score is relatively low (0.31).

I will implement a **Batched Randomized Greedy LPT (Longest Processing Time)** initialization to replace the "Snake Sort".
Snake sort is a fixed pattern filling strategy that is suboptimal for general weight distributions. Greedy LPT is a standard approximation algorithm for multiprocessor scheduling (equivalent to this packing problem) and typically yields better balance. By randomizing the input weights (perturbation) and running multiple candidates in parallel (Batched Greedy), we can explore the solution space more effectively than a single deterministic pass or the rigid snake pattern.

The new implementation involves:
1.  Increasing `num_candidates` to 16 to explore more permutations.
2.  Replacing the sorting-and-snake-assignment logic with a loop that greedily assigns the heaviest remaining item to the least loaded valid pack.
3.  Keeping the batched refinement step to fine-tune the result.
4.  Ensuring the refinement uses the real weights, not the perturbed ones used for greedy decisions.

This change is targeted at `balanced_packing`.

<NAME>
randomized_greedy_packing
</NAME>

<DESCRIPTION>
Replace the "Snake Sort" initialization in `balanced_packing` with a "Batched Randomized Greedy LPT" initialization.
Greedy LPT (Longest Processing Time) is a superior heuristic for packing problems compared to fixed patterns like Snake Sort. By vectorizing it over multiple randomized candidates (perturbed weights), we explore the solution space effectively.
We also increase `num_candidates` to 16 to improve the probability of finding a good configuration.
The refinement step remains to polish the greedy result.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses Parallel Randomized Snake-Sort Initialization followed by Batched Refinement.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64,
                                  device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Parameters
    num_candidates = 8

    # 1. Expand and Perturb
    # [L, 1, N]
    w_expanded = weight.unsqueeze(1).expand(-1, num_candidates, -1)

    # Noise: [L, K, N]. K=0 is clean. K>0 has noise.
    noise = torch.rand(num_layers, num_candidates - 1, num_groups, device=device) * 0.02 + 0.99
    # Prefix with ones
    ones = torch.ones(num_layers, 1, num_groups, device=device)
    noise = torch.cat([ones, noise], dim=1)

    w_perturbed = w_expanded * noise

    # 2. Sort and Initialize (Snake Pattern)
    # [L, K, N]
    sorted_res = w_perturbed.sort(dim=-1, descending=True)
    sorted_indices = sorted_res.indices

    # Snake pattern mapping: Rank r -> Pack p
    # [N]
    ranks = torch.arange(num_groups, device=device)
    cycle_len = 2 * num_packs
    mod_ranks = ranks % cycle_len
    pack_ids_by_rank = torch.where(mod_ranks < num_packs, mod_ranks, cycle_len - 1 - mod_ranks)

    # Slot indices
    slot_indices = (ranks // cycle_len) * 2 + (mod_ranks >= num_packs).long()

    # Flatten destination index: p * C + s
    flat_dest_idx = pack_ids_by_rank * groups_per_pack + slot_indices

    # Assign items to packs based on rank
    # assignments: [L, K, P*G]
    assignments = torch.zeros((num_layers, num_candidates, num_groups), dtype=torch.int64, device=device)

    # Scatter sorted_indices into positions
    dest_expanded = flat_dest_idx.view(1, 1, -1).expand(num_layers, num_candidates, -1)
    assignments.scatter_(2, dest_expanded, sorted_indices)

    # Reshape to [L, K, P, G] -> Flatten to [B, P, G]
    B = num_layers * num_candidates
    assignments_flat = assignments.view(B, num_packs, groups_per_pack)

    # Compute initial pack weights
    # We need actual weights, not perturbed ones for accurate refinement
    w_real = weight.unsqueeze(1).expand(-1, num_candidates, -1).reshape(B, -1)

    # Pack weights: [B, P]
    w_assigned = torch.gather(w_real, 1, assignments_flat.view(B, -1)) # [B, N]
    w_assigned = w_assigned.view(B, num_packs, groups_per_pack)
    pack_weights = w_assigned.sum(dim=2) # [B, P]

    # 3. Batched Refinement
    refined_assignments = _refine_packing_batched(
        w_real, assignments_flat, pack_weights, num_packs, groups_per_pack, max_iters=20
    )

    # 4. Selection
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses Parallel Randomized Greedy LPT Initialization followed by Batched Refinement.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64,
                                  device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Parameters
    num_candidates = 16

    # 1. Expand and Perturb
    # [L, 1, N]
    w_expanded = weight.unsqueeze(1).expand(-1, num_candidates, -1)

    # Noise: [L, K, N]. K=0 is clean. K>0 has noise.
    noise = torch.rand(num_layers, num_candidates - 1, num_groups, device=device) * 0.02 + 0.99
    # Prefix with ones
    ones = torch.ones(num_layers, 1, num_groups, device=device)
    noise = torch.cat([ones, noise], dim=1)

    w_perturbed = w_expanded * noise

    # 2. Sort for Greedy LPT
    # [L, K, N]
    sorted_res = w_perturbed.sort(dim=-1, descending=True)
    sorted_indices = sorted_res.indices
    sorted_weights = sorted_res.values

    # 3. Batched Greedy Assignment
    B = num_layers * num_candidates

    # Reshape for batch processing
    sorted_indices_flat = sorted_indices.view(B, num_groups)
    sorted_weights_flat = sorted_weights.view(B, num_groups)

    # Track pack state: [B, P]
    pack_weights = torch.zeros((B, num_packs), device=device, dtype=weight.dtype)
    pack_counts = torch.zeros((B, num_packs), device=device, dtype=torch.int64)

    # Result buffer: [B, P, G] - flattened to 1D for efficient scatter
    assignments_flat = torch.zeros((B, num_packs, groups_per_pack), dtype=torch.int64, device=device)
    assignments_flat_view = assignments_flat.view(-1)

    # Helper for indexing
    batch_offsets = torch.arange(B, device=device) * (num_packs * groups_per_pack)

    for i in range(num_groups):
        # Current item to place
        item_idx = sorted_indices_flat[:, i] # [B]
        item_w = sorted_weights_flat[:, i]   # [B]

        # Mask full packs
        is_full = pack_counts >= groups_per_pack
        # We want to find min weight among non-full packs
        w_masked = pack_weights.clone()
        w_masked[is_full] = float('inf')

        # Greedy choice
        best_pack = torch.argmin(w_masked, dim=1) # [B]

        # Determine slot index
        # slot = pack_counts[b, best_pack[b]]
        chosen_slots = pack_counts.gather(1, best_pack.unsqueeze(1)).squeeze(1) # [B]

        # Calculate flat index into assignments_flat
        # idx = b * (P*G) + pack * G + slot
        flat_idx = batch_offsets + best_pack * groups_per_pack + chosen_slots

        # Assign
        assignments_flat_view[flat_idx] = item_idx

        # Update weights and counts
        pack_weights.scatter_add_(1, best_pack.unsqueeze(1), item_w.unsqueeze(1))
        pack_counts.scatter_add_(1, best_pack.unsqueeze(1), torch.ones((B, 1), device=device, dtype=torch.int64))

    # 4. Prepare for Refinement
    # Compute REAL pack weights (since greedy used perturbed weights)
    w_real = weight.unsqueeze(1).expand(-1, num_candidates, -1).reshape(B, -1)

    # Gather real weights for assignments
    # assignments_flat is [B, P, G]
    w_assigned = torch.gather(w_real, 1, assignments_flat.view(B, -1))
    w_assigned = w_assigned.view(B, num_packs, groups_per_pack)
    pack_weights_real = w_assigned.sum(dim=2) # [B, P]

    # 5. Batched Refinement
    refined_assignments = _refine_packing_batched(
        w_real, assignments_flat, pack_weights_real, num_packs, groups_per_pack, max_iters=50
    )

    # 6. Selection
>>>>>>> REPLACE

</DIFF>