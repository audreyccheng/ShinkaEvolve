<NAME>
refine_all_candidates_with_noise_schedule
</NAME>

<DESCRIPTION>
This edit improves the `balanced_packing` function by removing the pre-selection step ("Generate Many, Refine Few") and instead refining *all* generated candidates. The "Speed Score" of 1.0 suggests ample compute budget, while the "Balancedness" has been stuck. Refining all 64 candidates significantly increases the diversity of local optima explored, preventing the algorithm from discarding a candidate that is poor initially but reachable to a global optimum. Additionally, a linear noise schedule (`0.0` to `0.15`) is introduced to systematically explore the trade-off between greedy (LPT) and randomized initialization, rather than using uniform noise for all. The refinement iterations are increased to 100 to ensure convergence.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int,
                     num_restarts: int = 4) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using Parallel Randomized Greedy LPT
    followed by Variance-Minimizing Refinement on the best candidates.

    Implements a "Generate Many, Refine Few" strategy to improve solution quality.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs
        num_restarts: parameter kept for API compatibility, but internally overriden
                      or used as the count of refined candidates.

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Optimization for trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64,
                                  device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Configuration for Generate Many, Refine Few
    # Generate many candidates (cheap) to explore the search space
    num_candidates = 64
    # Refine only the top K (expensive) to optimize the best ones
    num_refine = 4

    # 1. Parallel Randomized Greedy Initialization
    # We generate multiple candidate packings in parallel by sorting weights with random noise.

    # Expand weights for candidates: [L, R, G]
    B = num_layers * num_candidates

    # Create sort keys
    weights_expanded = weight.unsqueeze(1).expand(-1, num_candidates, -1) # [L, R, G]

    # Noise generation
    # First candidate is always deterministic LPT (clean weights)
    noise = torch.rand_like(weights_expanded) * 0.05
    mask = torch.ones(num_candidates, device=device)
    mask[0] = 0
    noise = noise * mask.view(1, num_candidates, 1)

    sort_keys = weights_expanded * (1.0 + noise)

    # Sort descending
    sorted_res = sort_keys.sort(dim=-1, descending=True)
    sorted_indices = sorted_res.indices # [L, R, G]

    # Flatten to [B, G]
    flat_indices = sorted_indices.view(B, num_groups)
    flat_weights_src = weights_expanded.reshape(B, num_groups)

    # Gather actual weights in sorted order: [B, G]
    flat_sorted_weights = torch.gather(flat_weights_src, 1, flat_indices)

    # Greedy Packing Loop
    pack_loads = torch.zeros((B, num_packs), dtype=weight.dtype, device=device)
    pack_counts = torch.zeros((B, num_packs), dtype=torch.int64, device=device)
    pack_assignment = torch.zeros((B, num_packs, groups_per_pack), dtype=torch.int64, device=device)

    batch_idx = torch.arange(B, device=device)
    inf_tensor = torch.tensor(float('inf'), device=device)

    # Vectorized loop over items
    for j in range(num_groups):
        w = flat_sorted_weights[:, j]
        item_id = flat_indices[:, j]

        # Identify valid packs (not full)
        is_full = (pack_counts >= groups_per_pack)

        # Find min load among valid packs
        masked_loads = torch.where(is_full, inf_tensor, pack_loads)
        best_pack = torch.argmin(masked_loads, dim=1) # [B]

        # Update
        pack_loads[batch_idx, best_pack] += w
        slot = pack_counts[batch_idx, best_pack]
        pack_assignment[batch_idx, best_pack, slot] = item_id
        pack_counts[batch_idx, best_pack] += 1

    # 2. Selection of Top Candidates for Refinement
    # Compute max load for each candidate
    candidate_max_loads, _ = pack_loads.max(dim=1) # [B]
    candidate_max_loads = candidate_max_loads.view(num_layers, num_candidates)

    # Select best num_refine candidates indices
    # We want indices that minimize max load
    _, topk_local_indices = torch.topk(candidate_max_loads, num_refine, dim=1, largest=False) # [L, K]

    # Convert local indices to global batch indices
    offsets = (torch.arange(num_layers, device=device) * num_candidates).unsqueeze(1)
    topk_global_indices = (offsets + topk_local_indices).flatten() # [L*K]

    # Gather data for refinement
    refined_weights = flat_weights_src[topk_global_indices]
    refined_assignment = pack_assignment[topk_global_indices]
    refined_loads = pack_loads[topk_global_indices]

    # 3. Refinement Loop (Batched on top candidates)
    refined_assignment = _refine_packing(
        refined_weights,
        refined_assignment,
        refined_loads,
        num_packs,
        groups_per_pack,
        max_iters=50
    )

    # 4. Final Selection
    # Recalculate max loads after refinement
    final_max_loads, _ = refined_loads.max(dim=1) # [L*K]
    final_max_loads = final_max_loads.view(num_layers, num_refine)

    best_local_idx = torch.argmin(final_max_loads, dim=1) # [L]

    # Calculate indices into the refined batch
    offsets_refined = torch.arange(num_layers, device=device) * num_refine
    best_refined_indices = offsets_refined + best_local_idx

    best_assignment = refined_assignment[best_refined_indices] # [L, M, C]

    # 5. Reconstruct Outputs
    pack_index = torch.empty_like(weight, dtype=torch.int64)
    rank_in_pack = torch.empty_like(weight, dtype=torch.int64)

    flat_assignment = best_assignment.view(num_layers, -1)

    grid = torch.arange(num_groups, device=device)
    p_vals = (grid // groups_per_pack).unsqueeze(0).expand(num_layers, -1)
    c_vals = (grid % groups_per_pack).unsqueeze(0).expand(num_layers, -1)

    pack_index.scatter_(1, flat_assignment, p_vals)
    rank_in_pack.scatter_(1, flat_assignment, c_vals)

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int,
                     num_restarts: int = 4) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using Parallel Randomized Greedy LPT
    followed by Variance-Minimizing Refinement on ALL candidates.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs
        num_restarts: parameter kept for API compatibility.

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Optimization for trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64,
                                  device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Number of parallel candidates to explore
    num_candidates = 64

    # 1. Parallel Randomized Greedy Initialization
    # We generate multiple candidate packings in parallel by sorting weights with random noise.

    # Expand weights for candidates: [L, R, G]
    B = num_layers * num_candidates

    # Create sort keys
    weights_expanded = weight.unsqueeze(1).expand(-1, num_candidates, -1) # [L, R, G]

    # Noise generation with Linear Schedule
    # candidate 0: 0.0 noise (Pure LPT)
    # candidate N-1: max_noise
    max_noise = 0.15
    noise_levels = torch.linspace(0, max_noise, steps=num_candidates, device=device)
    # [1, R, 1]
    noise_levels = noise_levels.view(1, num_candidates, 1)

    noise = torch.rand_like(weights_expanded) * noise_levels

    sort_keys = weights_expanded * (1.0 + noise)

    # Sort descending
    sorted_res = sort_keys.sort(dim=-1, descending=True)
    sorted_indices = sorted_res.indices # [L, R, G]

    # Flatten to [B, G]
    flat_indices = sorted_indices.view(B, num_groups)
    flat_weights_src = weights_expanded.reshape(B, num_groups)

    # Gather actual weights in sorted order: [B, G]
    flat_sorted_weights = torch.gather(flat_weights_src, 1, flat_indices)

    # Greedy Packing Loop
    pack_loads = torch.zeros((B, num_packs), dtype=weight.dtype, device=device)
    pack_counts = torch.zeros((B, num_packs), dtype=torch.int64, device=device)
    pack_assignment = torch.zeros((B, num_packs, groups_per_pack), dtype=torch.int64, device=device)

    batch_idx = torch.arange(B, device=device)
    inf_tensor = torch.tensor(float('inf'), device=device)

    # Vectorized loop over items
    for j in range(num_groups):
        w = flat_sorted_weights[:, j]
        item_id = flat_indices[:, j]

        # Identify valid packs (not full)
        is_full = (pack_counts >= groups_per_pack)

        # Find min load among valid packs
        masked_loads = torch.where(is_full, inf_tensor, pack_loads)
        best_pack = torch.argmin(masked_loads, dim=1) # [B]

        # Update
        pack_loads[batch_idx, best_pack] += w
        slot = pack_counts[batch_idx, best_pack]
        pack_assignment[batch_idx, best_pack, slot] = item_id
        pack_counts[batch_idx, best_pack] += 1

    # 2. Refinement Loop (Batched on ALL candidates)
    # Now we refine all candidates, not just top-k.
    refined_assignment = _refine_packing(
        flat_weights_src,
        pack_assignment,
        pack_loads,
        num_packs,
        groups_per_pack,
        max_iters=100
    )

    # 3. Final Selection
    # Recalculate max loads after refinement
    final_max_loads, _ = pack_loads.max(dim=1) # [L*K]
    final_max_loads = final_max_loads.view(num_layers, num_candidates)

    best_local_idx = torch.argmin(final_max_loads, dim=1) # [L]

    # Calculate indices into the refined batch
    offsets_refined = torch.arange(num_layers, device=device) * num_candidates
    best_refined_indices = offsets_refined + best_local_idx

    best_assignment = refined_assignment[best_refined_indices] # [L, M, C]

    # 4. Reconstruct Outputs
    pack_index = torch.empty_like(weight, dtype=torch.int64)
    rank_in_pack = torch.empty_like(weight, dtype=torch.int64)

    flat_assignment = best_assignment.view(num_layers, -1)

    grid = torch.arange(num_groups, device=device)
    p_vals = (grid // groups_per_pack).unsqueeze(0).expand(num_layers, -1)
    c_vals = (grid % groups_per_pack).unsqueeze(0).expand(num_layers, -1)

    pack_index.scatter_(1, flat_assignment, p_vals)
    rank_in_pack.scatter_(1, flat_assignment, c_vals)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>