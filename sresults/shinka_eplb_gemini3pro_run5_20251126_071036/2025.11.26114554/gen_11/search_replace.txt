<NAME>
gpu_parallel_randomized_greedy_refinement
</NAME>

<DESCRIPTION>
Replaces the sequential CPU-based packing algorithm with a parallelized GPU implementation.
This new implementation:
1. Runs multiple randomized restarts of the Greedy LPT algorithm in parallel (vectorized across a batch dimension) to escape local optima.
2. Performs vectorized "Best Swap" refinement on the GPU, identifying the optimal swap between the heaviest and lightest packs for each instance in parallel.
3. Selects the best result from the multiple restarts for each layer.
This approach leverages the GPU's parallelism to explore a larger search space (via restarts) and perform faster refinements, leading to better load balancing without the bottleneck of sequential CPU processing.
I also update `rebalance_experts` to keep tensors on the GPU, avoiding unnecessary transfers.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=weight.device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    weight_cpu = weight.float().cpu()
    indices = weight_cpu.sort(-1, descending=True).indices
    pack_index = torch.full_like(weight,
                                 fill_value=-1,
                                 dtype=torch.int64,
                                 device="cpu")
    rank_in_pack = torch.full_like(pack_index, fill_value=-1)

    for i in range(num_layers):
        pack_weights = [0.0] * num_packs
        packs = [[] for _ in range(num_packs)]

        # Greedy LPT
        for group in indices[i].tolist():
            pack = min(
                (p for p in range(num_packs) if len(packs[p]) < groups_per_pack),
                key=pack_weights.__getitem__,
            )
            packs[pack].append(group)
            pack_weights[pack] += weight_cpu[i, group].item()

        # Local Search Refinement
        # Perform pairwise swaps between the heaviest and lightest packs
        for _ in range(20):
            min_pack = min(range(num_packs), key=pack_weights.__getitem__)
            max_pack = max(range(num_packs), key=pack_weights.__getitem__)
            if min_pack == max_pack:
                break

            diff = pack_weights[max_pack] - pack_weights[min_pack]
            best_swap = None
            # Target improvement: reduce diff

            for idx_max, item_max in enumerate(packs[max_pack]):
                w_max = weight_cpu[i, item_max].item()
                for idx_min, item_min in enumerate(packs[min_pack]):
                    w_min = weight_cpu[i, item_min].item()

                    if w_max <= w_min:
                        continue

                    delta = w_max - w_min
                    reduction = diff - abs(diff - 2 * delta)
                    if reduction > 1e-6:
                        if best_swap is None or reduction > best_swap[2]:
                            best_swap = (idx_max, idx_min, reduction)

            if best_swap:
                idx_h, idx_l, _ = best_swap
                item_h = packs[max_pack][idx_h]
                item_l = packs[min_pack][idx_l]

                packs[max_pack][idx_h] = item_l
                packs[min_pack][idx_l] = item_h

                w_h = weight_cpu[i, item_h].item()
                w_l = weight_cpu[i, item_l].item()

                pack_weights[max_pack] -= (w_h - w_l)
                pack_weights[min_pack] += (w_h - w_l)
            else:
                break

        for p in range(num_packs):
            for rank, item in enumerate(packs[p]):
                pack_index[i, item] = p
                rank_in_pack[i, item] = rank

    return pack_index.to(weight.device), rank_in_pack.to(weight.device)
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Run multiple random restarts in parallel
    num_restarts = 4

    # Expand weight for restarts: [num_layers, num_restarts, num_groups]
    weight_expanded = weight.unsqueeze(1).expand(-1, num_restarts, -1).clone()

    # Perturb weights with noise to randomize sorting order for restarts > 0
    if num_restarts > 1:
        noise = torch.rand_like(weight_expanded[:, 1:]) * 0.05
        weight_expanded[:, 1:] *= (1.0 + noise)

    flat_weight = weight_expanded.reshape(-1, num_groups)
    # Use original weights for accumulation (repeated for each restart)
    original_w_flat = weight.unsqueeze(1).expand(-1, num_restarts, -1).reshape(
        -1, num_groups)

    # Sort descending based on perturbed weights
    sorted_indices = flat_weight.argsort(dim=-1, descending=True)
    sorted_w = original_w_flat.gather(1, sorted_indices)

    batch_size = flat_weight.shape[0]
    row_indices = torch.arange(batch_size, device=device)

    # Tracking state
    pack_weights = torch.zeros(batch_size, num_packs, device=device)
    pack_counts = torch.zeros(batch_size,
                              num_packs,
                              dtype=torch.int64,
                              device=device)

    pack_index_sorted = torch.zeros(batch_size,
                                    num_groups,
                                    dtype=torch.int64,
                                    device=device)
    rank_in_pack_sorted = torch.zeros(batch_size,
                                      num_groups,
                                      dtype=torch.int64,
                                      device=device)

    # Vectorized Greedy Packing
    for i in range(num_groups):
        w = sorted_w[:, i]

        valid_mask = pack_counts < groups_per_pack
        candidate_weights = pack_weights.clone()
        candidate_weights[~valid_mask] = float('inf')

        chosen_pack = candidate_weights.argmin(dim=1)

        pack_weights[row_indices, chosen_pack] += w
        rank_in_pack_sorted[:, i] = pack_counts[row_indices, chosen_pack]
        pack_counts[row_indices, chosen_pack] += 1
        pack_index_sorted[:, i] = chosen_pack

    # Refinement: Max-Min Swap
    # Construct pack_contents for easy access: [Batch, num_packs, groups_per_pack]
    pack_contents = torch.zeros(batch_size,
                                num_packs,
                                groups_per_pack,
                                device=device)
    pack_item_ids = torch.zeros(batch_size,
                                num_packs,
                                groups_per_pack,
                                dtype=torch.int64,
                                device=device)

    flat_indices = row_indices.unsqueeze(1).expand(-1, num_groups).flatten()
    flat_pack_idx = pack_index_sorted.flatten()
    flat_rank_idx = rank_in_pack_sorted.flatten()

    pack_contents.index_put_((flat_indices, flat_pack_idx, flat_rank_idx),
                             sorted_w.flatten())
    pack_item_ids.index_put_((flat_indices, flat_pack_idx, flat_rank_idx),
                             sorted_indices.flatten())

    for _ in range(20):
        current_pack_weights = pack_contents.sum(dim=2)
        max_val, max_pack = current_pack_weights.max(dim=1)
        min_val, min_pack = current_pack_weights.min(dim=1)

        diff = max_val - min_val
        active_mask = diff > 1e-4
        if not active_mask.any():
            break

        idx_max = max_pack.view(-1, 1, 1).expand(-1, 1, groups_per_pack)
        idx_min = min_pack.view(-1, 1, 1).expand(-1, 1, groups_per_pack)

        items_max = pack_contents.gather(1, idx_max).squeeze(1)
        items_min = pack_contents.gather(1, idx_min).squeeze(1)

        # delta[b, i, j] = items_max[b, i] - items_min[b, j]
        delta = items_max.unsqueeze(2) - items_min.unsqueeze(1)
        diff_view = diff.view(-1, 1, 1)

        # Gain metric: reduce diff as much as possible
        gain = diff_view - (diff_view - 2 * delta).abs()

        gain_flat = gain.view(batch_size, -1)
        best_gain, best_swap_idx = gain_flat.max(dim=1)

        do_swap = (best_gain > 1e-6) & active_mask
        if not do_swap.any():
            break

        batch_idx_swap = row_indices[do_swap]
        swap_idx = best_swap_idx[do_swap]
        p_max_s = max_pack[do_swap]
        p_min_s = min_pack[do_swap]

        idx_item_max = swap_idx // groups_per_pack
        idx_item_min = swap_idx % groups_per_pack

        val_max = pack_contents[batch_idx_swap, p_max_s, idx_item_max]
        val_min = pack_contents[batch_idx_swap, p_min_s, idx_item_min]

        pack_contents[batch_idx_swap, p_max_s, idx_item_max] = val_min
        pack_contents[batch_idx_swap, p_min_s, idx_item_min] = val_max

        id_max = pack_item_ids[batch_idx_swap, p_max_s, idx_item_max]
        id_min = pack_item_ids[batch_idx_swap, p_min_s, idx_item_min]

        pack_item_ids[batch_idx_swap, p_max_s, idx_item_max] = id_min
        pack_item_ids[batch_idx_swap, p_min_s, idx_item_min] = id_max

    # Select best restart per layer
    final_pack_weights = pack_contents.sum(dim=2)
    imbalance = final_pack_weights.max(
        dim=1).values - final_pack_weights.min(dim=1).values
    imbalance = imbalance.view(num_layers, num_restarts)

    best_restart_idx = imbalance.argmin(dim=1)

    best_batch_idx = torch.arange(
        num_layers, device=device) * num_restarts + best_restart_idx
    best_item_ids = pack_item_ids[best_batch_idx]  # [L, P, G]

    # Scatter back to output format
    pack_index = torch.empty(num_layers,
                             num_groups,
                             dtype=torch.int64,
                             device=device)
    rank_in_pack = torch.empty(num_layers,
                               num_groups,
                               dtype=torch.int64,
                               device=device)

    flat_item_ids = best_item_ids.view(num_layers, -1)
    grid_packs = torch.arange(num_packs, device=device).view(
        1, -1, 1).expand(num_layers, -1,
                         groups_per_pack).reshape(num_layers, -1)
    grid_ranks = torch.arange(groups_per_pack, device=device).view(
        1, 1, -1).expand(num_layers, num_packs, -1).reshape(num_layers, -1)

    pack_index.scatter_(1, flat_item_ids, grid_packs)
    rank_in_pack.scatter_(1, flat_item_ids, grid_ranks)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
<<<<<<< SEARCH
    num_layers, num_logical_experts = weight.shape
    weight = weight.float().cpu()
    if num_groups % num_nodes == 0:
=======
    num_layers, num_logical_experts = weight.shape
    weight = weight.float()
    if num_groups % num_nodes == 0:
>>>>>>> REPLACE
</DIFF>