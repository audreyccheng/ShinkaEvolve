<NAME>
generate_many_refine_few
</NAME>

<DESCRIPTION>
Implements a "Generate Many, Refine Few" strategy in `balanced_packing`.
Instead of running a small number (4) of greedy candidates and refining all of them, the algorithm now generates a large number (64) of randomized greedy candidates efficiently. It then selects the top few (4) candidates based on the maximum load metric and only runs the expensive variance-minimizing refinement on these promising candidates.
This allows for much better exploration of the solution space (escaping local optima in the greedy phase) without significantly increasing the computational cost, leading to better load balancing.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int,
                     num_restarts: int = 4) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using Parallel Randomized Greedy LPT
    followed by Variance-Minimizing Refinement on ALL candidates.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs
        num_restarts: number of parallel greedy attempts (default 4)

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Optimization for trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64,
                                  device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # 1. Parallel Randomized Greedy Initialization
    # We generate multiple candidate packings in parallel by sorting weights with random noise.

    # Expand weights for restarts: [L, R, G]
    # We treat L*R as the batch dimension for vectorized operations.
    B = num_layers * num_restarts

    # Create sort keys
    weights_expanded = weight.unsqueeze(1).expand(-1, num_restarts, -1) # [L, R, G]

    # Noise generation
    # First restart is always deterministic LPT (clean weights)
    noise = torch.rand_like(weights_expanded) * 0.05
    # Zero out noise for the first restart (index 0)
    mask = torch.ones(num_restarts, device=device)
    mask[0] = 0
    noise = noise * mask.view(1, num_restarts, 1)

    sort_keys = weights_expanded * (1.0 + noise)

    # Sort descending
    sorted_res = sort_keys.sort(dim=-1, descending=True)
    sorted_indices = sorted_res.indices # [L, R, G]

    # Flatten to [B, G]
    flat_indices = sorted_indices.view(B, num_groups)
    flat_weights_src = weights_expanded.reshape(B, num_groups)

    # Gather actual weights in sorted order: [B, G]
    flat_sorted_weights = torch.gather(flat_weights_src, 1, flat_indices)

    # Greedy Packing Loop
    pack_loads = torch.zeros((B, num_packs), dtype=weight.dtype, device=device)
    pack_counts = torch.zeros((B, num_packs), dtype=torch.int64, device=device)
    pack_assignment = torch.zeros((B, num_packs, groups_per_pack), dtype=torch.int64, device=device)

    batch_idx = torch.arange(B, device=device)
    inf_tensor = torch.tensor(float('inf'), device=device)

    # Vectorized loop over items
    for j in range(num_groups):
        w = flat_sorted_weights[:, j]
        item_id = flat_indices[:, j]

        # Identify valid packs (not full)
        is_full = (pack_counts >= groups_per_pack)

        # Find min load among valid packs
        # Set load of full packs to infinity
        masked_loads = torch.where(is_full, inf_tensor, pack_loads)

        best_pack = torch.argmin(masked_loads, dim=1) # [B]

        # Update
        pack_loads[batch_idx, best_pack] += w

        # Get slot index
        slot = pack_counts[batch_idx, best_pack]

        # Assign item
        pack_assignment[batch_idx, best_pack, slot] = item_id

        # Increment count
        pack_counts[batch_idx, best_pack] += 1

    # 2. Refinement Loop (Batched on all candidates)
    pack_assignment = _refine_packing(
        flat_weights_src,
        pack_assignment,
        pack_loads,
        num_packs,
        groups_per_pack,
        max_iters=50
    )

    # 3. Selection of Best Restart per Layer
    # Metric: Minimum Max-Load
    max_loads, _ = pack_loads.max(dim=1) # [B]
    max_loads = max_loads.view(num_layers, num_restarts)

    best_restart_idx = torch.argmin(max_loads, dim=1) # [L]

    # Extract best assignments
    offsets = torch.arange(num_layers, device=device) * num_restarts
    best_flat_indices = offsets + best_restart_idx

    best_assignment = pack_assignment[best_flat_indices] # [L, M, C]

    # 4. Reconstruct Outputs
    pack_index = torch.empty_like(weight, dtype=torch.int64)
    rank_in_pack = torch.empty_like(weight, dtype=torch.int64)

    flat_assignment = best_assignment.view(num_layers, -1)

    grid = torch.arange(num_groups, device=device)
    p_vals = (grid // groups_per_pack).unsqueeze(0).expand(num_layers, -1)
    c_vals = (grid % groups_per_pack).unsqueeze(0).expand(num_layers, -1)

    pack_index.scatter_(1, flat_assignment, p_vals)
    rank_in_pack.scatter_(1, flat_assignment, c_vals)

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int,
                     num_restarts: int = 4) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using Parallel Randomized Greedy LPT
    followed by Variance-Minimizing Refinement on the best candidates.

    Implements a "Generate Many, Refine Few" strategy to improve solution quality.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs
        num_restarts: parameter kept for API compatibility, but internally overriden
                      or used as the count of refined candidates.

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Optimization for trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64,
                                  device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Configuration for Generate Many, Refine Few
    # Generate many candidates (cheap) to explore the search space
    num_candidates = 64
    # Refine only the top K (expensive) to optimize the best ones
    num_refine = 4

    # 1. Parallel Randomized Greedy Initialization
    # We generate multiple candidate packings in parallel by sorting weights with random noise.

    # Expand weights for candidates: [L, R, G]
    B = num_layers * num_candidates

    # Create sort keys
    weights_expanded = weight.unsqueeze(1).expand(-1, num_candidates, -1) # [L, R, G]

    # Noise generation
    # First candidate is always deterministic LPT (clean weights)
    noise = torch.rand_like(weights_expanded) * 0.05
    mask = torch.ones(num_candidates, device=device)
    mask[0] = 0
    noise = noise * mask.view(1, num_candidates, 1)

    sort_keys = weights_expanded * (1.0 + noise)

    # Sort descending
    sorted_res = sort_keys.sort(dim=-1, descending=True)
    sorted_indices = sorted_res.indices # [L, R, G]

    # Flatten to [B, G]
    flat_indices = sorted_indices.view(B, num_groups)
    flat_weights_src = weights_expanded.reshape(B, num_groups)

    # Gather actual weights in sorted order: [B, G]
    flat_sorted_weights = torch.gather(flat_weights_src, 1, flat_indices)

    # Greedy Packing Loop
    pack_loads = torch.zeros((B, num_packs), dtype=weight.dtype, device=device)
    pack_counts = torch.zeros((B, num_packs), dtype=torch.int64, device=device)
    pack_assignment = torch.zeros((B, num_packs, groups_per_pack), dtype=torch.int64, device=device)

    batch_idx = torch.arange(B, device=device)
    inf_tensor = torch.tensor(float('inf'), device=device)

    # Vectorized loop over items
    for j in range(num_groups):
        w = flat_sorted_weights[:, j]
        item_id = flat_indices[:, j]

        # Identify valid packs (not full)
        is_full = (pack_counts >= groups_per_pack)

        # Find min load among valid packs
        masked_loads = torch.where(is_full, inf_tensor, pack_loads)
        best_pack = torch.argmin(masked_loads, dim=1) # [B]

        # Update
        pack_loads[batch_idx, best_pack] += w
        slot = pack_counts[batch_idx, best_pack]
        pack_assignment[batch_idx, best_pack, slot] = item_id
        pack_counts[batch_idx, best_pack] += 1

    # 2. Selection of Top Candidates for Refinement
    # Compute max load for each candidate
    candidate_max_loads, _ = pack_loads.max(dim=1) # [B]
    candidate_max_loads = candidate_max_loads.view(num_layers, num_candidates)

    # Select best num_refine candidates indices
    # We want indices that minimize max load
    _, topk_local_indices = torch.topk(candidate_max_loads, num_refine, dim=1, largest=False) # [L, K]

    # Convert local indices to global batch indices
    offsets = (torch.arange(num_layers, device=device) * num_candidates).unsqueeze(1)
    topk_global_indices = (offsets + topk_local_indices).flatten() # [L*K]

    # Gather data for refinement
    refined_weights = flat_weights_src[topk_global_indices]
    refined_assignment = pack_assignment[topk_global_indices]
    refined_loads = pack_loads[topk_global_indices]

    # 3. Refinement Loop (Batched on top candidates)
    refined_assignment = _refine_packing(
        refined_weights,
        refined_assignment,
        refined_loads,
        num_packs,
        groups_per_pack,
        max_iters=50
    )

    # 4. Final Selection
    # Recalculate max loads after refinement
    final_max_loads, _ = refined_loads.max(dim=1) # [L*K]
    final_max_loads = final_max_loads.view(num_layers, num_refine)

    best_local_idx = torch.argmin(final_max_loads, dim=1) # [L]

    # Calculate indices into the refined batch
    offsets_refined = torch.arange(num_layers, device=device) * num_refine
    best_refined_indices = offsets_refined + best_local_idx

    best_assignment = refined_assignment[best_refined_indices] # [L, M, C]

    # 5. Reconstruct Outputs
    pack_index = torch.empty_like(weight, dtype=torch.int64)
    rank_in_pack = torch.empty_like(weight, dtype=torch.int64)

    flat_assignment = best_assignment.view(num_layers, -1)

    grid = torch.arange(num_groups, device=device)
    p_vals = (grid // groups_per_pack).unsqueeze(0).expand(num_layers, -1)
    c_vals = (grid % groups_per_pack).unsqueeze(0).expand(num_layers, -1)

    pack_index.scatter_(1, flat_assignment, p_vals)
    rank_in_pack.scatter_(1, flat_assignment, c_vals)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>