<NAME>
vectorize_and_optimize
</NAME>

<DESCRIPTION>
1. Vectorize `balanced_packing` to use tensor operations instead of Python loops, enabling parallel processing of layers and GPU acceleration.
2. Optimize `replicate_experts` to incrementally update scores instead of recomputing division for the entire tensor in each iteration.
3. Remove forced CPU synchronization in `rebalance_experts` to allow end-to-end GPU execution.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=weight.device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    indices = weight.float().sort(-1, descending=True).indices.cpu()
    pack_index = torch.full_like(weight,
                                 fill_value=-1,
                                 dtype=torch.int64,
                                 device="cpu")
    rank_in_pack = torch.full_like(pack_index, fill_value=-1)
    for i in range(num_layers):
        pack_weights = [0] * num_packs
        pack_items = [0] * num_packs
        for group in indices[i]:
            pack = min(
                (i
                 for i in range(num_packs) if pack_items[i] < groups_per_pack),
                key=pack_weights.__getitem__,
            )
            assert pack_items[pack] < groups_per_pack
            pack_index[i, group] = pack
            rank_in_pack[i, group] = pack_items[pack]
            pack_weights[pack] += weight[i, group]
            pack_items[pack] += 1
    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Sort weights descending to apply LPT (Longest Processing Time) heuristic
    sorted_weight, sorted_indices = weight.float().sort(dim=-1, descending=True)

    # Track state for all layers in parallel
    pack_weights = torch.zeros(num_layers, num_packs, device=device)
    pack_counts = torch.zeros(num_layers,
                              num_packs,
                              dtype=torch.int64,
                              device=device)

    # Temporary storage for results in sorted order
    pack_index_sorted = torch.empty(num_layers,
                                    num_groups,
                                    dtype=torch.int64,
                                    device=device)
    rank_in_pack_sorted = torch.empty(num_layers,
                                      num_groups,
                                      dtype=torch.int64,
                                      device=device)

    # Vectorized greedy assignment
    # We assign items (columns in sorted_weight) one by one
    for i in range(num_groups):
        w = sorted_weight[:, i]  # [num_layers]

        # Identify valid packs (not full)
        valid_mask = pack_counts < groups_per_pack  # [num_layers, num_packs]

        # Find pack with min weight among valid packs
        # We add a large penalty to invalid packs to exclude them from argmin
        current_weights_with_penalty = pack_weights.clone()
        current_weights_with_penalty[~valid_mask] = float('inf')

        chosen_pack = torch.argmin(current_weights_with_penalty,
                                   dim=1)  # [num_layers]

        # Update pack weights and counts
        # scatter_add_ requires index to have same number of dimensions
        chosen_pack_idx = chosen_pack.unsqueeze(1)
        pack_weights.scatter_add_(1, chosen_pack_idx, w.unsqueeze(1))

        # Store rank (current count before increment)
        prev_count = pack_counts.gather(1, chosen_pack_idx).squeeze(1)
        rank_in_pack_sorted[:, i] = prev_count

        # Increment count
        pack_counts.scatter_add_(
            1, chosen_pack_idx,
            torch.ones(num_layers, 1, dtype=torch.int64, device=device))

        # Store pack assignment
        pack_index_sorted[:, i] = chosen_pack

    # Scatter results back to original order
    pack_index = torch.empty_like(pack_index_sorted)
    rank_in_pack = torch.empty_like(rank_in_pack_sorted)

    pack_index.scatter_(1, sorted_indices, pack_index_sorted)
    rank_in_pack.scatter_(1, sorted_indices, rank_in_pack_sorted)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
<<<<<<< SEARCH
    arangen = torch.arange(n, dtype=torch.int64, device=device)
    for i in range(num_log, num_phy):
        redundant_indices = (weight / logcnt).max(dim=-1).indices
        phy2log[:, i] = redundant_indices
        rank[:, i] = logcnt[arangen, redundant_indices]
        logcnt[arangen, redundant_indices] += 1
    return phy2log, rank, logcnt
=======
    arangen = torch.arange(n, dtype=torch.int64, device=device)
    # Pre-compute scores. Score = weight / count. Initially count is 1.
    current_scores = weight.float() / logcnt.float()

    for i in range(num_log, num_phy):
        redundant_indices = current_scores.argmax(dim=-1)
        phy2log[:, i] = redundant_indices
        rank[:, i] = logcnt[arangen, redundant_indices]

        # Update logcnt
        logcnt[arangen, redundant_indices] += 1

        # Incrementally update scores for modified experts
        # score = weight / new_count
        new_cnt = logcnt[arangen, redundant_indices].float()
        chosen_weight = weight[arangen, redundant_indices].float()
        current_scores[arangen, redundant_indices] = chosen_weight / new_cnt

    return phy2log, rank, logcnt
>>>>>>> REPLACE
<<<<<<< SEARCH
def rebalance_experts(
    weight: torch.Tensor,
    num_replicas: int,
    num_groups: int,
    num_nodes: int,
    num_gpus: int,
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Entry point for expert-parallelism load balancer.

    Parameters:
        weight: [layers, num_logical_experts], the load statistics for all
            logical experts
        num_replicas: number of physical experts, must be a multiple of
            `num_gpus`
        num_groups: number of expert groups
        num_nodes: number of server nodes, where the intra-node network
            (e.g, NVLink) is faster
        num_gpus: number of GPUs, must be a multiple of `num_nodes`

    Returns:
        physical_to_logical_map: [layers, num_replicas], the expert index of
            each replica
        logical_to_physical_map: [layers, num_logical_experts, X], the replica
            indices for each expert
        expert_count: [layers, num_logical_experts], number of physical
            replicas for each logical expert
    """
    num_layers, num_logical_experts = weight.shape
    weight = weight.float().cpu()
    if num_groups % num_nodes == 0:
=======
def rebalance_experts(
    weight: torch.Tensor,
    num_replicas: int,
    num_groups: int,
    num_nodes: int,
    num_gpus: int,
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Entry point for expert-parallelism load balancer.

    Parameters:
        weight: [layers, num_logical_experts], the load statistics for all
            logical experts
        num_replicas: number of physical experts, must be a multiple of
            `num_gpus`
        num_groups: number of expert groups
        num_nodes: number of server nodes, where the intra-node network
            (e.g, NVLink) is faster
        num_gpus: number of GPUs, must be a multiple of `num_nodes`

    Returns:
        physical_to_logical_map: [layers, num_replicas], the expert index of
            each replica
        logical_to_physical_map: [layers, num_logical_experts, X], the replica
            indices for each expert
        expert_count: [layers, num_logical_experts], number of physical
            replicas for each logical expert
    """
    num_layers, num_logical_experts = weight.shape
    weight = weight.float()
    if num_groups % num_nodes == 0:
>>>>>>> REPLACE
</DIFF>