<NAME>
massive_parallel_pruned_packing
</NAME>

<DESCRIPTION>
Implement a Massive Parallel Random Restarts with Pruned Refinement strategy.
1. Increase the number of random restarts from 8 to 2048 to significantly expand the search space for the initial packing. Use a linear noise schedule to perturb the sorting order.
2. Perform vectorized greedy packing on all 2048 candidates in parallel.
3. Compute the load imbalance for all candidates and prune the set, keeping only the top 64 most promising candidates (lowest imbalance) per layer.
4. Perform the computationally expensive L2-norm minimizing pairwise swap refinement (Top-4 vs Bottom-4) only on these selected 64 candidates.
5. Select the single best packing from the refined candidates.
This allows for massive exploration of the search space while concentrating the refinement compute budget on high-potential solutions.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Run multiple random restarts in parallel
    num_restarts = 8

    # Expand weight for restarts: [num_layers, num_restarts, num_groups]
    weight_expanded = weight.unsqueeze(1).expand(-1, num_restarts, -1).clone()

    # Perturb weights with varied noise levels to randomize sorting order
    # Use a linear spread of noise scales from 0.01 to 0.15
    if num_restarts > 1:
        noise_scales = torch.linspace(0.01,
                                      0.15,
                                      steps=num_restarts - 1,
                                      device=device)
        noise = torch.rand_like(weight_expanded[:, 1:]) * noise_scales.view(
            1, -1, 1)
        weight_expanded[:, 1:] *= (1.0 + noise)

    flat_weight = weight_expanded.reshape(-1, num_groups)
    # Use original weights for accumulation (repeated for each restart)
    original_w_flat = weight.unsqueeze(1).expand(-1, num_restarts, -1).reshape(
        -1, num_groups)

    # Sort descending based on perturbed weights
    sorted_indices = flat_weight.argsort(dim=-1, descending=True)
    sorted_w = original_w_flat.gather(1, sorted_indices)

    batch_size = flat_weight.shape[0]
    row_indices = torch.arange(batch_size, device=device)

    # Tracking state
    pack_weights = torch.zeros(batch_size, num_packs, device=device)
    pack_counts = torch.zeros(batch_size,
                              num_packs,
                              dtype=torch.int64,
                              device=device)

    pack_index_sorted = torch.zeros(batch_size,
                                    num_groups,
                                    dtype=torch.int64,
                                    device=device)
    rank_in_pack_sorted = torch.zeros(batch_size,
                                      num_groups,
                                      dtype=torch.int64,
                                      device=device)

    inf_tensor = torch.full((batch_size, num_packs),
                            float('inf'),
                            device=device)

    # Vectorized Greedy Packing
    for i in range(num_groups):
        w = sorted_w[:, i]

        valid_mask = pack_counts < groups_per_pack
        # Use torch.where to avoid cloning large tensors repeatedly
        candidate_weights = torch.where(valid_mask, pack_weights, inf_tensor)

        chosen_pack = candidate_weights.argmin(dim=1)

        pack_weights[row_indices, chosen_pack] += w
        rank_in_pack_sorted[:, i] = pack_counts[row_indices, chosen_pack]
        pack_counts[row_indices, chosen_pack] += 1
        pack_index_sorted[:, i] = chosen_pack

    # Refinement: Top-K / Bottom-K Swap minimizing L2 norm (Sum of Squared Weights)
    # Construct pack_contents for easy access: [Batch, num_packs, groups_per_pack]
    pack_contents = torch.zeros(batch_size,
                                num_packs,
                                groups_per_pack,
                                device=device)
    pack_item_ids = torch.zeros(batch_size,
                                num_packs,
                                groups_per_pack,
                                dtype=torch.int64,
                                device=device)

    flat_indices = row_indices.unsqueeze(1).expand(-1, num_groups).flatten()
    flat_pack_idx = pack_index_sorted.flatten()
    flat_rank_idx = rank_in_pack_sorted.flatten()

    pack_contents.index_put_((flat_indices, flat_pack_idx, flat_rank_idx),
                             sorted_w.flatten())
    pack_item_ids.index_put_((flat_indices, flat_pack_idx, flat_rank_idx),
                             sorted_indices.flatten())

    # Number of top/bottom packs to consider
    K = min(num_packs, 4)

    for _ in range(20):
        current_pack_weights = pack_contents.sum(dim=2)  # [B, P]

        # Identify Top-K and Bottom-K packs
        # vals_top: [B, K], idx_top: [B, K]
        vals_top, idx_top = torch.topk(current_pack_weights, k=K, largest=True)
        vals_bot, idx_bot = torch.topk(current_pack_weights,
                                       k=K,
                                       largest=False)

        # Check convergence (difference between heaviest and lightest)
        diff = vals_top[:, 0] - vals_bot[:, 0]
        active_mask = diff > 1e-4
        if not active_mask.any():
            break

        # Gather items from Top-K and Bottom-K packs
        # Expand indices for gathering: [B, K, G]
        gather_top = idx_top.unsqueeze(2).expand(-1, -1, groups_per_pack)
        gather_bot = idx_bot.unsqueeze(2).expand(-1, -1, groups_per_pack)

        items_top = pack_contents.gather(1, gather_top)  # [B, K, G]
        items_bot = pack_contents.gather(1, gather_bot)  # [B, K, G]

        # Compute delta for all pairs: w_u - w_v (u from Top, v from Bot)
        # Shape: [B, K, G, K, G]
        # items_top: [B, K, G, 1, 1]
        # items_bot: [B, 1, 1, K, G]
        delta = items_top.unsqueeze(3).unsqueeze(4) - items_bot.unsqueeze(
            1).unsqueeze(2)

        # Compute L2 gain: reduction in sum of squared weights
        # Gain = (W_A^2 + W_B^2) - ((W_A - d)^2 + (W_B + d)^2)
        #      = 2*d*(W_A - W_B - d)
        # W_A corresponds to vals_top, W_B to vals_bot
        W_A = vals_top.unsqueeze(2).unsqueeze(3).unsqueeze(4)
        W_B = vals_bot.unsqueeze(1).unsqueeze(2).unsqueeze(4)
        diff_packs = W_A - W_B

        gain = 2 * delta * (diff_packs - delta)

        # Mask invalid swaps:
        # 1. Must be distinct packs (if K is large enough to overlap)
        # idx_top: [B, K, 1, 1, 1], idx_bot: [B, 1, 1, K, 1]
        p_top_exp = idx_top.unsqueeze(2).unsqueeze(3).unsqueeze(4)
        p_bot_exp = idx_bot.unsqueeze(1).unsqueeze(2).unsqueeze(4)
        valid_pair = p_top_exp != p_bot_exp

        # 2. Only consider positive gain
        gain_mask = (gain > 1e-6) & valid_pair
        gain[~gain_mask] = -float('inf')

        # Find best swap per batch
        gain_flat = gain.view(batch_size, -1)
        best_gain, best_flat_idx = gain_flat.max(dim=1)

        do_swap = (best_gain > 1e-6) & active_mask
        if not do_swap.any():
            break

        # Decode indices
        batch_active = row_indices[do_swap]
        idx_tuple = best_flat_idx[do_swap]

        KG = K * groups_per_pack
        # Index in flattened [K*G, K*G]
        idx_pair_top = idx_tuple // KG
        idx_pair_bot = idx_tuple % KG

        k_t = idx_pair_top // groups_per_pack
        g_t = idx_pair_top % groups_per_pack
        k_b = idx_pair_bot // groups_per_pack
        g_b = idx_pair_bot % groups_per_pack

        # Retrieve pack indices
        p_top = idx_top[batch_active, k_t]
        p_bot = idx_bot[batch_active, k_b]

        # Perform swap
        val_top = pack_contents[batch_active, p_top, g_t]
        val_bot = pack_contents[batch_active, p_bot, g_b]

        pack_contents[batch_active, p_top, g_t] = val_bot
        pack_contents[batch_active, p_bot, g_b] = val_top

        id_top = pack_item_ids[batch_active, p_top, g_t]
        id_bot = pack_item_ids[batch_active, p_bot, g_b]

        pack_item_ids[batch_active, p_top, g_t] = id_bot
        pack_item_ids[batch_active, p_bot, g_b] = id_top

    # Select best restart per layer
    final_pack_weights = pack_contents.sum(dim=2)
    imbalance = final_pack_weights.max(
        dim=1).values - final_pack_weights.min(dim=1).values
    imbalance = imbalance.view(num_layers, num_restarts)

    best_restart_idx = imbalance.argmin(dim=1)

    best_batch_idx = torch.arange(
        num_layers, device=device) * num_restarts + best_restart_idx
    best_item_ids = pack_item_ids[best_batch_idx]  # [L, P, G]

    # Scatter back to output format
    pack_index = torch.empty(num_layers,
                             num_groups,
                             dtype=torch.int64,
                             device=device)
    rank_in_pack = torch.empty(num_layers,
                               num_groups,
                               dtype=torch.int64,
                               device=device)

    flat_item_ids = best_item_ids.view(num_layers, -1)
    grid_packs = torch.arange(num_packs, device=device).view(
        1, -1, 1).expand(num_layers, -1,
                         groups_per_pack).reshape(num_layers, -1)
    grid_ranks = torch.arange(groups_per_pack, device=device).view(
        1, 1, -1).expand(num_layers, num_packs, -1).reshape(num_layers, -1)

    pack_index.scatter_(1, flat_item_ids, grid_packs)
    rank_in_pack.scatter_(1, flat_item_ids, grid_ranks)

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Massive Parallel Restarts with Pruning
    # We explore a very large number of seeds but only refine the best ones
    num_restarts = 2048
    num_refined = 64

    # Expand weight for restarts: [num_layers, num_restarts, num_groups]
    weight_expanded = weight.unsqueeze(1).expand(-1, num_restarts, -1).clone()

    # Apply noise scales linearly from 0.0 to 0.20
    noise_scales = torch.linspace(0.0, 0.20, steps=num_restarts, device=device)
    noise = torch.rand_like(weight_expanded) * noise_scales.view(1, -1, 1)
    weight_expanded *= (1.0 + noise)

    flat_weight = weight_expanded.reshape(-1, num_groups)
    # Use original weights for accumulation (repeated for each restart)
    original_w_flat = weight.unsqueeze(1).expand(-1, num_restarts, -1).reshape(
        -1, num_groups)

    # Sort descending based on perturbed weights
    sorted_indices = flat_weight.argsort(dim=-1, descending=True)
    sorted_w = original_w_flat.gather(1, sorted_indices)

    batch_size = flat_weight.shape[0]
    row_indices = torch.arange(batch_size, device=device)

    # State tracking
    pack_weights = torch.zeros(batch_size, num_packs, device=device)
    pack_counts = torch.zeros(batch_size,
                              num_packs,
                              dtype=torch.int64,
                              device=device)

    # Record assignments for reconstruction
    pack_index_sorted = torch.zeros(batch_size,
                                    num_groups,
                                    dtype=torch.int64,
                                    device=device)
    rank_in_pack_sorted = torch.zeros(batch_size,
                                      num_groups,
                                      dtype=torch.int64,
                                      device=device)

    inf_tensor = torch.full((batch_size, num_packs),
                            float('inf'),
                            device=device)

    # Vectorized Greedy Packing
    for i in range(num_groups):
        w = sorted_w[:, i]

        valid_mask = pack_counts < groups_per_pack
        candidate_weights = torch.where(valid_mask, pack_weights, inf_tensor)
        chosen_pack = candidate_weights.argmin(dim=1)

        pack_weights[row_indices, chosen_pack] += w
        rank_in_pack_sorted[:, i] = pack_counts[row_indices, chosen_pack]
        pack_counts[row_indices, chosen_pack] += 1
        pack_index_sorted[:, i] = chosen_pack

    # Pruning: Select top-N candidates based on initial imbalance
    imbalance = pack_weights.max(dim=1).values - pack_weights.min(dim=1).values
    imbalance = imbalance.view(num_layers, num_restarts)

    _, best_restart_indices = imbalance.topk(num_refined, dim=1, largest=False)

    # Gather indices for the selected best restarts
    layer_offsets = (torch.arange(num_layers, device=device) *
                     num_restarts).unsqueeze(1)
    selected_flat_indices = (layer_offsets + best_restart_indices).flatten()

    # Subset the data for refinement
    sel_sorted_w = sorted_w[selected_flat_indices]
    sel_sorted_idx = sorted_indices[selected_flat_indices]
    sel_pack_idx = pack_index_sorted[selected_flat_indices]
    sel_rank_idx = rank_in_pack_sorted[selected_flat_indices]

    refined_batch_size = selected_flat_indices.size(0)
    ref_row_indices = torch.arange(refined_batch_size, device=device)

    # Construct pack_contents for refinement on the pruned set
    pack_contents = torch.zeros(refined_batch_size,
                                num_packs,
                                groups_per_pack,
                                device=device)
    pack_item_ids = torch.zeros(refined_batch_size,
                                num_packs,
                                groups_per_pack,
                                dtype=torch.int64,
                                device=device)

    flat_indices = ref_row_indices.unsqueeze(1).expand(-1,
                                                       num_groups).flatten()
    flat_pack_idx = sel_pack_idx.flatten()
    flat_rank_idx = sel_rank_idx.flatten()

    pack_contents.index_put_((flat_indices, flat_pack_idx, flat_rank_idx),
                             sel_sorted_w.flatten())
    pack_item_ids.index_put_((flat_indices, flat_pack_idx, flat_rank_idx),
                             sel_sorted_idx.flatten())

    # Refinement: Top-K / Bottom-K Swap on Pruned Candidates
    K = min(num_packs, 4)

    for _ in range(50):
        current_pack_weights = pack_contents.sum(dim=2)  # [B, P]

        vals_top, idx_top = torch.topk(current_pack_weights, k=K, largest=True)
        vals_bot, idx_bot = torch.topk(current_pack_weights,
                                       k=K,
                                       largest=False)

        diff = vals_top[:, 0] - vals_bot[:, 0]
        active_mask = diff > 1e-4
        if not active_mask.any():
            break

        gather_top = idx_top.unsqueeze(2).expand(-1, -1, groups_per_pack)
        gather_bot = idx_bot.unsqueeze(2).expand(-1, -1, groups_per_pack)

        items_top = pack_contents.gather(1, gather_top)  # [B, K, G]
        items_bot = pack_contents.gather(1, gather_bot)  # [B, K, G]

        delta = items_top.unsqueeze(3).unsqueeze(4) - items_bot.unsqueeze(
            1).unsqueeze(2)

        W_A = vals_top.unsqueeze(2).unsqueeze(3).unsqueeze(4)
        W_B = vals_bot.unsqueeze(1).unsqueeze(2).unsqueeze(4)
        diff_packs = W_A - W_B

        gain = 2 * delta * (diff_packs - delta)

        p_top_exp = idx_top.unsqueeze(2).unsqueeze(3).unsqueeze(4)
        p_bot_exp = idx_bot.unsqueeze(1).unsqueeze(2).unsqueeze(4)
        valid_pair = p_top_exp != p_bot_exp

        gain_mask = (gain > 1e-6) & valid_pair
        gain[~gain_mask] = -float('inf')

        gain_flat = gain.view(refined_batch_size, -1)
        best_gain, best_flat_idx = gain_flat.max(dim=1)

        do_swap = (best_gain > 1e-6) & active_mask
        if not do_swap.any():
            break

        batch_active = ref_row_indices[do_swap]
        idx_tuple = best_flat_idx[do_swap]

        KG = K * groups_per_pack
        idx_pair_top = idx_tuple // KG
        idx_pair_bot = idx_tuple % KG

        k_t = idx_pair_top // groups_per_pack
        g_t = idx_pair_top % groups_per_pack
        k_b = idx_pair_bot // groups_per_pack
        g_b = idx_pair_bot % groups_per_pack

        p_top = idx_top[batch_active, k_t]
        p_bot = idx_bot[batch_active, k_b]

        val_top = pack_contents[batch_active, p_top, g_t]
        val_bot = pack_contents[batch_active, p_bot, g_b]

        pack_contents[batch_active, p_top, g_t] = val_bot
        pack_contents[batch_active, p_bot, g_b] = val_top

        id_top = pack_item_ids[batch_active, p_top, g_t]
        id_bot = pack_item_ids[batch_active, p_bot, g_b]

        pack_item_ids[batch_active, p_top, g_t] = id_bot
        pack_item_ids[batch_active, p_bot, g_b] = id_top

    # Final Selection from Refined Set
    final_pack_weights = pack_contents.sum(dim=2)
    final_imbalance = final_pack_weights.max(
        dim=1).values - final_pack_weights.min(dim=1).values
    final_imbalance = final_imbalance.view(num_layers, num_refined)

    best_idx_in_refined = final_imbalance.argmin(dim=1)
    offsets = torch.arange(num_layers, device=device) * num_refined
    best_flat_idx = offsets + best_idx_in_refined

    best_item_ids = pack_item_ids[best_flat_idx]

    # Reconstruct Output
    pack_index = torch.empty(num_layers,
                             num_groups,
                             dtype=torch.int64,
                             device=device)
    rank_in_pack = torch.empty(num_layers,
                               num_groups,
                               dtype=torch.int64,
                               device=device)

    flat_item_ids = best_item_ids.view(num_layers, -1)
    grid_packs = torch.arange(num_packs, device=device).view(
        1, -1, 1).expand(num_layers, -1,
                         groups_per_pack).reshape(num_layers, -1)
    grid_ranks = torch.arange(groups_per_pack, device=device).view(
        1, 1, -1).expand(num_layers, num_packs, -1).reshape(num_layers, -1)

    pack_index.scatter_(1, flat_item_ids, grid_packs)
    rank_in_pack.scatter_(1, flat_item_ids, grid_ranks)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>