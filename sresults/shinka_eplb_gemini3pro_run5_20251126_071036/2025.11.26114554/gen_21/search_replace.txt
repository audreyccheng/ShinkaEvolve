<NAME>
parallel_randomized_packing_v2
</NAME>

<DESCRIPTION>
Implements a Parallel Randomized Greedy LPT approach for the balanced packing algorithm. Instead of a single deterministic greedy initialization, the algorithm now runs K=4 parallel trials for each layer. The first trial uses the original weights (deterministic), while the other three apply small random multiplicative noise (Â±2.5%) to the weights before sorting. This induces slight variations in the processing order, allowing the greedy heuristic to explore different starting points.

All K candidate packings are then refined simultaneously using a batched version of the variance-minimization local search (`_refine_packing`). This vectorization leverages the GPU's parallelism to process multiple candidates with minimal overhead. Finally, the candidate resulting in the minimum maximum load is selected for the final assignment. This approach improves load balancing by escaping local optima while maintaining high execution speed.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def _refine_packing(weights: torch.Tensor,
                    pack_indices: torch.Tensor,
                    pack_weights: torch.Tensor,
                    num_packs: int,
                    groups_per_pack: int,
                    max_iters: int = 50) -> torch.Tensor:
    """
    Iteratively refines the packing by swapping items between any two packs
    to reduce the variance of pack weights.

    Args:
        weights: [num_groups] weights of items
        pack_indices: [num_packs, groups_per_pack] indices of items in each pack
        pack_weights: [num_packs] total weight of each pack

    Returns:
        pack_indices: Refined item assignments
    """
    N = num_packs * groups_per_pack
    # Precompute mask for same-pack swaps
    pack_ids = torch.arange(num_packs, device=weights.device).repeat_interleave(groups_per_pack)
    # Mask [N, N] where pack_ids[i] == pack_ids[j]
    mask = pack_ids.view(-1, 1) == pack_ids.view(1, -1)

    for _ in range(max_iters):
        # Flatten indices to [N]
        flat_indices = pack_indices.view(-1)
        w_flat = weights[flat_indices]  # [N]
        l_flat = pack_weights[pack_ids] # [N] (broadcast pack weights to items)

        # Calculate Delta matrix: D[i, j] = w[i] - w[j]
        # Item i is at flat index i, currently in pack pack_ids[i]
        # Item j is at flat index j, currently in pack pack_ids[j]
        # We consider swapping item i into j's pack and item j into i's pack
        D = w_flat.view(-1, 1) - w_flat.view(1, -1)

        # Calculate Load Diff matrix: L_diff[i, j] = L[pack(j)] - L[pack(i)]
        L_diff = l_flat.view(1, -1) - l_flat.view(-1, 1)

        # Change in variance = 2 * D * (L_diff + D)
        # We want to minimize this change.
        change = D * (L_diff + D)

        # Mask invalid swaps (same pack)
        change.masked_fill_(mask, float('inf'))

        # Find best swap
        min_val, min_idx = torch.min(change.view(-1), 0)

        if min_val > -1e-6:
            break

        # Decode indices
        idx_i = min_idx // N
        idx_j = min_idx % N

        p1 = (idx_i // groups_per_pack).item()
        g1 = (idx_i % groups_per_pack).item()

        p2 = (idx_j // groups_per_pack).item()
        g2 = (idx_j % groups_per_pack).item()

        # Execute swap
        item1 = pack_indices[p1, g1].item()
        item2 = pack_indices[p2, g2].item()

        pack_indices[p1, g1] = item2
        pack_indices[p2, g2] = item1

        # Update weights
        delta = D[idx_i, idx_j].item()
        pack_weights[p1] -= delta
        pack_weights[p2] += delta

    return pack_indices


def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Implements a Greedy LPT initialization followed by Iterative Swapping refinement.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Optimization for trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64,
                                  device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Pre-allocate outputs
    pack_index = torch.full_like(weight, -1, dtype=torch.int64)
    rank_in_pack = torch.full_like(weight, -1, dtype=torch.int64)

    # Sort weights for Greedy LPT (Longest Processing Time) initialization
    # Sorting descending helps placing largest items first
    sorted_res = weight.sort(dim=-1, descending=True)
    sorted_indices = sorted_res.indices
    sorted_weights = sorted_res.values

    # Pre-compute grid indices for scattering results later
    p_ids_grid = torch.arange(num_packs, device=device).unsqueeze(1).expand(-1, groups_per_pack)
    r_ids_grid = torch.arange(groups_per_pack, device=device).unsqueeze(0).expand(num_packs, -1)
    flat_p_ids = p_ids_grid.flatten()
    flat_r_ids = r_ids_grid.flatten()

    for i in range(num_layers):
        # 1. Greedy Initialization
        current_pack_weights = torch.zeros(num_packs, device=device, dtype=weight.dtype)
        current_pack_counts = torch.zeros(num_packs, device=device, dtype=torch.int64)

        # Matrix to store assignments: [Pack, Slot] -> Item Index
        pack_assignment = torch.zeros((num_packs, groups_per_pack),
                                      dtype=torch.int64, device=device)

        layer_indices = sorted_indices[i]
        layer_vals = sorted_weights[i]

        for j in range(num_groups):
            w = layer_vals[j]
            item_idx = layer_indices[j]

            # Vectorized greedy choice: choose the valid pack with min weight
            # Mask out full packs by setting their weight to infinity
            is_full = current_pack_counts >= groups_per_pack
            masked_weights = torch.where(is_full, float('inf'), current_pack_weights)
            best_pack = torch.argmin(masked_weights)

            # Assign
            slot = current_pack_counts[best_pack]
            pack_assignment[best_pack, slot] = item_idx
            current_pack_weights[best_pack] += w
            current_pack_counts[best_pack] += 1

        # 2. Iterative Refinement (Swapping)
        pack_assignment = _refine_packing(
            weight[i], pack_assignment, current_pack_weights,
            num_packs, groups_per_pack
        )

        # 3. Store results
        flat_items = pack_assignment.flatten()
        pack_index[i, flat_items] = flat_p_ids
        rank_in_pack[i, flat_items] = flat_r_ids

    return pack_index, rank_in_pack
=======
def _refine_packing(weights: torch.Tensor,
                    pack_indices: torch.Tensor,
                    pack_weights: torch.Tensor,
                    num_packs: int,
                    groups_per_pack: int,
                    max_iters: int = 50) -> torch.Tensor:
    """
    Iteratively refines the packing by swapping items between any two packs
    to reduce the variance of pack weights. Supports batched candidates.

    Args:
        weights: [num_groups] weights of items
        pack_indices: [batch, num_packs, groups_per_pack] indices of items
        pack_weights: [batch, num_packs] total weight of each pack

    Returns:
        pack_indices: Refined item assignments
    """
    batch_size = pack_indices.shape[0]
    N = num_packs * groups_per_pack
    device = weights.device

    pack_ids = torch.arange(num_packs, device=device).repeat_interleave(groups_per_pack)
    # Mask [B, N, N]
    mask = (pack_ids.view(-1, 1) == pack_ids.view(1, -1)).unsqueeze(0).expand(batch_size, -1, -1)

    batch_range = torch.arange(batch_size, device=device)

    for _ in range(max_iters):
        # Flatten indices to [B, N]
        flat_indices = pack_indices.view(batch_size, -1)
        w_flat = weights[flat_indices]  # [B, N]
        l_flat = pack_weights[:, pack_ids] # [B, N]

        # Calculate Delta matrix: D[b, i, j] = w[b, i] - w[b, j]
        # [B, N, N]
        D = w_flat.unsqueeze(2) - w_flat.unsqueeze(1)

        # Calculate Load Diff matrix: L_diff[b, i, j] = L[b, pack(j)] - L[b, pack(i)]
        # [B, N, N]
        L_diff = l_flat.unsqueeze(1) - l_flat.unsqueeze(2)

        # Change in variance = 2 * D * (L_diff + D)
        change = D * (L_diff + D)

        # Mask invalid swaps (same pack)
        change.masked_fill_(mask, float('inf'))

        # Find best swap per batch
        min_val, min_idx = torch.min(change.view(batch_size, -1), dim=1)

        # Determine which batches have valid swaps
        update_mask = min_val < -1e-6
        if not update_mask.any():
            break

        # Indices for active batches
        active_b = batch_range[update_mask]
        idx = min_idx[update_mask]

        idx_i = idx // N
        idx_j = idx % N

        p1 = idx_i // groups_per_pack
        g1 = idx_i % groups_per_pack
        p2 = idx_j // groups_per_pack
        g2 = idx_j % groups_per_pack

        # Execute swap for active batches
        item1 = pack_indices[active_b, p1, g1]
        item2 = pack_indices[active_b, p2, g2]

        pack_indices[active_b, p1, g1] = item2
        pack_indices[active_b, p2, g2] = item1

        # Update weights
        delta = D[active_b, idx_i, idx_j]
        pack_weights[active_b, p1] -= delta
        pack_weights[active_b, p2] += delta

    return pack_indices


def balanced_packing(weight: torch.Tensor,
                     num_packs: int,
                     num_restarts: int = 4) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Implements a Parallel Randomized Greedy LPT initialization followed by
    Iterative Swapping refinement.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs
        num_restarts: number of parallel candidates to evaluate

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Optimization for trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64,
                                  device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Pre-allocate outputs
    pack_index = torch.full_like(weight, -1, dtype=torch.int64)
    rank_in_pack = torch.full_like(weight, -1, dtype=torch.int64)

    # Pre-compute grid indices for scattering results later
    p_ids_grid = torch.arange(num_packs, device=device).unsqueeze(1).expand(-1, groups_per_pack)
    r_ids_grid = torch.arange(groups_per_pack, device=device).unsqueeze(0).expand(num_packs, -1)
    flat_p_ids = p_ids_grid.flatten()
    flat_r_ids = r_ids_grid.flatten()

    batch_range = torch.arange(num_restarts, device=device)

    for i in range(num_layers):
        w_layer = weight[i]

        # Create candidates: [K, G]
        w_candidates = w_layer.unsqueeze(0).expand(num_restarts, -1).clone()
        if num_restarts > 1:
            # Noise +/- 2.5%
            noise = (torch.rand((num_restarts - 1, num_groups), device=device) * 0.05) + 0.975
            w_candidates[1:] *= noise

        # Sort candidates
        # Sorting is based on noisy weights to explore different orders
        sorted_res = w_candidates.sort(dim=-1, descending=True)
        sorted_indices = sorted_res.indices

        # Use original weights for allocation logic to ensure correctness
        # [K, G]
        sorted_vals = w_layer[sorted_indices]

        # 1. Greedy Initialization (Parallel)
        pack_weights = torch.zeros(num_restarts, num_packs, device=device, dtype=weight.dtype)
        pack_counts = torch.zeros(num_restarts, num_packs, device=device, dtype=torch.int64)

        # [K, num_packs, groups_per_pack]
        assignment = torch.zeros((num_restarts, num_packs, groups_per_pack),
                                      dtype=torch.int64, device=device)

        for j in range(num_groups):
            w = sorted_vals[:, j] # [K]
            item_idx = sorted_indices[:, j] # [K]

            # Vectorized greedy choice
            is_full = pack_counts >= groups_per_pack
            masked_weights = torch.where(is_full, float('inf'), pack_weights)
            best_pack = torch.argmin(masked_weights, dim=1) # [K]

            # Assign
            slot = pack_counts[batch_range, best_pack]
            assignment[batch_range, best_pack, slot] = item_idx

            pack_weights[batch_range, best_pack] += w
            pack_counts[batch_range, best_pack] += 1

        # 2. Iterative Refinement (Swapping) - Batched
        assignment = _refine_packing(
            w_layer, assignment, pack_weights,
            num_packs, groups_per_pack
        )

        # 3. Select best candidate
        # Minimize max load
        max_loads = pack_weights.max(dim=1).values
        best_k = torch.argmin(max_loads)

        best_assignment = assignment[best_k] # [P, G]

        # 4. Store results
        flat_items = best_assignment.flatten()
        pack_index[i, flat_items] = flat_p_ids
        rank_in_pack[i, flat_items] = flat_r_ids

    return pack_index, rank_in_pack
>>>>>>> REPLACE

</DIFF>