# EVOLVE-BLOCK-START
"""
Expert parallelism load balancer (EPLB) for vLLM.

This module implements the core rearrangement algorithm.

The rearrangement algorithm is adapted from
[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).

Please find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example
on how the EPLB algorithm works.
"""

import torch


def _refine_packing(weights: torch.Tensor,
                    pack_indices: torch.Tensor,
                    pack_weights: torch.Tensor,
                    num_packs: int,
                    groups_per_pack: int,
                    max_iters: int = 20) -> torch.Tensor:
    """
    Iteratively refines the packing by swapping items between any pair of packs
    to minimize the sum of squared pack weights (variance).
    
    Uses fully vectorized cost calculation for all pairs.
    """
    device = weights.device
    
    # Identify the pack ID for every position in the flattened pack_indices
    # [num_packs * groups_per_pack]
    # e.g. [0, 0, 1, 1, ...]
    p_ids_flat = torch.arange(num_packs, device=device).repeat_interleave(groups_per_pack)
    
    # Mask to ignore swaps within the same pack
    # [TotalItems, TotalItems]
    same_pack_mask = p_ids_flat.view(-1, 1) == p_ids_flat.view(1, -1)

    for _ in range(max_iters):
        # Current item weights: [P, G]
        item_weights = weights[pack_indices]
        w_flat = item_weights.view(-1)
        
        # Current pack weights: [P] -> mapped to items -> [TotalItems]
        L_flat = pack_weights[p_ids_flat]
        
        # Calculate D[i, j] = w[j] - w[i]
        # This is the weight change for Pack(i) if we swap Item(i) with Item(j)
        # Pack(i) new = L(i) - w(i) + w(j) = L(i) + D[i,j]
        D = w_flat.view(1, -1) - w_flat.view(-1, 1)
        
        # Calculate L_diff[i, j] = L(i) - L(j)
        L_diff = L_flat.view(-1, 1) - L_flat.view(1, -1)
        
        # The change in sum of squares (variance) for the system if swap (i, j) happens:
        # Change = (L(i) + D)^2 + (L(j) - D)^2 - L(i)^2 - L(j)^2
        #        = 2*D^2 + 2*D*L(i) - 2*D*L(j)
        #        = 2*D * (D + L(i) - L(j))
        #        = 2*D * (D + L_diff)
        # We want to minimize this.
        
        cost_change = D * (D + L_diff)
        
        # Apply mask
        cost_change.masked_fill_(same_pack_mask, float('inf'))
        
        # Find best swap
        min_val, min_idx = torch.min(cost_change.view(-1), dim=0)
        
        # Threshold for improvement (accounting for float precision)
        if min_val > -1e-5:
            break
            
        # Decode indices
        total_items = num_packs * groups_per_pack
        idx_i = min_idx // total_items # Item currently in p1
        idx_j = min_idx % total_items  # Item currently in p2
        
        p1 = (idx_i // groups_per_pack).item()
        g1 = (idx_i % groups_per_pack).item()
        p2 = (idx_j // groups_per_pack).item()
        g2 = (idx_j % groups_per_pack).item()
        
        # Apply swap
        item1 = pack_indices[p1, g1].item()
        item2 = pack_indices[p2, g2].item()
        
        pack_indices[p1, g1] = item2
        pack_indices[p2, g2] = item1
        
        # Update weights
        delta = D[idx_i, idx_j].item()
        pack_weights[p1] += delta
        pack_weights[p2] -= delta

    return pack_indices


def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses Parallel Randomized ZigZag Initialization followed by Variance Reduction.

    Parameters:
        weight: [layers, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [layers, n], the pack index of each item
        rank_in_pack: [layers, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64,
                                  device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # --- Parallel Randomized Initialization ---
    num_candidates = 64
    
    # Create perturbations: [Layers, Candidates, Groups]
    # Candidate 0 is clean (noise=1.0)
    # Candidates 1..K have noise ~ U[0.99, 1.01]
    noise = torch.rand(num_layers, num_candidates - 1, num_groups, device=device) * 0.02 + 0.99
    ones = torch.ones(num_layers, 1, num_groups, device=device)
    noise = torch.cat([ones, noise], dim=1)
    
    # Broadcast weights and apply noise
    w_expanded = weight.unsqueeze(1) * noise 
    
    # Sort to get assignments: [Layers, Candidates, Groups]
    # We use these indices to assign items to packs
    sorted_indices = torch.argsort(w_expanded, dim=-1, descending=True)
    
    # ZigZag Pattern Construction
    # Map sorted position [0..G-1] to Pack ID [0..P-1]
    # 0..P-1, P-1..0, 0..P-1 ...
    seq = torch.arange(num_groups, device=device)
    row = seq // num_packs
    col = seq % num_packs
    # If row is even, p = col. If row is odd, p = P - 1 - col.
    pack_ids_zigzag = torch.where(row % 2 == 0, col, num_packs - 1 - col)
    
    # Calculate loads for all candidates
    # We gather the original weights using the sorted indices
    w_sorted_real = torch.gather(weight.unsqueeze(1).expand(-1, num_candidates, -1), 
                                 2, sorted_indices)
    
    # Sum weights by pack
    # We use scatter_add. 
    # Indices must be expanded to [Layers, Candidates, Groups]
    p_ids_exp = pack_ids_zigzag.view(1, 1, -1).expand(num_layers, num_candidates, -1)
    
    candidate_loads = torch.zeros(num_layers, num_candidates, num_packs, device=device)
    candidate_loads.scatter_add_(2, p_ids_exp, w_sorted_real)
    
    # Select best candidate (Minimizing Max Load)
    max_loads = candidate_loads.max(dim=-1).values
    best_candidate_idx = torch.argmin(max_loads, dim=1) # [Layers]
    
    # --- Refinement and Output Construction ---
    
    # Pre-allocate output
    pack_index = torch.full_like(weight, -1, dtype=torch.int64)
    rank_in_pack = torch.full_like(weight, -1, dtype=torch.int64)
    
    # Grid for reconstructing output
    p_grid = torch.arange(num_packs, device=device).unsqueeze(1).expand(-1, groups_per_pack)
    r_grid = torch.arange(groups_per_pack, device=device).unsqueeze(0).expand(num_packs, -1)
    p_grid_flat = p_grid.flatten()
    r_grid_flat = r_grid.flatten()
    
    # For ZigZag reconstruction
    slot_ids_zigzag = seq // num_packs
    
    for l in range(num_layers):
        best_k = best_candidate_idx[l]
        
        # Reconstruct the assignment for the best candidate
        # sorted_items: [Groups] (Item IDs in sorted order)
        sorted_items = sorted_indices[l, best_k]
        
        # Fill the pack_matrix: [P, G_per_P] -> ItemID
        pack_matrix = torch.empty((num_packs, groups_per_pack), dtype=torch.int64, device=device)
        pack_matrix[pack_ids_zigzag, slot_ids_zigzag] = sorted_items
        
        # Initial pack weights
        current_pack_weights = candidate_loads[l, best_k].clone()
        
        # Refine
        pack_matrix = _refine_packing(
            weight[l],
            pack_matrix,
            current_pack_weights,
            num_packs,
            groups_per_pack,
            max_iters=20
        )
        
        # Store results
        # pack_matrix[p, s] = item_id
        # We need pack_index[l, item_id] = p
        flat_items = pack_matrix.flatten()
        pack_index[l, flat_items] = p_grid_flat
        rank_in_pack[l, flat_items] = r_grid_flat

    return pack_index, rank_in_pack


def replicate_experts(
        weight: torch.Tensor,
        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Replicate experts to `num_phy` replicas, minimizing the maximum load.

    Uses a vectorized greedy approach (Jefferson/D'Hondt method).

    Parameters:
        weight: [X, num_log]
        num_phy: total number of experts after replication

    Returns:
        phy2log: [X, num_phy], logical expert id of each physical expert
        rank: [X, num_phy], the replica rank
        logcnt: [X, num_log], number of replicas for each logical expert
    """
    n, num_log = weight.shape
    device = weight.device

    # Initialize: Each expert gets at least 1 replica
    logcnt = torch.ones((n, num_log), dtype=torch.int64, device=device)
    
    # Greedily assign remaining replicas
    # Loop for (num_phy - num_log) iterations
    # In each step, assign replica to expert with max (weight / count)
    for _ in range(num_log, num_phy):
        scores = weight / logcnt
        indices = torch.argmax(scores, dim=-1)
        
        # Vectorized increment
        logcnt[torch.arange(n, device=device), indices] += 1

    # Reconstruction
    phy2log = torch.zeros((n, num_phy), dtype=torch.int64, device=device)
    rank = torch.zeros((n, num_phy), dtype=torch.int64, device=device)

    for i in range(n):
        counts = logcnt[i]
        
        # Create logical IDs
        l_ids = torch.repeat_interleave(
            torch.arange(num_log, device=device), counts
        )
        phy2log[i] = l_ids
        
        # Create ranks
        # Since we just need [0..c-1] for each expert, we can construct it 
        # by checking boundaries or simple accumulation.
        curr = 0
        for idx in range(num_log):
            c = counts[idx].item()
            rank[i, curr:curr+c] = torch.arange(c, device=device)
            curr += c
            
    return phy2log, rank, logcnt


def rebalance_experts_hierarchical(
    weight: torch.Tensor,
    num_physical_experts: int,
    num_groups: int,
    num_nodes: int,
    num_gpus: int,
):
    """
    Hierarchical rebalancing: Groups->Nodes, then Replicas->GPUs.
    """
    num_layers, num_logical_experts = weight.shape
    assert num_logical_experts % num_groups == 0
    group_size = num_logical_experts // num_groups
    assert num_groups % num_nodes == 0
    groups_per_node = num_groups // num_nodes
    assert num_gpus % num_nodes == 0
    assert num_physical_experts % num_gpus == 0
    phy_experts_per_gpu = num_physical_experts // num_gpus

    def inverse(perm: torch.Tensor) -> torch.Tensor:
        inv = torch.empty_like(perm)
        inv.scatter_(
            1,
            perm,
            torch.arange(perm.size(1), dtype=torch.int64,
                         device=perm.device).expand(perm.shape),
        )
        return inv

    # Step 1: pack groups to nodes
    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)
    
    group_pack_index, group_rank_in_pack = balanced_packing(
        tokens_per_group, num_nodes)

    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *
                 group_size).unsqueeze(-1) +
                torch.arange(group_size,
                             dtype=torch.int64,
                             device=group_pack_index.device)).flatten(-2)
    mlog2log = inverse(log2mlog)

    # Step 2: construct redundant experts within nodes
    tokens_per_mlog = weight.gather(-1, mlog2log).view(
        -1, num_logical_experts // num_nodes)
    phy2mlog, phyrank, mlogcnt = replicate_experts(
        tokens_per_mlog, num_physical_experts // num_nodes)

    # Step 3: pack physical_experts to GPUs
    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
    
    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,
                                                num_gpus // num_nodes)

    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
    pphy2phy = inverse(phy2pphy)

    pphy2mlog = phy2mlog.gather(
        -1, pphy2phy) 
    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(
        0,
        num_logical_experts,
        num_logical_experts // num_nodes,
        device=group_pack_index.device,
    ).view(1, -1, 1)).flatten(-2)
    pphy2log = mlog2log.gather(-1, pphy2mlog)
    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)
    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)
    return pphy2log, pphyrank, logcnt


def rebalance_experts(
    weight: torch.Tensor,
    num_replicas: int,
    num_groups: int,
    num_nodes: int,
    num_gpus: int,
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Entry point for expert-parallelism load balancer.
    """
    num_layers, num_logical_experts = weight.shape
    weight = weight.float().cpu()

    if num_groups % num_nodes == 0:
        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
            weight, num_replicas, num_groups, num_nodes, num_gpus)
    else:
        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
            weight, num_replicas, 1, 1, num_gpus)

    num_redundant_experts = num_replicas - num_logical_experts
    maxlogcnt = num_redundant_experts + 1

    log2phy: torch.Tensor = torch.full(
        (num_layers, num_logical_experts, maxlogcnt),
        -1,
        dtype=torch.int64,
        device=logcnt.device,
    )

    log2phy.view(num_layers, -1).scatter_(
        -1,
        phy2log * maxlogcnt + phyrank,
        torch.arange(num_replicas, dtype=torch.int64,
                     device=log2phy.device).expand(num_layers, -1),
    )
    return phy2log, log2phy, logcnt
# EVOLVE-BLOCK-END


# This part remains fixed (not evolved)
def run_eplb(weight: torch.Tensor, num_replicas: int, num_groups: int, 
             num_nodes: int, num_gpus: int):
    """Run the expert parallelism load balancer"""
    phy2log, log2phy, logcnt = rebalance_experts(
        weight, num_replicas, num_groups, num_nodes, num_gpus
    )
    return phy2log, log2phy, logcnt


__all__ = ["rebalance_experts", "run_eplb"]