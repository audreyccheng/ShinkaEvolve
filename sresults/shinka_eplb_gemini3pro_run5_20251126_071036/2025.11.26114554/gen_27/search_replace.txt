<NAME>
parallel_randomized_greedy_packing
</NAME>

<DESCRIPTION>
Replace the deterministic Snake Sort initialization in `balanced_packing` with a Parallel Randomized Greedy LPT approach.
This method generates `num_restarts` (default 4) candidate packings: one using standard sorted weights (Greedy LPT), and others using randomly perturbed weights to induce different greedy choices.
It then selects the packing with the minimum maximum load for each layer and refines it using the existing variance-minimizing swap algorithm.
This allows the algorithm to explore a wider solution space and escape local optima that deterministic heuristics might get stuck in, improving load balance without significant performance penalty due to vectorization over trials.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Uses Snake-Sort Initialization followed by Variance-Minimizing Refinement.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Optimization for trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64,
                                  device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # 1. Snake Sort Initialization (Vectorized)
    # Sort weights descending
    sorted_res = weight.sort(dim=-1, descending=True)
    sorted_weights = sorted_res.values
    sorted_indices = sorted_res.indices

    # Construct snake pattern mapping for ranks
    # Rank r goes to pack p(r).
    # Pattern 0..M-1, M-1..0, repeated
    cycle_len = 2 * num_packs
    ranks = torch.arange(num_groups, device=device)
    mod_ranks = ranks % cycle_len

    # Calculate Pack IDs for each rank
    # if mod < M: p = mod; else: p = 2M - 1 - mod
    pack_ids_by_rank = torch.where(mod_ranks < num_packs, mod_ranks, cycle_len - 1 - mod_ranks)

    # Calculate Slot IDs for each rank
    # Each full cycle consumes 2 slots per pack.
    # slot = (cycle_idx * 2) + (1 if in second half of cycle else 0)
    slot_indices = (ranks // cycle_len) * 2 + (mod_ranks >= num_packs).long()

    # Determine the flattened index in the destination [L, M, C] array
    # dest_idx = pack_id * C + slot_id
    flat_dest_idx = pack_ids_by_rank * groups_per_pack + slot_indices

    # Create the packed assignment structure [L, M, C]
    # We map items from sorted_indices to this structure
    pack_assignment = torch.zeros((num_layers, num_packs, groups_per_pack),
                                  dtype=torch.int64, device=device)

    # Scatter sorted items to their snake-assigned positions
    # pack_assignment.view(L, -1)[:, flat_dest_idx[r]] = sorted_indices[:, r]
    pack_assignment.view(num_layers, -1)[:, flat_dest_idx] = sorted_indices

    # Calculate initial pack weights
    pack_weights = torch.zeros((num_layers, num_packs), dtype=weight.dtype, device=device)
    # We can sum up using index_add_ or scatter_add_
    # Expand pack_ids to [L, N]
    pack_ids_expanded = pack_ids_by_rank.unsqueeze(0).expand(num_layers, -1)
    pack_weights.scatter_add_(1, pack_ids_expanded, sorted_weights)

    # 2. Refinement Loop
    # Process each layer
    for i in range(num_layers):
        pack_assignment[i] = _refine_packing(
            weight[i],
            pack_assignment[i],
            pack_weights[i],
            num_packs,
            groups_per_pack,
            max_iters=50
        )

    # 3. Reconstruct Outputs
    pack_index = torch.empty_like(weight, dtype=torch.int64)
    rank_in_pack = torch.empty_like(weight, dtype=torch.int64)

    # flat_assignment [L, N] contains item IDs
    flat_assignment = pack_assignment.view(num_layers, -1)

    # Map back: We know the Pack ID and Slot ID for each position in flat_assignment
    # Position k corresponds to pack k // C, slot k % C
    grid = torch.arange(num_groups, device=device)
    p_vals = (grid // groups_per_pack).unsqueeze(0).expand(num_layers, -1)
    c_vals = (grid % groups_per_pack).unsqueeze(0).expand(num_layers, -1)

    # pack_index[layer, item_id] = p_val
    pack_index.scatter_(1, flat_assignment, p_vals)
    rank_in_pack.scatter_(1, flat_assignment, c_vals)

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int,
                     num_restarts: int = 4) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using Parallel Randomized Greedy LPT
    followed by Variance-Minimizing Refinement.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs
        num_restarts: number of parallel greedy attempts (default 4)

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Optimization for trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64,
                                  device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # 1. Parallel Randomized Greedy Initialization
    # We generate multiple candidate packings in parallel by sorting weights with random noise.

    # Expand weights for restarts: [L, R, G]
    # We treat L*R as the batch dimension for vectorized operations.
    B = num_layers * num_restarts

    # Create sort keys
    weights_expanded = weight.unsqueeze(1).expand(-1, num_restarts, -1) # [L, R, G]

    # Noise generation
    # First restart is always deterministic LPT (clean weights)
    noise = torch.rand_like(weights_expanded) * 0.05
    # Zero out noise for the first restart (index 0)
    mask = torch.ones(num_restarts, device=device)
    mask[0] = 0
    noise = noise * mask.view(1, num_restarts, 1)

    sort_keys = weights_expanded * (1.0 + noise)

    # Sort descending
    sorted_res = sort_keys.sort(dim=-1, descending=True)
    sorted_indices = sorted_res.indices # [L, R, G]

    # Flatten to [B, G]
    flat_indices = sorted_indices.view(B, num_groups)
    flat_weights_src = weights_expanded.reshape(B, num_groups)

    # Gather actual weights in sorted order: [B, G]
    flat_sorted_weights = torch.gather(flat_weights_src, 1, flat_indices)

    # Greedy Packing Loop
    pack_loads = torch.zeros((B, num_packs), dtype=weight.dtype, device=device)
    pack_counts = torch.zeros((B, num_packs), dtype=torch.int64, device=device)
    pack_assignment = torch.zeros((B, num_packs, groups_per_pack), dtype=torch.int64, device=device)

    batch_idx = torch.arange(B, device=device)
    inf_tensor = torch.tensor(float('inf'), device=device)

    # Vectorized loop over items
    for j in range(num_groups):
        w = flat_sorted_weights[:, j]
        item_id = flat_indices[:, j]

        # Identify valid packs (not full)
        is_full = (pack_counts >= groups_per_pack)

        # Find min load among valid packs
        # Set load of full packs to infinity
        masked_loads = torch.where(is_full, inf_tensor, pack_loads)

        best_pack = torch.argmin(masked_loads, dim=1) # [B]

        # Update
        pack_loads[batch_idx, best_pack] += w

        # Get slot index
        slot = pack_counts[batch_idx, best_pack]

        # Assign item
        pack_assignment[batch_idx, best_pack, slot] = item_id

        # Increment count
        pack_counts[batch_idx, best_pack] += 1

    # 2. Selection of Best Restart per Layer
    # Metric: Minimum Max-Load
    max_loads, _ = pack_loads.max(dim=1) # [B]
    max_loads = max_loads.view(num_layers, num_restarts)

    best_restart_idx = torch.argmin(max_loads, dim=1) # [L]

    # Extract best assignments
    offsets = torch.arange(num_layers, device=device) * num_restarts
    best_flat_indices = offsets + best_restart_idx

    best_assignment = pack_assignment[best_flat_indices] # [L, M, C]
    best_pack_weights = pack_loads[best_flat_indices]    # [L, M]

    # 3. Refinement Loop
    for i in range(num_layers):
        best_assignment[i] = _refine_packing(
            weight[i],
            best_assignment[i],
            best_pack_weights[i],
            num_packs,
            groups_per_pack,
            max_iters=50
        )

    # 4. Reconstruct Outputs
    pack_index = torch.empty_like(weight, dtype=torch.int64)
    rank_in_pack = torch.empty_like(weight, dtype=torch.int64)

    flat_assignment = best_assignment.view(num_layers, -1)

    grid = torch.arange(num_groups, device=device)
    p_vals = (grid // groups_per_pack).unsqueeze(0).expand(num_layers, -1)
    c_vals = (grid % groups_per_pack).unsqueeze(0).expand(num_layers, -1)

    pack_index.scatter_(1, flat_assignment, p_vals)
    rank_in_pack.scatter_(1, flat_assignment, c_vals)

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>