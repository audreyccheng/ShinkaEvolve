# EVOLVE-BLOCK-START
"""
Expert parallelism load balancer (EPLB) for vLLM.

This module implements the core rearrangement algorithm.

The rearrangement algorithm is adapted from
[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).

Please find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example
on how the EPLB algorithm works.
"""

import torch


def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs using Vectorized Greedy LPT with 
    Top-K Local Search Refinement.
    
    Parameters:
        weight: [Batch, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [Batch, n], the pack index of each item
        rank_in_pack: [Batch, n], the rank of the item in the pack
    """
    batch_size, num_items = weight.shape
    device = weight.device
    assert num_items % num_packs == 0
    items_per_pack = num_items // num_packs

    if items_per_pack == 1:
        pack_index = torch.arange(num_packs,
                                  dtype=torch.int64,
                                  device=device).view(1, -1).expand(batch_size, -1)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # --- 1. Greedy LPT Initialization ---
    # Sort weights descending for LPT
    sorted_weight, sorted_indices = weight.float().sort(dim=-1, descending=True)

    # Tracking state
    pack_weights = torch.zeros(batch_size, num_packs, device=device)
    pack_counts = torch.zeros(batch_size, num_packs, dtype=torch.int64, device=device)
    
    # Storage for pack contents to enable swapping: [Batch, Packs, ItemsPerPack]
    pack_contents = torch.zeros(batch_size, num_packs, items_per_pack, device=device)
    pack_item_ids = torch.zeros(batch_size, num_packs, items_per_pack, dtype=torch.int64, device=device)

    row_indices = torch.arange(batch_size, device=device)

    # Assign items one by one
    for i in range(num_items):
        w = sorted_weight[:, i]
        original_idx = sorted_indices[:, i]

        # Identify valid packs (count < limit)
        valid_mask = pack_counts < items_per_pack
        
        # Select pack with minimum weight among valid packs
        # Use simple additive penalty for invalid packs
        # Since weights are >= 0, adding sum(weight)+1 is sufficient, or just float('inf')
        candidates = pack_weights.clone()
        candidates[~valid_mask] = float('inf')
        
        chosen_pack = torch.argmin(candidates, dim=1)
        chosen_rank = pack_counts[row_indices, chosen_pack]

        # Update state
        pack_weights[row_indices, chosen_pack] += w
        pack_counts[row_indices, chosen_pack] += 1
        
        pack_contents[row_indices, chosen_pack, chosen_rank] = w
        pack_item_ids[row_indices, chosen_pack, chosen_rank] = original_idx

    # --- 2. Top-K Local Search Refinement ---
    # Try to swap items between K heaviest and K lightest packs
    K = 4
    if num_packs < 2 * K:
        K = num_packs // 2
    
    if K > 0:
        num_iters = 20
        for _ in range(num_iters):
            # Recalculate weights
            current_pack_weights = pack_contents.sum(dim=-1)
            
            # Find Top-K heavy and light
            sorted_pack_idx = current_pack_weights.argsort(dim=1)
            heavy_packs = sorted_pack_idx[:, -K:] # [Batch, K]
            light_packs = sorted_pack_idx[:, :K]  # [Batch, K]
            
            # Extract weights
            w_heavy = torch.gather(current_pack_weights, 1, heavy_packs)
            w_light = torch.gather(current_pack_weights, 1, light_packs)
            
            # Diff matrix: [Batch, K_h, K_l]
            # heavy i vs light j
            pack_diff = w_heavy.unsqueeze(2) - w_light.unsqueeze(1)
            
            # Early exit if perfectly balanced
            if (pack_diff < 1e-4).all():
                break

            # Extract Items: [Batch, K, G]
            idx_heavy_exp = heavy_packs.unsqueeze(2).expand(-1, -1, items_per_pack)
            idx_light_exp = light_packs.unsqueeze(2).expand(-1, -1, items_per_pack)
            
            items_heavy = torch.gather(pack_contents, 1, idx_heavy_exp)
            items_light = torch.gather(pack_contents, 1, idx_light_exp)
            
            # Compute gains for all pairs (K*G) x (K*G)
            # Shapes for broadcasting:
            # Heavy: [Batch, K, G, 1, 1]
            # Light: [Batch, 1, 1, K, G]
            delta = items_heavy.view(batch_size, K, items_per_pack, 1, 1) - \
                    items_light.view(batch_size, 1, 1, K, items_per_pack)
            
            # Pack Diff: [Batch, K, 1, K, 1]
            pd_expanded = pack_diff.view(batch_size, K, 1, K, 1)
            
            # Improvement = diff - |diff - 2*delta|
            # We want to maximize this.
            # Mask invalid swaps: delta must be > 0 (flow from heavy to light)
            # Mask non-improving swaps: improvement > epsilon
            improvement = pd_expanded - (pd_expanded - 2 * delta).abs()
            
            valid_mask = (delta > 0) & (improvement > 1e-5)
            improvement[~valid_mask] = -1.0
            
            # Find best swap
            # Flatten the search space: K*G*K*G
            imp_flat = improvement.view(batch_size, -1)
            best_imp, best_idx_flat = imp_flat.max(dim=1)
            
            do_swap = best_imp > 0
            if not do_swap.any():
                break
                
            # Perform Swaps
            batch_idx_swap = torch.where(do_swap)[0]
            flat_indices = best_idx_flat[do_swap]
            
            # Decode indices
            # Structure: K_h * G_h * K_l * G_l
            G = items_per_pack
            
            idx_item_l = flat_indices % G
            flat_indices = flat_indices // G
            idx_pack_l_k = flat_indices % K
            flat_indices = flat_indices // K
            idx_item_h = flat_indices % G
            idx_pack_h_k = flat_indices // G
            
            # Map K-index back to pack-index
            p_h = heavy_packs[batch_idx_swap, idx_pack_h_k]
            p_l = light_packs[batch_idx_swap, idx_pack_l_k]
            
            # Update values
            val_h = pack_contents[batch_idx_swap, p_h, idx_item_h]
            val_l = pack_contents[batch_idx_swap, p_l, idx_item_l]
            
            pack_contents[batch_idx_swap, p_h, idx_item_h] = val_l
            pack_contents[batch_idx_swap, p_l, idx_item_l] = val_h
            
            # Update IDs
            id_h = pack_item_ids[batch_idx_swap, p_h, idx_item_h]
            id_l = pack_item_ids[batch_idx_swap, p_l, idx_item_l]
            
            pack_item_ids[batch_idx_swap, p_h, idx_item_h] = id_l
            pack_item_ids[batch_idx_swap, p_l, idx_item_l] = id_h

    # --- 3. Finalize Output ---
    pack_index = torch.empty(batch_size, num_items, dtype=torch.int64, device=device)
    rank_in_pack = torch.empty(batch_size, num_items, dtype=torch.int64, device=device)
    
    # pack_item_ids has shape [Batch, Packs, ItemsPerPack] and contains Item Indices
    # We want to invert this: index[Item ID] = Pack ID
    
    flat_ids = pack_item_ids.view(batch_size, -1)
    
    # Create grids of Pack IDs and Rank IDs
    grid_packs = torch.arange(num_packs, device=device).view(1, -1, 1).expand(batch_size, -1, items_per_pack).reshape(batch_size, -1)
    grid_ranks = torch.arange(items_per_pack, device=device).view(1, 1, -1).expand(batch_size, num_packs, -1).reshape(batch_size, -1)
    
    pack_index.scatter_(1, flat_ids, grid_packs)
    rank_in_pack.scatter_(1, flat_ids, grid_ranks)
    
    return pack_index, rank_in_pack


def replicate_experts(
        weight: torch.Tensor,
        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Replicate `num_log` experts to `num_phy` replicas.
    Vectorized over batch dimension.
    """
    batch_size, num_log = weight.shape
    num_redundant = num_phy - num_log
    assert num_redundant >= 0
    device = weight.device
    
    phy2log = torch.arange(num_phy, dtype=torch.int64, device=device).expand(batch_size, -1).clone()
    rank = torch.zeros(batch_size, num_phy, dtype=torch.int64, device=device)
    logcnt = torch.ones(batch_size, num_log, dtype=torch.int64, device=device)
    
    # Use float for score calculation
    current_scores = weight.float() # score = weight / count (count=1)
    
    row_indices = torch.arange(batch_size, device=device)

    # Greedy replication loop
    # Note: Vectorizing the loop itself is hard because steps are sequential dependencies.
    # But we process all batches in parallel.
    for i in range(num_log, num_phy):
        # Pick expert with max score
        redundant_indices = current_scores.argmax(dim=-1)
        
        phy2log[:, i] = redundant_indices
        rank[:, i] = logcnt[row_indices, redundant_indices]
        
        # Update count
        logcnt[row_indices, redundant_indices] += 1
        
        # Update score
        # Only for the chosen experts
        new_cnt = logcnt[row_indices, redundant_indices].float()
        chosen_weight = weight[row_indices, redundant_indices].float()
        current_scores[row_indices, redundant_indices] = chosen_weight / new_cnt

    return phy2log, rank, logcnt


def rebalance_experts_hierarchical(
    weight: torch.Tensor,
    num_physical_experts: int,
    num_groups: int,
    num_nodes: int,
    num_gpus: int,
):
    # This handles the hierarchical steps.
    # Assumes weight is [Batch, NumLogical]
    
    batch_size, num_logical_experts = weight.shape
    device = weight.device
    
    group_size = num_logical_experts // num_groups
    groups_per_node = num_groups // num_nodes
    phy_experts_per_gpu = num_physical_experts // num_gpus

    def inverse(perm: torch.Tensor) -> torch.Tensor:
        inv = torch.empty_like(perm)
        inv.scatter_(1, perm, torch.arange(perm.size(1), dtype=torch.int64, device=device).expand(perm.shape))
        return inv

    # Step 1: Pack Groups to Nodes
    # weight: [B, L] -> unflatten -> [B, G, S] -> sum -> [B, G]
    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(dim=-1)
    
    # group_pack_index: [B, G] (Node ID)
    group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)
    
    # Map logical experts to Meta-Logical experts (grouped by Node)
    # logic: (NodeID * GrpsPerNode + RankInNode) * GrpSize + RankInGrp
    # [B, G, 1]
    base_mlog = (group_pack_index * groups_per_node + group_rank_in_pack) * group_size
    # [1, 1, S]
    offset_mlog = torch.arange(group_size, dtype=torch.int64, device=device).view(1, 1, -1)
    
    log2mlog = (base_mlog.unsqueeze(-1) + offset_mlog).flatten(1) # [B, L]
    mlog2log = inverse(log2mlog)

    # Step 2: Replicate within Nodes
    # Gather weights into mlog order
    tokens_per_mlog = weight.gather(1, mlog2log)
    
    # Reshape for independent node processing: [B * Nodes, LogPerNode]
    # We must flatten batch and node dimension together
    log_per_node = num_logical_experts // num_nodes
    phy_per_node = num_physical_experts // num_nodes
    
    tokens_per_mlog_node_view = tokens_per_mlog.view(-1, log_per_node)
    
    # Run replication
    phy2mlog_node, phyrank_node, mlogcnt_node = replicate_experts(tokens_per_mlog_node_view, phy_per_node)
    
    # Step 3: Pack Physical Experts to GPUs
    # Calculate weights of replicas
    tokens_per_phy_node = (tokens_per_mlog_node_view / mlogcnt_node).gather(1, phy2mlog_node)
    
    # Pack to GPUs within Node
    gpu_per_node = num_gpus // num_nodes
    pack_index_node, rank_in_pack_node = balanced_packing(tokens_per_phy_node, gpu_per_node)
    
    # Construct pphy (Physical index ordered by GPU)
    # pphy = GPU_ID * PhyPerGPU + RankInGPU
    phy2pphy_node = pack_index_node * phy_experts_per_gpu + rank_in_pack_node
    pphy2phy_node = inverse(phy2pphy_node)
    
    # -- Reconstruction of global mappings --
    
    # 1. Expand node results back to [B, Nodes, ...]
    phy2mlog = phy2mlog_node.view(batch_size, num_nodes, -1)
    pphy2phy = pphy2phy_node.view(batch_size, num_nodes, -1)
    mlogcnt = mlogcnt_node.view(batch_size, -1)
    
    # 2. Get mlog indices for ordered physical experts
    # [B, Nodes, PhyPerNode]
    pphy2mlog_local = phy2mlog.gather(2, pphy2phy)
    
    # 3. Convert local mlog indices (0..LogPerNode-1) to global mlog indices
    # GlobalMlog = LocalMlog + NodeIndex * LogPerNode
    offset_global = torch.arange(num_nodes, device=device).view(1, -1, 1) * log_per_node
    pphy2mlog_global = (pphy2mlog_local + offset_global).flatten(1)
    
    # 4. Map to original logical ID
    pphy2log = mlog2log.gather(1, pphy2mlog_global)
    
    # 5. Phyrank and Logcnt
    pphyrank = phyrank_node.view(batch_size, num_nodes, -1).gather(2, pphy2phy).flatten(1)
    
    # mlogcnt is already in mlog order, scatter back to log order
    logcnt = torch.empty_like(mlogcnt)
    logcnt.scatter_(1, mlog2log, mlogcnt)
    
    return pphy2log, pphyrank, logcnt


def rebalance_experts(
    weight: torch.Tensor,
    num_replicas: int,
    num_groups: int,
    num_nodes: int,
    num_gpus: int,
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Entry point with Parallel Randomized Restarts.
    """
    num_layers, num_logical_experts = weight.shape
    device = weight.device
    
    # Parallel Restarts Configuration
    num_restarts = 16 
    
    # 1. Expand Inputs
    # [L, G] -> [L*R, G]
    weight_expanded = weight.float().repeat_interleave(num_restarts, dim=0)
    
    # 2. Inject Noise for Randomization
    # Only into restarts 1..R-1
    noise = torch.rand_like(weight_expanded) * 0.01
    
    # Zero out noise for restart 0 (Deterministic Baseline)
    mask_det = torch.arange(weight_expanded.shape[0], device=device) % num_restarts == 0
    noise[mask_det] = 0.0
    
    weight_noisy = weight_expanded * (1.0 + noise)
    
    # 3. Run Hierarchical Algorithm on Noisy Weights
    if num_groups % num_nodes == 0:
        phy2log_cand, phyrank_cand, logcnt_cand = rebalance_experts_hierarchical(
            weight_noisy, num_replicas, num_groups, num_nodes, num_gpus)
    else:
        phy2log_cand, phyrank_cand, logcnt_cand = rebalance_experts_hierarchical(
            weight_noisy, num_replicas, 1, 1, num_gpus)
            
    # 4. Evaluate Candidates on Original Weights
    # Load on each GPU is what we care about.
    # phy2log_cand is ordered by Physical Expert Index (which corresponds to GPU slots).
    # [B, NumPhy]. 
    # Experts 0..K-1 are GPU 0, K..2K-1 are GPU 1, etc.
    
    experts_per_gpu = num_replicas // num_gpus
    
    # Get logical weights for the assigned experts
    # [B, NumPhy]
    assigned_weights = weight_expanded.gather(1, phy2log_cand)
    
    # Divide weights by replica count to get actual load per replica
    # [B, NumLog]
    counts = logcnt_cand.gather(1, phy2log_cand)
    replica_loads = assigned_weights / counts
    
    # Sum loads per GPU
    # Reshape to [B, NumGPUs, ExpertsPerGPU]
    gpu_loads = replica_loads.view(-1, num_gpus, experts_per_gpu).sum(dim=-1)
    
    # Max load per batch
    max_loads = gpu_loads.max(dim=1).values # [B]
    
    # 5. Select Best Restart
    # Reshape to [Layers, Restarts]
    max_loads_view = max_loads.view(num_layers, num_restarts)
    best_restart_idx = max_loads_view.argmin(dim=1) # [Layers]
    
    # 6. Gather Best Result
    # Indices into the expanded batch
    base_idx = torch.arange(num_layers, device=device) * num_restarts
    best_batch_idx = base_idx + best_restart_idx
    
    phy2log = phy2log_cand[best_batch_idx]
    
    # Logic to construct log2phy map (same as original)
    num_redundant_experts = num_replicas - num_logical_experts
    maxlogcnt = num_redundant_experts + 1
    
    # Need final logcnt and phyrank
    logcnt = logcnt_cand[best_batch_idx]
    phyrank = phyrank_cand[best_batch_idx]
    
    log2phy = torch.full(
        (num_layers, num_logical_experts, maxlogcnt),
        -1,
        dtype=torch.int64,
        device=device,
    )
    
    scatter_idx = phy2log * maxlogcnt + phyrank
    src = torch.arange(num_replicas, dtype=torch.int64, device=device).expand(num_layers, -1)
    
    log2phy.view(num_layers, -1).scatter_(-1, scatter_idx, src)
    
    return phy2log, log2phy, logcnt

# EVOLVE-BLOCK-END