<NAME>
optimize_expert_load_balancing_with_replication
</NAME>

<DESCRIPTION>
This edit optimizes the `rebalance_experts` algorithm by moving the computation to the CPU to avoid GPU synchronization overhead and introducing expert replication to achieve better load balancing.

1.  **CPU Offloading**: The original algorithm used PyTorch operations (`argsort`, `argmin`) on the GPU within a loop, causing significant synchronization latency. The new approach moves `expert_load` to the CPU and uses Python's native `list` and `heapq` for assignment, which is much faster for the typical problem scale.
2.  **Expert Replication**: To better handle skewed load distributions, experts with loads exceeding the average target load are split into multiple replicas. The number of replicas is proportional to the expert's load (capped at `num_gpus`).
3.  **LPT Greedy Assignment**: Uses the Longest Processing Time (LPT) first greedy strategy with a min-heap to assign expert replicas to GPUs, minimizing the maximum load on any GPU.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    sorted_experts = torch.argsort(expert_load, descending=True)

    gpu_loads = torch.zeros(num_gpus, device=expert_load.device)
    expert_assignment: List[List[int]] = [[] for _ in range(num_gpus)]

    for expert_id in sorted_experts:
        min_gpu_idx = torch.argmin(gpu_loads).item()
        expert_assignment[min_gpu_idx].append(expert_id.item())
        gpu_loads[min_gpu_idx] += expert_load[expert_id]
=======
    # Move to CPU to avoid synchronization overhead
    expert_load_cpu = expert_load.to(torch.float32).cpu()
    num_experts = expert_load_cpu.numel()
    total_load = expert_load_cpu.sum().item()
    target_load = total_load / num_gpus

    # Create tasks (replicas)
    import math
    import heapq

    tasks = []
    for i in range(num_experts):
        load = expert_load_cpu[i].item()
        if target_load > 0 and load > target_load:
            # Heuristic: Replicate if expert load is significantly larger than target load.
            # Cap the number of replicas to num_gpus.
            num_replicas = min(math.ceil(load / target_load), num_gpus)
        else:
            num_replicas = 1

        replica_load = load / num_replicas
        for _ in range(num_replicas):
            tasks.append((replica_load, i))

    # Sort tasks by load descending (LPT)
    tasks.sort(key=lambda x: x[0], reverse=True)

    # Greedy assignment using a min-heap
    # Heap stores (current_load, gpu_idx)
    gpu_heap = [(0.0, i) for i in range(num_gpus)]

    expert_assignment: List[List[int]] = [[] for _ in range(num_gpus)]

    for load, expert_id in tasks:
        min_load, gpu_idx = heapq.heappop(gpu_heap)
        expert_assignment[gpu_idx].append(expert_id)
        heapq.heappush(gpu_heap, (min_load + load, gpu_idx))
>>>>>>> REPLACE
</DIFF>