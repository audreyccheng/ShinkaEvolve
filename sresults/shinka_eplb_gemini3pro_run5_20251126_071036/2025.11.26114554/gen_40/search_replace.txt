<NAME>
iterative_reweighting_packing
</NAME>

<DESCRIPTION>
Implements an iterative re-weighting strategy in `balanced_packing`. Instead of a single pass of Greedy LPT + Refinement, the algorithm runs multiple attempts. After each attempt, it identifies the items in the heaviest pack and increases their "virtual weight". This forces the greedy heuristic to prioritize these difficult items in subsequent iterations, placing them earlier into emptier bins, thereby escaping local optima.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Implements a Greedy LPT initialization followed by Iterative Swapping refinement.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Optimization for trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64,
                                  device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Pre-allocate outputs
    pack_index = torch.full_like(weight, -1, dtype=torch.int64)
    rank_in_pack = torch.full_like(weight, -1, dtype=torch.int64)

    # Sort weights for Greedy LPT (Longest Processing Time) initialization
    # Sorting descending helps placing largest items first
    sorted_res = weight.sort(dim=-1, descending=True)
    sorted_indices = sorted_res.indices
    sorted_weights = sorted_res.values

    # Pre-compute grid indices for scattering results later
    p_ids_grid = torch.arange(num_packs, device=device).unsqueeze(1).expand(-1, groups_per_pack)
    r_ids_grid = torch.arange(groups_per_pack, device=device).unsqueeze(0).expand(num_packs, -1)
    flat_p_ids = p_ids_grid.flatten()
    flat_r_ids = r_ids_grid.flatten()

    for i in range(num_layers):
        # 1. Greedy Initialization
        current_pack_weights = torch.zeros(num_packs, device=device, dtype=weight.dtype)
        current_pack_counts = torch.zeros(num_packs, device=device, dtype=torch.int64)

        # Matrix to store assignments: [Pack, Slot] -> Item Index
        pack_assignment = torch.zeros((num_packs, groups_per_pack),
                                      dtype=torch.int64, device=device)

        layer_indices = sorted_indices[i]
        layer_vals = sorted_weights[i]

        for j in range(num_groups):
            w = layer_vals[j]
            item_idx = layer_indices[j]

            # Vectorized greedy choice: choose the valid pack with min weight
            # Mask out full packs by setting their weight to infinity
            is_full = current_pack_counts >= groups_per_pack
            masked_weights = torch.where(is_full, float('inf'), current_pack_weights)
            best_pack = torch.argmin(masked_weights)

            # Assign
            slot = current_pack_counts[best_pack]
            pack_assignment[best_pack, slot] = item_idx
            current_pack_weights[best_pack] += w
            current_pack_counts[best_pack] += 1

        # 2. Iterative Refinement (Swapping)
        pack_assignment = _refine_packing(
            weight[i], pack_assignment, current_pack_weights,
            num_packs, groups_per_pack
        )

        # 3. Store results
        flat_items = pack_assignment.flatten()
        pack_index[i, flat_items] = flat_p_ids
        rank_in_pack[i, flat_items] = flat_r_ids

    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int,
                     num_attempts: int = 5) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Implements a Greedy LPT initialization with Iterative Re-weighting and
    Swapping refinement.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs
        num_attempts: number of iterations for re-weighting optimization

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    device = weight.device
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    # Optimization for trivial case
    if groups_per_pack == 1:
        pack_index = torch.arange(num_groups, dtype=torch.int64,
                                  device=device).expand(num_layers, -1)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Pre-allocate outputs
    pack_index = torch.full_like(weight, -1, dtype=torch.int64)
    rank_in_pack = torch.full_like(weight, -1, dtype=torch.int64)

    # Pre-compute grid indices for scattering results later
    p_ids_grid = torch.arange(num_packs, device=device).unsqueeze(1).expand(-1, groups_per_pack)
    r_ids_grid = torch.arange(groups_per_pack, device=device).unsqueeze(0).expand(num_packs, -1)
    flat_p_ids = p_ids_grid.flatten()
    flat_r_ids = r_ids_grid.flatten()

    for i in range(num_layers):
        best_max_load = float('inf')
        best_assignment = None

        # Virtual weights for iterative re-weighting
        virtual_weight = weight[i].clone().float()

        for attempt in range(num_attempts):
            # Sort based on virtual weights to determine order
            sorted_indices = torch.argsort(virtual_weight, descending=True)

            # 1. Greedy Initialization using Virtual Weights for order, Real Weights for load
            current_pack_weights = torch.zeros(num_packs, device=device, dtype=weight.dtype)
            current_pack_counts = torch.zeros(num_packs, device=device, dtype=torch.int64)
            pack_assignment = torch.zeros((num_packs, groups_per_pack),
                                          dtype=torch.int64, device=device)

            for j in range(num_groups):
                item_idx = sorted_indices[j]
                w = weight[i, item_idx] # Real weight

                # Find valid pack with min weight
                is_full = current_pack_counts >= groups_per_pack
                masked_weights = torch.where(is_full, float('inf'), current_pack_weights)
                best_pack = torch.argmin(masked_weights)

                # Assign
                slot = current_pack_counts[best_pack]
                pack_assignment[best_pack, slot] = item_idx
                current_pack_weights[best_pack] += w
                current_pack_counts[best_pack] += 1

            # 2. Iterative Refinement (Swapping) using Real Weights
            pack_assignment = _refine_packing(
                weight[i], pack_assignment, current_pack_weights,
                num_packs, groups_per_pack
            )

            # 3. Check solution quality
            max_load, max_pid = torch.max(current_pack_weights, dim=0)

            if max_load < best_max_load:
                best_max_load = max_load
                best_assignment = pack_assignment.clone()

            # 4. Re-weighting: Boost virtual weights of items in the heaviest pack
            # This forces them to be processed earlier in the next greedy pass
            if attempt < num_attempts - 1:
                items_in_max = pack_assignment[max_pid]
                virtual_weight[items_in_max] *= 1.05

        # Store best result for this layer
        if best_assignment is not None:
            flat_items = best_assignment.flatten()
            pack_index[i, flat_items] = flat_p_ids
            rank_in_pack[i, flat_items] = flat_r_ids

    return pack_index, rank_in_pack
>>>>>>> REPLACE
</DIFF>