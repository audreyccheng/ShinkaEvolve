<NAME>
optimize_balanced_packing_restarts
</NAME>

<DESCRIPTION>
1. Increase `num_restarts` from 4 to 8 to explore more solutions in parallel, improving load balancing quality.
2. Diversify the noise injected into weights for restarts. Instead of a fixed 0.05 scale, we use a linspace of noise scales (e.g. 0.01 to 0.15) across the restarts. This ensures we cover both conservative (near deterministic LPT) and aggressive (randomized) orderings.
3. Optimize the greedy loop in `balanced_packing` by replacing `clone()` and masked assignment with `torch.where()` and a pre-allocated infinity tensor, which avoids memory allocation in the loop.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Run multiple random restarts in parallel
    num_restarts = 4

    # Expand weight for restarts: [num_layers, num_restarts, num_groups]
    weight_expanded = weight.unsqueeze(1).expand(-1, num_restarts, -1).clone()

    # Perturb weights with noise to randomize sorting order for restarts > 0
    if num_restarts > 1:
        noise = torch.rand_like(weight_expanded[:, 1:]) * 0.05
        weight_expanded[:, 1:] *= (1.0 + noise)

    flat_weight = weight_expanded.reshape(-1, num_groups)
    # Use original weights for accumulation (repeated for each restart)
    original_w_flat = weight.unsqueeze(1).expand(-1, num_restarts, -1).reshape(
        -1, num_groups)

    # Sort descending based on perturbed weights
    sorted_indices = flat_weight.argsort(dim=-1, descending=True)
    sorted_w = original_w_flat.gather(1, sorted_indices)

    batch_size = flat_weight.shape[0]
    row_indices = torch.arange(batch_size, device=device)

    # Tracking state
    pack_weights = torch.zeros(batch_size, num_packs, device=device)
    pack_counts = torch.zeros(batch_size,
                              num_packs,
                              dtype=torch.int64,
                              device=device)

    pack_index_sorted = torch.zeros(batch_size,
                                    num_groups,
                                    dtype=torch.int64,
                                    device=device)
    rank_in_pack_sorted = torch.zeros(batch_size,
                                      num_groups,
                                      dtype=torch.int64,
                                      device=device)

    # Vectorized Greedy Packing
    for i in range(num_groups):
        w = sorted_w[:, i]

        valid_mask = pack_counts < groups_per_pack
        candidate_weights = pack_weights.clone()
        candidate_weights[~valid_mask] = float('inf')

        chosen_pack = candidate_weights.argmin(dim=1)

        pack_weights[row_indices, chosen_pack] += w
        rank_in_pack_sorted[:, i] = pack_counts[row_indices, chosen_pack]
        pack_counts[row_indices, chosen_pack] += 1
        pack_index_sorted[:, i] = chosen_pack
=======
    # Run multiple random restarts in parallel
    num_restarts = 8

    # Expand weight for restarts: [num_layers, num_restarts, num_groups]
    weight_expanded = weight.unsqueeze(1).expand(-1, num_restarts, -1).clone()

    # Perturb weights with varied noise levels to randomize sorting order
    # Use a linear spread of noise scales from 0.01 to 0.15
    if num_restarts > 1:
        noise_scales = torch.linspace(0.01,
                                      0.15,
                                      steps=num_restarts - 1,
                                      device=device)
        noise = torch.rand_like(weight_expanded[:, 1:]) * noise_scales.view(
            1, -1, 1)
        weight_expanded[:, 1:] *= (1.0 + noise)

    flat_weight = weight_expanded.reshape(-1, num_groups)
    # Use original weights for accumulation (repeated for each restart)
    original_w_flat = weight.unsqueeze(1).expand(-1, num_restarts, -1).reshape(
        -1, num_groups)

    # Sort descending based on perturbed weights
    sorted_indices = flat_weight.argsort(dim=-1, descending=True)
    sorted_w = original_w_flat.gather(1, sorted_indices)

    batch_size = flat_weight.shape[0]
    row_indices = torch.arange(batch_size, device=device)

    # Tracking state
    pack_weights = torch.zeros(batch_size, num_packs, device=device)
    pack_counts = torch.zeros(batch_size,
                              num_packs,
                              dtype=torch.int64,
                              device=device)

    pack_index_sorted = torch.zeros(batch_size,
                                    num_groups,
                                    dtype=torch.int64,
                                    device=device)
    rank_in_pack_sorted = torch.zeros(batch_size,
                                      num_groups,
                                      dtype=torch.int64,
                                      device=device)

    inf_tensor = torch.full((batch_size, num_packs),
                            float('inf'),
                            device=device)

    # Vectorized Greedy Packing
    for i in range(num_groups):
        w = sorted_w[:, i]

        valid_mask = pack_counts < groups_per_pack
        # Use torch.where to avoid cloning large tensors repeatedly
        candidate_weights = torch.where(valid_mask, pack_weights, inf_tensor)

        chosen_pack = candidate_weights.argmin(dim=1)

        pack_weights[row_indices, chosen_pack] += w
        rank_in_pack_sorted[:, i] = pack_counts[row_indices, chosen_pack]
        pack_counts[row_indices, chosen_pack] += 1
        pack_index_sorted[:, i] = chosen_pack
>>>>>>> REPLACE
</DIFF>