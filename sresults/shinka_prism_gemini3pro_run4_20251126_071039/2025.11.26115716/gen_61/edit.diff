--- a/original.py
+++ b/original.py
@@ -1,373 +1,366 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 import random
 import math
 
 GPU_MEM_SIZE = 80.0  # GB
 
 def compute_model_placement(gpu_num, models):
     """
-    Minimizes max KVPR using Binary Search on the target pressure K.
-    The feasibility check transforms the problem into Bin Packing with item size (w + K*s).
-    Uses deterministic and stochastic ordering strategies for robust packing.
+    Computes a model placement minimizing the maximum KV Cache Pressure (KVPR).
+    Uses Binary Search on target KVPR, transforming the problem into a 
+    constrained Bin Packing problem solved via adaptive stochastic heuristics.
     """
 
-    # 0. Precompute model data
+    # ------------------------------------------------------------------
+    # 0. Data Preparation
+    # ------------------------------------------------------------------
     m_data = []
     total_w = 0.0
     total_s = 0.0
     for i, m in enumerate(models):
         w = m.req_rate / m.slo
         s = m.model_size
         m_data.append({'w': w, 's': s, 'obj': m, 'id': i})
         total_w += w
         total_s += s
 
-    # Theoretical Lower Bound
-    # sum(w) / (N*C - sum(s))
+    # Lower bound for Binary Search
     rem_global = gpu_num * GPU_MEM_SIZE - total_s
     if rem_global <= 1e-9:
-        if total_w > 0: lb = float('inf')
-        else: lb = 0.0
+        lb = float('inf') if total_w > 0 else 0.0
     else:
         lb = total_w / rem_global
 
-    # -------------------------------------------------------
-    # Helper: Feasibility Check for Pressure K
-    # -------------------------------------------------------
-    def try_pack(target_k, max_random_trials=0):
+    # ------------------------------------------------------------------
+    # Helper: Calculate Max KVPR
+    # ------------------------------------------------------------------
+    def get_max_kvpr_from_indices(placement_dict):
+        max_p = 0.0
+        for idxs in placement_dict.values():
+            w_sum = sum(m_data[i]['w'] for i in idxs)
+            s_sum = sum(m_data[i]['s'] for i in idxs)
+            rem = GPU_MEM_SIZE - s_sum
+            if rem <= 1e-9:
+                if w_sum > 0: return float('inf')
+                else: continue
+            val = w_sum / rem
+            if val > max_p: max_p = val
+        return max_p
+
+    # ------------------------------------------------------------------
+    # 1. Feasibility Check with Adaptive Stochastic Packing
+    # ------------------------------------------------------------------
+    def try_pack(target_k, effort_level=1):
         """
-        Attempts to pack all models such that for every GPU:
-        sum(w) / (C - sum(s)) <= target_k
-        Equivalent to: sum(w + target_k * s) <= target_k * C
-        Returns placement dict if successful, else None.
+        Attempts to pack models such that for all GPUs: w_sum <= target_k * (C - s_sum).
+        Uses deterministic strategies first, then stochastic strategies with failure memory.
         """
         
-        # Strategies to determine order of packing
-        # Each strategy is (key_function, reverse_bool)
-        # 1. Virtual Size: w + K*s (Standard for variable size bin packing)
-        # 2. Physical Size: s
-        # 3. Weight: w
-        # 4. Density: w/s
-        strategies = [
-            (lambda x: x['w'] + target_k * x['s'], True),
-            (lambda x: x['s'], True),
-            (lambda x: x['w'], True),
-            (lambda x: x['w'] / (x['s'] + 1e-9), True),
-        ]
-
-        # Indices for trials
-        trials = list(range(len(strategies)))
-        if max_random_trials > 0:
-            trials.extend(['rand'] * max_random_trials)
-
-        for t in trials:
-            # Sort indices based on strategy
-            if isinstance(t, int):
-                key_func, reverse = strategies[t]
-                ordered_indices = sorted(range(len(m_data)), key=lambda i: key_func(m_data[i]), reverse=reverse)
-            else:
-                # Stochastic: Perturbed Virtual Size
-                # (w + K*s) * random noise (0.9 to 1.1)
-                # This breaks ties and local optima of deterministic sorts
-                ordered_indices = sorted(range(len(m_data)), 
-                    key=lambda i: (m_data[i]['w'] + target_k * m_data[i]['s']) * random.uniform(0.9, 1.1), 
-                    reverse=True)
-
-            # Perform Packing (Best Fit Decreasing on Virtual Slack)
+        # Internal packing function (Best Fit Decreasing on Virtual Slack)
+        def pack_indices(indices, record_failure=False, failed_set=None):
             bins = [{'w': 0.0, 's': 0.0, 'idxs': []} for _ in range(gpu_num)]
-            possible = True
-
-            for idx in ordered_indices:
+            
+            for idx in indices:
                 item = m_data[idx]
                 w, s = item['w'], item['s']
                 
                 best_bin = None
-                min_virtual_slack = float('inf')
-
-                # We want to find a bin that fits physically AND satisfies pressure constraint
-                # And minimizes the leftover "pressure budget" (Slack)
-                
+                min_slack = float('inf')
+                
+                # Check all bins
                 for b_idx in range(gpu_num):
                     b = bins[b_idx]
-                    
-                    # 1. Physical Fit
                     if b['s'] + s > GPU_MEM_SIZE: continue
-
-                    # 2. Pressure Fit
-                    # w_new / rem_new <= K  <=>  w_new <= K * rem_new
-                    rem_new = GPU_MEM_SIZE - (b['s'] + s)
-                    if rem_new < 0: rem_new = 0.0
-                    
-                    max_w = target_k * rem_new
-                    new_w = b['w'] + w
-                    
-                    if new_w <= max_w + 1e-5:
-                        # Feasible. Calculate slack (virtual remaining capacity)
-                        slack = max_w - new_w
-                        if slack < min_virtual_slack:
-                            min_virtual_slack = slack
+                    
+                    # Constraint: New_W <= K * New_Rem_S
+                    rem_s_new = GPU_MEM_SIZE - (b['s'] + s)
+                    limit_w = target_k * rem_s_new
+                    current_w = b['w'] + w
+                    
+                    if current_w <= limit_w + 1e-6:
+                        slack = limit_w - current_w
+                        if slack < min_slack:
+                            min_slack = slack
                             best_bin = b_idx
                 
                 if best_bin is None:
-                    possible = False
-                    break
+                    if record_failure and failed_set is not None:
+                        failed_set.add(idx)
+                    return None
                 
                 bins[best_bin]['idxs'].append(idx)
                 bins[best_bin]['w'] += w
                 bins[best_bin]['s'] += s
-            
-            if possible:
-                return {i: bins[i]['idxs'] for i in range(gpu_num)}
-
+                
+            return {i: bins[i]['idxs'] for i in range(gpu_num)}
+
+        # Deterministic Strategies
+        # 1. Linearized Constraint: w + K*s
+        # 2. Physical Size: s
+        # 3. Asymptotic Pressure: w / (C - s)
+        det_strategies = [
+            lambda x: x['w'] + target_k * x['s'],
+            lambda x: x['s'],
+            lambda x: x['w'] / (GPU_MEM_SIZE - x['s'] + 1e-6)
+        ]
+
+        for key_fn in det_strategies:
+            sorted_indices = sorted(range(len(m_data)), key=lambda i: key_fn(m_data[i]), reverse=True)
+            res = pack_indices(sorted_indices)
+            if res: return res
+
+        # Stochastic Strategies with Failure Memory
+        failed_items = set()
+        num_attempts = 20 if effort_level == 0 else 100
+
+        for attempt in range(num_attempts):
+            # Rotate base strategy
+            mode = attempt % 3
+            
+            def sort_key(idx):
+                item = m_data[idx]
+                if mode == 0:
+                    val = item['w'] + target_k * item['s']
+                elif mode == 1:
+                    val = item['s']
+                else:
+                    val = item['w'] / (GPU_MEM_SIZE - item['s'] + 1e-6)
+                
+                # Add noise
+                val *= random.uniform(0.9, 1.1)
+                
+                # Prioritize failed items from previous attempts
+                if idx in failed_items:
+                    val += 1e9
+                return val
+
+            sorted_indices = sorted(range(len(m_data)), key=sort_key, reverse=True)
+            res = pack_indices(sorted_indices, record_failure=True, failed_set=failed_items)
+            if res: return res
+            
         return None
 
-    # -------------------------------------------------------
-    # 1. Search Initialization
-    # -------------------------------------------------------
+    # ------------------------------------------------------------------
+    # 2. Initialization and Binary Search
+    # ------------------------------------------------------------------
     best_placement = None
-    best_max_kvpr = float('inf')
-
-    # Establish an upper bound with a very loose pressure constraint (essentially physical packing)
-    # Then evaluate its real pressure.
-    init_k = 2000.0 
-    initial_sol = try_pack(init_k, max_random_trials=0)
-    
-    if initial_sol:
-        # Evaluate
-        curr_max = 0.0
-        for idxs in initial_sol.values():
-            w = sum(m_data[i]['w'] for i in idxs)
-            s = sum(m_data[i]['s'] for i in idxs)
-            rem = GPU_MEM_SIZE - s
-            if rem > 1e-9:
-                val = w/rem
-            elif w > 0:
-                val = float('inf')
-            else:
-                val = 0.0
-            curr_max = max(curr_max, val)
-        best_max_kvpr = curr_max
-        best_placement = initial_sol
-    
-    # Define Binary Search Range
-    high = best_max_kvpr if best_max_kvpr != float('inf') else 5000.0
+    best_score = float('inf')
+
+    # Initial Heuristic Upper Bound
+    k_est = 1000.0
+    if total_w > 0 and rem_global > 0:
+        k_est = (total_w / rem_global) * 1.5
+
+    # Try heuristic K
+    sol = try_pack(k_est, effort_level=0)
+    if sol:
+        best_placement = sol
+        best_score = get_max_kvpr_from_indices(sol)
+    else:
+        # Fallback to loose upper bound
+        sol = try_pack(5000.0, effort_level=0)
+        if sol:
+            best_placement = sol
+            best_score = get_max_kvpr_from_indices(sol)
+
+    # Binary Search
     low = lb
-
-    # -------------------------------------------------------
-    # 2. Binary Search
-    # -------------------------------------------------------
-    # Perform search to push K down. 
-    # The stochastic packer gives us a good chance to find a valid config if one exists near 'mid'.
+    high = best_score if best_score != float('inf') else 3000.0
+
     if high > low + 1e-4:
-        # Number of BS iterations
-        for _ in range(16):
+        for _ in range(20):
             mid = (low + high) / 2.0
-            
-            # Use randomization to try harder to fit into 'mid'
-            sol = try_pack(mid, max_random_trials=8)
+            sol = try_pack(mid, effort_level=1)
             
             if sol:
-                # Found a valid packing.
-                # However, the packing only guarantees KVPR <= mid (approx).
-                # We save it and try to find an even smaller mid.
-                
-                # Recalculate actual max pressure to keep the best real solution found
-                curr_max = 0.0
-                for idxs in sol.values():
-                    w = sum(m_data[i]['w'] for i in idxs)
-                    s = sum(m_data[i]['s'] for i in idxs)
-                    rem = GPU_MEM_SIZE - s
-                    val = w/rem if rem > 1e-9 else (float('inf') if w > 0 else 0.0)
-                    curr_max = max(curr_max, val)
-                
-                if curr_max < best_max_kvpr:
-                    best_max_kvpr = curr_max
+                # Feasible at mid, check actual score
+                # (Actual score might be less than mid due to slack)
+                actual_score = get_max_kvpr_from_indices(sol)
+                if actual_score < best_score:
+                    best_score = actual_score
                     best_placement = sol
                 
+                # Try lower
                 high = mid
             else:
+                # Infeasible
                 low = mid
 
     if best_placement is None:
          raise ValueError("Unable to place models on GPUs with available memory.")
 
-    # -------------------------------------------------------
+    # ------------------------------------------------------------------
     # 3. Local Search Refinement
-    # -------------------------------------------------------
-    # Convert best_placement (indices) to lists of objects for output, 
-    # but keep working with indices for local search speed.
-    curr_map = best_placement # dict: gpu_id -> list of indices
-
-    # Precalculate stats
+    # ------------------------------------------------------------------
+    curr_map = best_placement
+    
+    # Calculate stats for LS
     g_stats = []
     for g in range(gpu_num):
         idxs = curr_map[g]
         w = sum(m_data[i]['w'] for i in idxs)
         s = sum(m_data[i]['s'] for i in idxs)
         rem = GPU_MEM_SIZE - s
-        p = w / rem if rem > 1e-9 else (float('inf') if w > 0 else 0.0)
+        p = w / rem if rem > 1e-9 else float('inf')
         g_stats.append({'w': w, 's': s, 'p': p})
 
-    # Optimization Loop
-    for _ in range(50):
-        # Identify bottleneck
+    for _ in range(100): # Limit LS steps
+        # Find bottleneck GPU
         max_p = -1.0
         src_gpu = -1
         for g in range(gpu_num):
             if g_stats[g]['p'] > max_p:
                 max_p = g_stats[g]['p']
                 src_gpu = g
         
         if src_gpu == -1 or max_p < 1e-9: break
         
         improved = False
-        src_list = curr_map[src_gpu]
-        
-        # 3.1 Try Move
-        for list_idx, m_idx in enumerate(src_list):
+        src_idxs = curr_map[src_gpu]
+        
+        # 3.1 Best Move (Gradient Descent)
+        best_move = None # (idx_in_list, m_idx, dst, new_max_impact)
+
+        for i_idx, m_idx in enumerate(src_idxs):
             m = m_data[m_idx]
             
-            # Src State if moved
-            src_rem_new = GPU_MEM_SIZE - (g_stats[src_gpu]['s'] - m['s'])
-            src_w_new = g_stats[src_gpu]['w'] - m['w']
-            src_p_new = src_w_new / src_rem_new if src_rem_new > 1e-9 else (float('inf') if src_w_new > 0 else 0.0)
-            
-            best_move_dst = None
+            # Predict Src if removed
+            src_rem = GPU_MEM_SIZE - (g_stats[src_gpu]['s'] - m['s'])
+            if src_rem <= 1e-9: src_p = float('inf') # Should not happen if removing makes it smaller
+            else: src_p = (g_stats[src_gpu]['w'] - m['w']) / src_rem
             
             for dst in range(gpu_num):
                 if dst == src_gpu: continue
                 if g_stats[dst]['s'] + m['s'] > GPU_MEM_SIZE: continue
                 
-                dst_rem_new = GPU_MEM_SIZE - (g_stats[dst]['s'] + m['s'])
-                dst_w_new = g_stats[dst]['w'] + m['w']
-                dst_p_new = dst_w_new / dst_rem_new if dst_rem_new > 1e-9 else (float('inf') if dst_w_new > 0 else 0.0)
-                
-                if max(src_p_new, dst_p_new) < max_p - 1e-5:
-                    best_move_dst = dst
-                    break # First improvement is fine
-            
-            if best_move_dst is not None:
-                # Apply Move
-                curr_map[src_gpu].pop(list_idx)
-                curr_map[best_move_dst].append(m_idx)
-                
-                # Update Stats
-                g_stats[src_gpu]['w'] = src_w_new
-                g_stats[src_gpu]['s'] = GPU_MEM_SIZE - src_rem_new
-                g_stats[src_gpu]['p'] = src_p_new
-                
-                dst_rem = GPU_MEM_SIZE - (g_stats[best_move_dst]['s'] + m['s'])
-                dst_w = g_stats[best_move_dst]['w'] + m['w']
-                dst_p = dst_w / dst_rem if dst_rem > 1e-9 else (float('inf') if dst_w > 0 else 0.0)
-                g_stats[best_move_dst] = {'w': dst_w, 's': GPU_MEM_SIZE - dst_rem, 'p': dst_p}
-                
-                improved = True
-                break
+                dst_rem = GPU_MEM_SIZE - (g_stats[dst]['s'] + m['s'])
+                if dst_rem <= 1e-9: continue
+                dst_p = (g_stats[dst]['w'] + m['w']) / dst_rem
+                
+                # Check improvement
+                impact = max(src_p, dst_p)
+                if impact < max_p - 1e-6:
+                    if best_move is None or impact < best_move[3]:
+                        best_move = (i_idx, m_idx, dst, impact)
+
+        if best_move:
+            i_idx, m_idx, dst, _ = best_move
+            # Execute Move
+            curr_map[src_gpu].pop(i_idx)
+            curr_map[dst].append(m_idx)
+            
+            # Update Stats
+            for g in [src_gpu, dst]:
+                idxs = curr_map[g]
+                w = sum(m_data[i]['w'] for i in idxs)
+                s = sum(m_data[i]['s'] for i in idxs)
+                rem = GPU_MEM_SIZE - s
+                p = w / rem if rem > 1e-9 else float('inf')
+                g_stats[g] = {'w': w, 's': s, 'p': p}
+            
+            improved = True
         
         if improved: continue
-        
-        # 3.2 Try Swap (only if move failed)
-        # To save time, only check top heavy items from src or random subset? No, check all for now (usually few items per GPU)
-        for s_list_idx, m_src_idx in enumerate(src_list):
+
+        # 3.2 Greedy Swap
+        for i_src, m_src_idx in enumerate(src_idxs):
             m_src = m_data[m_src_idx]
             
             for dst in range(gpu_num):
                 if dst == src_gpu: continue
-                dst_list = curr_map[dst]
-                
-                for d_list_idx, m_dst_idx in enumerate(dst_list):
+                dst_idxs = curr_map[dst]
+                
+                for i_dst, m_dst_idx in enumerate(dst_idxs):
                     m_dst = m_data[m_dst_idx]
                     
-                    # Capacity Check
-                    new_src_s = g_stats[src_gpu]['s'] - m_src['s'] + m_dst['s']
-                    if new_src_s > GPU_MEM_SIZE: continue
-                    
-                    new_dst_s = g_stats[dst]['s'] - m_dst['s'] + m_src['s']
-                    if new_dst_s > GPU_MEM_SIZE: continue
+                    # Physical Check
+                    s_src_new = g_stats[src_gpu]['s'] - m_src['s'] + m_dst['s']
+                    s_dst_new = g_stats[dst]['s'] - m_dst['s'] + m_src['s']
+                    if s_src_new > GPU_MEM_SIZE or s_dst_new > GPU_MEM_SIZE: continue
                     
                     # Pressure Check
-                    new_src_rem = GPU_MEM_SIZE - new_src_s
-                    new_src_w = g_stats[src_gpu]['w'] - m_src['w'] + m_dst['w']
-                    new_src_p = new_src_w / new_src_rem if new_src_rem > 1e-9 else float('inf')
-                    
-                    new_dst_rem = GPU_MEM_SIZE - new_dst_s
-                    new_dst_w = g_stats[dst]['w'] - m_dst['w'] + m_src['w']
-                    new_dst_p = new_dst_w / new_dst_rem if new_dst_rem > 1e-9 else float('inf')
-                    
-                    if max(new_src_p, new_dst_p) < max_p - 1e-5:
-                        # Apply Swap
-                        curr_map[src_gpu].pop(s_list_idx)
-                        curr_map[src_gpu].append(m_dst_idx)
+                    rem_src = GPU_MEM_SIZE - s_src_new
+                    rem_dst = GPU_MEM_SIZE - s_dst_new
+                    if rem_src <= 1e-9 or rem_dst <= 1e-9: continue
+                    
+                    p_src_new = (g_stats[src_gpu]['w'] - m_src['w'] + m_dst['w']) / rem_src
+                    p_dst_new = (g_stats[dst]['w'] - m_dst['w'] + m_src['w']) / rem_dst
+                    
+                    if max(p_src_new, p_dst_new) < max_p - 1e-6:
+                        # Execute Swap
+                        curr_map[src_gpu][i_src] = m_dst_idx
+                        curr_map[dst][i_dst] = m_src_idx
                         
-                        curr_map[dst].pop(d_list_idx)
-                        curr_map[dst].append(m_src_idx)
+                        # Update Stats
+                        g_stats[src_gpu]['w'] = g_stats[src_gpu]['w'] - m_src['w'] + m_dst['w']
+                        g_stats[src_gpu]['s'] = s_src_new
+                        g_stats[src_gpu]['p'] = p_src_new
                         
-                        g_stats[src_gpu] = {'w': new_src_w, 's': new_src_s, 'p': new_src_p}
-                        g_stats[dst] = {'w': new_dst_w, 's': new_dst_s, 'p': new_dst_p}
+                        g_stats[dst]['w'] = g_stats[dst]['w'] - m_dst['w'] + m_src['w']
+                        g_stats[dst]['s'] = s_dst_new
+                        g_stats[dst]['p'] = p_dst_new
                         
                         improved = True
                         break
                 if improved: break
             if improved: break
             
         if not improved: break
 
-    # Final conversion
+    # 4. Result Formatting
     result = {}
     for g, idxs in curr_map.items():
         result[g] = [m_data[i]['obj'] for i in idxs]
-    
     return result
-
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")