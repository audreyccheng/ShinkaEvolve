--- a/original.py
+++ b/original.py
@@ -1,338 +1,406 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
-GPU_MEM_SIZE = 80  # GB
+import math
+import heapq
+
+GPU_MEM_SIZE = 80.0  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
-    Combines Greedy Heuristics, Binary Search (Bin Packing), and Local Search.
+    
+    Architecture:
+    1. Beam Search Construction: Explores multiple placement paths simultaneously 
+       using different sorting strategies and pruning symmetric states.
+    2. Binary Search refinement: Solves the Min-Max problem by converting it to 
+       Bin Packing with variable item sizes, using Best Fit Decreasing.
+    3. Local Search: Optimizes the best found solution using Move and Swap operations.
     """
 
-    # Helper to calculate max KVPR of a placement
-    def get_max_kvpr(placement):
+    # 0. Preprocessing
+    if not models:
+        return {i: [] for i in range(gpu_num)}
+
+    m_data = []
+    total_w = 0.0
+    total_s = 0.0
+    for i, m in enumerate(models):
+        w = m.req_rate / m.slo
+        s = m.model_size
+        m_data.append({
+            'id': i,
+            'w': w,
+            's': s,
+            'obj': m
+        })
+        total_w += w
+        total_s += s
+
+    # Helper to calculate max KVPR from a placement list (list of lists of indices)
+    def calculate_score(placement_indices):
         max_p = 0.0
-        for assigned in placement.values():
-            w = sum(m.req_rate / m.slo for m in assigned)
-            s = sum(m.model_size for m in assigned)
-            rem = GPU_MEM_SIZE - s
+        for indices in placement_indices:
+            cur_w = sum(m_data[idx]['w'] for idx in indices)
+            cur_s = sum(m_data[idx]['s'] for idx in indices)
+            rem = GPU_MEM_SIZE - cur_s
+            
             if rem <= 1e-9:
-                # If usage is exactly max memory (or close), and weight > 0, pressure is inf.
-                # If weight is 0, it's 0. But models have weight.
-                if w > 0: return float('inf')
+                if cur_w > 0: return float('inf')
                 else: continue
-            max_p = max(max_p, w / rem)
+            
+            p = cur_w / rem
+            if p > max_p: max_p = p
         return max_p
 
-    best_placement = None
-    best_score = float('inf')
-
-    # ---------------------------------------------------------
-    # 1. Greedy Heuristics Ensemble
-    # ---------------------------------------------------------
-    # Strategies:
-    #   'min_result': Place on GPU that minimizes the resulting KVPR (Greedy Min-Max)
-    #   'min_current': Place on GPU that has the lowest current KVPR (Load Balancing/Valley Filling)
-
-    heuristics = [
-        (lambda m: m.req_rate / m.slo, 'min_result'),
-        (lambda m: m.req_rate / m.slo, 'min_current'),
-        (lambda m: m.model_size, 'min_result'),
-        (lambda m: (m.req_rate / m.slo) / (GPU_MEM_SIZE - m.model_size + 1e-6), 'min_result'),
+    best_placement_indices = None
+    best_max_kvpr = float('inf')
+
+    # ---------------------------------------------------------
+    # 1. Beam Search Construction
+    # ---------------------------------------------------------
+    # We use beam search with limited width to explore placements.
+    # To handle identical GPUs, we prune states that are permutations of each other.
+    
+    BEAM_WIDTH = 8
+    
+    # Sorting strategies dictate the order models are added
+    strategies = [
+        lambda x: x['w'],                           # Weight Descending
+        lambda x: x['s'],                           # Size Descending
+        lambda x: x['w'] / (x['s'] + 1e-6),         # Density Descending
+        lambda x: x['w'] / (GPU_MEM_SIZE - x['s'] + 1e-6) # Isolated Pressure Descending
     ]
 
-    for key_fn, strategy in heuristics:
-        sorted_models = sorted(models, key=key_fn, reverse=True)
-
-        placement = {i: [] for i in range(gpu_num)}
-        gpu_w = [0.0] * gpu_num
-        gpu_s = [0.0] * gpu_num
-        possible = True
-
-        for model in sorted_models:
-            w = model.req_rate / model.slo
-            s = model.model_size
-
-            best_idx = None
-            best_val = float('inf')
-
-            for i in range(gpu_num):
-                if gpu_s[i] + s > GPU_MEM_SIZE: continue
-
-                rem = GPU_MEM_SIZE - gpu_s[i]
-
-                if strategy == 'min_result':
-                    new_rem = rem - s
-                    if new_rem > 1e-9:
-                        val = (gpu_w[i] + w) / new_rem
+    for sort_key in strategies:
+        sorted_indices = sorted(range(len(m_data)), key=lambda i: sort_key(m_data[i]), reverse=True)
+        
+        # State: (current_max_kvpr, gpu_states_tuple, placement_tuple)
+        # gpu_states_tuple: tuple of (w, s) for each GPU
+        # placement_tuple: tuple of tuples of model indices
+        
+        init_states = tuple([(0.0, 0.0)] * gpu_num)
+        init_placement = tuple([() for _ in range(gpu_num)])
+        
+        # Beam: list of states
+        beam = [(0.0, init_states, init_placement)]
+        
+        for m_idx in sorted_indices:
+            item = m_data[m_idx]
+            w, s = item['w'], item['s']
+            
+            candidates = []
+            seen_configs = set()
+            
+            for score, states, pl in beam:
+                # Try placing on each unique GPU state to avoid symmetry redundancy
+                # (e.g., if GPU 0 and GPU 1 are empty, placing on 0 is same as 1)
+                
+                # To efficiently implement symmetry breaking:
+                # We iterate all GPUs, but we track the (w,s) state of the GPU we placed on.
+                # Actually, simpler: Generate all valid children, then filter duplicates by sorting states.
+                
+                current_step_candidates = []
+                
+                for g in range(gpu_num):
+                    gw, gs = states[g]
+                    if gs + s > GPU_MEM_SIZE: continue
+                    
+                    new_gs = gs + s
+                    new_gw = gw + w
+                    
+                    # Calculate new local pressure
+                    rem = GPU_MEM_SIZE - new_gs
+                    if rem <= 1e-9:
+                        if new_gw > 0: local_p = float('inf')
+                        else: local_p = 0.0
                     else:
-                        val = float('inf')
-                else: # min_current
-                    if rem > 1e-9:
-                        val = gpu_w[i] / rem
-                    else:
-                        val = float('inf')
-
-                if val < best_val:
-                    best_val = val
-                    best_idx = i
-                elif val == best_val and best_idx is None:
-                    best_idx = i
-
-            if best_idx is None:
-                possible = False
+                        local_p = new_gw / rem
+                    
+                    new_score = max(score, local_p)
+                    
+                    # Update state structures
+                    # Tuples are immutable, create new ones
+                    new_states_list = list(states)
+                    new_states_list[g] = (new_gw, new_gs)
+                    new_states = tuple(new_states_list)
+                    
+                    new_pl_list = list(pl)
+                    new_pl_list[g] = pl[g] + (m_idx,)
+                    new_pl = tuple(new_pl_list)
+                    
+                    # Canonical representation for symmetry pruning: sorted states
+                    canonical = tuple(sorted(new_states))
+                    if canonical in seen_configs:
+                        continue
+                    seen_configs.add(canonical)
+                    
+                    current_step_candidates.append((new_score, new_states, new_pl))
+                
+                candidates.extend(current_step_candidates)
+            
+            if not candidates:
+                beam = []
                 break
-
-            placement[best_idx].append(model)
-            gpu_w[best_idx] += w
-            gpu_s[best_idx] += s
-
-        if possible:
-            score = get_max_kvpr(placement)
-            if score < best_score:
-                best_score = score
-                best_placement = placement
-
-    # ---------------------------------------------------------
-    # 2. Binary Search on Target KVPR (Transformation to Bin Packing)
-    # ---------------------------------------------------------
-    # We want to check if there exists a placement such that KVPR <= K for all GPUs.
-    # Constraint: sum(w) / (C - sum(s)) <= K  <==>  sum(w) + K*sum(s) <= K*C
-    # Let v_i(K) = w_i + K*s_i. We pack items of size v_i into bins of capacity K*C.
-
-    # Lower bound: Perfect fluid balance
-    total_w = sum(m.req_rate / m.slo for m in models)
-    total_s = sum(m.model_size for m in models)
+            
+            # Prune beam - keep top K best scores
+            # If many candidates, use nsmallest for efficiency
+            if len(candidates) > BEAM_WIDTH:
+                beam = heapq.nsmallest(BEAM_WIDTH, candidates, key=lambda x: x[0])
+            else:
+                beam = candidates
+
+        # Evaluate final beam states
+        for score, states, pl in beam:
+            # Re-verify score just to be safe (though tracked incrementally)
+            if score < best_max_kvpr:
+                best_max_kvpr = score
+                best_placement_indices = [list(x) for x in pl]
+
+    # ---------------------------------------------------------
+    # 2. Binary Search (Bin Packing Transformation)
+    # ---------------------------------------------------------
+    # Check if we can fit models with KVPR <= K.
+    # Logic: item size v_i = w_i + K * s_i, Bin Capacity = K * GPU_MEM_SIZE
+    # We use "Best Fit Decreasing" on these virtual sizes.
+    
     rem_global = gpu_num * GPU_MEM_SIZE - total_s
-
-    if rem_global > 1e-6:
-        low = total_w / rem_global
-        high = best_score if best_score != float('inf') else 1000.0
-
-        # Binary Search loop
-        # Only if the range is significant
-        if high > low + 1e-4:
-            for _ in range(20):
-                mid = (low + high) / 2
-
-                # Sort items by virtual size v_i(mid) descending
-                # This heuristics works well for Bin Packing (Best Fit Decreasing)
-                bs_models = sorted(models, key=lambda m: (m.req_rate/m.slo) + mid * m.model_size, reverse=True)
-
-                temp_placement = {i: [] for i in range(gpu_num)}
-                gpu_w = [0.0] * gpu_num
-                gpu_s = [0.0] * gpu_num
-                possible_k = True
-
-                for model in bs_models:
-                    w = model.req_rate / model.slo
-                    s = model.model_size
-
-                    best_idx = None
-                    min_slack = float('inf')
-
-                    # Best Fit Decreasing logic
-                    for i in range(gpu_num):
-                        if gpu_s[i] + s > GPU_MEM_SIZE: continue
-
-                        # Check KVPR constraint: (W + w) <= mid * (C - S - s)
-                        lhs = gpu_w[i] + w
-                        rhs = mid * (GPU_MEM_SIZE - gpu_s[i] - s)
-
-                        if lhs <= rhs + 1e-5:
-                            slack = rhs - lhs
-                            if slack < min_slack:
-                                min_slack = slack
-                                best_idx = i
-
-                    if best_idx is None:
-                        possible_k = False
-                        break
-
-                    temp_placement[best_idx].append(model)
-                    gpu_w[best_idx] += w
-                    gpu_s[best_idx] += s
-
-                if possible_k:
-                    # Found a valid placement with max KVPR <= mid
-                    current_actual_max = get_max_kvpr(temp_placement)
-                    if current_actual_max < best_score:
-                        best_score = current_actual_max
-                        best_placement = temp_placement
-                    high = mid
-                else:
-                    low = mid
-
-    if best_placement is None:
+    low = total_w / rem_global if rem_global > 1e-6 else best_max_kvpr
+    high = best_max_kvpr if best_max_kvpr != float('inf') else 1000.0
+
+    if high > low + 1e-4:
+        for _ in range(15):
+            mid = (low + high) / 2.0
+            
+            # Sort by effective size for this K
+            # s + w/K is proportional to w + K*s
+            check_indices = sorted(range(len(m_data)), 
+                                 key=lambda i: m_data[i]['s'] + m_data[i]['w']/mid, 
+                                 reverse=True)
+            
+            temp_alloc = [[] for _ in range(gpu_num)]
+            temp_states = [{'w': 0.0, 's': 0.0} for _ in range(gpu_num)]
+            possible = True
+            
+            for idx in check_indices:
+                item = m_data[idx]
+                w, s = item['w'], item['s']
+                
+                best_g = None
+                min_slack = float('inf')
+                
+                # Best Fit Decreasing on "Effective Capacity"
+                # Constraint: w_new + mid*s_new <= mid*C_new
+                # <=> w + mid*s <= mid * (C - current_s) - current_w (Wait, this is wrong)
+                # Correct: (current_w + w) / (C - current_s - s) <= mid
+                # <=> current_w + w <= mid * (C - current_s - s)
+                
+                for g in range(gpu_num):
+                    st = temp_states[g]
+                    if st['s'] + s > GPU_MEM_SIZE: continue
+                    
+                    phys_rem = GPU_MEM_SIZE - st['s'] - s
+                    if phys_rem < 0: phys_rem = 0.0
+                    
+                    lhs = st['w'] + w
+                    rhs = mid * phys_rem
+                    
+                    if lhs <= rhs + 1e-7:
+                        # Feasible
+                        # Minimize slack (Best Fit) to pack tightly
+                        slack = rhs - lhs
+                        if slack < min_slack:
+                            min_slack = slack
+                            best_g = g
+                
+                if best_g is None:
+                    possible = False
+                    break
+                
+                temp_alloc[best_g].append(idx)
+                temp_states[best_g]['w'] += w
+                temp_states[best_g]['s'] += s
+            
+            if possible:
+                score = calculate_score(temp_alloc)
+                if score < best_max_kvpr:
+                    best_max_kvpr = score
+                    best_placement_indices = temp_alloc
+                high = mid
+            else:
+                low = mid
+
+    if best_placement_indices is None:
         raise ValueError("Unable to place models on GPUs with available memory.")
 
     # ---------------------------------------------------------
     # 3. Local Search Refinement
     # ---------------------------------------------------------
-    # Iteratively move or swap models to reduce the peak KVPR
-
-    # Calculate initial states
-    gpu_states = []
-    current_kvpr = []
-    for i in range(gpu_num):
-        assigned = best_placement[i]
-        w = sum(m.req_rate / m.slo for m in assigned)
-        s = sum(m.model_size for m in assigned)
-        gpu_states.append({'w': w, 's': s})
+    # Convert best placement to mutable format
+    # best_placement_indices is list of lists
+    
+    # Precompute GPU states
+    current_states = []
+    for g in range(gpu_num):
+        indices = best_placement_indices[g]
+        w = sum(m_data[i]['w'] for i in indices)
+        s = sum(m_data[i]['s'] for i in indices)
         rem = GPU_MEM_SIZE - s
         p = w / rem if rem > 1e-9 else float('inf')
-        current_kvpr.append(p)
-
-    for _ in range(100):
-        # Find current global max pressure
-        max_p = max(current_kvpr)
-        if max_p < 1e-9: break
-
-        # Identify bottleneck GPUs
-        src_gpus = [i for i, p in enumerate(current_kvpr) if abs(p - max_p) < 1e-9]
-        # Pick one to optimize
-        src_gpu = src_gpus[0]
-        src_models = best_placement[src_gpu]
-
+        current_states.append({'w': w, 's': s, 'p': p})
+
+    # Optimization Loop
+    for _ in range(200):
+        # Identify bottleneck
+        max_p = -1.0
+        src_g = -1
+        for g in range(gpu_num):
+            if current_states[g]['p'] > max_p:
+                max_p = current_states[g]['p']
+                src_g = g
+        
+        if src_g == -1 or max_p < 1e-9: break
+        
         improved = False
-
-        # 1. Try MOVE (src -> dst)
-        for m_idx, model in enumerate(src_models):
-            w = model.req_rate / model.slo
-            s = model.model_size
-
-            # Hypothetical Src State
-            new_src_s = gpu_states[src_gpu]['s'] - s
-            new_src_w = gpu_states[src_gpu]['w'] - w
-            new_src_rem = GPU_MEM_SIZE - new_src_s
-            new_src_p = new_src_w / new_src_rem if new_src_rem > 1e-9 else float('inf')
-
-            for dst_gpu in range(gpu_num):
-                if dst_gpu == src_gpu: continue
-
-                if gpu_states[dst_gpu]['s'] + s > GPU_MEM_SIZE: continue
-
-                new_dst_s = gpu_states[dst_gpu]['s'] + s
-                new_dst_w = gpu_states[dst_gpu]['w'] + w
-                new_dst_rem = GPU_MEM_SIZE - new_dst_s
-                new_dst_p = new_dst_w / new_dst_rem if new_dst_rem > 1e-9 else float('inf')
-
-                # Check if this move reduces the pressure of the bottleneck
-                # and doesn't create a new bottleneck worse than current max_p
-                if max(new_src_p, new_dst_p) < max_p - 1e-6:
-                    # Apply Move
-                    moved_model = src_models.pop(m_idx)
-                    best_placement[dst_gpu].append(moved_model)
-
-                    gpu_states[src_gpu] = {'w': new_src_w, 's': new_src_s}
-                    gpu_states[dst_gpu] = {'w': new_dst_w, 's': new_dst_s}
-                    current_kvpr[src_gpu] = new_src_p
-                    current_kvpr[dst_gpu] = new_dst_p
+        src_indices = best_placement_indices[src_g]
+        
+        # 3.1 MOVE Operation
+        for i, m_idx in enumerate(src_indices):
+            m = m_data[m_idx]
+            
+            # Helper to predict p
+            def predict_p(w, s):
+                rem = GPU_MEM_SIZE - s
+                return w / rem if rem > 1e-9 else float('inf')
+
+            # Hypo src
+            src_new_w = current_states[src_g]['w'] - m['w']
+            src_new_s = current_states[src_g]['s'] - m['s']
+            src_new_p = predict_p(src_new_w, src_new_s)
+            
+            best_dst = None
+            
+            for dst_g in range(gpu_num):
+                if dst_g == src_g: continue
+                if current_states[dst_g]['s'] + m['s'] > GPU_MEM_SIZE: continue
+                
+                dst_new_w = current_states[dst_g]['w'] + m['w']
+                dst_new_s = current_states[dst_g]['s'] + m['s']
+                dst_new_p = predict_p(dst_new_w, dst_new_s)
+                
+                # Improvement criteria: Reduce global max
+                if max(src_new_p, dst_new_p) < max_p - 1e-6:
+                    # Apply move
+                    src_indices.pop(i)
+                    best_placement_indices[dst_g].append(m_idx)
+                    
+                    current_states[src_g].update({'w': src_new_w, 's': src_new_s, 'p': src_new_p})
+                    current_states[dst_g].update({'w': dst_new_w, 's': dst_new_s, 'p': dst_new_p})
                     improved = True
                     break
             if improved: break
-
+        
         if improved: continue
 
-        # 2. Try SWAP (src <-> dst)
-        # Iterate over all models in src and all models in other GPUs
-        for m_idx, m_src in enumerate(src_models):
-            w_src = m_src.req_rate / m_src.slo
-            s_src = m_src.model_size
-
-            for dst_gpu in range(gpu_num):
-                if dst_gpu == src_gpu: continue
-
-                dst_models = best_placement[dst_gpu]
-                for d_idx, m_dst in enumerate(dst_models):
-                    w_dst = m_dst.req_rate / m_dst.slo
-                    s_dst = m_dst.model_size
-
-                    # Check Capacity
-                    new_src_s = gpu_states[src_gpu]['s'] - s_src + s_dst
-                    if new_src_s > GPU_MEM_SIZE: continue
-
-                    new_dst_s = gpu_states[dst_gpu]['s'] - s_dst + s_src
-                    if new_dst_s > GPU_MEM_SIZE: continue
-
-                    # Check Pressure
-                    new_src_w = gpu_states[src_gpu]['w'] - w_src + w_dst
-                    new_src_rem = GPU_MEM_SIZE - new_src_s
-                    new_src_p = new_src_w / new_src_rem if new_src_rem > 1e-9 else float('inf')
-
-                    new_dst_w = gpu_states[dst_gpu]['w'] - w_dst + w_src
-                    new_dst_rem = GPU_MEM_SIZE - new_dst_s
-                    new_dst_p = new_dst_w / new_dst_rem if new_dst_rem > 1e-9 else float('inf')
-
-                    if max(new_src_p, new_dst_p) < max_p - 1e-6:
+        # 3.2 SWAP Operation
+        # Only swap if move didn't work.
+        for i, m1_idx in enumerate(src_indices):
+            m1 = m_data[m1_idx]
+            
+            for dst_g in range(gpu_num):
+                if dst_g == src_g: continue
+                # Optimization: Skip if dst is also high pressure
+                if current_states[dst_g]['p'] > max_p - 1.0: continue
+                
+                dst_indices = best_placement_indices[dst_g]
+                for j, m2_idx in enumerate(dst_indices):
+                    m2 = m_data[m2_idx]
+                    
+                    # Capacity Check
+                    new_src_s = current_states[src_g]['s'] - m1['s'] + m2['s']
+                    new_dst_s = current_states[dst_g]['s'] - m2['s'] + m1['s']
+                    
+                    if new_src_s > GPU_MEM_SIZE or new_dst_s > GPU_MEM_SIZE: continue
+                    
+                    # Pressure Check
+                    new_src_w = current_states[src_g]['w'] - m1['w'] + m2['w']
+                    new_dst_w = current_states[dst_g]['w'] - m2['w'] + m1['w']
+                    
+                    # Recalculate pressures
+                    rem_src = GPU_MEM_SIZE - new_src_s
+                    p_src = new_src_w / rem_src if rem_src > 1e-9 else float('inf')
+                    
+                    rem_dst = GPU_MEM_SIZE - new_dst_s
+                    p_dst = new_dst_w / rem_dst if rem_dst > 1e-9 else float('inf')
+                    
+                    if max(p_src, p_dst) < max_p - 1e-6:
                         # Apply Swap
-                        mod_src = src_models.pop(m_idx)
-                        mod_dst = dst_models.pop(d_idx)
-
-                        src_models.append(mod_dst) # Put dst model into src list
-                        dst_models.append(mod_src) # Put src model into dst list
-
-                        gpu_states[src_gpu] = {'w': new_src_w, 's': new_src_s}
-                        gpu_states[dst_gpu] = {'w': new_dst_w, 's': new_dst_s}
-                        current_kvpr[src_gpu] = new_src_p
-                        current_kvpr[dst_gpu] = new_dst_p
+                        src_indices[i] = m2_idx
+                        dst_indices[j] = m1_idx
+                        
+                        current_states[src_g].update({'w': new_src_w, 's': new_src_s, 'p': p_src})
+                        current_states[dst_g].update({'w': new_dst_w, 's': new_dst_s, 'p': p_dst})
                         improved = True
                         break
                 if improved: break
             if improved: break
-
-        if not improved:
-            break
-
-    return best_placement
+            
+        if not improved: break
+
+    # Final result mapping
+    final_placement = {}
+    for g in range(gpu_num):
+        final_placement[g] = [m_data[i]['obj'] for i in best_placement_indices[g]]
+
+    return final_placement
 
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")