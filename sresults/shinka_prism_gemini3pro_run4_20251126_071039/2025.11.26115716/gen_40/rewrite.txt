# EVOLVE-BLOCK-START
"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""

import random
import math

GPU_MEM_SIZE = 80.0  # GB

def compute_model_placement(gpu_num, models):
    """
    Minimizes max KVPR using Binary Search on the target pressure K.
    The feasibility check transforms the problem into Bin Packing with item size (w + K*s).
    Uses deterministic and stochastic ordering strategies for robust packing.
    """

    # 0. Precompute model data
    m_data = []
    total_w = 0.0
    total_s = 0.0
    for i, m in enumerate(models):
        w = m.req_rate / m.slo
        s = m.model_size
        m_data.append({'w': w, 's': s, 'obj': m, 'id': i})
        total_w += w
        total_s += s

    # Theoretical Lower Bound
    # sum(w) / (N*C - sum(s))
    rem_global = gpu_num * GPU_MEM_SIZE - total_s
    if rem_global <= 1e-9:
        if total_w > 0: lb = float('inf')
        else: lb = 0.0
    else:
        lb = total_w / rem_global

    # -------------------------------------------------------
    # Helper: Feasibility Check for Pressure K
    # -------------------------------------------------------
    def try_pack(target_k, max_random_trials=0):
        """
        Attempts to pack all models such that for every GPU:
        sum(w) / (C - sum(s)) <= target_k
        Equivalent to: sum(w + target_k * s) <= target_k * C
        Returns placement dict if successful, else None.
        """
        
        # Strategies to determine order of packing
        # Each strategy is (key_function, reverse_bool)
        # 1. Virtual Size: w + K*s (Standard for variable size bin packing)
        # 2. Physical Size: s
        # 3. Weight: w
        # 4. Density: w/s
        strategies = [
            (lambda x: x['w'] + target_k * x['s'], True),
            (lambda x: x['s'], True),
            (lambda x: x['w'], True),
            (lambda x: x['w'] / (x['s'] + 1e-9), True),
        ]

        # Indices for trials
        trials = list(range(len(strategies)))
        if max_random_trials > 0:
            trials.extend(['rand'] * max_random_trials)

        for t in trials:
            # Sort indices based on strategy
            if isinstance(t, int):
                key_func, reverse = strategies[t]
                ordered_indices = sorted(range(len(m_data)), key=lambda i: key_func(m_data[i]), reverse=reverse)
            else:
                # Stochastic: Perturbed Virtual Size
                # (w + K*s) * random noise (0.9 to 1.1)
                # This breaks ties and local optima of deterministic sorts
                ordered_indices = sorted(range(len(m_data)), 
                    key=lambda i: (m_data[i]['w'] + target_k * m_data[i]['s']) * random.uniform(0.9, 1.1), 
                    reverse=True)

            # Perform Packing (Best Fit Decreasing on Virtual Slack)
            bins = [{'w': 0.0, 's': 0.0, 'idxs': []} for _ in range(gpu_num)]
            possible = True

            for idx in ordered_indices:
                item = m_data[idx]
                w, s = item['w'], item['s']
                
                best_bin = None
                min_virtual_slack = float('inf')

                # We want to find a bin that fits physically AND satisfies pressure constraint
                # And minimizes the leftover "pressure budget" (Slack)
                
                for b_idx in range(gpu_num):
                    b = bins[b_idx]
                    
                    # 1. Physical Fit
                    if b['s'] + s > GPU_MEM_SIZE: continue

                    # 2. Pressure Fit
                    # w_new / rem_new <= K  <=>  w_new <= K * rem_new
                    rem_new = GPU_MEM_SIZE - (b['s'] + s)
                    if rem_new < 0: rem_new = 0.0
                    
                    max_w = target_k * rem_new
                    new_w = b['w'] + w
                    
                    if new_w <= max_w + 1e-5:
                        # Feasible. Calculate slack (virtual remaining capacity)
                        slack = max_w - new_w
                        if slack < min_virtual_slack:
                            min_virtual_slack = slack
                            best_bin = b_idx
                
                if best_bin is None:
                    possible = False
                    break
                
                bins[best_bin]['idxs'].append(idx)
                bins[best_bin]['w'] += w
                bins[best_bin]['s'] += s
            
            if possible:
                return {i: bins[i]['idxs'] for i in range(gpu_num)}

        return None

    # -------------------------------------------------------
    # 1. Search Initialization
    # -------------------------------------------------------
    best_placement = None
    best_max_kvpr = float('inf')

    # Establish an upper bound with a very loose pressure constraint (essentially physical packing)
    # Then evaluate its real pressure.
    init_k = 2000.0 
    initial_sol = try_pack(init_k, max_random_trials=0)
    
    if initial_sol:
        # Evaluate
        curr_max = 0.0
        for idxs in initial_sol.values():
            w = sum(m_data[i]['w'] for i in idxs)
            s = sum(m_data[i]['s'] for i in idxs)
            rem = GPU_MEM_SIZE - s
            if rem > 1e-9:
                val = w/rem
            elif w > 0:
                val = float('inf')
            else:
                val = 0.0
            curr_max = max(curr_max, val)
        best_max_kvpr = curr_max
        best_placement = initial_sol
    
    # Define Binary Search Range
    high = best_max_kvpr if best_max_kvpr != float('inf') else 5000.0
    low = lb

    # -------------------------------------------------------
    # 2. Binary Search
    # -------------------------------------------------------
    # Perform search to push K down. 
    # The stochastic packer gives us a good chance to find a valid config if one exists near 'mid'.
    if high > low + 1e-4:
        # Number of BS iterations
        for _ in range(16):
            mid = (low + high) / 2.0
            
            # Use randomization to try harder to fit into 'mid'
            sol = try_pack(mid, max_random_trials=8)
            
            if sol:
                # Found a valid packing.
                # However, the packing only guarantees KVPR <= mid (approx).
                # We save it and try to find an even smaller mid.
                
                # Recalculate actual max pressure to keep the best real solution found
                curr_max = 0.0
                for idxs in sol.values():
                    w = sum(m_data[i]['w'] for i in idxs)
                    s = sum(m_data[i]['s'] for i in idxs)
                    rem = GPU_MEM_SIZE - s
                    val = w/rem if rem > 1e-9 else (float('inf') if w > 0 else 0.0)
                    curr_max = max(curr_max, val)
                
                if curr_max < best_max_kvpr:
                    best_max_kvpr = curr_max
                    best_placement = sol
                
                high = mid
            else:
                low = mid

    if best_placement is None:
         raise ValueError("Unable to place models on GPUs with available memory.")

    # -------------------------------------------------------
    # 3. Local Search Refinement
    # -------------------------------------------------------
    # Convert best_placement (indices) to lists of objects for output, 
    # but keep working with indices for local search speed.
    curr_map = best_placement # dict: gpu_id -> list of indices

    # Precalculate stats
    g_stats = []
    for g in range(gpu_num):
        idxs = curr_map[g]
        w = sum(m_data[i]['w'] for i in idxs)
        s = sum(m_data[i]['s'] for i in idxs)
        rem = GPU_MEM_SIZE - s
        p = w / rem if rem > 1e-9 else (float('inf') if w > 0 else 0.0)
        g_stats.append({'w': w, 's': s, 'p': p})

    # Optimization Loop
    for _ in range(50):
        # Identify bottleneck
        max_p = -1.0
        src_gpu = -1
        for g in range(gpu_num):
            if g_stats[g]['p'] > max_p:
                max_p = g_stats[g]['p']
                src_gpu = g
        
        if src_gpu == -1 or max_p < 1e-9: break
        
        improved = False
        src_list = curr_map[src_gpu]
        
        # 3.1 Try Move
        for list_idx, m_idx in enumerate(src_list):
            m = m_data[m_idx]
            
            # Src State if moved
            src_rem_new = GPU_MEM_SIZE - (g_stats[src_gpu]['s'] - m['s'])
            src_w_new = g_stats[src_gpu]['w'] - m['w']
            src_p_new = src_w_new / src_rem_new if src_rem_new > 1e-9 else (float('inf') if src_w_new > 0 else 0.0)
            
            best_move_dst = None
            
            for dst in range(gpu_num):
                if dst == src_gpu: continue
                if g_stats[dst]['s'] + m['s'] > GPU_MEM_SIZE: continue
                
                dst_rem_new = GPU_MEM_SIZE - (g_stats[dst]['s'] + m['s'])
                dst_w_new = g_stats[dst]['w'] + m['w']
                dst_p_new = dst_w_new / dst_rem_new if dst_rem_new > 1e-9 else (float('inf') if dst_w_new > 0 else 0.0)
                
                if max(src_p_new, dst_p_new) < max_p - 1e-5:
                    best_move_dst = dst
                    break # First improvement is fine
            
            if best_move_dst is not None:
                # Apply Move
                curr_map[src_gpu].pop(list_idx)
                curr_map[best_move_dst].append(m_idx)
                
                # Update Stats
                g_stats[src_gpu]['w'] = src_w_new
                g_stats[src_gpu]['s'] = GPU_MEM_SIZE - src_rem_new
                g_stats[src_gpu]['p'] = src_p_new
                
                dst_rem = GPU_MEM_SIZE - (g_stats[best_move_dst]['s'] + m['s'])
                dst_w = g_stats[best_move_dst]['w'] + m['w']
                dst_p = dst_w / dst_rem if dst_rem > 1e-9 else (float('inf') if dst_w > 0 else 0.0)
                g_stats[best_move_dst] = {'w': dst_w, 's': GPU_MEM_SIZE - dst_rem, 'p': dst_p}
                
                improved = True
                break
        
        if improved: continue
        
        # 3.2 Try Swap (only if move failed)
        # To save time, only check top heavy items from src or random subset? No, check all for now (usually few items per GPU)
        for s_list_idx, m_src_idx in enumerate(src_list):
            m_src = m_data[m_src_idx]
            
            for dst in range(gpu_num):
                if dst == src_gpu: continue
                dst_list = curr_map[dst]
                
                for d_list_idx, m_dst_idx in enumerate(dst_list):
                    m_dst = m_data[m_dst_idx]
                    
                    # Capacity Check
                    new_src_s = g_stats[src_gpu]['s'] - m_src['s'] + m_dst['s']
                    if new_src_s > GPU_MEM_SIZE: continue
                    
                    new_dst_s = g_stats[dst]['s'] - m_dst['s'] + m_src['s']
                    if new_dst_s > GPU_MEM_SIZE: continue
                    
                    # Pressure Check
                    new_src_rem = GPU_MEM_SIZE - new_src_s
                    new_src_w = g_stats[src_gpu]['w'] - m_src['w'] + m_dst['w']
                    new_src_p = new_src_w / new_src_rem if new_src_rem > 1e-9 else float('inf')
                    
                    new_dst_rem = GPU_MEM_SIZE - new_dst_s
                    new_dst_w = g_stats[dst]['w'] - m_dst['w'] + m_src['w']
                    new_dst_p = new_dst_w / new_dst_rem if new_dst_rem > 1e-9 else float('inf')
                    
                    if max(new_src_p, new_dst_p) < max_p - 1e-5:
                        # Apply Swap
                        curr_map[src_gpu].pop(s_list_idx)
                        curr_map[src_gpu].append(m_dst_idx)
                        
                        curr_map[dst].pop(d_list_idx)
                        curr_map[dst].append(m_src_idx)
                        
                        g_stats[src_gpu] = {'w': new_src_w, 's': new_src_s, 'p': new_src_p}
                        g_stats[dst] = {'w': new_dst_w, 's': new_dst_s, 'p': new_dst_p}
                        
                        improved = True
                        break
                if improved: break
            if improved: break
            
        if not improved: break

    # Final conversion
    result = {}
    for g, idxs in curr_map.items():
        result[g] = [m_data[i]['obj'] for i in idxs]
    
    return result

# EVOLVE-BLOCK-END