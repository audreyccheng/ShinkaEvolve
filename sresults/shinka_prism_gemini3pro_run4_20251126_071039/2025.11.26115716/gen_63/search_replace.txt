system
I will provide the edit in the specified format.

<NAME>
improved_bs_and_ls
</NAME>

<DESCRIPTION>
1.  **Multi-Strategy Stochastic Packing in Binary Search**: Replaces the single "Effective Size" sort key with a rotation of strategies (Effective Size, Physical Size, Weight, Density). This diversifies the search for a feasible packing at a given pressure $K$, especially useful for escaping local optima that a single heuristic might fall into.
2.  **Best-Fit Destination in Local Search**: Modifies the "Move" operator in the local search phase. Instead of accepting the *first* destination GPU that satisfies the pressure constraint (First Fit), it scans all valid destinations and selects the one that results in the *minimum* new pressure (Best Fit). This strategy promotes better load balancing, creating more slack on non-bottleneck GPUs to absorb future moves from bottleneck GPUs.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
                # Randomized Best Fit Decreasing Check with Failure Memory
                found_placement = None
                failed_items = set()

                for attempt in range(20):
                    noise = 0.0
                    if attempt > 0:
                        noise = 0.02 + (attempt * 0.005)

                    def sort_key(m):
                        # Base: Effective size
                        val = m.model_size + (m.req_rate / m.slo) / mid
                        # Noise perturbation
                        if noise > 0:
                            val *= random.uniform(1.0 - noise, 1.0 + noise)
                        # Memory: Prioritize items that failed previously
                        if id(m) in failed_items:
                            val += 1e6
                        return val

                    bs_models = sorted(models, key=sort_key, reverse=True)

                    temp_placement = {i: [] for i in range(gpu_num)}
=======
                # Randomized Best Fit Decreasing Check with Failure Memory
                found_placement = None
                failed_items = set()

                # Strategy pool for sorting models
                strategies = [
                    (lambda m: m.model_size + (m.req_rate / m.slo) / mid, 'Effective'),
                    (lambda m: m.model_size, 'Physical'),
                    (lambda m: m.req_rate / m.slo, 'Weight'),
                    (lambda m: (m.req_rate / m.slo) / (m.model_size + 1e-6), 'Density')
                ]

                for attempt in range(32):
                    # Cycle through strategies
                    base_key, _ = strategies[attempt % len(strategies)]

                    # Adaptive Noise
                    noise = 0.0
                    if attempt >= 4:
                        noise = 0.01 + (attempt * 0.002)

                    def sort_key(m):
                        val = base_key(m)
                        if noise > 0:
                            val *= random.uniform(1.0 - noise, 1.0 + noise)
                        # Failure memory boost
                        if id(m) in failed_items:
                            val += 1e9
                        return val

                    bs_models = sorted(models, key=sort_key, reverse=True)

                    temp_placement = {i: [] for i in range(gpu_num)}
>>>>>>> REPLACE
<<<<<<< SEARCH
        # 1. Try MOVE (Src -> Dst)
        for m_idx, model in enumerate(src_models):
            w = model.req_rate / model.slo
            s = model.model_size

            best_dst = None
            for dst in range(gpu_num):
                if dst == src_gpu: continue
                if gpu_stats[dst]['s'] + s > GPU_MEM_SIZE: continue

                dst_rem = GPU_MEM_SIZE - (gpu_stats[dst]['s'] + s)
                if dst_rem <= 1e-9: continue

                dst_w = gpu_stats[dst]['w'] + w
                new_dst_p = dst_w / dst_rem

                if new_dst_p < max_p - 1e-5:
                     best_dst = dst
                     break

            if best_dst is not None:
                moved_model = src_models.pop(m_idx)
                best_placement[best_dst].append(moved_model)
                improved = True
                break
=======
        # 1. Try MOVE (Src -> Dst)
        for m_idx, model in enumerate(src_models):
            w = model.req_rate / model.slo
            s = model.model_size

            best_dst = None
            min_dst_p = float('inf')

            for dst in range(gpu_num):
                if dst == src_gpu: continue
                if gpu_stats[dst]['s'] + s > GPU_MEM_SIZE: continue

                dst_rem = GPU_MEM_SIZE - (gpu_stats[dst]['s'] + s)
                if dst_rem <= 1e-9: continue

                dst_w = gpu_stats[dst]['w'] + w
                new_dst_p = dst_w / dst_rem

                if new_dst_p < max_p - 1e-5:
                     # Find best fit destination (lowest resulting pressure)
                     if new_dst_p < min_dst_p:
                         min_dst_p = new_dst_p
                         best_dst = dst

            if best_dst is not None:
                moved_model = src_models.pop(m_idx)
                best_placement[best_dst].append(moved_model)
                improved = True
                break
>>>>>>> REPLACE
</DIFF>