<NAME>
hybrid_placement_search
</NAME>

<DESCRIPTION>
Implements a hybrid placement strategy combining an ensemble of greedy heuristics, a binary search over the target KVPR (formulated as a Bin Packing problem with variable item sizes), and a post-placement local search for load balancing. This approach leverages the theoretical connection between the ratio-minimization problem and parameterized bin packing to find tighter bounds than simple greedy methods.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Uses an ensemble of greedy heuristics. Each heuristic sorts models differently
    and then greedily assigns them to the GPU that minimizes the resulting KVPR.
    """

    # Sorting strategies to try:
    # 1. Weight (req/slo) primary, Size secondary
    # 2. Size primary, Weight secondary
    # 3. Isolated KVPR (Weight / Available_If_Alone)
    # 4. Density (Weight / Size)
    sorting_keys = [
        lambda m: (m.req_rate / m.slo, m.model_size),
        lambda m: (m.model_size, m.req_rate / m.slo),
        lambda m: (m.req_rate / m.slo) / (GPU_MEM_SIZE - m.model_size + 1e-6),
        lambda m: (m.req_rate / m.slo) / m.model_size
    ]

    best_placement = None
    best_max_kvpr = float('inf')

    # Try each sorting strategy
    for key_fn in sorting_keys:
        sorted_models = sorted(models, key=key_fn, reverse=True)

        # Per-pass state
        placement = {gpu_id: [] for gpu_id in range(gpu_num)}
        shared_kv = [GPU_MEM_SIZE for _ in range(gpu_num)]
        weighted_req_rate = [0.0 for _ in range(gpu_num)]
        possible = True

        # Greedy assignment
        for model in sorted_models:
            best_idx = None
            best_new_kvpr = float('inf')

            w = model.req_rate / model.slo
            s = model.model_size

            for gpu_id in range(gpu_num):
                if s <= shared_kv[gpu_id]:
                    # Calculate KVPR if we place model here
                    new_rem = shared_kv[gpu_id] - s
                    new_rate = weighted_req_rate[gpu_id] + w

                    if new_rem > 1e-6:
                        new_kvpr = new_rate / new_rem
                    else:
                        new_kvpr = float('inf')

                    if new_kvpr < best_new_kvpr:
                        best_new_kvpr = new_kvpr
                        best_idx = gpu_id

            if best_idx is None:
                possible = False
                break

            placement[best_idx].append(model)
            weighted_req_rate[best_idx] += w
            shared_kv[best_idx] -= s

        if possible:
            # Calculate global max KVPR for this placement
            current_max = 0.0
            for gpu_id in range(gpu_num):
                rem = shared_kv[gpu_id]
                rate = weighted_req_rate[gpu_id]
                if rem < GPU_MEM_SIZE: # If GPU is used
                    if rem > 1e-6:
                        current_max = max(current_max, rate / rem)
                    else:
                        current_max = float('inf')

            if current_max < best_max_kvpr:
                best_max_kvpr = current_max
                best_placement = placement

    if best_placement is None:
        raise ValueError("Unable to place models on GPUs with available memory.")

    return best_placement
=======
def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.
    Combines Greedy Heuristics, Binary Search (Bin Packing), and Local Search.
    """

    # Helper to calculate max KVPR of a placement
    def get_max_kvpr(placement):
        max_p = 0.0
        for assigned in placement.values():
            w = sum(m.req_rate / m.slo for m in assigned)
            s = sum(m.model_size for m in assigned)
            rem = GPU_MEM_SIZE - s
            if rem <= 1e-9:
                if w > 0: return float('inf')
                else: continue
            max_p = max(max_p, w / rem)
        return max_p

    best_placement = None
    best_score = float('inf')

    # ---------------------------------------------------------
    # 1. Greedy Heuristics Ensemble
    # ---------------------------------------------------------
    # Fast initial solutions to set an upper bound
    heuristics = [
        # (Key Function, Strategy Name)
        # Strategy 'min_result': Place on GPU minimizing resulting KVPR
        # Strategy 'min_current': Place on GPU minimizing current KVPR (Load Balancing)
        (lambda m: (m.req_rate / m.slo, m.model_size), 'min_result'),
        (lambda m: m.model_size, 'min_result'),
        (lambda m: (m.req_rate / m.slo) / (GPU_MEM_SIZE - m.model_size + 1e-6), 'min_result'),
        (lambda m: m.req_rate / m.slo, 'min_current'),
    ]

    for key_fn, strategy in heuristics:
        sorted_models = sorted(models, key=key_fn, reverse=True)
        placement = {i: [] for i in range(gpu_num)}
        gpu_w = [0.0] * gpu_num
        gpu_s = [0.0] * gpu_num
        possible = True

        for model in sorted_models:
            w = model.req_rate / model.slo
            s = model.model_size
            best_idx = None
            best_val = float('inf')

            for i in range(gpu_num):
                if gpu_s[i] + s > GPU_MEM_SIZE: continue
                rem = GPU_MEM_SIZE - gpu_s[i]

                if strategy == 'min_result':
                    new_rem = rem - s
                    if new_rem > 1e-9:
                        val = (gpu_w[i] + w) / new_rem
                    else:
                        val = float('inf')
                else: # min_current
                    if rem > 1e-9:
                        val = gpu_w[i] / rem
                    else:
                        val = float('inf')

                if val < best_val:
                    best_val = val
                    best_idx = i
                elif val == best_val and best_idx is None:
                    best_idx = i

            if best_idx is None:
                possible = False
                break

            placement[best_idx].append(model)
            gpu_w[best_idx] += w
            gpu_s[best_idx] += s

        if possible:
            score = get_max_kvpr(placement)
            if score < best_score:
                best_score = score
                best_placement = placement

    # ---------------------------------------------------------
    # 2. Binary Search on Target KVPR (Transformation to Bin Packing)
    # ---------------------------------------------------------
    # Problem: Minimize K such that sum(w_i)/ (C - sum(s_i)) <= K
    # Equivalent to Bin Packing with item size v_i(K) = s_i + w_i/K, bin capacity C.

    total_w = sum(m.req_rate / m.slo for m in models)
    total_s = sum(m.model_size for m in models)
    rem_global = gpu_num * GPU_MEM_SIZE - total_s

    # Theoretical lower bound: Average case fluid distribution
    if rem_global > 1e-6:
        low = total_w / rem_global
        high = best_score if best_score != float('inf') else 1000.0

        if high > low + 1e-4:
            for _ in range(20):
                mid = (low + high) / 2

                # Sort items by effective size s + w/K descending (Best Fit Decreasing)
                bs_models = sorted(models, key=lambda m: m.model_size + (m.req_rate/m.slo)/mid, reverse=True)

                temp_placement = {i: [] for i in range(gpu_num)}
                gpu_w = [0.0] * gpu_num   # Actual w
                gpu_s = [0.0] * gpu_num   # Actual s
                possible_k = True

                for model in bs_models:
                    w = model.req_rate / model.slo
                    s = model.model_size
                    eff = s + w/mid

                    best_idx = None
                    min_rem_eff = float('inf')

                    # Best Fit on Effective Capacity
                    for i in range(gpu_num):
                        # Must fit physically
                        if gpu_s[i] + s > GPU_MEM_SIZE: continue

                        # Must fit effectively: current_eff + eff <= C
                        curr_eff = gpu_s[i] + gpu_w[i]/mid

                        if curr_eff + eff <= GPU_MEM_SIZE + 1e-6:
                            rem_eff = GPU_MEM_SIZE - (curr_eff + eff)
                            # Best Fit: minimize remaining effective space
                            if rem_eff < min_rem_eff:
                                min_rem_eff = rem_eff
                                best_idx = i

                    if best_idx is None:
                        possible_k = False
                        break

                    temp_placement[best_idx].append(model)
                    gpu_w[best_idx] += w
                    gpu_s[best_idx] += s

                if possible_k:
                    # Check actual score to be sure (approximation errors)
                    actual_score = get_max_kvpr(temp_placement)
                    if actual_score < best_score:
                        best_score = actual_score
                        best_placement = temp_placement
                    # We found a valid config for K=mid, try lower
                    high = mid
                else:
                    low = mid

    if best_placement is None:
        raise ValueError("Unable to place models on GPUs with available memory.")

    # ---------------------------------------------------------
    # 3. Local Search Refinement
    # ---------------------------------------------------------
    # Iteratively move models from the bottleneck GPU to others
    for _ in range(50):
        # Find bottleneck GPU
        max_p = -1.0
        src_gpu = -1
        gpu_stats = []

        for i in range(gpu_num):
            assigned = best_placement[i]
            w = sum(m.req_rate / m.slo for m in assigned)
            s = sum(m.model_size for m in assigned)
            rem = GPU_MEM_SIZE - s
            p = w / rem if rem > 1e-9 else float('inf')
            gpu_stats.append({'w': w, 's': s, 'p': p})
            if p > max_p:
                max_p = p
                src_gpu = i

        if src_gpu == -1 or max_p < 1e-9: break

        improved = False
        src_models = best_placement[src_gpu]

        # Try to move one model
        for m_idx, model in enumerate(src_models):
            w = model.req_rate / model.slo
            s = model.model_size

            # Predict source pressure if moved
            src_rem = GPU_MEM_SIZE - (gpu_stats[src_gpu]['s'] - s)
            src_w = gpu_stats[src_gpu]['w'] - w
            # new_src_p = src_w / src_rem

            # We need to find a dst where new_dst_p < max_p
            best_dst = None

            for dst in range(gpu_num):
                if dst == src_gpu: continue
                if gpu_stats[dst]['s'] + s > GPU_MEM_SIZE: continue

                dst_rem = GPU_MEM_SIZE - (gpu_stats[dst]['s'] + s)
                if dst_rem < 1e-9: continue

                dst_w = gpu_stats[dst]['w'] + w
                new_dst_p = dst_w / dst_rem

                # Criteria: Reduces the global max.
                # Since src_p reduces, we just need new_dst_p < old_max_p
                if new_dst_p < max_p - 1e-5:
                     best_dst = dst
                     break

            if best_dst is not None:
                moved_model = src_models.pop(m_idx)
                best_placement[best_dst].append(moved_model)
                improved = True
                break

        if not improved: break

    return best_placement
>>>>>>> REPLACE
</DIFF>