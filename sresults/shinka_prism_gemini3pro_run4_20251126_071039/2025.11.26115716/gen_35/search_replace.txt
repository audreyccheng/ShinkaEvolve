<NAME>
beam_search_packing
</NAME>

<DESCRIPTION>
I will replace the greedy Best-Fit Decreasing packing check with a limited Beam Search (width=4).
This allows the algorithm to explore multiple promising placement paths simultaneously, avoiding local optima where a greedy choice leads to infeasibility due to packing constraints.
I also add a "Density" sorting strategy to the deterministic pool.
The beam search uses a scoring function that maximizes the sum of squares of bin fill ratios, which generalizes the Best Fit heuristic (favoring tight packing) while maintaining diversity in the beam.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    def solve_packing(k_target, attempts=1):
        """
        Attempt to place models such that KVPR <= k_target for all GPUs.
        Constraint: w <= k * (C - s)  <==>  w + k*s <= k*C
        """
        limit_val = k_target * GPU_MEM_SIZE

        # Strategies to generate ordering of items
        strategies = []
        # 1. Linear Cost: w + k*s (Matches the constraint boundary)
        strategies.append(lambda x: x['w'] + k_target * x['s'])
        # 2. Size (Classic bin packing, good for tight memory)
        strategies.append(lambda x: x['s'])
        # 3. Weight (Good for load balancing)
        strategies.append(lambda x: x['w'])

        def try_pack(ordered_items):
            # State: current w, s per GPU
            bins = [{'w': 0.0, 's': 0.0} for _ in range(gpu_num)]
            allocation = [[] for _ in range(gpu_num)] # List of lists

            for item in ordered_items:
                item_lin = item['w'] + k_target * item['s']

                best_idx = -1
                min_slack = float('inf') # Best Fit: Minimize remaining space

                for i in range(gpu_num):
                    b = bins[i]
                    # Hard Capacity Check
                    if b['s'] + item['s'] > GPU_MEM_SIZE: continue

                    # KVPR / Linearized Check
                    # Current load + item load
                    new_lin = (b['w'] + item['w']) + k_target * (b['s'] + item['s'])

                    if new_lin > limit_val + 1e-5: continue

                    slack = limit_val - new_lin
                    if slack < min_slack:
                        min_slack = slack
                        best_idx = i

                if best_idx != -1:
                    allocation[best_idx].append(item)
                    bins[best_idx]['w'] += item['w']
                    bins[best_idx]['s'] += item['s']
                else:
                    return None # Failed to place this item
            return allocation

        # Run Deterministic Strategies
        for key_fn in strategies:
            # Sort Descending
            res = try_pack(sorted(items, key=key_fn, reverse=True))
            if res: return res

        # Run Randomized Strategies
        if attempts > 1:
            rng = random.Random(42 + int(k_target))
            # Base order: Linear cost
            for _ in range(attempts - 1):
                # Noisy sort: key * random noise
                noisy_items = sorted(items, key=lambda x: (x['w'] + k_target * x['s']) * rng.uniform(0.9, 1.1), reverse=True)
                res = try_pack(noisy_items)
                if res: return res

        return None
=======
    def solve_packing(k_target, attempts=1):
        """
        Attempt to place models such that KVPR <= k_target for all GPUs.
        Constraint: w <= k * (C - s)  <==>  w + k*s <= k*C
        Uses Beam Search to find a valid packing.
        """
        limit_val = k_target * GPU_MEM_SIZE
        # Avoid div by zero in scoring
        score_denom = max(limit_val, 1e-6)

        # Strategies to generate ordering of items
        strategies = []
        # 1. Linear Cost: w + k*s
        strategies.append(lambda x: x['w'] + k_target * x['s'])
        # 2. Size
        strategies.append(lambda x: x['s'])
        # 3. Density (KVPR of item itself)
        strategies.append(lambda x: x['w'] / (GPU_MEM_SIZE - x['s'] + 1e-9) if GPU_MEM_SIZE > x['s'] else x['w']*1e9)
        # 4. Weight
        strategies.append(lambda x: x['w'])

        def try_pack(ordered_items):
            # Beam Search Settings
            beam_width = 4

            # State: (score, allocations_indices, bins_state)
            # score: sum of squares of bin fill ratios (favors tight packing)
            # bins_state: list of {'w':.., 's':..}

            start_bins = [{'w': 0.0, 's': 0.0} for _ in range(gpu_num)]
            # Beam: list of tuples. Start with one empty state.
            beam = [(0.0, [], start_bins)]

            for item in ordered_items:
                item_w = item['w']
                item_s = item['s']

                candidates = []

                # Expand each state in the beam
                for score, allocs, bins in beam:
                    # Optimization: Symmetry Breaking
                    # If multiple bins have identical (w, s) states, trying one is sufficient.
                    seen_states = set()

                    for i in range(gpu_num):
                        b = bins[i]
                        state_sig = (b['w'], b['s'])
                        if state_sig in seen_states:
                            continue
                        seen_states.add(state_sig)

                        # Check Hard Capacity
                        if b['s'] + item_s > GPU_MEM_SIZE: continue

                        # Check Linearized Capacity (KVPR constraint)
                        new_lin = (b['w'] + item_w) + k_target * (b['s'] + item_s)
                        if new_lin > limit_val + 1e-5: continue

                        # Valid placement - Create new state
                        new_bins = bins[:] # Shallow copy list
                        new_bins[i] = {'w': b['w'] + item_w, 's': b['s'] + item_s} # Update modified bin

                        # Update Score: Incremental update of sum-of-squares
                        old_load = b['w'] + k_target * b['s']
                        new_load = new_bins[i]['w'] + k_target * new_bins[i]['s']

                        old_ratio = old_load / score_denom
                        new_ratio = new_load / score_denom

                        new_score = score - (old_ratio * old_ratio) + (new_ratio * new_ratio)

                        new_allocs = allocs + [i]
                        candidates.append((new_score, new_allocs, new_bins))

                if not candidates:
                    return None

                # Prune: Keep top W candidates
                candidates.sort(key=lambda x: x[0], reverse=True)
                beam = candidates[:beam_width]

            # Reconstruction from the best path found
            best_sol = beam[0]
            final_alloc_indices = best_sol[1]

            allocation = [[] for _ in range(gpu_num)]
            for item, idx in zip(ordered_items, final_alloc_indices):
                allocation[idx].append(item)
            return allocation

        # Run Deterministic Strategies
        for key_fn in strategies:
            res = try_pack(sorted(items, key=key_fn, reverse=True))
            if res: return res

        # Run Randomized Strategies
        if attempts > 1:
            rng = random.Random(42 + int(k_target))
            base_key = lambda x: x['w'] + k_target * x['s']
            for _ in range(attempts - 1):
                # Noisy sort
                noisy_items = sorted(items, key=lambda x: base_key(x) * rng.uniform(0.95, 1.05), reverse=True)
                res = try_pack(noisy_items)
                if res: return res

        return None
>>>>>>> REPLACE
</DIFF>