<NAME>
improved_heuristics_and_search
</NAME>

<DESCRIPTION>
1. Enhance `check_placement` by adding more deterministic packing heuristics (sorting by weight and density) and introducing a randomized shuffling strategy (Las Vegas) to escape local optima when deterministic sorts fail.
2. Optimize the binary search loop to faster contract the upper bound `high` by using the actual maximum KVPR of the found valid placement, rather than just the target `mid`. This speeds up convergence to the true optimum.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    def check_placement(k_target):
        """
        Determines if it is possible to place all models such that for every GPU:
        KVPR <= k_target.

        Uses multiple heuristics:
        1. Dynamic Sort (Weight + K*Size), Best Fit Decreasing
        2. Dynamic Sort (Weight + K*Size), First Fit Decreasing
        3. Size Sort, Best Fit Decreasing
        """
        # Strategies: (key_lambda, use_best_fit)
        strategies = [
            (lambda x: x['w'] + k_target * x['s'], True),
            (lambda x: x['w'] + k_target * x['s'], False),
            (lambda x: x['s'], True)
        ]

        for key_func, use_best_fit in strategies:
            sorted_items = sorted(items, key=key_func, reverse=True)
            placement = {i: [] for i in range(gpu_num)}
            # Track current s and w for valid checking
            gpu_state = [{'w': 0.0, 's': 0.0} for _ in range(gpu_num)]

            possible = True
            for item in sorted_items:
                best_idx = -1
                best_fill = -1.0

                # Iterate GPUs to find valid placements
                for i in range(gpu_num):
                    new_s = gpu_state[i]['s'] + item['s']
                    new_w = gpu_state[i]['w'] + item['w']

                    # Hard memory limit
                    if new_s > GPU_MEM_SIZE:
                        continue

                    # KVPR constraint: new_w <= k * (MEM - new_s)
                    rem_mem = GPU_MEM_SIZE - new_s
                    # Avoid float issues near 0
                    if rem_mem <= 1e-9:
                        if k_target > 1e12: pass
                        elif new_w > 1e-9: continue
                    elif new_w > k_target * rem_mem + 1e-9:
                        continue

                    if not use_best_fit:
                        # First Fit
                        best_idx = i
                        break
                    else:
                        # Best Fit: Maximize w + k*s (tightest packing)
                        # Equivalent to maximizing new_w + k_target * new_s
                        current_fill = new_w + k_target * new_s
                        if current_fill > best_fill:
                            best_fill = current_fill
                            best_idx = i

                if best_idx != -1:
                    placement[best_idx].append(item['model'])
                    gpu_state[best_idx]['s'] += item['s']
                    gpu_state[best_idx]['w'] += item['w']
                else:
                    possible = False
                    break

            if possible:
                return placement

        return None
=======
    def check_placement(k_target):
        import random
        """
        Determines if it is possible to place all models such that for every GPU:
        KVPR <= k_target.
        """

        def try_pack(item_order):
            placement = {i: [] for i in range(gpu_num)}
            gpu_state = [{'w': 0.0, 's': 0.0} for _ in range(gpu_num)]

            for item in item_order:
                best_idx = -1
                best_fill = -1.0

                for i in range(gpu_num):
                    new_s = gpu_state[i]['s'] + item['s']
                    if new_s > GPU_MEM_SIZE: continue

                    new_w = gpu_state[i]['w'] + item['w']
                    rem_mem = GPU_MEM_SIZE - new_s

                    # KVPR Check
                    if rem_mem <= 1e-9:
                        if k_target > 1e12: pass
                        elif new_w > 1e-9: continue
                    elif new_w > k_target * rem_mem + 1e-9:
                        continue

                    # Best Fit: Maximize w + k*s
                    current_fill = new_w + k_target * new_s
                    if current_fill > best_fill:
                        best_fill = current_fill
                        best_idx = i

                if best_idx != -1:
                    placement[best_idx].append(item['model'])
                    gpu_state[best_idx]['s'] += item['s']
                    gpu_state[best_idx]['w'] += item['w']
                else:
                    return None
            return placement

        # 1. Deterministic Strategies
        # Sort keys (descending)
        strategies = [
            lambda x: x['w'] + k_target * x['s'],
            lambda x: x['s'],
            lambda x: x['w'],
            lambda x: x['w'] / (x['s'] + 1e-9)
        ]

        for key in strategies:
            res = try_pack(sorted(items, key=key, reverse=True))
            if res: return res

        # 2. Randomized Strategies (Shuffle + Best Fit)
        rng = random.Random(42)
        indices = list(range(len(items)))
        for _ in range(50):
            rng.shuffle(indices)
            res = try_pack([items[i] for i in indices])
            if res: return res

        return None
>>>>>>> REPLACE
<<<<<<< SEARCH
    # Binary Search Loop
    for _ in range(25):
        mid = (low + high) / 2
        result = check_placement(mid)
        if result is not None:
            best_placement = result
            high = mid
        else:
            low = mid
=======
    # Binary Search Loop
    for _ in range(30):
        mid = (low + high) / 2
        result = check_placement(mid)
        if result is not None:
            best_placement = result

            # Refine high bound using the actual max KVPR of the valid solution
            actual_max = 0.0
            for gpu_p in best_placement.values():
                w_sum = sum(m.req_rate / m.slo for m in gpu_p)
                s_sum = sum(m.model_size for m in gpu_p)
                rem = GPU_MEM_SIZE - s_sum
                if rem > 1e-9:
                    val = w_sum / rem
                elif w_sum > 0:
                    val = float('inf')
                else:
                    val = 0.0
                if val > actual_max:
                    actual_max = val

            high = min(mid, actual_max)
        else:
            low = mid
>>>>>>> REPLACE
</DIFF>