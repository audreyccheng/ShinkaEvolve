--- a/original.py
+++ b/original.py
@@ -1,172 +1,268 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 GPU_MEM_SIZE = 80  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         A placement of models to GPUs
     """
-    
-    # Pre-process models to extract relevant metrics: weight (req/slo) and size
+
+    # Pre-process models
     items = []
     for m in models:
         items.append({
             'model': m,
             'w': m.req_rate / m.slo,
             's': m.model_size
         })
 
+    def get_max_kvpr(placement):
+        """Calculate the actual maximum KVPR of a given placement."""
+        current_max = 0.0
+        for p in placement.values():
+            w_sum = sum(m.req_rate / m.slo for m in p)
+            s_sum = sum(m.model_size for m in p)
+            rem = GPU_MEM_SIZE - s_sum
+            if rem <= 1e-9:
+                if w_sum > 1e-9: return float('inf')
+                val = 0.0
+            else:
+                val = w_sum / rem
+            if val > current_max:
+                current_max = val
+        return current_max
+
     def check_placement(k_target):
         """
-        Determines if it is possible to place all models such that for every GPU:
-        KVPR <= k_target.
-        
-        Uses First-Fit-Decreasing heuristic with a dynamic sorting key:
-        Sort Key = Weight + k_target * Size
-        
-        This key is derived from the linearized constraint:
-        Sum(Weight) <= k_target * (Capacity - Sum(Size))
-        Sum(Weight) + k_target * Sum(Size) <= k_target * Capacity
+        Check if models can be placed with KVPR <= k_target using multiple heuristics.
         """
-        # Sort items based on the "cost" at this specific pressure level K
-        # High K (memory bound) -> Size dominates sorting
-        # Low K (load bound) -> Weight dominates sorting
-        sorted_items = sorted(items, key=lambda x: x['w'] + k_target * x['s'], reverse=True)
-        
-        placement = {i: [] for i in range(gpu_num)}
-        gpu_load = [{'w': 0.0, 's': 0.0} for _ in range(gpu_num)]
-        
-        for item in sorted_items:
-            placed = False
-            for i in range(gpu_num):
-                # Proposed new state
-                new_s = gpu_load[i]['s'] + item['s']
-                new_w = gpu_load[i]['w'] + item['w']
-                
-                # Check hard memory constraint (with slight tolerance)
-                if new_s > GPU_MEM_SIZE:
-                    continue
-                    
-                # Check KVPR constraint: new_w / (M - new_s) <= k_target
-                # Transform to multiplication to handle denominator approaching zero safely
-                rem_mem = GPU_MEM_SIZE - new_s
-                
-                # If remaining memory is near zero, KVPR blows up unless load is 0
-                if rem_mem <= 1e-9:
-                    # If K is extremely large (initialization), we allow tight packing
-                    if k_target > 1e12:
-                        pass 
-                    elif new_w > 1e-9: 
+        # Strategies: (key_lambda, reverse, use_best_fit)
+        # 1. Linearized cost: w + k*s. Adapts to K.
+        # 2. Size: s. Good for memory bound.
+        # 3. Weight: w. Good for load bound.
+        # 4. Density: w/s.
+        strategies = [
+            (lambda x: x['w'] + k_target * x['s'], True, True),   # Best Fit
+            (lambda x: x['w'] + k_target * x['s'], True, False),  # First Fit
+            (lambda x: x['s'], True, True),                       # Size Desc, Best Fit
+            (lambda x: x['w'], True, True),                       # Weight Desc, Best Fit
+            (lambda x: x['w'] / (x['s'] + 1e-5), True, True)      # Density Desc, Best Fit
+        ]
+
+        for key_func, reverse, use_best_fit in strategies:
+            sorted_items = sorted(items, key=key_func, reverse=reverse)
+            placement = {i: [] for i in range(gpu_num)}
+            gpu_state = [{'w': 0.0, 's': 0.0} for _ in range(gpu_num)]
+            
+            possible = True
+            for item in sorted_items:
+                best_idx = -1
+                best_score = -1.0 # For Best Fit (maximize filled "volume")
+                
+                # Check all GPUs
+                for i in range(gpu_num):
+                    new_s = gpu_state[i]['s'] + item['s']
+                    if new_s > GPU_MEM_SIZE: continue
+                    
+                    new_w = gpu_state[i]['w'] + item['w']
+                    rem = GPU_MEM_SIZE - new_s
+                    
+                    # KVPR constraint: w <= k * rem
+                    # Use multiplication to avoid division by zero/small numbers
+                    limit = k_target * rem
+                    if new_w > limit + 1e-7: 
                         continue
-                elif new_w > k_target * rem_mem:
-                    continue
-                
-                # Assign to this GPU
-                placement[i].append(item['model'])
-                gpu_load[i]['s'] = new_s
-                gpu_load[i]['w'] = new_w
-                placed = True
-                break
-            
-            if not placed:
-                return None
-                
-        return placement
-
-    # Binary Search for the Minimum Maximum KVPR (K)
-    
-    # 1. Initialization
-    # Try a very loose constraint (High K) to check feasibility and get a baseline.
-    # K=1e9 makes the sort key dominated by Size, effectively "First Fit Decreasing by Size".
+                        
+                    if not use_best_fit:
+                        # First Fit
+                        best_idx = i
+                        break
+                    
+                    # Best Fit criteria: Maximize current usage (Leave least space)
+                    # Usage metric: new_w + k * new_s
+                    # This corresponds to filling the "linearized bin" as much as possible
+                    score = new_w + k_target * new_s
+                    if score > best_score:
+                        best_score = score
+                        best_idx = i
+                
+                if best_idx != -1:
+                    placement[best_idx].append(item['model'])
+                    gpu_state[best_idx]['w'] += item['w']
+                    gpu_state[best_idx]['s'] += item['s']
+                else:
+                    possible = False
+                    break
+            
+            if possible:
+                return placement
+        return None
+
+    # 1. Binary Search for K
     high = 1e9
+    # Initial feasibility check
     best_placement = check_placement(high)
-    
     if best_placement is None:
-        # If we cannot pack even with relaxed constraints, the models physically don't fit.
         raise ValueError("Unable to place models on GPUs (insufficient total memory).")
-        
-    # Refine 'high' to the actual KVPR of the feasible solution found
-    current_max = 0.0
-    for gpu_p in best_placement.values():
-        w_sum = sum(m.req_rate / m.slo for m in gpu_p)
-        s_sum = sum(m.model_size for m in gpu_p)
-        rem = GPU_MEM_SIZE - s_sum
-        if rem > 1e-9:
-            current_max = max(current_max, w_sum / rem)
-        elif w_sum > 0:
-            current_max = high 
-            
-    high = current_max
+    
+    # Tighten initial high bound
+    high = get_max_kvpr(best_placement)
     low = 0.0
     
-    # 2. Binary Search Loop
-    # 25 iterations gives sufficient precision for this scale
-    for _ in range(25):
+    # 20 iterations is sufficient for high precision
+    for _ in range(20):
         mid = (low + high) / 2
-        result = check_placement(mid)
-        if result is not None:
-            best_placement = result
-            high = mid
+        res = check_placement(mid)
+        if res is not None:
+            best_placement = res
+            # Aggressive bound tightening: the actual max is <= mid
+            actual_max = get_max_kvpr(res)
+            high = min(mid, actual_max)
         else:
             low = mid
-            
-    return best_placement
+
+    # 2. Local Search Optimization (Hill Climbing / Descent)
+    # Attempt to reduce the peak load by moving models from the bottleneck GPU
+    
+    # Initialize state for fast updates
+    gpu_states = []
+    for i in range(gpu_num):
+        w = sum(m.req_rate / m.slo for m in best_placement[i])
+        s = sum(m.model_size for m in best_placement[i])
+        gpu_states.append({'w': w, 's': s, 'models': list(best_placement[i])})
+
+    for _ in range(100): # Max iterations
+        # Calculate costs and identify max
+        max_cost = -1.0
+        max_idx = -1
+        
+        for i in range(gpu_num):
+            rem = GPU_MEM_SIZE - gpu_states[i]['s']
+            if rem <= 1e-9:
+                c = float('inf') if gpu_states[i]['w'] > 1e-9 else 0.0
+            else:
+                c = gpu_states[i]['w'] / rem
+            
+            if c > max_cost:
+                max_cost = c
+                max_idx = i
+        
+        if max_cost == 0: break
+        
+        improved = False
+        src = gpu_states[max_idx]
+        
+        # Try to move a model from src to dst
+        for m_idx, m in enumerate(src['models']):
+            m_w = m.req_rate / m.slo
+            m_s = m.model_size
+            
+            # Predict source cost after removal
+            ns_s = src['s'] - m_s
+            ns_w = src['w'] - m_w
+            ns_rem = GPU_MEM_SIZE - ns_s
+            if ns_rem <= 1e-9:
+                 ns_cost = float('inf') if ns_w > 1e-9 else 0.0
+            else:
+                 ns_cost = ns_w / ns_rem
+            
+            # Optimization: if src doesn't improve much, still valid, but we want global improvement
+            if ns_cost >= max_cost: continue 
+
+            for dst_idx in range(gpu_num):
+                if dst_idx == max_idx: continue
+                dst = gpu_states[dst_idx]
+                
+                # Check mem
+                nd_s = dst['s'] + m_s
+                if nd_s > GPU_MEM_SIZE: continue
+                
+                # Predict dest cost
+                nd_w = dst['w'] + m_w
+                nd_rem = GPU_MEM_SIZE - nd_s
+                if nd_rem <= 1e-9:
+                    nd_cost = float('inf') if nd_w > 1e-9 else 0.0
+                else:
+                    nd_cost = nd_w / nd_rem
+                
+                # Check if this move improves the local bottleneck situation
+                # We require that both the new source and new dest are strictly better than the OLD max
+                if max(ns_cost, nd_cost) < max_cost - 1e-9:
+                    # Apply move
+                    model = src['models'].pop(m_idx)
+                    src['s'] = ns_s
+                    src['w'] = ns_w
+                    
+                    dst['models'].append(model)
+                    dst['s'] = nd_s
+                    dst['w'] = nd_w
+                    
+                    improved = True
+                    break
+            if improved: break
+        
+        if not improved:
+            break
+
+    # Reconstruct result
+    final_placement = {i: gpu_states[i]['models'] for i in range(gpu_num)}
+    return final_placement
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
     
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
     
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
     
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
     
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
     
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")
 
