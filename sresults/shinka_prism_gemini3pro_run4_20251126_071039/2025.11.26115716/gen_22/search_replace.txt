<NAME>
bs_multisort_and_ls_swap
</NAME>

<DESCRIPTION>
Improve the Binary Search feasibility check by trying multiple sorting strategies (Effective Size, Physical Size, Weight) to find a valid packing for a given target KVPR. This allows finding better placements that might be missed by a single sorting heuristic.
Additionally, enhance the Local Search phase to include Swap operations (exchanging models between GPUs) alongside Move operations. This helps reduce the maximum pressure even when GPUs are near memory capacity and simple moves are impossible.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # ---------------------------------------------------------
    # 2. Binary Search on Target KVPR (Transformation to Bin Packing)
    # ---------------------------------------------------------
    # Problem: Minimize K such that sum(w_i)/ (C - sum(s_i)) <= K
    # Equivalent to Bin Packing with item size v_i(K) = s_i + w_i/K, bin capacity C.

    total_w = sum(m.req_rate / m.slo for m in models)
    total_s = sum(m.model_size for m in models)
    rem_global = gpu_num * GPU_MEM_SIZE - total_s

    # Theoretical lower bound: Average case fluid distribution
    if rem_global > 1e-6:
        low = total_w / rem_global
        high = best_score if best_score != float('inf') else 1000.0

        if high > low + 1e-4:
            for _ in range(20):
                mid = (low + high) / 2

                # Sort items by effective size s + w/K descending (Best Fit Decreasing)
                bs_models = sorted(models, key=lambda m: m.model_size + (m.req_rate/m.slo)/mid, reverse=True)

                temp_placement = {i: [] for i in range(gpu_num)}
                gpu_w = [0.0] * gpu_num   # Actual w
                gpu_s = [0.0] * gpu_num   # Actual s
                possible_k = True

                for model in bs_models:
                    w = model.req_rate / model.slo
                    s = model.model_size
                    eff = s + w/mid

                    best_idx = None
                    min_rem_eff = float('inf')

                    # Best Fit on Effective Capacity
                    for i in range(gpu_num):
                        # Must fit physically
                        if gpu_s[i] + s > GPU_MEM_SIZE: continue

                        # Must fit effectively: current_eff + eff <= C
                        curr_eff = gpu_s[i] + gpu_w[i]/mid

                        if curr_eff + eff <= GPU_MEM_SIZE + 1e-6:
                            rem_eff = GPU_MEM_SIZE - (curr_eff + eff)
                            # Best Fit: minimize remaining effective space
                            if rem_eff < min_rem_eff:
                                min_rem_eff = rem_eff
                                best_idx = i

                    if best_idx is None:
                        possible_k = False
                        break

                    temp_placement[best_idx].append(model)
                    gpu_w[best_idx] += w
                    gpu_s[best_idx] += s

                if possible_k:
                    # Check actual score to be sure (approximation errors)
                    actual_score = get_max_kvpr(temp_placement)
                    if actual_score < best_score:
                        best_score = actual_score
                        best_placement = temp_placement
                    # We found a valid config for K=mid, try lower
                    high = mid
                else:
                    low = mid
=======
    # ---------------------------------------------------------
    # 2. Binary Search on Target KVPR (Transformation to Bin Packing)
    # ---------------------------------------------------------
    # Problem: Minimize K such that sum(w_i)/ (C - sum(s_i)) <= K
    # Equivalent to Bin Packing with item size v_i(K) = s_i + w_i/K, bin capacity C.

    total_w = sum(m.req_rate / m.slo for m in models)
    total_s = sum(m.model_size for m in models)
    rem_global = gpu_num * GPU_MEM_SIZE - total_s

    # Theoretical lower bound: Average case fluid distribution
    if rem_global > 1e-6:
        low = total_w / rem_global
        high = best_score if best_score != float('inf') else 1000.0

        if high > low + 1e-4:
            for _ in range(20):
                mid = (low + high) / 2

                # Check feasibility with multiple heuristics to increase success rate
                check_keys = [
                    lambda m: m.model_size + (m.req_rate/m.slo)/mid, # Effective Size
                    lambda m: m.model_size,                          # Physical Size
                    lambda m: m.req_rate/m.slo                       # Weight
                ]

                found_valid_for_mid = False

                for key_fn in check_keys:
                    bs_models = sorted(models, key=key_fn, reverse=True)

                    temp_placement = {i: [] for i in range(gpu_num)}
                    gpu_w = [0.0] * gpu_num   # Actual w
                    gpu_s = [0.0] * gpu_num   # Actual s
                    possible_k = True

                    for model in bs_models:
                        w = model.req_rate / model.slo
                        s = model.model_size

                        best_idx = None
                        min_slack = float('inf')

                        # Best Fit Decreasing on Effective Constraint
                        # Constraint: w_new <= mid * rem_phys_new
                        for i in range(gpu_num):
                            if gpu_s[i] + s > GPU_MEM_SIZE: continue

                            rem_phys_new = GPU_MEM_SIZE - (gpu_s[i] + s)
                            # Allow slight float tolerance
                            if rem_phys_new < 0: rem_phys_new = 0.0

                            lhs = gpu_w[i] + w
                            rhs = mid * rem_phys_new

                            if lhs <= rhs + 1e-5:
                                # Slack: unused effective capacity
                                slack = rhs - lhs
                                if slack < min_slack:
                                    min_slack = slack
                                    best_idx = i

                        if best_idx is None:
                            possible_k = False
                            break

                        temp_placement[best_idx].append(model)
                        gpu_w[best_idx] += w
                        gpu_s[best_idx] += s

                    if possible_k:
                        actual_score = get_max_kvpr(temp_placement)
                        if actual_score < best_score:
                            best_score = actual_score
                            best_placement = temp_placement
                        found_valid_for_mid = True
                        break # Found a valid packing, no need to check other sorts

                if found_valid_for_mid:
                    high = mid
                else:
                    low = mid
>>>>>>> REPLACE
<<<<<<< SEARCH
    # ---------------------------------------------------------
    # 3. Local Search Refinement
    # ---------------------------------------------------------
    # Iteratively move models from the bottleneck GPU to others
    for _ in range(50):
        # Find bottleneck GPU
        max_p = -1.0
        src_gpu = -1
        gpu_stats = []

        for i in range(gpu_num):
            assigned = best_placement[i]
            w = sum(m.req_rate / m.slo for m in assigned)
            s = sum(m.model_size for m in assigned)
            rem = GPU_MEM_SIZE - s
            p = w / rem if rem > 1e-9 else float('inf')
            gpu_stats.append({'w': w, 's': s, 'p': p})
            if p > max_p:
                max_p = p
                src_gpu = i

        if src_gpu == -1 or max_p < 1e-9: break

        improved = False
        src_models = best_placement[src_gpu]

        # Try to move one model
        for m_idx, model in enumerate(src_models):
            w = model.req_rate / model.slo
            s = model.model_size

            # Predict source pressure if moved
            src_rem = GPU_MEM_SIZE - (gpu_stats[src_gpu]['s'] - s)
            src_w = gpu_stats[src_gpu]['w'] - w
            # new_src_p = src_w / src_rem

            # We need to find a dst where new_dst_p < max_p
            best_dst = None

            for dst in range(gpu_num):
                if dst == src_gpu: continue
                if gpu_stats[dst]['s'] + s > GPU_MEM_SIZE: continue

                dst_rem = GPU_MEM_SIZE - (gpu_stats[dst]['s'] + s)
                if dst_rem < 1e-9: continue

                dst_w = gpu_stats[dst]['w'] + w
                new_dst_p = dst_w / dst_rem

                # Criteria: Reduces the global max.
                # Since src_p reduces, we just need new_dst_p < old_max_p
                if new_dst_p < max_p - 1e-5:
                     best_dst = dst
                     break

            if best_dst is not None:
                moved_model = src_models.pop(m_idx)
                best_placement[best_dst].append(moved_model)
                improved = True
                break

        if not improved: break
=======
    # ---------------------------------------------------------
    # 3. Local Search Refinement
    # ---------------------------------------------------------
    # Iteratively move or swap models to reduce the peak KVPR

    # Cache state for faster evaluation
    gpu_w = [0.0] * gpu_num
    gpu_s = [0.0] * gpu_num
    for i in range(gpu_num):
        assigned = best_placement[i]
        gpu_w[i] = sum(m.req_rate / m.slo for m in assigned)
        gpu_s[i] = sum(m.model_size for m in assigned)

    for _ in range(100):
        # Identify bottleneck
        max_p = -1.0
        src_gpu = -1

        # Recalculate pressures
        for i in range(gpu_num):
            rem = GPU_MEM_SIZE - gpu_s[i]
            p = gpu_w[i] / rem if rem > 1e-9 else float('inf')
            if p > max_p:
                max_p = p
                src_gpu = i

        if src_gpu == -1 or max_p < 1e-9: break

        improved = False
        src_models = best_placement[src_gpu]

        # 1. Try MOVE (Source -> Dest)
        for m_idx, model in enumerate(src_models):
            w = model.req_rate / model.slo
            s = model.model_size

            for dst in range(gpu_num):
                if dst == src_gpu: continue

                if gpu_s[dst] + s > GPU_MEM_SIZE: continue

                new_dst_rem = GPU_MEM_SIZE - (gpu_s[dst] + s)
                if new_dst_rem <= 1e-9: continue

                new_dst_p = (gpu_w[dst] + w) / new_dst_rem

                # Move if destination doesn't become worse than current max
                if new_dst_p < max_p - 1e-6:
                    moved_model = src_models.pop(m_idx)
                    best_placement[dst].append(moved_model)

                    gpu_w[src_gpu] -= w
                    gpu_s[src_gpu] -= s
                    gpu_w[dst] += w
                    gpu_s[dst] += s
                    improved = True
                    break
            if improved: break

        if improved: continue

        # 2. Try SWAP (Source <-> Dest)
        for m_idx, m_src in enumerate(src_models):
            w_src = m_src.req_rate / m_src.slo
            s_src = m_src.model_size

            for dst in range(gpu_num):
                if dst == src_gpu: continue

                dst_models = best_placement[dst]
                for d_idx, m_dst in enumerate(dst_models):
                    w_dst = m_dst.req_rate / m_dst.slo
                    s_dst = m_dst.model_size

                    new_src_s = gpu_s[src_gpu] - s_src + s_dst
                    new_dst_s = gpu_s[dst] - s_dst + s_src

                    if new_src_s > GPU_MEM_SIZE or new_dst_s > GPU_MEM_SIZE: continue

                    new_src_rem = GPU_MEM_SIZE - new_src_s
                    new_dst_rem = GPU_MEM_SIZE - new_dst_s

                    if new_src_rem <= 1e-9 or new_dst_rem <= 1e-9: continue

                    new_src_p = (gpu_w[src_gpu] - w_src + w_dst) / new_src_rem
                    new_dst_p = (gpu_w[dst] - w_dst + w_src) / new_dst_rem

                    if max(new_src_p, new_dst_p) < max_p - 1e-6:
                        mod_src = src_models.pop(m_idx)
                        mod_dst = dst_models.pop(d_idx)

                        src_models.append(mod_dst)
                        dst_models.append(mod_src)

                        gpu_w[src_gpu] = gpu_w[src_gpu] - w_src + w_dst
                        gpu_s[src_gpu] = gpu_s[src_gpu] - s_src + s_dst
                        gpu_w[dst] = gpu_w[dst] - w_dst + w_src
                        gpu_s[dst] = gpu_s[dst] - s_dst + s_src

                        improved = True
                        break
                if improved: break
            if improved: break

        if not improved: break
>>>>>>> REPLACE
</DIFF>