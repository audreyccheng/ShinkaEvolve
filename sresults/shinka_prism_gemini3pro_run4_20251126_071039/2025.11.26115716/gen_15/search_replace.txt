<NAME>
repair_and_tighten_bounds
</NAME>

<DESCRIPTION>
1.  **Refined Lower Bound**: Updates `k_min` to account for single-item constraints (an item's inherent KVPR), which tightens the binary search range.
2.  **Repair Logic in Feasibility Check**: Enhances the `pack` function with a failure-driven swap mechanism. If an item fails to fit greedily, the algorithm attempts to swap it with an already placed item to resolve the conflict. This allows finding feasible packings for tighter $K$ targets.
3.  **Fast Placement Smoothing**: Integrates a quick load-balancing pass (`smooth_placement`) within the binary search loop. When a valid placement is found, this function attempts to reduce its maximum KVPR by moving items from the bottleneck GPU. This allows the binary search to lower the upper bound (`high`) more aggressively, accelerating convergence to the global optimum.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Theoretical Lower Bound: K >= Sum(w) / (M*C - Sum(s))
    rem_global = gpu_num * GPU_MEM_SIZE - total_s
    k_min = 0.0
    if rem_global > 1e-9:
        k_min = total_w / rem_global
    elif total_w > 1e-9:
        # If no global memory remains but we have load, it's impossible.
        # But we let the search handle the failure.
        k_min = 1e9

    def get_max_kvpr(placement_list):
=======
    # Theoretical Lower Bound: K >= Sum(w) / (M*C - Sum(s))
    rem_global = gpu_num * GPU_MEM_SIZE - total_s
    k_min = 0.0
    if rem_global > 1e-9:
        k_min = total_w / rem_global
    elif total_w > 1e-9:
        # If no global memory remains but we have load, it's impossible.
        # But we let the search handle the failure.
        k_min = 1e9

    # Refine LB with single-item constraints
    for item in items:
        rem_item = GPU_MEM_SIZE - item['s']
        if rem_item > 1e-9:
            k_min = max(k_min, item['w'] / rem_item)

    def get_max_kvpr(placement_list):
>>>>>>> REPLACE
<<<<<<< SEARCH
    def pack(k_target, max_attempts=5):
        """
        Try to pack items into gpu_num bins such that for all j:
        sum(w) / (C - sum(s)) <= k_target
        Equivalently: sum(w + k_target*s) <= k_target * C
        """

        # Helper for a single packing attempt
        def try_order(ordered_items):
            # Track current usage
            # Linear Capacity = K * C
            # Item Linear Size = w + K * s
            # We use Best Fit Decreasing on Linear Size to minimize wasted Linear Capacity

            pl = [[] for _ in range(gpu_num)]
            g_w = [0.0] * gpu_num
            g_s = [0.0] * gpu_num

            for item in ordered_items:
                w = item['w']
                s = item['s']
                # Cost in the linearized domain
                cost = w + k_target * s

                best_idx = -1
                best_residue = float('inf')

                for i in range(gpu_num):
                    # Hard memory limit
                    if g_s[i] + s > GPU_MEM_SIZE:
                        continue

                    # KVPR / Linear Capacity Limit
                    # Current load: g_w + K * g_s
                    # New load: (g_w + w) + K * (g_s + s)
                    current_lin_load = g_w[i] + k_target * g_s[i]
                    new_lin_load = current_lin_load + cost
                    limit = k_target * GPU_MEM_SIZE

                    if new_lin_load > limit + 1e-5:
                        continue

                    # Best Fit: Minimize remaining space in the linearized bin
                    # residue = limit - new_lin_load
                    # We want to minimize residue => maximize new_lin_load
                    residue = limit - new_lin_load

                    if residue < best_residue:
                        best_residue = residue
                        best_idx = i

                if best_idx != -1:
                    pl[best_idx].append(item['model'])
                    g_w[best_idx] += w
                    g_s[best_idx] += s
                else:
                    return None
            return pl

        # 1. Deterministic Strategy
        # Sort by decreasing linear cost: w + k*s
        base_items = sorted(items, key=lambda x: x['w'] + k_target * x['s'], reverse=True)
        res = try_order(base_items)
        if res: return res

        # 2. Randomized Restarts (Noisy Sorting)
        if max_attempts > 0:
            # Seed based on k_target to ensure determinism for the same call
            rng = random.Random(hash(k_target))
            for _ in range(max_attempts):
                # Perturb the sort keys by +/- 10%
                # This keeps the general "heavy first" heuristic but changes local ordering
                noisy_items = sorted(items, key=lambda x: (x['w'] + k_target * x['s']) * rng.uniform(0.9, 1.1), reverse=True)
                res = try_order(noisy_items)
                if res: return res

        return None
=======
    def pack(k_target, max_attempts=5):
        """
        Try to pack items into gpu_num bins such that for all j:
        sum(w) / (C - sum(s)) <= k_target
        Equivalently: sum(w + k_target*s) <= k_target * C
        """

        # Helper for a single packing attempt
        def try_order(ordered_items):
            # Track current usage
            pl = [[] for _ in range(gpu_num)] # Stores item dicts
            g_w = [0.0] * gpu_num
            g_s = [0.0] * gpu_num

            for item in ordered_items:
                w = item['w']
                s = item['s']
                cost = w + k_target * s

                best_idx = -1
                best_residue = float('inf')

                # Try to place in a GPU
                for i in range(gpu_num):
                    if g_s[i] + s > GPU_MEM_SIZE: continue

                    current_lin_load = g_w[i] + k_target * g_s[i]
                    new_lin_load = current_lin_load + cost
                    limit = k_target * GPU_MEM_SIZE

                    if new_lin_load > limit + 1e-5: continue

                    residue = limit - new_lin_load
                    if residue < best_residue:
                        best_residue = residue
                        best_idx = i

                if best_idx != -1:
                    pl[best_idx].append(item)
                    g_w[best_idx] += w
                    g_s[best_idx] += s
                else:
                    # Repair Logic: Try to swap with an existing item
                    placed_flag = False
                    for j in range(gpu_num):
                        # Candidate GPU j: check if swapping a victim allows 'item'
                        for v_idx, victim in enumerate(pl[j]):
                            v_w, v_s = victim['w'], victim['s']

                            # Check if item fits in j replacing victim
                            if g_s[j] - v_s + s > GPU_MEM_SIZE: continue

                            new_load_j = (g_w[j] - v_w + w) + k_target * (g_s[j] - v_s + s)
                            limit = k_target * GPU_MEM_SIZE
                            if new_load_j > limit + 1e-5: continue

                            # Item fits in j. Now check if victim fits elsewhere
                            for k in range(gpu_num):
                                if j == k: continue

                                if g_s[k] + v_s > GPU_MEM_SIZE: continue
                                new_load_k = (g_w[k] + v_w) + k_target * (g_s[k] + v_s)
                                if new_load_k > limit + 1e-5: continue

                                # Found valid swap
                                pl[j][v_idx] = item
                                pl[k].append(victim)

                                g_w[j] += (w - v_w)
                                g_s[j] += (s - v_s)
                                g_w[k] += v_w
                                g_s[k] += v_s

                                placed_flag = True
                                break
                            if placed_flag: break
                        if placed_flag: break

                    if not placed_flag:
                        return None

            # Convert item dicts back to models
            return [[x['model'] for x in gpu_items] for gpu_items in pl]

        # 1. Deterministic Strategy
        base_items = sorted(items, key=lambda x: x['w'] + k_target * x['s'], reverse=True)
        res = try_order(base_items)
        if res: return res

        # 2. Randomized Restarts
        if max_attempts > 0:
            rng = random.Random(hash(k_target))
            for _ in range(max_attempts):
                # Noisy sort: value * random(0.9, 1.1)
                # We use a tuple key to ensure stability is broken randomly
                noisy_items = sorted(items, key=lambda x: (x['w'] + k_target * x['s']) * rng.uniform(0.9, 1.1), reverse=True)
                res = try_order(noisy_items)
                if res: return res

        return None
>>>>>>> REPLACE
<<<<<<< SEARCH
    # Search Loop
    for _ in range(25):
        if high - low < 1e-4: break
        mid = (low + high) / 2

        # Try to pack with K = mid
        res = pack(mid, max_attempts=5)

        if res:
            best_pl_list = res
            actual_max = get_max_kvpr(res)
            # If we found a valid packing with actual max KVPR 'actual_max',
            # we know a solution exists for K = actual_max.
            # And by definition actual_max <= mid (since it fit in mid).
            # So we can safely lower high to actual_max.
            high = min(mid, actual_max)
        else:
            low = mid

    # Convert to dict format
    final_pl = {i: best_pl_list[i] for i in range(gpu_num)}
=======
    # Fast smoothing helper
    def smooth_placement(pl_list):
        # Quick pass to move items from highest KVPR GPU to others
        for _ in range(3): # Small number of passes
            # Compute current metrics
            stats = []
            for idx, p in enumerate(pl_list):
                w = sum(m.req_rate / m.slo for m in p)
                s = sum(m.model_size for m in p)
                rem = GPU_MEM_SIZE - s
                kvpr = w/rem if rem > 1e-9 else (float('inf') if w > 1e-9 else 0.0)
                stats.append({'idx': idx, 'w': w, 's': s, 'kvpr': kvpr})

            stats.sort(key=lambda x: x['kvpr'], reverse=True)
            src = stats[0]
            if src['kvpr'] == 0: break

            improved = False
            src_models = pl_list[src['idx']]

            for m_idx, m in enumerate(src_models):
                mw = m.req_rate / m.slo
                ms = m.model_size

                # Check src improvement
                ns_rem = GPU_MEM_SIZE - (src['s'] - ms)
                ns_kvpr = (src['w'] - mw)/ns_rem if ns_rem > 1e-9 else 0.0
                if ns_kvpr >= src['kvpr']: continue

                # Try find dst
                for dst in stats[1:]:
                    if dst['s'] + ms > GPU_MEM_SIZE: continue
                    nd_rem = GPU_MEM_SIZE - (dst['s'] + ms)
                    nd_kvpr = (dst['w'] + mw)/nd_rem if nd_rem > 1e-9 else float('inf')

                    if nd_kvpr < src['kvpr']:
                        # Move
                        item = src_models.pop(m_idx)
                        pl_list[dst['idx']].append(item)
                        improved = True
                        break
                if improved: break
            if not improved: break
        return pl_list

    # Search Loop
    for _ in range(25):
        if high - low < 1e-4: break
        mid = (low + high) / 2

        # Try to pack with K = mid
        res = pack(mid, max_attempts=5)

        if res:
            # Optimize the found solution to potentially lower the upper bound further
            res = smooth_placement(res)
            best_pl_list = res

            actual_max = get_max_kvpr(res)
            high = min(mid, actual_max)
        else:
            low = mid

    # Convert to dict format
    final_pl = {i: best_pl_list[i] for i in range(gpu_num)}
>>>>>>> REPLACE
</DIFF>