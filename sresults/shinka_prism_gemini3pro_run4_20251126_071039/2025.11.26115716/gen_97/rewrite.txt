# EVOLVE-BLOCK-START
"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""

import random

GPU_MEM_SIZE = 80.0  # GB

def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.
    Combines Greedy Heuristics, Binary Search (Bin Packing) with Failure Memory, 
    and Steepest Descent Local Search.
    """

    # Helper to calculate max KVPR of a placement
    def get_max_kvpr(placement):
        max_p = 0.0
        for assigned in placement.values():
            w = sum(m.req_rate / m.slo for m in assigned)
            s = sum(m.model_size for m in assigned)
            rem = GPU_MEM_SIZE - s
            if rem <= 1e-9:
                if w > 0: return float('inf')
                else: continue
            max_p = max(max_p, w / rem)
        return max_p

    # Helper: Steepest Descent Refinement
    # Iteratively finds the single best Move or Swap that reduces the bottleneck pressure.
    def refine_placement(placement, max_iters=20):
        # Create efficient mutable structure
        # g_stats: list of dicts {'w': float, 's': float, 'models': list}
        g_stats = []
        for g in range(gpu_num):
            m_list = placement[g]
            w = sum(m.req_rate / m.slo for m in m_list)
            s = sum(m.model_size for m in m_list)
            g_stats.append({'w': w, 's': s, 'models': list(m_list)})

        for _ in range(max_iters):
            # 1. Identify Bottleneck
            max_p = -1.0
            src_gpu = -1
            
            # Recalculate pressures
            for g in range(gpu_num):
                rem = GPU_MEM_SIZE - g_stats[g]['s']
                p = g_stats[g]['w'] / rem if rem > 1e-9 else (float('inf') if g_stats[g]['w'] > 0 else 0.0)
                if p > max_p:
                    max_p = p
                    src_gpu = g
            
            if src_gpu == -1 or max_p < 1e-9:
                break
            
            src_models = g_stats[src_gpu]['models']
            best_action = None # ('move', m_idx, dst) or ('swap', m_src_idx, dst, m_dst_idx)
            best_val = max_p 

            # 2. Evaluate MOVES (Src -> Dst)
            for m_idx, model in enumerate(src_models):
                w = model.req_rate / model.slo
                s = model.model_size
                
                # Src stats after move
                src_s_new = g_stats[src_gpu]['s'] - s
                src_w_new = g_stats[src_gpu]['w'] - w
                src_rem = GPU_MEM_SIZE - src_s_new
                src_p_new = src_w_new / src_rem if src_rem > 1e-9 else (float('inf') if src_w_new > 0 else 0.0)

                for dst in range(gpu_num):
                    if dst == src_gpu: continue
                    if g_stats[dst]['s'] + s > GPU_MEM_SIZE: continue
                    
                    dst_rem = GPU_MEM_SIZE - (g_stats[dst]['s'] + s)
                    if dst_rem <= 1e-9: continue
                    
                    dst_p_new = (g_stats[dst]['w'] + w) / dst_rem
                    
                    local_max = max(src_p_new, dst_p_new)
                    if local_max < best_val - 1e-6:
                        best_val = local_max
                        best_action = ('move', m_idx, dst)

            # 3. Evaluate SWAPS (Src <-> Dst)
            for m_src_idx, m_src in enumerate(src_models):
                w_src, s_src = m_src.req_rate / m_src.slo, m_src.model_size
                
                for dst in range(gpu_num):
                    if dst == src_gpu: continue
                    
                    dst_models = g_stats[dst]['models']
                    for m_dst_idx, m_dst in enumerate(dst_models):
                        w_dst, s_dst = m_dst.req_rate / m_dst.slo, m_dst.model_size
                        
                        # Capacity Check
                        new_src_s = g_stats[src_gpu]['s'] - s_src + s_dst
                        if new_src_s > GPU_MEM_SIZE: continue
                        new_dst_s = g_stats[dst]['s'] - s_dst + s_src
                        if new_dst_s > GPU_MEM_SIZE: continue
                        
                        # Pressure Check
                        new_src_rem = GPU_MEM_SIZE - new_src_s
                        if new_src_rem <= 1e-9: continue
                        new_src_p = (g_stats[src_gpu]['w'] - w_src + w_dst) / new_src_rem
                        
                        new_dst_rem = GPU_MEM_SIZE - new_dst_s
                        if new_dst_rem <= 1e-9: continue
                        new_dst_p = (g_stats[dst]['w'] - w_dst + w_src) / new_dst_rem
                        
                        local_max = max(new_src_p, new_dst_p)
                        if local_max < best_val - 1e-6:
                            best_val = local_max
                            best_action = ('swap', m_src_idx, dst, m_dst_idx)

            # Apply Best Action
            if best_action:
                if best_action[0] == 'move':
                    _, m_idx, dst = best_action
                    model = g_stats[src_gpu]['models'].pop(m_idx)
                    g_stats[src_gpu]['w'] -= (model.req_rate / model.slo)
                    g_stats[src_gpu]['s'] -= model.model_size
                    
                    g_stats[dst]['models'].append(model)
                    g_stats[dst]['w'] += (model.req_rate / model.slo)
                    g_stats[dst]['s'] += model.model_size
                else:
                    _, m_src_idx, dst, m_dst_idx = best_action
                    m_src = g_stats[src_gpu]['models'][m_src_idx]
                    m_dst = g_stats[dst]['models'][m_dst_idx]
                    
                    # Swap in lists
                    g_stats[src_gpu]['models'][m_src_idx] = m_dst
                    g_stats[dst]['models'][m_dst_idx] = m_src
                    
                    # Update stats
                    w_src, s_src = m_src.req_rate / m_src.slo, m_src.model_size
                    w_dst, s_dst = m_dst.req_rate / m_dst.slo, m_dst.model_size
                    
                    g_stats[src_gpu]['w'] += (w_dst - w_src)
                    g_stats[src_gpu]['s'] += (s_dst - s_src)
                    g_stats[dst]['w'] += (w_src - w_dst)
                    g_stats[dst]['s'] += (s_src - s_dst)
            else:
                break
        
        return {g: g_stats[g]['models'] for g in range(gpu_num)}

    best_placement = None
    best_score = float('inf')

    # ---------------------------------------------------------
    # 1. Greedy Heuristics Ensemble
    # ---------------------------------------------------------
    t_w = sum(m.req_rate / m.slo for m in models)
    t_s = sum(m.model_size for m in models)
    t_rem = gpu_num * GPU_MEM_SIZE - t_s
    k_est = t_w / t_rem if t_rem > 1e-9 else 1.0

    heuristics = [
        (lambda m: (m.req_rate / m.slo, m.model_size), 'min_result'),
        (lambda m: m.model_size, 'min_result'),
        (lambda m: (m.req_rate / m.slo) / (GPU_MEM_SIZE - m.model_size + 1e-6), 'min_result'),
        (lambda m: m.req_rate / m.slo, 'min_current'),
        (lambda m: (m.req_rate / m.slo) + k_est * m.model_size, 'min_result'),
    ]

    for key_fn, strategy in heuristics:
        sorted_models = sorted(models, key=key_fn, reverse=True)
        placement = {i: [] for i in range(gpu_num)}
        gpu_w = [0.0] * gpu_num
        gpu_s = [0.0] * gpu_num
        possible = True

        for model in sorted_models:
            w = model.req_rate / model.slo
            s = model.model_size
            best_idx = None
            best_val = float('inf')

            for i in range(gpu_num):
                if gpu_s[i] + s > GPU_MEM_SIZE: continue
                rem = GPU_MEM_SIZE - gpu_s[i]
                if strategy == 'min_result':
                    new_rem = rem - s
                    val = (gpu_w[i] + w) / new_rem if new_rem > 1e-9 else float('inf')
                else: 
                    val = gpu_w[i] / rem if rem > 1e-9 else float('inf')

                if val < best_val:
                    best_val = val
                    best_idx = i
                elif val == best_val and best_idx is None:
                    best_idx = i

            if best_idx is None:
                possible = False
                break
            placement[best_idx].append(model)
            gpu_w[best_idx] += w
            gpu_s[best_idx] += s

        if possible:
            score = get_max_kvpr(placement)
            if score < best_score:
                best_score = score
                best_placement = placement

    # ---------------------------------------------------------
    # 2. Binary Search with Adaptive Packing
    # ---------------------------------------------------------
    total_w = sum(m.req_rate / m.slo for m in models)
    total_s = sum(m.model_size for m in models)
    rem_global = gpu_num * GPU_MEM_SIZE - total_s

    if rem_global > 1e-6:
        low = total_w / rem_global
        high = best_score if best_score != float('inf') else 2000.0

        if high > low + 1e-4:
            for _ in range(16):
                mid = (low + high) / 2
                
                found_placement = None
                failed_items = set()
                
                # Packing Strategies
                strategies = [
                    (lambda m: m.model_size + (m.req_rate / m.slo) / mid, 'best_fit'),
                    (lambda m: m.model_size, 'best_fit'),
                    (lambda m: m.req_rate / m.slo, 'best_fit'),
                    (lambda m: (m.req_rate / m.slo) / (m.model_size + 1e-6), 'best_fit')
                ]

                # Try packing with randomized strategies and failure memory
                for attempt in range(40):
                    base_key, fit_mode = strategies[attempt % len(strategies)]
                    if attempt % 6 == 5: fit_mode = 'worst_fit'

                    noise = 0.0
                    if attempt >= 4:
                        noise = 0.01 + (attempt * 0.003)

                    def sort_key(m):
                        val = base_key(m)
                        if noise > 0: val *= random.uniform(1.0 - noise, 1.0 + noise)
                        if id(m) in failed_items: val += 1e9
                        return val

                    bs_models = sorted(models, key=sort_key, reverse=True)
                    temp_placement = {i: [] for i in range(gpu_num)}
                    gpu_w = [0.0] * gpu_num
                    gpu_s = [0.0] * gpu_num
                    possible_k = True
                    first_fail_model = None

                    for model in bs_models:
                        w = model.req_rate / model.slo
                        s = model.model_size
                        eff = s + w/mid

                        best_idx = None
                        if fit_mode == 'best_fit':
                            best_metric = float('inf')
                        else:
                            best_metric = -1.0

                        for i in range(gpu_num):
                            if gpu_s[i] + s > GPU_MEM_SIZE: continue
                            
                            curr_eff = gpu_s[i] + gpu_w[i]/mid
                            if curr_eff + eff <= GPU_MEM_SIZE + 1e-6:
                                rem_eff = GPU_MEM_SIZE - (curr_eff + eff)
                                
                                if fit_mode == 'best_fit':
                                    if rem_eff < best_metric:
                                        best_metric = rem_eff
                                        best_idx = i
                                else:
                                    if rem_eff > best_metric:
                                        best_metric = rem_eff
                                        best_idx = i
                        
                        if best_idx is None:
                            possible_k = False
                            first_fail_model = model
                            break
                        
                        temp_placement[best_idx].append(model)
                        gpu_w[best_idx] += w
                        gpu_s[best_idx] += s

                    if possible_k:
                        found_placement = temp_placement
                        break
                    else:
                        if first_fail_model: failed_items.add(id(first_fail_model))

                if found_placement:
                    # Instant refinement
                    refined = refine_placement(found_placement, max_iters=15)
                    actual_score = get_max_kvpr(refined)
                    
                    if actual_score < best_score:
                        best_score = actual_score
                        best_placement = refined
                    
                    high = min(mid, actual_score)
                else:
                    low = mid

    if best_placement is None:
        raise ValueError("Unable to place models on GPUs with available memory.")

    # ---------------------------------------------------------
    # 3. Final Local Search Refinement
    # ---------------------------------------------------------
    best_placement = refine_placement(best_placement, max_iters=50)
    
    return best_placement
# EVOLVE-BLOCK-END