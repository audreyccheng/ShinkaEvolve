<NAME>
greedy_post_placement_min_kvpr
</NAME>

<DESCRIPTION>
Change the greedy placement strategy to minimize the *resulting* KVPR of the target GPU (lookahead) instead of choosing the GPU with the minimum *current* KVPR. This better accounts for the impact of the current model's size on the GPU's pressure (denominator of the KVPR formula).
Also, sort models by size (descending) primarily, then by density (req_rate/slo). This ensures large models are placed first when GPUs are emptier, preserving large remaining memory blocks and preventing pressure spikes that occur when large models are forced into partially full GPUs.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Greedy KVPR-minimizing placement based on Algorithm 1 (without Ï„ check)
    # 1) Sort models by r_j / s_j in descending order
    sorted_models = sorted(models, key=lambda m: (m.req_rate / m.slo), reverse=True)

    # 2) Initialize per-GPU states
    placement = {gpu_id: [] for gpu_id in range(gpu_num)}
    shared_kv = [GPU_MEM_SIZE for _ in range(gpu_num)]  # remaining memory per GPU
    weighted_req_rate = [0.0 for _ in range(gpu_num)]   # sum of r_j / s_j per GPU

    # 3) Assign each model to the GPU that minimizes current KVPR while fitting in memory
    for model in sorted_models:
        best_idx = None
        best_ratio = float('inf')

        for gpu_id in range(gpu_num):
            if model.model_size <= shared_kv[gpu_id] and shared_kv[gpu_id] > 0:
                current_ratio = weighted_req_rate[gpu_id] / shared_kv[gpu_id]
                if current_ratio < best_ratio:
                    best_ratio = current_ratio
                    best_idx = gpu_id
=======
    # Greedy KVPR-minimizing placement with lookahead
    # 1) Sort models by size then density in descending order to handle large/heavy models first
    sorted_models = sorted(models, key=lambda m: (m.model_size, m.req_rate / m.slo), reverse=True)

    # 2) Initialize per-GPU states
    placement = {gpu_id: [] for gpu_id in range(gpu_num)}
    shared_kv = [GPU_MEM_SIZE for _ in range(gpu_num)]  # remaining memory per GPU
    weighted_req_rate = [0.0 for _ in range(gpu_num)]   # sum of r_j / s_j per GPU

    # 3) Assign each model to the GPU that minimizes the KVPR AFTER placement
    for model in sorted_models:
        best_idx = None
        best_new_kvpr = float('inf')

        for gpu_id in range(gpu_num):
            if model.model_size <= shared_kv[gpu_id]:
                # Calculate potential new state
                new_rem = shared_kv[gpu_id] - model.model_size
                new_rate = weighted_req_rate[gpu_id] + (model.req_rate / model.slo)

                # Calculate resulting KVPR
                if new_rem > 1e-9:
                    new_kvpr = new_rate / new_rem
                else:
                    new_kvpr = float('inf')

                # Update best if this is the first valid option or better than previous
                if best_idx is None or new_kvpr < best_new_kvpr:
                    best_new_kvpr = new_kvpr
                    best_idx = gpu_id
>>>>>>> REPLACE
</DIFF>