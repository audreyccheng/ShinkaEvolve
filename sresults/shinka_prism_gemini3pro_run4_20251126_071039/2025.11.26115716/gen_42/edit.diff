--- a/original.py
+++ b/original.py
@@ -1,304 +1,307 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 import math
+import random
 
 GPU_MEM_SIZE = 80.0
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
     
-    Approach:
-    1.  Transform the KVPR minimization problem into a series of feasibility checks for a target KVPR K.
-        Constraint: sum(req/slo) / (Capacity - sum(size)) <= K
-        Linearized: sum(req/slo + K * size) <= K * Capacity
-    2.  Use Binary Search to find the minimum K.
-    3.  Check feasibility using Beam Search (Width=5) with symmetry breaking (canonicalized bin states).
-        Heuristic: Maximize sum of squared loads (Best-Fit preference).
-    4.  Refine feasible solutions using local search (load balancing) to tighten binary search bounds faster.
-
+    Strategy:
+    1. Binary Search for the optimal target KVPR (K).
+    2. Feasibility Check (Bin Packing):
+       - Transforms the problem into checking if items fit in bins with capacity K*C 
+         and item sizes w + K*s.
+       - Uses a Multi-Heuristic Greedy approach:
+         - Tries multiple deterministic sort orders (Linearized Cost, Size, Weight, Density).
+         - If deterministic fails, tries Randomized Greedy with perturbed sort keys.
+    3. Local Search Optimization:
+       - Refines any valid placement found using Steepest Descent Hill Climbing.
+       - Uses both 'Move' (shift item to another GPU) and 'Swap' (exchange items) operators.
+       
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         A placement of models to GPUs
     """
     
     # 1. Preprocess items
     items = []
-    total_w = 0.0
-    total_s = 0.0
     for i, m in enumerate(models):
         w = m.req_rate / m.slo
         s = m.model_size
-        items.append({'w': w, 's': s, 'model': m, 'id': i})
-        total_w += w
-        total_s += s
-
-    # 2. Lower Bound for K
-    # sum(w) / (Total_Cap - sum(s)) is a theoretical lower bound
-    total_cap = gpu_num * GPU_MEM_SIZE
-    rem_global = total_cap - total_s
-    if rem_global <= 1e-9:
-        k_min = 0.0
-    else:
-        k_min = total_w / rem_global
-
-    def get_actual_kvpr(placement):
-        """Calculate the actual maximum KVPR of a placement."""
-        max_k = 0.0
+        items.append({'m': m, 'w': w, 's': s})
+
+    # Helper: Calculate KVPR given w and s
+    def calc_kvpr(w, s):
+        rem = GPU_MEM_SIZE - s
+        if rem <= 1e-9:
+            return float('inf') if w > 1e-9 else 0.0
+        return w / rem
+
+    def get_max_kvpr(placement):
+        mx = 0.0
         for p in placement.values():
-            w_sum = sum(m.req_rate / m.slo for m in p)
-            s_sum = sum(m.model_size for m in p)
-            rem = GPU_MEM_SIZE - s_sum
-            if rem <= 1e-9:
-                # If memory is full, pressure is infinite if there is load, else 0
-                if w_sum > 1e-9: return float('inf')
-                val = 0.0
-            else:
-                val = w_sum / rem
-            if val > max_k: max_k = val
-        return max_k
-
-    def check_feasibility(k_target):
-        """
-        Check if models can be packed with KVPR <= k_target using Beam Search.
-        """
-        # Linear capacity limit
-        cap_lin = k_target * GPU_MEM_SIZE
-        
-        # Calculate item weights for this K: v_i = w_i + K * s_i
-        item_data = []
-        for it in items:
-            v = it['w'] + k_target * it['s']
-            # Optimization: If a single item is larger than bin capacity, impossible
-            if v > cap_lin + 1e-7: return None 
-            item_data.append((v, it))
-            
-        # Sort items descending by linear size (BFD heuristic)
-        item_data.sort(key=lambda x: x[0], reverse=True)
-        
-        # Beam Search
-        # State: (score, loads_tuple, placement_tuple)
-        # We use canonicalized loads (sorted) to break symmetry.
-        
-        # Initial State: 0 load on all GPUs, empty placements
-        init_loads = tuple([0.0] * gpu_num)
-        init_pl = tuple([[] for _ in range(gpu_num)])
-        
-        beam = [(0.0, init_loads, init_pl)]
-        beam_width = 5
-        
-        for v, it in item_data:
-            next_beam = []
-            seen_states = set()
-            
-            for score, loads, pl in beam:
-                # To break symmetry, we track tried load values for this state
-                tried_loads = set()
-                
-                for i in range(gpu_num):
-                    current_l = loads[i]
-                    
-                    # Optimization: Only try one bin for each unique load value
-                    if current_l in tried_loads:
-                        continue
-                        
-                    # Check capacity
-                    if current_l + v <= cap_lin + 1e-7:
-                        tried_loads.add(current_l)
-                        
-                        # Create new state components
-                        new_l = current_l + v
-                        new_bin_pl = pl[i] + [it['model']]
-                        
-                        # Canonicalize: Sort bins by load to maintain unique state representation
-                        # Combine load and placement to sort together
-                        temp_state = []
-                        for j in range(gpu_num):
-                            if i == j:
-                                temp_state.append((new_l, new_bin_pl))
-                            else:
-                                temp_state.append((loads[j], pl[j]))
-                        
-                        # Sort by load descending
-                        temp_state.sort(key=lambda x: x[0], reverse=True)
-                        
-                        new_loads = tuple(x[0] for x in temp_state)
-                        new_pl = tuple(x[1] for x in temp_state)
-                        
-                        if new_loads not in seen_states:
-                            seen_states.add(new_loads)
-                            # Heuristic: Maximize sum of squares (Best-Fit preference)
-                            # This encourages filling bins tightly, leaving others empty
-                            new_score = sum(l*l for l in new_loads)
-                            next_beam.append((new_score, new_loads, new_pl))
-            
-            # If no valid states for this item, this path fails
-            if not next_beam:
-                return None
-            
-            # Prune beam: Keep top W states
-            next_beam.sort(key=lambda x: x[0], reverse=True)
-            beam = next_beam[:beam_width]
-            
-        # Return best placement found
-        best_pl_tuple = beam[0][2]
-        final_placement = {i: best_pl_tuple[i] for i in range(gpu_num)}
-        return final_placement
-
-    def optimize_placement(placement):
-        """
-        Local search to reduce max KVPR of a valid placement.
-        Tries to move models from the highest pressure GPU to others.
-        """
-        for _ in range(30): # Limited iterations
-            # Identify max KVPR GPU
+            w = sum(m.req_rate / m.slo for m in p)
+            s = sum(m.model_size for m in p)
+            mx = max(mx, calc_kvpr(w, s))
+        return mx
+
+    # 2. Local Search (Hill Climbing with Move & Swap)
+    def local_optimize(placement):
+        # Convert placement to mutable state
+        state = []
+        for i in range(gpu_num):
+            p = placement[i]
+            w = sum(m.req_rate / m.slo for m in p)
+            s = sum(m.model_size for m in p)
+            state.append({'w': w, 's': s, 'items': list(p)})
+
+        # Optimization loop
+        for _ in range(100):
+            # Find bottleneck GPU
             max_k = -1.0
-            max_idx = -1
-            gpu_stats = []
-            
-            for i in range(gpu_num):
-                w = sum(m.req_rate / m.slo for m in placement[i])
-                s = sum(m.model_size for m in placement[i])
-                rem = GPU_MEM_SIZE - s
-                if rem <= 1e-9:
-                    k = float('inf') if w > 1e-9 else 0.0
-                else:
-                    k = w / rem
-                gpu_stats.append({'k': k, 'w': w, 's': s, 'idx': i})
+            src_idx = -1
+            gpu_k = []
+            
+            for i, st in enumerate(state):
+                k = calc_kvpr(st['w'], st['s'])
+                gpu_k.append(k)
                 if k > max_k:
                     max_k = k
-                    max_idx = i
-            
-            if max_idx == -1 or max_k == 0: break
-            
+                    src_idx = i
+            
+            if max_k <= 1e-9: break
+            
+            src = state[src_idx]
             improved = False
-            src = gpu_stats[max_idx]
-            
-            # Try to move a model from bottleneck source to any destination
-            src_models = placement[src['idx']]
-            for m_idx, m in enumerate(src_models):
-                mw = m.req_rate / m.slo
-                ms = m.model_size
-                
-                # Predict src K after removal
-                ns_rem = GPU_MEM_SIZE - (src['s'] - ms)
-                if ns_rem <= 1e-9:
-                    ns_k = float('inf') if (src['w'] - mw) > 1e-9 else 0.0
-                else:
-                    ns_k = (src['w'] - mw) / ns_rem
-                
-                # Optimization: if removing doesn't help enough, skip (optional)
-                
-                for dst in gpu_stats:
-                    if dst['idx'] == src['idx']: continue
-                    
-                    # Check memory fit
-                    if dst['s'] + ms > GPU_MEM_SIZE: continue
-                    
-                    # Predict dst K after addition
-                    nd_rem = GPU_MEM_SIZE - (dst['s'] + ms)
-                    if nd_rem <= 1e-9:
-                        nd_k = float('inf') if (dst['w'] + mw) > 1e-9 else 0.0
-                    else:
-                        nd_k = (dst['w'] + mw) / nd_rem
-                    
-                    # Move is valid if both new pressures are strictly less than current global max
-                    # (This ensures we are essentially 'lowering the water level')
-                    if max(ns_k, nd_k) < max_k - 1e-9:
-                        # Apply move
-                        model = placement[src['idx']].pop(m_idx)
-                        placement[dst['idx']].append(model)
+            
+            # Operator 1: Move item from bottleneck to other
+            for i, item in enumerate(src['items']):
+                iw = item.req_rate / item.slo
+                is_ = item.model_size
+                
+                # Check Src after removal
+                ns_w = src['w'] - iw
+                ns_s = src['s'] - is_
+                ns_k = calc_kvpr(ns_w, ns_s)
+                
+                # Pruning: If src doesn't drop below max_k, it's not a complete fix, 
+                # but we proceed if it improves the global situation (steepest descent).
+                # To be efficient, we check if max(ns_k, nd_k) < max_k
+                
+                for dst_idx in range(gpu_num):
+                    if dst_idx == src_idx: continue
+                    dst = state[dst_idx]
+                    
+                    if dst['s'] + is_ > GPU_MEM_SIZE: continue
+                    
+                    nd_w = dst['w'] + iw
+                    nd_s = dst['s'] + is_
+                    nd_k = calc_kvpr(nd_w, nd_s)
+                    
+                    if max(ns_k, nd_k) < max_k - 1e-7:
+                        # Apply Move
+                        src['items'].pop(i)
+                        src['w'], src['s'] = ns_w, ns_s
+                        dst['items'].append(item)
+                        dst['w'], dst['s'] = nd_w, nd_s
                         improved = True
                         break
                 if improved: break
             
+            if improved: continue
+            
+            # Operator 2: Swap item in bottleneck with item in other
+            for i, item1 in enumerate(src['items']):
+                iw1 = item1.req_rate / item1.slo
+                is1 = item1.model_size
+                
+                for dst_idx in range(gpu_num):
+                    if dst_idx == src_idx: continue
+                    dst = state[dst_idx]
+                    
+                    # Heuristic: Don't swap with a near-bottleneck GPU
+                    if gpu_k[dst_idx] > max_k * 0.95: continue
+                    
+                    for j, item2 in enumerate(dst['items']):
+                        iw2 = item2.req_rate / item2.slo
+                        is2 = item2.model_size
+                        
+                        # New Src
+                        ns_s = src['s'] - is1 + is2
+                        if ns_s > GPU_MEM_SIZE: continue
+                        ns_w = src['w'] - iw1 + iw2
+                        
+                        # New Dst
+                        nd_s = dst['s'] - is2 + is1
+                        if nd_s > GPU_MEM_SIZE: continue
+                        nd_w = dst['w'] - iw2 + iw1
+                        
+                        ns_k = calc_kvpr(ns_w, ns_s)
+                        nd_k = calc_kvpr(nd_w, nd_s)
+                        
+                        if max(ns_k, nd_k) < max_k - 1e-7:
+                            # Apply Swap
+                            src['items'][i] = item2
+                            src['w'], src['s'] = ns_w, ns_s
+                            dst['items'][j] = item1
+                            dst['w'], dst['s'] = nd_w, nd_s
+                            improved = True
+                            break
+                    if improved: break
+                if improved: break
+            
             if not improved: break
             
-        return placement
-
-    # Main Binary Search Loop
+        return {i: state[i]['items'] for i in range(gpu_num)}
+
+    # 3. Feasibility Check (Multi-Heuristic Greedy)
+    def check_placement(k_target):
+        lin_cap = k_target * GPU_MEM_SIZE
+        
+        def try_pack(ordered_items):
+            bins = [{'w': 0.0, 's': 0.0, 'items': []} for _ in range(gpu_num)]
+            
+            for item in ordered_items:
+                w, s = item['w'], item['s']
+                v_lin = w + k_target * s
+                
+                best_idx = -1
+                best_fill = -1.0
+                
+                for i in range(gpu_num):
+                    b = bins[i]
+                    # Physical Check
+                    if b['s'] + s > GPU_MEM_SIZE: continue
+                    
+                    # KVPR Check: (current_w + w) + K*(current_s + s) <= K*C
+                    # <=> current_lin + v_lin <= lin_cap
+                    lin_load = b['w'] + k_target * b['s']
+                    if lin_load + v_lin > lin_cap + 1e-7: continue
+                    
+                    # Best Fit: Maximize current linearized load
+                    if lin_load > best_fill:
+                        best_fill = lin_load
+                        best_idx = i
+                
+                if best_idx != -1:
+                    bins[best_idx]['w'] += w
+                    bins[best_idx]['s'] += s
+                    bins[best_idx]['items'].append(item['m'])
+                else:
+                    return None
+            return {i: bins[i]['items'] for i in range(gpu_num)}
+
+        # A. Deterministic Sort Strategies
+        keys = [
+            lambda x: x['w'] + k_target * x['s'],   # Linearized Cost
+            lambda x: x['s'],                       # Physical Size
+            lambda x: x['w'],                       # Weight
+            lambda x: x['w'] / (x['s'] + 1e-9)      # Density
+        ]
+        
+        for key in keys:
+            res = try_pack(sorted(items, key=key, reverse=True))
+            if res: return res
+
+        # B. Randomized Strategy
+        # Perturb the Linearized Cost key
+        rng = random.Random(42 + int(k_target * 100))
+        base_key = lambda x: x['w'] + k_target * x['s']
+        
+        for _ in range(50):
+            # Add noise to the key: key * uniform(0.9, 1.1)
+            noisy_items = []
+            for item in items:
+                score = base_key(item) * rng.uniform(0.9, 1.1)
+                noisy_items.append((score, item))
+            
+            noisy_items.sort(key=lambda x: x[0], reverse=True)
+            res = try_pack([x[1] for x in noisy_items])
+            if res: return res
+            
+        return None
+
+    # 4. Binary Search Driver
     high = 1e9
     
-    # 1. Initial Feasibility Check
-    bs_res = check_feasibility(high)
-    if not bs_res:
-         raise ValueError("Unable to place models on GPUs (insufficient total memory).")
-    
-    # 2. Refine bounds
-    # Optimization: tighten high bound immediately
-    bs_res = optimize_placement(bs_res)
-    best_placement = bs_res
-    high = min(high, get_actual_kvpr(bs_res))
-    low = k_min
-    
-    # 3. Search
-    for _ in range(25):
+    # Initial valid solution
+    best_placement = check_placement(high)
+    if best_placement is None:
+        raise ValueError("Unable to place models on GPUs (insufficient total memory).")
+    
+    # Initial Optimization
+    best_placement = local_optimize(best_placement)
+    high = get_max_kvpr(best_placement)
+    low = 0.0
+
+    for _ in range(30):
         mid = (low + high) / 2
-        res = check_feasibility(mid)
+        res = check_placement(mid)
         if res:
-            # Found a valid placement, optimize it to see if we can go lower
-            opt_res = optimize_placement(res)
-            best_placement = opt_res
-            
-            # The actual max KVPR of the optimized solution is a valid upper bound
-            actual_k = get_actual_kvpr(opt_res)
-            high = min(mid, actual_k)
+            res = local_optimize(res)
+            mx = get_max_kvpr(res)
+            if mx < get_max_kvpr(best_placement):
+                best_placement = res
+            high = min(mid, mx)
         else:
             low = mid
-            
+
     return best_placement
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
     
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
     
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
     
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
     
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
     
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")
 
