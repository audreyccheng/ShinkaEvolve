--- a/original.py
+++ b/original.py
@@ -1,295 +1,315 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
-GPU_MEM_SIZE = 80  # GB
+import random
+
+GPU_MEM_SIZE = 80.0
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
-
-    Args:
-        gpu_num: Number of GPUs
-        models: List of models to place
-
-    Returns:
-        A placement of models to GPUs
+    
+    Uses Binary Search on target KVPR (K).
+    Inner check uses Best-Fit Decreasing with Adaptive Failure Prioritization.
+    Post-processes with Local Search (Hill Climbing).
     """
 
-    # Pre-process models to extract relevant metrics: weight (req/slo) and size
+    # Pre-process models
     items = []
-    for m in models:
+    for i, m in enumerate(models):
         items.append({
             'model': m,
             'w': m.req_rate / m.slo,
-            's': m.model_size
+            's': m.model_size,
+            'id': i
         })
 
-    def check_placement(k_target):
-        import random
-        """
-        Determines if it is possible to place all models such that for every GPU:
-        KVPR <= k_target.
-        """
-
-        def try_pack(item_order):
-            placement = {i: [] for i in range(gpu_num)}
-            gpu_state = [{'w': 0.0, 's': 0.0} for _ in range(gpu_num)]
-
-            for item in item_order:
-                best_idx = -1
-                best_fill = -1.0
-
-                for i in range(gpu_num):
-                    new_s = gpu_state[i]['s'] + item['s']
-                    if new_s > GPU_MEM_SIZE: continue
-
-                    new_w = gpu_state[i]['w'] + item['w']
-                    rem_mem = GPU_MEM_SIZE - new_s
-
-                    # KVPR Check
-                    if rem_mem <= 1e-9:
-                        if k_target > 1e12: pass
-                        elif new_w > 1e-9: continue
-                    elif new_w > k_target * rem_mem + 1e-9:
-                        continue
-
-                    # Best Fit: Maximize w + k*s
-                    current_fill = new_w + k_target * new_s
-                    if current_fill > best_fill:
-                        best_fill = current_fill
-                        best_idx = i
-
-                if best_idx != -1:
-                    placement[best_idx].append(item['model'])
-                    gpu_state[best_idx]['s'] += item['s']
-                    gpu_state[best_idx]['w'] += item['w']
-                else:
-                    return None
-            return placement
-
-        # 1. Deterministic Strategies
-        # Sort keys (descending)
-        strategies = [
-            lambda x: x['w'] + k_target * x['s'],
-            lambda x: x['s'],
-            lambda x: x['w'],
-            lambda x: x['w'] / (x['s'] + 1e-9),
-            lambda x: x['w'] / (GPU_MEM_SIZE - x['s'] + 1e-9)
-        ]
-
-        for key in strategies:
-            res = try_pack(sorted(items, key=key, reverse=True))
-            if res: return res
-
-        # 2. Randomized Strategies (Noisy Heuristic)
-        # Use a noisy version of the linearized cost to explore neighborhood of the best heuristic
-        rng = random.Random(42 + int(k_target))
-        base_key = lambda x: x['w'] + k_target * x['s']
-        for _ in range(25):
-            # Perturb the sort key with multiplicative noise
-            noisy_items = sorted(items, key=lambda x: base_key(x) * rng.uniform(0.9, 1.1), reverse=True)
-            res = try_pack(noisy_items)
-            if res: return res
-
-        return None
-
-    # Binary Search for the Minimum Maximum KVPR (K)
-
-    # Initialization
-    high = 1e9
-    best_placement = check_placement(high)
-
-    if best_placement is None:
-        raise ValueError("Unable to place models on GPUs (insufficient total memory).")
-
-    # Refine 'high' based on found solution
-    current_max = 0.0
-    for gpu_p in best_placement.values():
-        w_sum = sum(m.req_rate / m.slo for m in gpu_p)
-        s_sum = sum(m.model_size for m in gpu_p)
-        rem = GPU_MEM_SIZE - s_sum
-        if rem > 1e-9:
-            current_max = max(current_max, w_sum / rem)
-        elif w_sum > 0:
-            current_max = high
-
-    high = current_max
-    low = 0.0
-
-    # Binary Search Loop
-    for _ in range(30):
-        mid = (low + high) / 2
-        result = check_placement(mid)
-        if result is not None:
-            best_placement = result
-
-            # Refine high bound using the actual max KVPR of the valid solution
-            actual_max = 0.0
-            for gpu_p in best_placement.values():
-                w_sum = sum(m.req_rate / m.slo for m in gpu_p)
-                s_sum = sum(m.model_size for m in gpu_p)
-                rem = GPU_MEM_SIZE - s_sum
-                if rem > 1e-9:
-                    val = w_sum / rem
-                elif w_sum > 0:
-                    val = float('inf')
-                else:
-                    val = 0.0
-                if val > actual_max:
-                    actual_max = val
-
-            high = min(mid, actual_max)
-        else:
-            low = mid
-
-    # --- Local Search Optimization (Hill Climbing) ---
     def calc_kvpr(w, s):
+        """Compute KVPR: w / (C - s)."""
         rem = GPU_MEM_SIZE - s
         if rem <= 1e-9:
             return float('inf') if w > 1e-9 else 0.0
         return w / rem
 
-    gpu_states = []
-    for i in range(gpu_num):
-        p = best_placement[i]
-        w = sum(m.req_rate / m.slo for m in p)
-        s = sum(m.model_size for m in p)
-        gpu_states.append({'w': w, 's': s, 'models': list(p)})
-
-    # Iterative improvement
-    for _ in range(150):
-        # Find bottleneck
-        max_k = -1.0
-        src_idx = -1
-        for i in range(gpu_num):
-            k = calc_kvpr(gpu_states[i]['w'], gpu_states[i]['s'])
-            if k > max_k:
-                max_k = k
-                src_idx = i
-
-        if max_k <= 1e-9: break
-
-        improved = False
-        src = gpu_states[src_idx]
-
-        # 1. Try Move (Src -> Dst)
-        for i, m in enumerate(src['models']):
-            m_w = m.req_rate / m.slo
-            m_s = m.model_size
-
-            ns_w = src['w'] - m_w
-            ns_s = src['s'] - m_s
-            ns_k = calc_kvpr(ns_w, ns_s)
-
-            if ns_k >= max_k - 1e-9: continue
-
-            for dst_idx in range(gpu_num):
-                if dst_idx == src_idx: continue
-                dst = gpu_states[dst_idx]
-
-                if dst['s'] + m_s > GPU_MEM_SIZE: continue
-                nd_k = calc_kvpr(dst['w'] + m_w, dst['s'] + m_s)
-
-                if nd_k < max_k - 1e-9:
-                    src['models'].pop(i)
-                    src['w'], src['s'] = ns_w, ns_s
-
-                    dst['models'].append(m)
-                    dst['w'] += m_w
-                    dst['s'] += m_s
-                    improved = True
-                    break
-            if improved: break
-
-        if improved: continue
-
-        # 2. Try Swap (Src <-> Dst)
-        for i, m1 in enumerate(src['models']):
-            m1_w = m1.req_rate / m1.slo
-            m1_s = m1.model_size
-
-            for dst_idx in range(gpu_num):
-                if dst_idx == src_idx: continue
-                dst = gpu_states[dst_idx]
-
-                if calc_kvpr(dst['w'], dst['s']) > max_k * 0.95: continue
-
-                for j, m2 in enumerate(dst['models']):
-                    m2_w = m2.req_rate / m2.slo
-                    m2_s = m2.model_size
-
-                    ns_s = src['s'] - m1_s + m2_s
-                    if ns_s > GPU_MEM_SIZE: continue
-                    ns_w = src['w'] - m1_w + m2_w
-
-                    nd_s = dst['s'] - m2_s + m1_s
-                    if nd_s > GPU_MEM_SIZE: continue
-                    nd_w = dst['w'] - m2_w + m1_w
-
-                    ns_k = calc_kvpr(ns_w, ns_s)
+    def refine_placement(placement_list):
+        """
+        Hill climbing local search to reduce max KVPR.
+        Input: list of lists of item dicts.
+        Output: list of dicts representing GPU states.
+        """
+        # Initialize state
+        state = []
+        for p in placement_list:
+            w = sum(x['w'] for x in p)
+            s = sum(x['s'] for x in p)
+            state.append({'w': w, 's': s, 'items': list(p)})
+            
+        # Optimize
+        for _ in range(150):
+            # Find bottleneck
+            max_k = -1.0
+            src_idx = -1
+            gpu_k = []
+            
+            for i, st in enumerate(state):
+                k = calc_kvpr(st['w'], st['s'])
+                gpu_k.append(k)
+                if k > max_k:
+                    max_k = k
+                    src_idx = i
+            
+            if max_k <= 1e-9: break
+            
+            improved = False
+            src = state[src_idx]
+            
+            # 1. Try Move (Src -> Dst)
+            # Sort items by some heuristic? Maybe largest size first?
+            # Iterating normally is usually fine for small N.
+            for i, item in enumerate(src['items']):
+                # Predicte source state after removal
+                ns_w = src['w'] - item['w']
+                ns_s = src['s'] - item['s']
+                # KVPR generally improves on source, so we focus on dest
+                
+                # Check all destinations
+                for dst_idx in range(gpu_num):
+                    if dst_idx == src_idx: continue
+                    dst = state[dst_idx]
+                    
+                    if dst['s'] + item['s'] > GPU_MEM_SIZE: continue
+                    
+                    nd_w = dst['w'] + item['w']
+                    nd_s = dst['s'] + item['s']
                     nd_k = calc_kvpr(nd_w, nd_s)
-
-                    if max(ns_k, nd_k) < max_k - 1e-9:
-                        src['models'][i] = m2
+                    
+                    # Acceptance criteria: Dest must stay below current bottleneck
+                    if nd_k < max_k - 1e-9:
+                        src['items'].pop(i)
                         src['w'], src['s'] = ns_w, ns_s
-
-                        dst['models'][j] = m1
+                        dst['items'].append(item)
                         dst['w'], dst['s'] = nd_w, nd_s
                         improved = True
                         break
                 if improved: break
-            if improved: break
-
-        if not improved: break
-
-    return {i: gpu_states[i]['models'] for i in range(gpu_num)}
+            
+            if improved: continue
+            
+            # 2. Try Swap (Src <-> Dst)
+            for i, item1 in enumerate(src['items']):
+                for dst_idx in range(gpu_num):
+                    if dst_idx == src_idx: continue
+                    dst = state[dst_idx]
+                    
+                    # Pruning: Don't swap with a GPU that is already near the limit
+                    if gpu_k[dst_idx] > max_k * 0.95: continue
+                    
+                    for j, item2 in enumerate(dst['items']):
+                        # Check capacity
+                        ns_s = src['s'] - item1['s'] + item2['s']
+                        nd_s = dst['s'] - item2['s'] + item1['s']
+                        
+                        if ns_s > GPU_MEM_SIZE or nd_s > GPU_MEM_SIZE: continue
+                        
+                        ns_w = src['w'] - item1['w'] + item2['w']
+                        nd_w = dst['w'] - item2['w'] + item1['w']
+                        
+                        ns_k = calc_kvpr(ns_w, ns_s)
+                        nd_k = calc_kvpr(nd_w, nd_s)
+                        
+                        # Acceptance: Both must be better than current max
+                        if ns_k < max_k - 1e-9 and nd_k < max_k - 1e-9:
+                            src['items'][i] = item2
+                            src['w'], src['s'] = ns_w, ns_s
+                            dst['items'][j] = item1
+                            dst['w'], dst['s'] = nd_w, nd_s
+                            improved = True
+                            break
+                    if improved: break
+                if improved: break
+                
+            if not improved: break
+            
+        return state
+
+    def check_placement(k_target):
+        limit_cost = k_target * GPU_MEM_SIZE
+        
+        def try_pack(ordered_items):
+            # Returns (placement_list, failed_index)
+            # Placement is list of lists of items
+            
+            # State: load (w), size (s), items
+            # Optimization: track linearized load directly
+            # lin_load = w + k*s
+            bins = [{'lin': 0.0, 's': 0.0, 'w': 0.0, 'items': []} for _ in range(gpu_num)]
+            
+            for idx, item in enumerate(ordered_items):
+                w, s = item['w'], item['s']
+                cost = w + k_target * s
+                
+                best_idx = -1
+                best_fill = -1.0
+                
+                for i in range(gpu_num):
+                    b = bins[i]
+                    if b['s'] + s > GPU_MEM_SIZE: continue
+                    
+                    # Capacity Check
+                    if b['lin'] + cost > limit_cost + 1e-7: continue
+                    
+                    # Best Fit: Maximize current fill (linearized)
+                    if b['lin'] > best_fill:
+                        best_fill = b['lin']
+                        best_idx = i
+                
+                if best_idx != -1:
+                    bins[best_idx]['lin'] += cost
+                    bins[best_idx]['s'] += s
+                    bins[best_idx]['w'] += w
+                    bins[best_idx]['items'].append(item)
+                else:
+                    return None, idx
+                    
+            return [b['items'] for b in bins], -1
+
+        # 1. Deterministic Heuristics
+        strategies = [
+            lambda x: x['w'] + k_target * x['s'],   # Linearized Cost
+            lambda x: x['s'],                       # Size
+            lambda x: x['w'],                       # Weight
+            lambda x: x['w'] / (GPU_MEM_SIZE - x['s'] + 1e-9) # Standalone Pressure
+        ]
+        
+        for key in strategies:
+            items_sorted = sorted(items, key=key, reverse=True)
+            res, _ = try_pack(items_sorted)
+            if res: return res
+
+        # 2. Adaptive Randomized Heuristics
+        rng = random.Random(42 + int(k_target * 10))
+        base_items = list(items)
+        rng.shuffle(base_items)
+        
+        failed_item_id = -1
+        
+        for _ in range(40):
+            # Perturb sort key
+            # Base key: Linearized Cost
+            # We sort items by (Cost * Noise)
+            # If we have a failed item, we artificially boost its score to move it to front? 
+            # Or just explicitly insert at 0. Explicit insert is more reliable.
+            
+            current_items = sorted(base_items, 
+                                 key=lambda x: (x['w'] + k_target * x['s']) * rng.uniform(0.9, 1.1), 
+                                 reverse=True)
+            
+            if failed_item_id != -1:
+                # Move failed item to front
+                for i in range(len(current_items)):
+                    if current_items[i]['id'] == failed_item_id:
+                        current_items.insert(0, current_items.pop(i))
+                        break
+            
+            res, fail_idx = try_pack(current_items)
+            if res: return res
+            
+            if fail_idx != -1:
+                failed_item_id = current_items[fail_idx]['id']
+            else:
+                failed_item_id = -1
+                
+        return None
+
+    # Binary Search
+    high = 1e9
+    
+    # Quick check at high
+    best_res = check_placement(high)
+    if best_res is None:
+        raise ValueError("Unable to place models on GPUs.")
+        
+    # Convert best_res to state for processing
+    best_state = refine_placement(best_res)
+    
+    # Calculate upper bound
+    mx = 0.0
+    for st in best_state:
+        mx = max(mx, calc_kvpr(st['w'], st['s']))
+    high = mx
+    low = 0.0
+    
+    for _ in range(25):
+        if high - low < 1e-4: break
+        mid = (low + high) / 2
+        
+        res = check_placement(mid)
+        if res:
+            refined = refine_placement(res)
+            rmx = 0.0
+            for st in refined:
+                rmx = max(rmx, calc_kvpr(st['w'], st['s']))
+            
+            if rmx < high:
+                best_state = refined
+                high = rmx
+            else:
+                # Valid placement found at mid, so high can be at least mid
+                high = min(mid, high)
+        else:
+            low = mid
+
+    # Final pass
+    best_state = refine_placement([st['items'] for st in best_state])
+    
+    return {i: [x['model'] for x in st['items']] for i, st in enumerate(best_state)}
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")