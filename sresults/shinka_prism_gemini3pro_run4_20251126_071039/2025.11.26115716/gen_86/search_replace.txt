<NAME>
failure_driven_packing_and_iterative_optimization
</NAME>

<DESCRIPTION>
1. Implements Failure-Driven Prioritization in the randomized packing phase. When a packing attempt fails, the unplaced items are prioritized (moved to the front) for the next attempt. This helps the greedy allocator handle the most difficult constraints first.
2. Refactors the Hill Climbing local search into a reusable function `optimize_placement`.
3. Integrates `optimize_placement` into the binary search loop. Whenever a feasible solution is found, it is locally optimized to find its true minimum KVPR, allowing the binary search upper bound (`high`) to be tightened more aggressively, leading to faster convergence to a lower global maximum KVPR.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    def solve_packing(k_target, ordered_items):
        """Try to pack items into GPUs such that KVPR <= k_target."""
        placement = {i: [] for i in range(gpu_num)}
        gpu_state = [{'w': 0.0, 's': 0.0} for _ in range(gpu_num)]

        for item in ordered_items:
            best_idx = -1
            best_score = -1.0

            for i in range(gpu_num):
                # Capacity Check
                if gpu_state[i]['s'] + item['s'] > GPU_MEM_SIZE: continue

                new_w = gpu_state[i]['w'] + item['w']
                new_s = gpu_state[i]['s'] + item['s']
                rem = GPU_MEM_SIZE - new_s

                # KVPR Constraint Check
                if rem <= 1e-9:
                    if new_w > 1e-9: continue
                elif new_w > k_target * rem + 1e-7:
                    continue

                # Best Fit Heuristic: Maximize usage (linearized)
                # This packs bins tightly.
                score = new_w + k_target * new_s
                if score > best_score:
                    best_score = score
                    best_idx = i

            if best_idx != -1:
                placement[best_idx].append(item['model'])
                gpu_state[best_idx]['w'] += item['w']
                gpu_state[best_idx]['s'] += item['s']
            else:
                return None
        return placement

    def check_placement(k_target, attempts=10):
        # 1. Deterministic Strategies
        # Keys: Linearized Cost, Size, Weight
        strategies = [
            lambda x: x['w'] + k_target * x['s'],
            lambda x: x['s'],
            lambda x: x['w']
        ]

        for key_func in strategies:
            res = solve_packing(k_target, sorted(items, key=key_func, reverse=True))
            if res: return res

        # 2. Randomized Noisy Strategies
        if attempts > 0:
            rng = random.Random(42 + int(k_target))
            base_key = lambda x: x['w'] + k_target * x['s']
            for _ in range(attempts):
                # Add multiplicative noise
                noisy_items = sorted(items, key=lambda x: base_key(x) * rng.uniform(0.85, 1.15), reverse=True)
                res = solve_packing(k_target, noisy_items)
                if res: return res

        return None

    # Binary Search
    high = 1e9
    best_placement = check_placement(high, attempts=1)

    if best_placement is None:
        raise ValueError("Unable to place models on GPUs (insufficient total memory).")

    high = get_placement_kvpr(best_placement)
    low = 0.0

    for _ in range(25):
        mid = (low + high) / 2
        res = check_placement(mid, attempts=20)
        if res is not None:
            best_placement = res
            high = min(mid, get_placement_kvpr(res))
        else:
            low = mid

    # --- Local Search Optimization (Hill Climbing) ---
    # Convert to manageable state
    gpu_states = []
    for i in range(gpu_num):
        models_p = best_placement[i]
        w = sum(m.req_rate / m.slo for m in models_p)
        s = sum(m.model_size for m in models_p)
        gpu_states.append({'w': w, 's': s, 'models': list(models_p)})

    def calc_kvpr(w, s):
        rem = GPU_MEM_SIZE - s
        if rem <= 1e-9: return float('inf') if w > 1e-9 else 0.0
        return w / rem

    for _ in range(100):
        # Find bottleneck GPU
        max_k = -1.0
        src_idx = -1
        for i in range(gpu_num):
            k = calc_kvpr(gpu_states[i]['w'], gpu_states[i]['s'])
            if k > max_k:
                max_k = k
                src_idx = i

        if max_k <= 0: break

        improved = False
        src = gpu_states[src_idx]

        # Try moving a model from src to any dst
        for i, m in enumerate(src['models']):
            m_w = m.req_rate / m.slo
            m_s = m.model_size

            # Predict src after removal
            ns_w = src['w'] - m_w
            ns_s = src['s'] - m_s
            ns_k = calc_kvpr(ns_w, ns_s)

            # Optimization: Don't move if src doesn't improve enough to matter
            if ns_k >= max_k - 1e-9: continue

            for dst_idx in range(gpu_num):
                if dst_idx == src_idx: continue
                dst = gpu_states[dst_idx]

                if dst['s'] + m_s > GPU_MEM_SIZE: continue

                nd_k = calc_kvpr(dst['w'] + m_w, dst['s'] + m_s)

                if nd_k < max_k - 1e-9:
                    # Apply Move
                    src['models'].pop(i)
                    src['w'] = ns_w
                    src['s'] = ns_s

                    dst['models'].append(m)
                    dst['w'] += m_w
                    dst['s'] += m_s
                    improved = True
                    break
            if improved: break

        if improved: continue

        # Try Swapping
        # Swap model from src with model from dst
        for i, m1 in enumerate(src['models']):
            m1_w = m1.req_rate / m1.slo
            m1_s = m1.model_size

            for dst_idx in range(gpu_num):
                if dst_idx == src_idx: continue
                dst = gpu_states[dst_idx]

                # Heuristic: Don't swap with a GPU that is already near the bottleneck limit
                if calc_kvpr(dst['w'], dst['s']) > max_k * 0.95: continue

                for j, m2 in enumerate(dst['models']):
                    m2_w = m2.req_rate / m2.slo
                    m2_s = m2.model_size

                    # Check capacities
                    new_src_s = src['s'] - m1_s + m2_s
                    new_dst_s = dst['s'] - m2_s + m1_s

                    if new_src_s > GPU_MEM_SIZE or new_dst_s > GPU_MEM_SIZE: continue

                    new_src_w = src['w'] - m1_w + m2_w
                    new_dst_w = dst['w'] - m2_w + m1_w

                    nk_src = calc_kvpr(new_src_w, new_src_s)
                    nk_dst = calc_kvpr(new_dst_w, new_dst_s)

                    if max(nk_src, nk_dst) < max_k - 1e-9:
                        # Apply Swap
                        src['models'][i] = m2
                        src['w'] = new_src_w
                        src['s'] = new_src_s

                        dst['models'][j] = m1
                        dst['w'] = new_dst_w
                        dst['s'] = new_dst_s
                        improved = True
                        break
                if improved: break
            if improved: break

        if not improved: break

    return {i: gpu_states[i]['models'] for i in range(gpu_num)}
=======
    def optimize_placement(placement, iterations=50):
        """Hill Climbing to reduce max KVPR of a valid placement."""
        # Convert to mutable state
        gpu_states = []
        for i in range(gpu_num):
            models_p = placement[i]
            w = sum(m.req_rate / m.slo for m in models_p)
            s = sum(m.model_size for m in models_p)
            gpu_states.append({'w': w, 's': s, 'models': list(models_p)})

        def calc_kvpr(w, s):
            rem = GPU_MEM_SIZE - s
            if rem <= 1e-9: return float('inf') if w > 1e-9 else 0.0
            return w / rem

        for _ in range(iterations):
            # Find bottleneck
            max_k = -1.0
            src_idx = -1
            gpu_ks = [] # Store K for all GPUs to use in heuristics
            for i in range(gpu_num):
                k = calc_kvpr(gpu_states[i]['w'], gpu_states[i]['s'])
                gpu_ks.append(k)
                if k > max_k:
                    max_k = k
                    src_idx = i

            if max_k <= 1e-9: break
            src = gpu_states[src_idx]
            improved = False

            # Move
            for i, m in enumerate(src['models']):
                m_w = m.req_rate / m.slo
                m_s = m.model_size
                ns_w, ns_s = src['w'] - m_w, src['s'] - m_s
                ns_k = calc_kvpr(ns_w, ns_s)

                if ns_k >= max_k - 1e-9: continue

                for dst_idx in range(gpu_num):
                    if dst_idx == src_idx: continue
                    dst = gpu_states[dst_idx]
                    if dst['s'] + m_s > GPU_MEM_SIZE: continue

                    nd_k = calc_kvpr(dst['w'] + m_w, dst['s'] + m_s)
                    if max(ns_k, nd_k) < max_k - 1e-9:
                        # Apply Move
                        src['models'].pop(i)
                        src['w'], src['s'] = ns_w, ns_s
                        dst['models'].append(m)
                        dst['w'] += m_w
                        dst['s'] += m_s
                        improved = True
                        break
                if improved: break

            if improved: continue

            # Swap
            for i, m1 in enumerate(src['models']):
                m1_w = m1.req_rate / m1.slo
                m1_s = m1.model_size

                for dst_idx in range(gpu_num):
                    if dst_idx == src_idx: continue
                    dst = gpu_states[dst_idx]
                    if gpu_ks[dst_idx] > max_k * 0.95: continue

                    for j, m2 in enumerate(dst['models']):
                        m2_w = m2.req_rate / m2.slo
                        m2_s = m2.model_size

                        ns_s = src['s'] - m1_s + m2_s
                        nd_s = dst['s'] - m2_s + m1_s
                        if ns_s > GPU_MEM_SIZE or nd_s > GPU_MEM_SIZE: continue

                        ns_w = src['w'] - m1_w + m2_w
                        nd_w = dst['w'] - m2_w + m1_w

                        ns_k = calc_kvpr(ns_w, ns_s)
                        nd_k = calc_kvpr(nd_w, nd_s)

                        if max(ns_k, nd_k) < max_k - 1e-9:
                            # Apply Swap
                            src['models'][i] = m2
                            src['w'], src['s'] = ns_w, ns_s
                            dst['models'][j] = m1
                            dst['w'], dst['s'] = nd_w, nd_s
                            improved = True
                            break
                    if improved: break
                if improved: break

            if not improved: break

        return {i: gpu_states[i]['models'] for i in range(gpu_num)}

    def solve_packing(k_target, ordered_items):
        """Try to pack items into GPUs such that KVPR <= k_target."""
        placement = {i: [] for i in range(gpu_num)}
        gpu_state = [{'w': 0.0, 's': 0.0} for _ in range(gpu_num)]
        unplaced = []

        for item in ordered_items:
            best_idx = -1
            best_score = -1.0

            for i in range(gpu_num):
                # Capacity Check
                if gpu_state[i]['s'] + item['s'] > GPU_MEM_SIZE: continue

                new_w = gpu_state[i]['w'] + item['w']
                new_s = gpu_state[i]['s'] + item['s']
                rem = GPU_MEM_SIZE - new_s

                # KVPR Constraint Check
                if rem <= 1e-9:
                    if new_w > 1e-9: continue
                elif new_w > k_target * rem + 1e-7:
                    continue

                # Best Fit Heuristic: Maximize usage (linearized)
                score = new_w + k_target * new_s
                if score > best_score:
                    best_score = score
                    best_idx = i

            if best_idx != -1:
                placement[best_idx].append(item['model'])
                gpu_state[best_idx]['w'] += item['w']
                gpu_state[best_idx]['s'] += item['s']
            else:
                unplaced.append(item)
        return placement, unplaced

    def check_placement(k_target, attempts=10):
        # 1. Deterministic Strategies
        strategies = [
            lambda x: x['w'] + k_target * x['s'],
            lambda x: x['s'],
            lambda x: x['w']
        ]

        for key_func in strategies:
            res, unplaced = solve_packing(k_target, sorted(items, key=key_func, reverse=True))
            if not unplaced: return res

        # 2. Randomized / Failure Driven Strategies
        if attempts > 0:
            rng = random.Random(42 + int(k_target))
            base_key = lambda x: x['w'] + k_target * x['s']

            # Start with a noisy sort of the base heuristic
            current_order = sorted(items, key=lambda x: base_key(x) * rng.uniform(0.85, 1.15), reverse=True)

            for _ in range(attempts):
                res, unplaced = solve_packing(k_target, current_order)
                if not unplaced: return res

                # Failure-Driven: Move unplaced items to front
                # Use item IDs to track uniqueness
                unplaced_ids = set(id(x) for x in unplaced)

                # Shuffle the unplaced ones to try different fit orders for them
                rng.shuffle(unplaced)

                placed = [x for x in current_order if id(x) not in unplaced_ids]

                # New order: Unplaced (Priority) + Placed
                current_order = unplaced + placed

        return None

    # Binary Search
    high = 1e9
    best_placement = check_placement(high, attempts=1)

    if best_placement is None:
        raise ValueError("Unable to place models on GPUs (insufficient total memory).")

    # Initial Optimization
    best_placement = optimize_placement(best_placement, iterations=100)
    high = get_placement_kvpr(best_placement)
    low = 0.0

    for _ in range(25):
        mid = (low + high) / 2
        res = check_placement(mid, attempts=30)
        if res is not None:
            # If feasible, optimize it immediately to see if we can tighten 'high' further
            res = optimize_placement(res, iterations=50)
            best_placement = res
            high = min(mid, get_placement_kvpr(res))
        else:
            low = mid

    # Final polish
    return optimize_placement(best_placement, iterations=150)
>>>>>>> REPLACE
</DIFF>