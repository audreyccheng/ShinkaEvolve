# EVOLVE-BLOCK-START
"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""

import random

GPU_MEM_SIZE = 80.0  # GB

def compute_model_placement(gpu_num, models):
    """
    Minimizes max KVPR using Binary Search on the target pressure K.
    The feasibility check transforms the problem into Bin Packing with item size (w + K*s).
    Uses deterministic and stochastic ordering strategies for robust packing.
    """

    # 1. Precompute model data for efficient access
    m_data = []
    total_w = 0.0
    total_s = 0.0
    for i, m in enumerate(models):
        w = m.req_rate / m.slo
        s = m.model_size
        m_data.append({'w': w, 's': s, 'obj': m, 'id': i})
        total_w += w
        total_s += s

    # Theoretical Lower Bound
    rem_global = gpu_num * GPU_MEM_SIZE - total_s
    if rem_global <= 1e-9:
        if total_w > 0: lb = float('inf')
        else: lb = 0.0
    else:
        lb = total_w / rem_global

    # -------------------------------------------------------
    # Helper: Feasibility Check for Pressure K
    # -------------------------------------------------------
    def pack(target_k, strategies):
        """
        Attempts to pack all models such that for every GPU:
        sum(w + target_k * s) <= target_k * C
        Returns placement dict if successful, else None.
        """
        for key_func in strategies:
            # Sort indices based on strategy
            # Using indices avoids copying model objects
            ordered_indices = sorted(range(len(m_data)), key=lambda i: key_func(m_data[i]), reverse=True)

            # Perform Packing (Best Fit Decreasing on Virtual Slack)
            bins = [{'w': 0.0, 's': 0.0, 'idxs': []} for _ in range(gpu_num)]
            possible = True

            for idx in ordered_indices:
                item = m_data[idx]
                w, s = item['w'], item['s']
                
                best_bin = None
                min_virtual_slack = float('inf')

                # We want to find a bin that fits physically AND satisfies pressure constraint
                # And minimizes the leftover "pressure budget" (Slack)
                
                for b_idx in range(gpu_num):
                    b = bins[b_idx]
                    
                    # 1. Physical Fit
                    if b['s'] + s > GPU_MEM_SIZE: continue

                    # 2. Pressure Fit
                    # w_new / rem_new <= K  <=>  w_new <= K * rem_new
                    rem_new = GPU_MEM_SIZE - (b['s'] + s)
                    
                    # Allow slight float tolerance
                    max_allowed_w = target_k * rem_new
                    new_w = b['w'] + w
                    
                    if new_w <= max_allowed_w + 1e-5:
                        # Feasible. Calculate slack (virtual remaining capacity)
                        slack = max_allowed_w - new_w
                        if slack < min_virtual_slack:
                            min_virtual_slack = slack
                            best_bin = b_idx
                
                if best_bin is None:
                    possible = False
                    break
                
                bins[best_bin]['idxs'].append(idx)
                bins[best_bin]['w'] += w
                bins[best_bin]['s'] += s
            
            if possible:
                return {i: bins[i]['idxs'] for i in range(gpu_num)}

        return None

    # -------------------------------------------------------
    # 2. Binary Search
    # -------------------------------------------------------
    best_placement = None
    best_max_kvpr = float('inf')

    # Establish an upper bound
    # Try a reasonably high K first to get a solution quickly
    # If this fails, we fall back to a very high K.
    initial_k = 2000.0
    init_res = pack(initial_k, [lambda x: x['w'] + initial_k * x['s'], lambda x: x['s']])
    
    if init_res:
        # Evaluate
        curr_max = 0.0
        for idxs in init_res.values():
            w = sum(m_data[i]['w'] for i in idxs)
            s = sum(m_data[i]['s'] for i in idxs)
            rem = GPU_MEM_SIZE - s
            val = w/rem if rem > 1e-9 else (float('inf') if w > 0 else 0.0)
            curr_max = max(curr_max, val)
        best_max_kvpr = curr_max
        best_placement = init_res
        high = best_max_kvpr
    else:
        # Fallback
        high = 10000.0

    low = lb

    if high > low + 1e-4:
        for _ in range(16):
            mid = (low + high) / 2.0
            
            # Strategies: Deterministic then Stochastic
            strategies = [
                lambda x: x['w'] + mid * x['s'],
                lambda x: x['s'],
                lambda x: x['w']
            ]
            # Add stochastic trials
            for _ in range(5):
                strategies.append(lambda x: (x['w'] + mid * x['s']) * random.uniform(0.9, 1.1))
            
            sol = pack(mid, strategies)
            
            if sol:
                # Calculate actual max pressure
                curr_max = 0.0
                for idxs in sol.values():
                    w = sum(m_data[i]['w'] for i in idxs)
                    s = sum(m_data[i]['s'] for i in idxs)
                    rem = GPU_MEM_SIZE - s
                    val = w/rem if rem > 1e-9 else float('inf')
                    curr_max = max(curr_max, val)
                
                if curr_max < best_max_kvpr:
                    best_max_kvpr = curr_max
                    best_placement = sol
                
                high = mid
            else:
                low = mid

    if best_placement is None:
         raise ValueError("Unable to place models on GPUs with available memory.")

    # -------------------------------------------------------
    # 3. Local Search Refinement
    # -------------------------------------------------------
    curr_map = best_placement # dict: gpu_id -> list of indices

    # Precalculate stats
    g_stats = []
    for g in range(gpu_num):
        idxs = curr_map[g]
        w = sum(m_data[i]['w'] for i in idxs)
        s = sum(m_data[i]['s'] for i in idxs)
        rem = GPU_MEM_SIZE - s
        p = w / rem if rem > 1e-9 else (float('inf') if w > 0 else 0.0)
        g_stats.append({'w': w, 's': s, 'p': p})

    # Optimization Loop
    for _ in range(60):
        # Identify bottleneck
        max_p = -1.0
        src_gpu = -1
        for g in range(gpu_num):
            if g_stats[g]['p'] > max_p:
                max_p = g_stats[g]['p']
                src_gpu = g
        
        if src_gpu == -1 or max_p < 1e-9: break
        
        improved = False
        src_list = curr_map[src_gpu]
        
        # 3.1 Try Move
        best_move = None # (idx_in_list, m_idx, dst, new_max)
        
        for list_idx, m_idx in enumerate(src_list):
            m = m_data[m_idx]
            
            # Src State if moved
            src_rem_new = GPU_MEM_SIZE - (g_stats[src_gpu]['s'] - m['s'])
            src_w_new = g_stats[src_gpu]['w'] - m['w']
            src_p_new = src_w_new / src_rem_new if src_rem_new > 1e-9 else float('inf')
            
            for dst in range(gpu_num):
                if dst == src_gpu: continue
                if g_stats[dst]['s'] + m['s'] > GPU_MEM_SIZE: continue
                
                dst_rem_new = GPU_MEM_SIZE - (g_stats[dst]['s'] + m['s'])
                dst_w_new = g_stats[dst]['w'] + m['w']
                dst_p_new = dst_w_new / dst_rem_new if dst_rem_new > 1e-9 else float('inf')
                
                # Check if this move helps reduce the global peak (currently max_p)
                # We need max(src_p_new, dst_p_new) < max_p
                local_peak = max(src_p_new, dst_p_new)
                if local_peak < max_p - 1e-5:
                    if best_move is None or local_peak < best_move[3]:
                        best_move = (list_idx, m_idx, dst, local_peak)
            
        if best_move:
            list_idx, m_idx, dst, _ = best_move
            
            # Apply Move
            curr_map[src_gpu].pop(list_idx)
            curr_map[dst].append(m_idx)
            
            # Update Stats
            m = m_data[m_idx]
            
            g_stats[src_gpu]['w'] -= m['w']
            g_stats[src_gpu]['s'] -= m['s']
            s_rem = GPU_MEM_SIZE - g_stats[src_gpu]['s']
            g_stats[src_gpu]['p'] = g_stats[src_gpu]['w'] / s_rem if s_rem > 1e-9 else float('inf')
            
            g_stats[dst]['w'] += m['w']
            g_stats[dst]['s'] += m['s']
            d_rem = GPU_MEM_SIZE - g_stats[dst]['s']
            g_stats[dst]['p'] = g_stats[dst]['w'] / d_rem if d_rem > 1e-9 else float('inf')
            
            improved = True
        
        if improved: continue
        
        # 3.2 Try Swap (only if move failed)
        for s_list_idx, m_src_idx in enumerate(src_list):
            m_src = m_data[m_src_idx]
            
            for dst in range(gpu_num):
                if dst == src_gpu: continue
                dst_list = curr_map[dst]
                
                for d_list_idx, m_dst_idx in enumerate(dst_list):
                    m_dst = m_data[m_dst_idx]
                    
                    # Capacity Check
                    new_src_s = g_stats[src_gpu]['s'] - m_src['s'] + m_dst['s']
                    if new_src_s > GPU_MEM_SIZE: continue
                    
                    new_dst_s = g_stats[dst]['s'] - m_dst['s'] + m_src['s']
                    if new_dst_s > GPU_MEM_SIZE: continue
                    
                    # Pressure Check
                    new_src_rem = GPU_MEM_SIZE - new_src_s
                    new_src_w = g_stats[src_gpu]['w'] - m_src['w'] + m_dst['w']
                    new_src_p = new_src_w / new_src_rem if new_src_rem > 1e-9 else float('inf')
                    
                    new_dst_rem = GPU_MEM_SIZE - new_dst_s
                    new_dst_w = g_stats[dst]['w'] - m_dst['w'] + m_src['w']
                    new_dst_p = new_dst_w / new_dst_rem if new_dst_rem > 1e-9 else float('inf')
                    
                    if max(new_src_p, new_dst_p) < max_p - 1e-5:
                        # Apply Swap
                        curr_map[src_gpu][s_list_idx] = m_dst_idx
                        curr_map[dst][d_list_idx] = m_src_idx
                        
                        # Update Stats
                        g_stats[src_gpu] = {'w': new_src_w, 's': new_src_s, 'p': new_src_p}
                        g_stats[dst] = {'w': new_dst_w, 's': new_dst_s, 'p': new_dst_p}
                        
                        improved = True
                        break
                if improved: break
            if improved: break
            
        if not improved: break

    # Final conversion
    result = {}
    for g, idxs in curr_map.items():
        result[g] = [m_data[i]['obj'] for i in idxs]
    
    return result
# EVOLVE-BLOCK-END