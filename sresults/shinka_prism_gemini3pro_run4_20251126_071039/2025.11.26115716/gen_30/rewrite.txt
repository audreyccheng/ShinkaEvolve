# EVOLVE-BLOCK-START
"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
import math

GPU_MEM_SIZE = 80.0  # GB

def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.
    
    Architecture:
    1. Ensemble of Greedy Heuristics (Fast baseline).
    2. Binary Search on Max KVPR using Beam Search for feasibility check (Global optimization).
    3. Local Search (Refinement via moves and swaps).
    """

    # --- Helper Functions ---
    def get_max_kvpr(placement):
        max_p = 0.0
        for assigned in placement.values():
            w = sum(m.req_rate / m.slo for m in assigned)
            s = sum(m.model_size for m in assigned)
            rem = GPU_MEM_SIZE - s
            if rem <= 1e-9:
                if w > 0: return float('inf')
                else: continue
            max_p = max(max_p, w / rem)
        return max_p

    # Precompute model attributes
    # Store as (index, model) to track assignments
    indexed_models = list(enumerate(models))
    
    best_placement = None
    best_score = float('inf')

    # ---------------------------------------------------------
    # 1. Greedy Heuristics Ensemble
    # ---------------------------------------------------------
    # Quick passes to establish a good upper bound
    greedy_strategies = [
        (lambda m: m.req_rate / m.slo, 'min_result'),
        (lambda m: m.model_size, 'min_result'),
        (lambda m: (m.req_rate / m.slo) / (GPU_MEM_SIZE - m.model_size + 1e-5), 'min_result'),
        (lambda m: m.req_rate / m.slo, 'min_current')
    ]

    for key_fn, strategy in greedy_strategies:
        sorted_m = sorted(models, key=key_fn, reverse=True)
        placement = {i: [] for i in range(gpu_num)}
        gpu_w = [0.0] * gpu_num
        gpu_s = [0.0] * gpu_num
        possible = True

        for model in sorted_m:
            w = model.req_rate / model.slo
            s = model.model_size
            best_idx = None
            best_val = float('inf')

            for i in range(gpu_num):
                if gpu_s[i] + s > GPU_MEM_SIZE: continue
                rem = GPU_MEM_SIZE - gpu_s[i]
                
                if strategy == 'min_result':
                    new_rem = rem - s
                    val = (gpu_w[i] + w) / new_rem if new_rem > 1e-9 else float('inf')
                else:
                    val = gpu_w[i] / rem if rem > 1e-9 else float('inf')

                if val < best_val:
                    best_val = val
                    best_idx = i
            
            if best_idx is None:
                possible = False
                break
            
            placement[best_idx].append(model)
            gpu_w[best_idx] += w
            gpu_s[best_idx] += s

        if possible:
            score = get_max_kvpr(placement)
            if score < best_score:
                best_score = score
                best_placement = placement

    # ---------------------------------------------------------
    # 2. Binary Search with Beam Search Feasibility
    # ---------------------------------------------------------
    # Target: Minimize K such that for all GPUs: w + K*s <= K*M
    
    # Range setup
    total_w = sum(m.req_rate / m.slo for m in models)
    total_s = sum(m.model_size for m in models)
    rem_global = gpu_num * GPU_MEM_SIZE - total_s
    
    low = total_w / rem_global if rem_global > 1e-5 else 0.0
    high = best_score if best_score != float('inf') else 1000.0
    
    # Only run if meaningful range
    if high > low + 1e-4:
        BEAM_WIDTH = 8
        
        for _ in range(16): # Binary search iterations
            mid = (low + high) / 2
            
            # Sort models by effective size: v = w + K*s
            # This is critical for Beam Search efficiency (Largest first)
            bs_models = sorted(indexed_models, key=lambda x: (x[1].req_rate/x[1].slo) + mid * x[1].model_size, reverse=True)
            
            # Beam State: tuple of ( (w, s), ... ) for each GPU
            # Initial state: ((0,0), (0,0), ...)
            initial_state = tuple([(0.0, 0.0)] * gpu_num)
            
            # Beam: List of (state, assignments)
            # assignments is a list of lists of model indices: [[m_idxs...], [m_idxs...], ...]
            beam = [(initial_state, tuple([] for _ in range(gpu_num)))]
            
            possible_mid = False

            for m_idx, model in bs_models:
                w = model.req_rate / model.slo
                s = model.model_size
                eff_cap = mid * GPU_MEM_SIZE
                
                next_candidates = []
                
                # Beam Expansion
                for (state, placement_struct) in beam:
                    tried_ws = set() # Symmetry optimization
                    
                    for i in range(gpu_num):
                        curr_w, curr_s = state[i]
                        
                        # Symmetry check: if multiple GPUs have identical state, only try one
                        if (curr_w, curr_s) in tried_ws:
                            continue
                        tried_ws.add((curr_w, curr_s))
                        
                        # Constraints
                        # 1. Physical Fit
                        if curr_s + s > GPU_MEM_SIZE: continue
                        
                        # 2. Pressure Fit: w_new + K*s_new <= K*M
                        new_w = curr_w + w
                        new_s = curr_s + s
                        
                        if new_w + mid * new_s > eff_cap + 1e-4: continue
                        
                        # Create new state
                        new_state_list = list(state)
                        new_state_list[i] = (new_w, new_s)
                        new_state = tuple(new_state_list)
                        
                        # Update assignments
                        new_pl_list = list(placement_struct)
                        new_pl_list[i] = placement_struct[i] + [model] 
                        new_placement = tuple(new_pl_list)
                        
                        next_candidates.append((new_state, new_placement))

                if not next_candidates:
                    possible_mid = False
                    break
                
                # Scoring / Pruning
                # Score: Sum of squares of effective utilization. 
                # Encourages tight packing (Best Fit intuition).
                scored_candidates = []
                for cand in next_candidates:
                    st = cand[0]
                    sc = 0.0
                    for (gw, gs) in st:
                        e = gw + mid * gs
                        sc += e*e
                    scored_candidates.append((sc, cand))
                
                scored_candidates.sort(key=lambda x: x[0], reverse=True)
                
                # Keep top BEAM_WIDTH
                beam = [x[1] for x in scored_candidates[:BEAM_WIDTH]]
                possible_mid = True

            if possible_mid:
                # Reconstruct best placement from the beam
                final_struct = beam[0][1] 
                temp_placement = {i: final_struct[i] for i in range(gpu_num)}
                
                actual_score = get_max_kvpr(temp_placement)
                if actual_score < best_score:
                    best_score = actual_score
                    best_placement = temp_placement
                
                high = mid
            else:
                low = mid

    if best_placement is None:
        raise ValueError("Unable to place models on GPUs with available memory.")

    # ---------------------------------------------------------
    # 3. Local Search Refinement
    # ---------------------------------------------------------
    # Use Hill Climbing with both Moves and Swaps
    
    def get_profile(plc):
        profile = []
        for i in range(gpu_num):
            assigned = plc[i]
            w = sum(m.req_rate / m.slo for m in assigned)
            s = sum(m.model_size for m in assigned)
            rem = GPU_MEM_SIZE - s
            p = w / rem if rem > 1e-9 else float('inf')
            profile.append({'w': w, 's': s, 'p': p, 'idx': i})
        return profile

    current_placement = best_placement
    
    for _ in range(50):
        profile = get_profile(current_placement)
        profile.sort(key=lambda x: x['p'], reverse=True)
        
        src_node = profile[0]
        max_p = src_node['p']
        if max_p < 1e-9: break
        
        src_gpu = src_node['idx']
        src_models = current_placement[src_gpu]
        improved = False
        
        # 1. Try Moving a model from Src to any Dst
        for m_idx, model in enumerate(src_models):
            w = model.req_rate / model.slo
            s = model.model_size
            
            new_src_rem = GPU_MEM_SIZE - (src_node['s'] - s)
            new_src_p = (src_node['w'] - w) / new_src_rem if new_src_rem > 1e-9 else float('inf')
            
            best_move = None
            
            for target in profile[1:]: # Try all others
                dst_gpu = target['idx']
                if target['s'] + s > GPU_MEM_SIZE: continue
                
                new_dst_rem = GPU_MEM_SIZE - (target['s'] + s)
                if new_dst_rem <= 1e-9: continue
                
                new_dst_p = (target['w'] + w) / new_dst_rem
                
                # Criteria: strictly reduce max pressure
                current_peak = max_p
                new_peak = max(new_src_p, new_dst_p)
                
                if new_peak < current_peak - 1e-6:
                    best_move = dst_gpu
                    break 
            
            if best_move is not None:
                mdl = src_models.pop(m_idx)
                current_placement[best_move].append(mdl)
                improved = True
                break
        
        if improved: continue

        # 2. Try Swapping models between Src and Dst
        for m_idx, m_src in enumerate(src_models):
            w_s = m_src.req_rate / m_src.slo
            s_s = m_src.model_size
            
            for target in profile[1:]:
                dst_gpu = target['idx']
                dst_models = current_placement[dst_gpu]
                
                for d_idx, m_dst in enumerate(dst_models):
                    w_d = m_dst.req_rate / m_dst.slo
                    s_d = m_dst.model_size
                    
                    new_src_s = src_node['s'] - s_s + s_d
                    new_dst_s = target['s'] - s_d + s_s
                    
                    if new_src_s > GPU_MEM_SIZE or new_dst_s > GPU_MEM_SIZE: continue
                    
                    new_src_rem = GPU_MEM_SIZE - new_src_s
                    new_dst_rem = GPU_MEM_SIZE - new_dst_s
                    
                    if new_src_rem <= 1e-9 or new_dst_rem <= 1e-9: continue
                    
                    new_src_p = (src_node['w'] - w_s + w_d) / new_src_rem
                    new_dst_p = (target['w'] - w_d + w_s) / new_dst_rem
                    
                    if max(new_src_p, new_dst_p) < max_p - 1e-6:
                        mdl_s = src_models.pop(m_idx)
                        mdl_d = dst_models.pop(d_idx)
                        src_models.append(mdl_d)
                        dst_models.append(mdl_s)
                        improved = True
                        break
                if improved: break
            if improved: break
            
        if not improved: break

    return current_placement