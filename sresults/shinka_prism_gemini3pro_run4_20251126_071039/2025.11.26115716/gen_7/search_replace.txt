<NAME>
ensemble_greedy_placement
</NAME>

<DESCRIPTION>
I propose replacing the single deterministic greedy strategy with an ensemble of greedy strategies using different sorting criteria.
The previous best-performing program (score 23.48) used an ensemble. The current program (score 22.67) uses a single sort order (Weight, then Size).
By iterating through multiple sort orders (Weight, Size, Isolated Pressure, Density) and picking the placement that minimizes the global maximum KVPR, we can handle a wider variety of model distributions effectively.
The placement logic remains "greedy minimize resulting KVPR", which locally optimizes the peak, but the order of placement significantly affects the outcome. The ensemble explores these orderings.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Args:
        gpu_num: Number of GPUs
        models: List of models to place

    Returns:
        A placement of models to GPUs
    """

    # Greedy KVPR-minimizing placement with lookahead
    # 1) Sort models by req_rate/slo (primary) and size (secondary) in descending order.
    # Prioritizing high-rate models ensures they are distributed early to avoid stacking.
    # Secondary size sort helps place larger models earlier among those with similar rates.
    sorted_models = sorted(models, key=lambda m: (m.req_rate / m.slo, m.model_size), reverse=True)

    # 2) Initialize per-GPU states
    placement = {gpu_id: [] for gpu_id in range(gpu_num)}
    shared_kv = [GPU_MEM_SIZE for _ in range(gpu_num)]  # remaining memory per GPU
    weighted_req_rate = [0.0 for _ in range(gpu_num)]   # sum of r_j / s_j per GPU

    # 3) Assign each model to the GPU that minimizes the KVPR AFTER placement
    for model in sorted_models:
        best_idx = None
        best_new_kvpr = float('inf')

        for gpu_id in range(gpu_num):
            if model.model_size <= shared_kv[gpu_id]:
                # Calculate potential new state
                new_rem = shared_kv[gpu_id] - model.model_size
                new_rate = weighted_req_rate[gpu_id] + (model.req_rate / model.slo)

                # Calculate resulting KVPR (minimize new_rate / new_rem)
                if new_rem > 1e-6:
                    new_kvpr = new_rate / new_rem
                else:
                    new_kvpr = float('inf')

                # Update best if this gives a lower resulting KVPR
                if new_kvpr < best_new_kvpr:
                    best_new_kvpr = new_kvpr
                    best_idx = gpu_id

        # Failure: if no GPU can fit, raise an error instead of overcommitting
        if best_idx is None:
            raise ValueError(
                f"Unable to place model of size {model.model_size} GB on any GPU. "
                f"Remaining per-GPU memory: {shared_kv}"
            )

        placement[best_idx].append(model)
        weighted_req_rate[best_idx] += model.req_rate / model.slo
        shared_kv[best_idx] -= model.model_size

    return placement
=======
def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Uses an ensemble of greedy heuristics. Each heuristic sorts models differently
    and then greedily assigns them to the GPU that minimizes the resulting KVPR.
    """

    # Sorting strategies to try:
    # 1. Weight (req/slo) primary, Size secondary
    # 2. Size primary, Weight secondary
    # 3. Isolated KVPR (Weight / Available_If_Alone)
    # 4. Density (Weight / Size)
    sorting_keys = [
        lambda m: (m.req_rate / m.slo, m.model_size),
        lambda m: (m.model_size, m.req_rate / m.slo),
        lambda m: (m.req_rate / m.slo) / (GPU_MEM_SIZE - m.model_size + 1e-6),
        lambda m: (m.req_rate / m.slo) / m.model_size
    ]

    best_placement = None
    best_max_kvpr = float('inf')

    # Try each sorting strategy
    for key_fn in sorting_keys:
        sorted_models = sorted(models, key=key_fn, reverse=True)

        # Per-pass state
        placement = {gpu_id: [] for gpu_id in range(gpu_num)}
        shared_kv = [GPU_MEM_SIZE for _ in range(gpu_num)]
        weighted_req_rate = [0.0 for _ in range(gpu_num)]
        possible = True

        # Greedy assignment
        for model in sorted_models:
            best_idx = None
            best_new_kvpr = float('inf')

            w = model.req_rate / model.slo
            s = model.model_size

            for gpu_id in range(gpu_num):
                if s <= shared_kv[gpu_id]:
                    # Calculate KVPR if we place model here
                    new_rem = shared_kv[gpu_id] - s
                    new_rate = weighted_req_rate[gpu_id] + w

                    if new_rem > 1e-6:
                        new_kvpr = new_rate / new_rem
                    else:
                        new_kvpr = float('inf')

                    if new_kvpr < best_new_kvpr:
                        best_new_kvpr = new_kvpr
                        best_idx = gpu_id

            if best_idx is None:
                possible = False
                break

            placement[best_idx].append(model)
            weighted_req_rate[best_idx] += w
            shared_kv[best_idx] -= s

        if possible:
            # Calculate global max KVPR for this placement
            current_max = 0.0
            for gpu_id in range(gpu_num):
                rem = shared_kv[gpu_id]
                rate = weighted_req_rate[gpu_id]
                if rem < GPU_MEM_SIZE: # If GPU is used
                    if rem > 1e-6:
                        current_max = max(current_max, rate / rem)
                    else:
                        current_max = float('inf')

            if current_max < best_max_kvpr:
                best_max_kvpr = current_max
                best_placement = placement

    if best_placement is None:
        raise ValueError("Unable to place models on GPUs with available memory.")

    return best_placement
>>>>>>> REPLACE
</DIFF>