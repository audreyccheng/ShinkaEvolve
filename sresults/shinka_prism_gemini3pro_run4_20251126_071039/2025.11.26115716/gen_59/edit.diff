--- a/original.py
+++ b/original.py
@@ -1,321 +1,315 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 import random
 
 GPU_MEM_SIZE = 80  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
 
-    Combines binary search for optimal max-KVPR target with:
-    1. Heuristic Packing with Repair: Tries to pack models using Best-Fit strategies.
-       If an item fails to fit, attempts to 'repair' by swapping with a placed item (from Prior Program 6).
-    2. Hybrid Randomization: Uses both noisy sorting (Current) and shuffling (Inspiration)
-       to escape local optima in packing order.
-    3. Hill Climbing: Post-optimizes valid placements to further reduce KVPR (Current).
+    Key Strategies:
+    1. Binary Search for optimal K.
+    2. Failure-Driven Adaptive Packing:
+       - Uses Best-Fit Decreasing packing.
+       - Prioritizes items that failed to fit in previous attempts by increasing their weights.
+       - Tries multiple deterministic and randomized adaptive passes.
+    3. Steepest Descent Hill Climbing:
+       - Refines placement by applying the SINGLE best move or swap that maximally reduces
+         the bottleneck KVPR in each iteration.
     """
 
     # Preprocessing
     items = []
-    for m in models:
+    for i, m in enumerate(models):
         items.append({
             'model': m,
+            'id': i,
             'w': m.req_rate / m.slo,
             's': m.model_size
         })
 
     def get_kvpr(w, s):
         rem = GPU_MEM_SIZE - s
         if rem <= 1e-9:
             return float('inf') if w > 1e-9 else 0.0
         return w / rem
 
     def get_placement_max_kvpr(placement):
         mx = 0.0
         for p in placement.values():
             w = sum(m.req_rate / m.slo for m in p)
             s = sum(m.model_size for m in p)
             mx = max(mx, get_kvpr(w, s))
         return mx
 
     def local_optimize(placement):
-        """Hill Climbing to reduce max KVPR."""
+        """Steepest Descent Hill Climbing"""
         state = []
         for i in range(gpu_num):
             p = placement[i]
             w = sum(m.req_rate / m.slo for m in p)
             s = sum(m.model_size for m in p)
             state.append({'w': w, 's': s, 'models': list(p), 'k': get_kvpr(w, s)})
 
-        # Limit iterations for speed, but sufficient for convergence
         for _ in range(100):
-            # Sort to find bottleneck
-            state.sort(key=lambda x: x['k'], reverse=True)
-            src = state[0]
-            current_max = src['k']
-
-            if current_max <= 1e-9: break
-
-            improved = False
-
-            # 1. Try Move
+            # Find bottleneck
+            max_k = -1.0
+            src_idx = -1
+            for i, st in enumerate(state):
+                if st['k'] > max_k:
+                    max_k = st['k']
+                    src_idx = i
+
+            if max_k <= 1e-9: break
+
+            # We look for the best move/swap that minimizes the new max KVPR involving the participants
+            best_action = None
+            min_local_max = max_k # Start with current bottleneck level
+
+            src = state[src_idx]
+
+            # 1. Try Moves (Src -> Dst)
             for i, m in enumerate(src['models']):
                 m_w = m.req_rate / m.slo
                 m_s = m.model_size
 
+                # New Src state
                 ns_w = src['w'] - m_w
                 ns_s = src['s'] - m_s
                 ns_k = get_kvpr(ns_w, ns_s)
 
-                # Only move if it helps the bottleneck significantly or at least clears the bar
-                if ns_k >= current_max - 1e-9: continue
-
-                for dst in state[1:]:
+                # Pruning: if source doesn't improve below current best found, skip
+                if ns_k >= min_local_max - 1e-9: continue
+
+                for dst_idx, dst in enumerate(state):
+                    if dst_idx == src_idx: continue
                     if dst['s'] + m_s > GPU_MEM_SIZE: continue
 
                     nd_w = dst['w'] + m_w
                     nd_s = dst['s'] + m_s
                     nd_k = get_kvpr(nd_w, nd_s)
 
-                    if nd_k < current_max - 1e-9:
-                        # Move
-                        src['models'].pop(i)
-                        src['w'], src['s'], src['k'] = ns_w, ns_s, ns_k
-
-                        dst['models'].append(m)
-                        dst['w'], dst['s'], dst['k'] = nd_w, nd_s, nd_k
-                        improved = True
-                        break
-                if improved: break
-
-            if improved: continue
-
-            # 2. Try Swap
+                    # Effective new max for this pair
+                    local_max = max(ns_k, nd_k)
+                    if local_max < min_local_max - 1e-9:
+                        min_local_max = local_max
+                        best_action = ('move', i, dst_idx)
+
+            # 2. Try Swaps (Src <-> Dst)
             for i, m1 in enumerate(src['models']):
                 m1_w = m1.req_rate / m1.slo
                 m1_s = m1.model_size
 
-                for dst in state[1:]:
-                    if dst['k'] > current_max * 0.95: continue # optimization
+                for dst_idx, dst in enumerate(state):
+                    if dst_idx == src_idx: continue
+                    # Heuristic: Don't swap with another high-load GPU
+                    if dst['k'] > max_k * 0.95: continue
 
                     for j, m2 in enumerate(dst['models']):
                         m2_w = m2.req_rate / m2.slo
                         m2_s = m2.model_size
 
                         # New Src
                         ns_s = src['s'] - m1_s + m2_s
                         if ns_s > GPU_MEM_SIZE: continue
                         ns_w = src['w'] - m1_w + m2_w
 
                         # New Dst
                         nd_s = dst['s'] - m2_s + m1_s
                         if nd_s > GPU_MEM_SIZE: continue
                         nd_w = dst['w'] - m2_w + m1_w
 
                         ns_k = get_kvpr(ns_w, ns_s)
                         nd_k = get_kvpr(nd_w, nd_s)
 
-                        if max(ns_k, nd_k) < current_max - 1e-9:
-                            # Swap
-                            src['models'][i] = m2
-                            src['w'], src['s'], src['k'] = ns_w, ns_s, ns_k
-
-                            dst['models'][j] = m1
-                            dst['w'], dst['s'], dst['k'] = nd_w, nd_s, nd_k
-                            improved = True
-                            break
-                    if improved: break
-                if improved: break
-
-            if not improved: break
+                        local_max = max(ns_k, nd_k)
+                        if local_max < min_local_max - 1e-9:
+                            min_local_max = local_max
+                            best_action = ('swap', i, dst_idx, j)
+
+            if best_action:
+                if best_action[0] == 'move':
+                    _, i, dst_idx = best_action
+                    dst = state[dst_idx]
+                    m = src['models'].pop(i)
+                    dst['models'].append(m)
+
+                    src['w'] -= m.req_rate/m.slo
+                    src['s'] -= m.model_size
+                    dst['w'] += m.req_rate/m.slo
+                    dst['s'] += m.model_size
+                else:
+                    _, i, dst_idx, j = best_action
+                    dst = state[dst_idx]
+                    m1 = src['models'][i]
+                    m2 = dst['models'][j]
+
+                    src['models'][i] = m2
+                    dst['models'][j] = m1
+
+                    src['w'] = src['w'] - (m1.req_rate/m1.slo) + (m2.req_rate/m2.slo)
+                    src['s'] = src['s'] - m1.model_size + m2.model_size
+
+                    dst['w'] = dst['w'] - (m2.req_rate/m2.slo) + (m1.req_rate/m1.slo)
+                    dst['s'] = dst['s'] - m2.model_size + m1.model_size
+
+                # Update K for affected GPUs
+                src['k'] = get_kvpr(src['w'], src['s'])
+                state[best_action[2]]['k'] = get_kvpr(state[best_action[2]]['w'], state[best_action[2]]['s'])
+            else:
+                break
 
         return {i: state[i]['models'] for i in range(gpu_num)}
 
     def solve_packing(k_target, ordered_items):
+        """Attempts to pack items. Returns (placement, failed_item)."""
         placement = {i: [] for i in range(gpu_num)}
         state = [{'w': 0.0, 's': 0.0} for _ in range(gpu_num)]
 
         for item in ordered_items:
             best_idx = -1
             best_score = -1.0
 
             # Try to place
             for i in range(gpu_num):
                 st = state[i]
                 if st['s'] + item['s'] > GPU_MEM_SIZE: continue
 
                 rem = GPU_MEM_SIZE - (st['s'] + item['s'])
-                # KVPR Check
+                # KVPR Constraint: new_w <= K * rem
                 if rem <= 1e-9:
                     if (st['w'] + item['w']) > 1e-9: continue
                 elif (st['w'] + item['w']) > k_target * rem + 1e-7:
                     continue
 
-                # Best Fit Score
+                # Best Fit: Maximize linearized fill
                 score = (st['w'] + item['w']) + k_target * (st['s'] + item['s'])
                 if score > best_score:
                     best_score = score
                     best_idx = i
 
             if best_idx != -1:
                 placement[best_idx].append(item['model'])
                 state[best_idx]['w'] += item['w']
                 state[best_idx]['s'] += item['s']
             else:
-                # Repair: Swap with victim
-                repaired = False
-                for i in range(gpu_num):
-                    st = state[i]
-                    for v_idx, victim in enumerate(placement[i]):
-                        # Check if item fits in i replacing victim
-                        new_s_i = st['s'] - victim.model_size + item['s']
-                        if new_s_i > GPU_MEM_SIZE: continue
-                        new_w_i = st['w'] - (victim.req_rate/victim.slo) + item['w']
-
-                        rem_i = GPU_MEM_SIZE - new_s_i
-                        if rem_i <= 1e-9:
-                            if new_w_i > 1e-9: continue
-                        elif new_w_i > k_target * rem_i + 1e-7:
-                            continue
-
-                        # Victim needs new home
-                        v_w = victim.req_rate/victim.slo
-                        v_s = victim.model_size
-
-                        for j in range(gpu_num):
-                            if i == j: continue
-                            st_j = state[j]
-                            if st_j['s'] + v_s > GPU_MEM_SIZE: continue
-
-                            rem_j = GPU_MEM_SIZE - (st_j['s'] + v_s)
-                            new_w_j = st_j['w'] + v_w
-
-                            if rem_j <= 1e-9:
-                                if new_w_j > 1e-9: continue
-                            elif new_w_j > k_target * rem_j + 1e-7:
-                                continue
-
-                            # Apply Swap
-                            placement[i][v_idx] = item['model']
-                            state[i]['w'], state[i]['s'] = new_w_i, new_s_i
-
-                            placement[j].append(victim)
-                            state[j]['w'], state[j]['s'] = new_w_j, st_j['s'] + v_s
-                            repaired = True
-                            break
-                        if repaired: break
-                    if repaired: break
-
-                if not repaired: return None
-        return placement
-
-    def check_placement(k_target):
-        # 1. Deterministic
+                # Failed to place this item
+                return None, item
+        return placement, None
+
+    def check_placement(k_target, max_attempts=50):
+        # 1. Deterministic Strategies
         strategies = [
             lambda x: x['w'] + k_target * x['s'],
+            lambda x: x['w'] / (GPU_MEM_SIZE - x['s'] + 1e-9), # Asymptotic Pressure
             lambda x: x['s'],
-            lambda x: x['w'],
-            lambda x: x['w'] / (x['s'] + 1e-9)
+            lambda x: x['w']
         ]
 
         for key in strategies:
-            res = solve_packing(k_target, sorted(items, key=key, reverse=True))
+            res, _ = solve_packing(k_target, sorted(items, key=key, reverse=True))
             if res: return res
 
-        # 2. Randomized
+        # 2. Adaptive Failure-Driven Strategies
+        # Track weights for items to prioritize hard-to-place ones
+        weights = {x['id']: 1.0 for x in items}
+        base_key_func = lambda x: x['w'] + k_target * x['s']
+
         rng = random.Random(42 + int(k_target))
-        base_key = lambda x: x['w'] + k_target * x['s']
-
-        # Noisy Sort (Current)
-        for _ in range(40):
-            noisy_items = sorted(items, key=lambda x: base_key(x) * rng.uniform(0.85, 1.15), reverse=True)
-            res = solve_packing(k_target, noisy_items)
-            if res: return res
-
-        # Shuffle (Inspiration) - good for diversity
-        indices = list(range(len(items)))
-        for _ in range(20):
-            rng.shuffle(indices)
-            res = solve_packing(k_target, [items[i] for i in indices])
-            if res: return res
+
+        for _ in range(max_attempts):
+            # Key = Base Score * Adaptive Weight * Small Noise
+            # Weight grows if item causes failure
+            def sort_key(x):
+                return base_key_func(x) * weights[x['id']] * rng.uniform(0.95, 1.05)
+
+            sorted_items = sorted(items, key=sort_key, reverse=True)
+            res, failed_item = solve_packing(k_target, sorted_items)
+
+            if res:
+                return res
+            else:
+                # Prioritize the item that caused failure
+                if failed_item:
+                    weights[failed_item['id']] += 0.5
 
         return None
 
     # Binary Search
     high = 1e9
-    best_placement = check_placement(high)
+    best_placement = check_placement(high, max_attempts=5)
 
     if best_placement is None:
-        raise ValueError("Unable to place models on GPUs (insufficient total memory).")
+        raise ValueError("Unable to place models on GPUs.")
 
     best_placement = local_optimize(best_placement)
     high = get_placement_max_kvpr(best_placement)
     low = 0.0
 
     for _ in range(25):
         if high - low < 1e-4: break
         mid = (low + high) / 2
 
         res = check_placement(mid)
         if res:
             res = local_optimize(res)
             mx = get_placement_max_kvpr(res)
             if mx < get_placement_max_kvpr(best_placement):
                 best_placement = res
             high = min(mid, mx)
         else:
             low = mid
 
     return local_optimize(best_placement)
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")