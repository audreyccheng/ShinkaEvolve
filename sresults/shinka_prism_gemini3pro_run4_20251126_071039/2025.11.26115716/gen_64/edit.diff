--- a/original.py
+++ b/original.py
@@ -1,408 +1,404 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 import random
 
 GPU_MEM_SIZE = 80.0  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
-
-    Phases:
-    1. Greedy Ensemble: Quick valid solutions to establish upper bounds.
-    2. Binary Search: Minimizes K using Bin Packing with Effective Size (s + w/K).
-    3. Local Search: Move and Swap operations to refine the bottleneck GPU.
+    
+    Architecture:
+    1. Greedy Heuristics: Establish valid upper bound using multiple deterministic strategies.
+    2. Binary Search on Pressure (K):
+       - Transforms optimization to feasibility problem: Can we pack with KVPR <= K?
+       - Feasibility Check: Adaptive Stochastic Bin Packing.
+         - Uses 'Effective Size' (s + w/K) for Best Fit Decreasing.
+         - Adds noise to sort keys to explore solution space.
+         - Implements 'Failure Memory': Items that fail to fit are prioritized in subsequent retries.
+    3. Local Search: Hill climbing on the best found solution using Move and Swap operators.
     """
 
-    # 0. Precompute model data for faster access
+    # ------------------------------------------------------------------
+    # 0. Data Preparation
+    # ------------------------------------------------------------------
     m_data = []
     total_w = 0.0
     total_s = 0.0
     for i, m in enumerate(models):
         w = m.req_rate / m.slo
         s = m.model_size
-        m_data.append({
-            'id': i,
-            'w': w,
-            's': s,
-            'obj': m
-        })
+        m_data.append({'w': w, 's': s, 'obj': m, 'id': i})
         total_w += w
         total_s += s
 
-    # ---------------------------------------------------------
-    # Helper Functions
-    # ---------------------------------------------------------
-    def evaluate_indices(placement_indices):
-        """Calc max KVPR given dict {gpu_idx: [list_of_m_data_indices]}"""
+    # Helper: Calculate Max KVPR for a solution (dict of indices)
+    def calc_score(placement_indices):
         max_p = 0.0
-        for indices in placement_indices.values():
-            w_sum = sum(m_data[idx]['w'] for idx in indices)
-            s_sum = sum(m_data[idx]['s'] for idx in indices)
+        for idxs in placement_indices.values():
+            w_sum = sum(m_data[i]['w'] for i in idxs)
+            s_sum = sum(m_data[i]['s'] for i in idxs)
             rem = GPU_MEM_SIZE - s_sum
-
+            
             if rem <= 1e-9:
                 if w_sum > 0: return float('inf')
-                else: continue
-
+                else: continue # Empty or 0 weight
+            
             p = w_sum / rem
             if p > max_p: max_p = p
         return max_p
 
-    def indices_to_objects(placement_indices):
-        res = {}
-        for g, indices in placement_indices.items():
-            res[g] = [m_data[idx]['obj'] for idx in indices]
-        return res
-
-    best_placement_indices = None
-    best_max_kvpr = float('inf')
-
-    # ---------------------------------------------------------
-    # 1. Beam Search Ensemble Initialization
-    # ---------------------------------------------------------
-    # Uses beam search with limited width to find better initial placements
-    # by exploring multiple GPU assignments for each model.
-
-    def run_beam_search(sorted_idxs, beam_width=8):
-        # Beam item: (max_kvpr, assignment_list, gpu_states)
-        # assignment_list: list of GPU IDs for models in sorted_idxs order
-        beam = [(0.0, [], [{'w': 0.0, 's': 0.0} for _ in range(gpu_num)])]
-
-        for idx in sorted_idxs:
+    best_sol_indices = None
+    best_val = float('inf')
+
+    # ------------------------------------------------------------------
+    # 1. Phase I: Greedy Heuristic Ensemble
+    # ------------------------------------------------------------------
+    # Quick deterministic passes to find a good initial solution/upper bound.
+    # We use a simple greedy packing that minimizes the resulting peak pressure locally.
+    greedy_strategies = [
+        (lambda x: x['w']/(GPU_MEM_SIZE - x['s'] + 1e-6), 'min_peak'), # Asymptotic Pressure
+        (lambda x: x['s'] + x['w'], 'min_peak'),                       # Sum
+        (lambda x: x['s'], 'min_peak'),                                # Physical Size
+        (lambda x: x['w'], 'min_load')                                 # Weight
+    ]
+    
+    for key_fn, mode in greedy_strategies:
+        sorted_indices = sorted(range(len(m_data)), key=lambda i: key_fn(m_data[i]), reverse=True)
+        bins = [{'w':0.0, 's':0.0, 'idxs':[]} for _ in range(gpu_num)]
+        possible = True
+        
+        for idx in sorted_indices:
             item = m_data[idx]
-            candidates = []
-
-            for curr_max_p, assign, g_states in beam:
-                for g in range(gpu_num):
-                    st = g_states[g]
-                    if st['s'] + item['s'] > GPU_MEM_SIZE: continue
-
-                    new_rem = GPU_MEM_SIZE - (st['s'] + item['s'])
-                    if new_rem <= 1e-9:
-                        # Full GPU
-                        p = float('inf') if (st['w'] + item['w']) > 1e-9 else 0.0
+            best_bin = None
+            best_metric = float('inf')
+            
+            for b_idx in range(gpu_num):
+                if bins[b_idx]['s'] + item['s'] > GPU_MEM_SIZE: continue
+                
+                rem = GPU_MEM_SIZE - bins[b_idx]['s']
+                
+                if mode == 'min_peak':
+                    # Look ahead: minimize resulting peak on this GPU
+                    new_rem = rem - item['s']
+                    if new_rem < 1e-9:
+                        val = float('inf') if (bins[b_idx]['w'] + item['w']) > 0 else 0.0
                     else:
-                        p = (st['w'] + item['w']) / new_rem
-
-                    new_max = max(curr_max_p, p)
-                    candidates.append((new_max, g, assign, g_states))
-
-            if not candidates:
-                return None
-
-            # Keep best candidates based on max KVPR
-            candidates.sort(key=lambda x: x[0])
-
-            new_beam = []
-            # Take top beam_width unique paths
-            for p_val, g, old_assign, old_states in candidates[:beam_width]:
-                new_st = [dict(s) for s in old_states]
-                new_st[g]['w'] += item['w']
-                new_st[g]['s'] += item['s']
-                new_beam.append((p_val, old_assign + [g], new_st))
-
-            beam = new_beam
-
-        if not beam: return None
-
-        # Reconstruct best placement
-        best_res = beam[0]
-        final_placement = {i: [] for i in range(gpu_num)}
-        for i, g_id in enumerate(best_res[1]):
-            final_placement[g_id].append(sorted_idxs[i])
-        return final_placement
-
-    # Heuristics: Just different sorting keys now. Beam search handles the placement logic.
-    sort_keys = [
-        lambda x: x['w'] / (x['s'] + 1e-6),              # Density
-        lambda x: x['w'],                                # Weight
-        lambda x: x['s'],                                # Size
-        lambda x: x['w'] / (GPU_MEM_SIZE - x['s'] + 1e-6) # Isolated Pressure
-    ]
-
-    for key_fn in sort_keys:
-        sorted_indices = sorted(range(len(m_data)), key=lambda i: key_fn(m_data[i]), reverse=True)
-        res_indices = run_beam_search(sorted_indices, beam_width=16)
-
-        if res_indices:
-            score = evaluate_indices(res_indices)
-            if score < best_max_kvpr:
-                best_max_kvpr = score
-                best_placement_indices = res_indices
-
-    # ---------------------------------------------------------
-    # 2. Binary Search with Multi-Heuristic Checking
-    # ---------------------------------------------------------
-    # Theoretical Lower Bound
+                        val = (bins[b_idx]['w'] + item['w']) / new_rem
+                else:
+                    # Load balancing: minimize current weight
+                    val = bins[b_idx]['w']
+                
+                if val < best_metric:
+                    best_metric = val
+                    best_bin = b_idx
+            
+            if best_bin is None:
+                possible = False
+                break
+            
+            bins[best_bin]['idxs'].append(idx)
+            bins[best_bin]['w'] += item['w']
+            bins[best_bin]['s'] += item['s']
+            
+        if possible:
+            sol = {i: bins[i]['idxs'] for i in range(gpu_num)}
+            score = calc_score(sol)
+            if score < best_val:
+                best_val = score
+                best_sol_indices = sol
+
+    # ------------------------------------------------------------------
+    # 2. Phase II: Binary Search with Adaptive Stochastic Packing
+    # ------------------------------------------------------------------
     rem_global = gpu_num * GPU_MEM_SIZE - total_s
-    low = total_w / rem_global if rem_global > 1e-6 else 0.0
-    if best_max_kvpr != float('inf'):
-        high = best_max_kvpr
-    else:
-        high = 5000.0
+    # Theoretical lower bound
+    lb = total_w / rem_global if rem_global > 1e-6 else 0.0
+    if best_val < lb: lb = 0.0 # Sanity check
+
+    low = lb
+    high = best_val if best_val != float('inf') else 5000.0
 
     if high > low + 1e-4:
-        for _ in range(20):
+        
+        def try_pack(target_k):
+            """
+            Attempts to pack items such that KVPR <= target_k.
+            Uses randomized sorting with failure feedback.
+            """
+            # Track failure counts to prioritize difficult items within this K check
+            fail_counts = {i: 0 for i in range(len(m_data))}
+            
+            # Number of stochastic trials
+            num_trials = 25
+            
+            for attempt in range(num_trials):
+                # Strategy rotation
+                strat_idx = attempt % 3
+                
+                # Noise increases with attempts
+                noise_scale = 0.0
+                if attempt > 4:
+                    noise_scale = 0.02 * (attempt - 4)
+
+                def sort_key(idx):
+                    item = m_data[idx]
+                    # Base Key
+                    if strat_idx == 0:
+                        # Effective Size: s + w/K
+                        val = item['s'] + item['w'] / target_k
+                    elif strat_idx == 1:
+                        # Physical Size
+                        val = item['s']
+                    else:
+                        # Asymptotic Pressure: w / (C - s)
+                        # Measures difficulty relative to bin capacity
+                        rem_cap = GPU_MEM_SIZE - item['s']
+                        val = item['w'] / (rem_cap + 1e-6)
+                    
+                    # Multiplicative noise
+                    if noise_scale > 0:
+                        val *= random.uniform(1.0 - noise_scale, 1.0 + noise_scale)
+                    
+                    # Failure boost: if item failed before, add huge value to put it first
+                    if fail_counts[idx] > 0:
+                        val += fail_counts[idx] * 1e6
+                    return val
+                
+                indices = sorted(range(len(m_data)), key=sort_key, reverse=True)
+                
+                # Bin Packing (Best Fit Decreasing on Effective Slack)
+                bins_eff = [0.0] * gpu_num # effective usage
+                bins_s = [0.0] * gpu_num   # physical usage
+                placement = [[] for _ in range(gpu_num)]
+                
+                possible_iter = True
+                first_fail_idx = None
+                
+                for idx in indices:
+                    item = m_data[idx]
+                    eff_size = item['s'] + item['w'] / target_k
+                    
+                    best_b = None
+                    min_rem_eff = float('inf')
+                    
+                    for b in range(gpu_num):
+                        # 1. Physical constraint
+                        if bins_s[b] + item['s'] > GPU_MEM_SIZE: continue
+                        
+                        # 2. Pressure constraint (Effective Size)
+                        if bins_eff[b] + eff_size <= GPU_MEM_SIZE + 1e-6:
+                            # Valid bin. Minimize remaining effective space (Best Fit)
+                            rem_eff = GPU_MEM_SIZE - (bins_eff[b] + eff_size)
+                            if rem_eff < min_rem_eff:
+                                min_rem_eff = rem_eff
+                                best_b = b
+                    
+                    if best_b is None:
+                        possible_iter = False
+                        first_fail_idx = idx
+                        break
+                    
+                    placement[best_b].append(idx)
+                    bins_eff[best_b] += eff_size
+                    bins_s[best_b] += item['s']
+                
+                if possible_iter:
+                    return {i: placement[i] for i in range(gpu_num)}
+                else:
+                    if first_fail_idx is not None:
+                        fail_counts[first_fail_idx] += 1
+            
+            return None
+
+        # Binary Search Loop
+        # 18 steps is sufficient for precision
+        for _ in range(18):
             mid = (low + high) / 2.0
-
-            # Check feasibility for target K = mid using Randomized Failure-Driven Packing
-            found_config = None
-            failed_items = set()
-
-            # Base sorting keys
-            strategies = [
-                (lambda x: x['s'] + x['w'] / mid, 'eff'),
-                (lambda x: x['s'], 'phys'),
-                (lambda x: x['w'], 'weight')
-            ]
-
-            # Try multiple attempts: deterministic first, then stochastic with feedback
-            for attempt in range(15):
-                # Select base strategy
-                if attempt < len(strategies):
-                    base_key = strategies[attempt][0]
-                    noise = 0.0
-                else:
-                    base_key = strategies[0][0] # Default to effective size
-                    noise = 0.02 * (attempt - 2)
-
-                def sort_key_fn(item):
-                    val = base_key(item)
-                    if noise > 0:
-                        val *= random.uniform(1.0 - noise, 1.0 + noise)
-                    # Priority boost for items that caused failure previously
-                    if item['id'] in failed_items:
-                        val += 1e9
-                    return val
-
-                sorted_idx = sorted(range(len(m_data)), key=lambda i: sort_key_fn(m_data[i]), reverse=True)
-
-                temp_p = {i: [] for i in range(gpu_num)}
-                temp_state = [{'w': 0.0, 's': 0.0} for _ in range(gpu_num)]
-                feasible = True
-                first_fail_id = None
-
-                for idx in sorted_idx:
-                    item = m_data[idx]
-
-                    best_gpu = None
-                    min_slack = float('inf')
-
-                    # Best Fit Decreasing on Effective Constraint
-                    for i in range(gpu_num):
-                        # 1. Physical Fit
-                        if temp_state[i]['s'] + item['s'] > GPU_MEM_SIZE: continue
-
-                        # 2. Effective Fit
-                        phys_rem_after = GPU_MEM_SIZE - (temp_state[i]['s'] + item['s'])
-                        if phys_rem_after < 0: phys_rem_after = 0.0
-
-                        lhs = temp_state[i]['w'] + item['w']
-                        rhs = mid * phys_rem_after
-
-                        if lhs <= rhs + 1e-6:
-                            # Slack: how much pressure budget is left
-                            slack = rhs - lhs
-                            if slack < min_slack:
-                                min_slack = slack
-                                best_gpu = i
-
-                    if best_gpu is None:
-                        feasible = False
-                        first_fail_id = item['id']
-                        break
-
-                    temp_p[best_gpu].append(idx)
-                    temp_state[best_gpu]['w'] += item['w']
-                    temp_state[best_gpu]['s'] += item['s']
-
-                if feasible:
-                    found_config = temp_p
-                    break
-                else:
-                    if first_fail_id is not None:
-                        failed_items.add(first_fail_id)
-
-            if found_config:
-                score = evaluate_indices(found_config)
-                if score < best_max_kvpr:
-                    best_max_kvpr = score
-                    best_placement_indices = found_config
+            sol = try_pack(mid)
+            
+            if sol:
+                # Found feasible configuration for K=mid.
+                actual_score = calc_score(sol)
+                if actual_score < best_val:
+                    best_val = actual_score
+                    best_sol_indices = sol
                 high = mid
             else:
                 low = mid
 
-    if best_placement_indices is None:
-         raise ValueError("Unable to place models on GPUs with available memory.")
-
-    # ---------------------------------------------------------
-    # 3. Local Search (Moves & Swaps)
-    # ---------------------------------------------------------
-    # Use best_placement_indices as starting point
-    # Convert to a mutable map: gpu_idx -> list of indices
-    curr_map = best_placement_indices
-
-    # Helper to get stats of a GPU
-    def get_gpu_stats(g_idx, indices_list):
-        w = sum(m_data[i]['w'] for i in indices_list)
-        s = sum(m_data[i]['s'] for i in indices_list)
-        rem = GPU_MEM_SIZE - s
-        p = w / rem if rem > 1e-9 else float('inf')
-        return w, s, p
-
+    if best_sol_indices is None:
+        raise ValueError("Unable to place models on GPUs with available memory.")
+
+    # ------------------------------------------------------------------
+    # 3. Phase III: Local Search Refinement
+    # ------------------------------------------------------------------
+    curr_sol = {g: list(idxs) for g, idxs in best_sol_indices.items()}
+    
     # Precalculate stats
     g_stats = []
     for g in range(gpu_num):
-        w, s, p = get_gpu_stats(g, curr_map[g])
+        w = sum(m_data[i]['w'] for i in curr_sol[g])
+        s = sum(m_data[i]['s'] for i in curr_sol[g])
+        rem = GPU_MEM_SIZE - s
+        p = w / rem if rem > 1e-9 else (float('inf') if w > 0 else 0.0)
         g_stats.append({'w': w, 's': s, 'p': p})
 
-    for _ in range(100):
-        # Find bottleneck
+    # Optimization Loop (Hill Climbing)
+    for _ in range(80):
+        # Identify bottleneck GPU
         max_p = -1.0
-        src_gpu = -1
+        src = -1
         for g in range(gpu_num):
             if g_stats[g]['p'] > max_p:
                 max_p = g_stats[g]['p']
-                src_gpu = g
-
-        if src_gpu == -1 or max_p < 1e-9: break
-
+                src = g
+        
+        if src == -1 or max_p < 1e-9: break
+        
         improved = False
-        src_list = curr_map[src_gpu]
-
-        # 3.1 Try MOVE (Src -> Dst)
-        for i_idx, m_idx in enumerate(src_list):
-            m = m_data[m_idx]
-
-            # Predict Src stats
-            src_rem_s = GPU_MEM_SIZE - (g_stats[src_gpu]['s'] - m['s'])
-            src_new_w = g_stats[src_gpu]['w'] - m['w']
-            src_new_p = src_new_w / src_rem_s if src_rem_s > 1e-9 else float('inf')
-
+        src_items = curr_sol[src]
+        
+        # 3.1 Move Operation
+        for i_idx, m_idx in enumerate(src_items):
+            item = m_data[m_idx]
+            
+            # Simulated removal from src
+            src_s_new = g_stats[src]['s'] - item['s']
+            src_w_new = g_stats[src]['w'] - item['w']
+            src_rem_new = GPU_MEM_SIZE - src_s_new
+            src_p_new = src_w_new / src_rem_new if src_rem_new > 1e-9 else float('inf')
+            
             best_dst = None
-
+            
             for dst in range(gpu_num):
-                if dst == src_gpu: continue
-                if g_stats[dst]['s'] + m['s'] > GPU_MEM_SIZE: continue
-
-                dst_rem_s = GPU_MEM_SIZE - (g_stats[dst]['s'] + m['s'])
-                dst_new_w = g_stats[dst]['w'] + m['w']
-                dst_new_p = dst_new_w / dst_rem_s if dst_rem_s > 1e-9 else float('inf')
-
-                # Condition: Reduce global max
-                if max(src_new_p, dst_new_p) < max_p - 1e-5:
+                if dst == src: continue
+                if g_stats[dst]['s'] + item['s'] > GPU_MEM_SIZE: continue
+                
+                # Simulated add to dst
+                dst_s_new = g_stats[dst]['s'] + item['s']
+                dst_w_new = g_stats[dst]['w'] + item['w']
+                dst_rem_new = GPU_MEM_SIZE - dst_s_new
+                dst_p_new = dst_w_new / dst_rem_new if dst_rem_new > 1e-9 else float('inf')
+                
+                # Check condition: reduce global max pressure
+                if max(src_p_new, dst_p_new) < max_p - 1e-5:
                     best_dst = dst
-                    break
-
+                    break # First valid move
+            
             if best_dst is not None:
                 # Apply Move
-                curr_map[src_gpu].pop(i_idx)
-                curr_map[best_dst].append(m_idx)
-
-                # Update Stats
-                g_stats[src_gpu] = dict(zip(['w','s','p'], get_gpu_stats(src_gpu, curr_map[src_gpu])))
-                g_stats[best_dst] = dict(zip(['w','s','p'], get_gpu_stats(best_dst, curr_map[best_dst])))
+                curr_sol[src].pop(i_idx)
+                curr_sol[best_dst].append(m_idx)
+                
+                g_stats[src] = {'w': src_w_new, 's': src_s_new, 'p': src_p_new}
+                
+                d_w = g_stats[best_dst]['w'] + item['w']
+                d_s = g_stats[best_dst]['s'] + item['s']
+                d_rem = GPU_MEM_SIZE - d_s
+                d_p = d_w / d_rem if d_rem > 1e-9 else float('inf')
+                g_stats[best_dst] = {'w': d_w, 's': d_s, 'p': d_p}
+                
                 improved = True
                 break
-
+        
         if improved: continue
-
-        # 3.2 Try SWAP (Src <-> Dst)
-        # Only if Move failed to improve
-        for s_i_idx, m_src_idx in enumerate(src_list):
-            m_src = m_data[m_src_idx]
-
+        
+        # 3.2 Swap Operation
+        for s_list_i, s_idx in enumerate(src_items):
+            s_item = m_data[s_idx]
+            
             for dst in range(gpu_num):
-                if dst == src_gpu: continue
-                dst_list = curr_map[dst]
-
-                for d_i_idx, m_dst_idx in enumerate(dst_list):
-                    m_dst = m_data[m_dst_idx]
-
-                    # Check Capacities
-                    new_src_s = g_stats[src_gpu]['s'] - m_src['s'] + m_dst['s']
+                if dst == src: continue
+                
+                dst_items = curr_sol[dst]
+                for d_list_i, d_idx in enumerate(dst_items):
+                    d_item = m_data[d_idx]
+                    
+                    # Capacity check for Swap
+                    new_src_s = g_stats[src]['s'] - s_item['s'] + d_item['s']
                     if new_src_s > GPU_MEM_SIZE: continue
-
-                    new_dst_s = g_stats[dst]['s'] - m_dst['s'] + m_src['s']
+                    
+                    new_dst_s = g_stats[dst]['s'] - d_item['s'] + s_item['s']
                     if new_dst_s > GPU_MEM_SIZE: continue
-
-                    # Check Pressures
+                    
+                    # Pressure check
+                    new_src_w = g_stats[src]['w'] - s_item['w'] + d_item['w']
                     new_src_rem = GPU_MEM_SIZE - new_src_s
-                    new_src_w = g_stats[src_gpu]['w'] - m_src['w'] + m_dst['w']
                     new_src_p = new_src_w / new_src_rem if new_src_rem > 1e-9 else float('inf')
-
+                    
+                    new_dst_w = g_stats[dst]['w'] - d_item['w'] + s_item['w']
                     new_dst_rem = GPU_MEM_SIZE - new_dst_s
-                    new_dst_w = g_stats[dst]['w'] - m_dst['w'] + m_src['w']
                     new_dst_p = new_dst_w / new_dst_rem if new_dst_rem > 1e-9 else float('inf')
-
+                    
                     if max(new_src_p, new_dst_p) < max_p - 1e-5:
                         # Apply Swap
-                        curr_map[src_gpu].pop(s_i_idx)
-                        curr_map[src_gpu].append(m_dst_idx)
-
-                        curr_map[dst].pop(d_i_idx)
-                        curr_map[dst].append(m_src_idx)
-
-                        g_stats[src_gpu] = dict(zip(['w','s','p'], get_gpu_stats(src_gpu, curr_map[src_gpu])))
-                        g_stats[dst] = dict(zip(['w','s','p'], get_gpu_stats(dst, curr_map[dst])))
+                        curr_sol[src][s_list_i] = d_idx
+                        curr_sol[dst][d_list_i] = s_idx
+                        
+                        g_stats[src] = {'w': new_src_w, 's': new_src_s, 'p': new_src_p}
+                        g_stats[dst] = {'w': new_dst_w, 's': new_dst_s, 'p': new_dst_p}
+                        
                         improved = True
                         break
                 if improved: break
             if improved: break
-
-        if not improved: break
-
-    return indices_to_objects(curr_map)
+            
+    # Final conversion to object list
+    result = {}
+    for g, idxs in curr_sol.items():
+        result[g] = [m_data[i]['obj'] for i in idxs]
+    
+    return result
 
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")