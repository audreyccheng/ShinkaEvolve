--- a/original.py
+++ b/original.py
@@ -1,291 +1,314 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 GPU_MEM_SIZE = 80  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         A placement of models to GPUs
     """
     import random
 
     # Pre-process models
     items = []
     for m in models:
         items.append({
             'model': m,
             'w': m.req_rate / m.slo,
             's': m.model_size
         })
 
     def get_placement_kvpr(placement):
         """Calculate max KVPR for a placement dict."""
         mx = 0.0
         for p in placement.values():
             w = sum(m.req_rate / m.slo for m in p)
             s = sum(m.model_size for m in p)
             rem = GPU_MEM_SIZE - s
             if rem <= 1e-9:
                 val = float('inf') if w > 1e-9 else 0.0
             else:
                 val = w / rem
             if val > mx: mx = val
         return mx
 
     def optimize_placement(placement, iterations=50):
-        """Hill Climbing to reduce max KVPR of a valid placement."""
+        """Steepest Descent Hill Climbing with Lexicographical Objective."""
         # Convert to mutable state
         gpu_states = []
         for i in range(gpu_num):
             models_p = placement[i]
             w = sum(m.req_rate / m.slo for m in models_p)
             s = sum(m.model_size for m in models_p)
             gpu_states.append({'w': w, 's': s, 'models': list(models_p)})
 
         def calc_kvpr(w, s):
             rem = GPU_MEM_SIZE - s
             if rem <= 1e-9: return float('inf') if w > 1e-9 else 0.0
             return w / rem
 
         for _ in range(iterations):
-            # Find bottleneck
-            max_k = -1.0
+            # Calculate current vector of loads
+            current_kvprs = [calc_kvpr(g['w'], g['s']) for g in gpu_states]
+            # Lexicographical minimization: sort descending and compare vectors
+            best_vec = sorted(current_kvprs, reverse=True)
+            max_k = best_vec[0]
+
+            if max_k <= 1e-9: break
+
+            # Identify bottleneck (source) - pick the one matching max_k
             src_idx = -1
-            gpu_ks = [] # Store K for all GPUs to use in heuristics
-            for i in range(gpu_num):
-                k = calc_kvpr(gpu_states[i]['w'], gpu_states[i]['s'])
-                gpu_ks.append(k)
-                if k > max_k:
-                    max_k = k
+            for i, k in enumerate(current_kvprs):
+                if abs(k - max_k) < 1e-9:
                     src_idx = i
-
-            if max_k <= 1e-9: break
+                    break
+
             src = gpu_states[src_idx]
-            improved = False
-
-            # Move
+            best_op = None # (type, arg1, arg2, arg3)
+
+            # 1. Evaluate Moves from src
             for i, m in enumerate(src['models']):
-                m_w = m.req_rate / m.slo
-                m_s = m.model_size
+                m_w, m_s = m.req_rate / m.slo, m.model_size
                 ns_w, ns_s = src['w'] - m_w, src['s'] - m_s
                 ns_k = calc_kvpr(ns_w, ns_s)
-
-                if ns_k >= max_k - 1e-9: continue
 
                 for dst_idx in range(gpu_num):
                     if dst_idx == src_idx: continue
                     dst = gpu_states[dst_idx]
                     if dst['s'] + m_s > GPU_MEM_SIZE: continue
 
-                    nd_k = calc_kvpr(dst['w'] + m_w, dst['s'] + m_s)
-                    if max(ns_k, nd_k) < max_k - 1e-9:
-                        # Apply Move
-                        src['models'].pop(i)
-                        src['w'], src['s'] = ns_w, ns_s
-                        dst['models'].append(m)
-                        dst['w'] += m_w
-                        dst['s'] += m_s
-                        improved = True
-                        break
-                if improved: break
-
-            if improved: continue
-
-            # Swap
-            for i, m1 in enumerate(src['models']):
-                m1_w = m1.req_rate / m1.slo
-                m1_s = m1.model_size
-
-                for dst_idx in range(gpu_num):
-                    if dst_idx == src_idx: continue
-                    dst = gpu_states[dst_idx]
-                    if gpu_ks[dst_idx] > max_k * 0.95: continue
-
-                    for j, m2 in enumerate(dst['models']):
-                        m2_w = m2.req_rate / m2.slo
-                        m2_s = m2.model_size
-
-                        ns_s = src['s'] - m1_s + m2_s
-                        nd_s = dst['s'] - m2_s + m1_s
-                        if ns_s > GPU_MEM_SIZE or nd_s > GPU_MEM_SIZE: continue
-
-                        ns_w = src['w'] - m1_w + m2_w
-                        nd_w = dst['w'] - m2_w + m1_w
-
-                        ns_k = calc_kvpr(ns_w, ns_s)
-                        nd_k = calc_kvpr(nd_w, nd_s)
-
-                        if max(ns_k, nd_k) < max_k - 1e-9:
-                            # Apply Swap
-                            src['models'][i] = m2
-                            src['w'], src['s'] = ns_w, ns_s
-                            dst['models'][j] = m1
-                            dst['w'], dst['s'] = nd_w, nd_s
-                            improved = True
-                            break
-                    if improved: break
-                if improved: break
-
-            if not improved: break
+                    nd_w, nd_s = dst['w'] + m_w, dst['s'] + m_s
+                    nd_k = calc_kvpr(nd_w, nd_s)
+
+                    # Form candidate vector
+                    cand_vec = list(current_kvprs)
+                    cand_vec[src_idx] = ns_k
+                    cand_vec[dst_idx] = nd_k
+                    cand_vec.sort(reverse=True)
+
+                    if cand_vec < best_vec:
+                        best_vec = cand_vec
+                        best_op = ('move', i, dst_idx, None)
+
+            # 2. Evaluate Swaps with src
+            # Optimization: Only consider swaps if move didn't fix things completely or for better exploration
+            # Checking all swaps is expensive. Limit to swapping with non-bottleneck GPUs.
+            if True:
+                for i, m1 in enumerate(src['models']):
+                    m1_w, m1_s = m1.req_rate / m1.slo, m1.model_size
+
+                    for dst_idx in range(gpu_num):
+                        if dst_idx == src_idx: continue
+                        if current_kvprs[dst_idx] > max_k * 0.95: continue # Don't swap with other heavily loaded GPUs
+
+                        dst = gpu_states[dst_idx]
+                        for j, m2 in enumerate(dst['models']):
+                            m2_w, m2_s = m2.req_rate / m2.slo, m2.model_size
+
+                            ns_s = src['s'] - m1_s + m2_s
+                            nd_s = dst['s'] - m2_s + m1_s
+                            if ns_s > GPU_MEM_SIZE or nd_s > GPU_MEM_SIZE: continue
+
+                            ns_w = src['w'] - m1_w + m2_w
+                            nd_w = dst['w'] - m2_w + m1_w
+
+                            ns_k = calc_kvpr(ns_w, ns_s)
+                            nd_k = calc_kvpr(nd_w, nd_s)
+
+                            cand_vec = list(current_kvprs)
+                            cand_vec[src_idx] = ns_k
+                            cand_vec[dst_idx] = nd_k
+                            cand_vec.sort(reverse=True)
+
+                            if cand_vec < best_vec:
+                                best_vec = cand_vec
+                                best_op = ('swap', i, dst_idx, j)
+
+            if best_op:
+                op_type, i, dst_idx, j = best_op
+                dst = gpu_states[dst_idx]
+                m1 = src['models'][i]
+
+                if op_type == 'move':
+                    src['models'].pop(i)
+                    src['w'] -= m1.req_rate / m1.slo
+                    src['s'] -= m1.model_size
+
+                    dst['models'].append(m1)
+                    dst['w'] += m1.req_rate / m1.slo
+                    dst['s'] += m1.model_size
+
+                elif op_type == 'swap':
+                    m2 = dst['models'][j]
+                    src['models'][i] = m2
+                    src['w'] = src['w'] - (m1.req_rate / m1.slo) + (m2.req_rate / m2.slo)
+                    src['s'] = src['s'] - m1.model_size + m2.model_size
+
+                    dst['models'][j] = m1
+                    dst['w'] = dst['w'] - (m2.req_rate / m2.slo) + (m1.req_rate / m1.slo)
+                    dst['s'] = dst['s'] - m2.model_size + m1.model_size
+            else:
+                break # No improvement found
 
         return {i: gpu_states[i]['models'] for i in range(gpu_num)}
 
     def solve_packing(k_target, ordered_items):
         """Try to pack items into GPUs such that KVPR <= k_target."""
         placement = {i: [] for i in range(gpu_num)}
         gpu_state = [{'w': 0.0, 's': 0.0} for _ in range(gpu_num)]
         unplaced = []
 
         for item in ordered_items:
             best_idx = -1
             best_score = -1.0
 
             for i in range(gpu_num):
                 # Capacity Check
                 if gpu_state[i]['s'] + item['s'] > GPU_MEM_SIZE: continue
 
                 new_w = gpu_state[i]['w'] + item['w']
                 new_s = gpu_state[i]['s'] + item['s']
                 rem = GPU_MEM_SIZE - new_s
 
                 # KVPR Constraint Check
                 if rem <= 1e-9:
                     if new_w > 1e-9: continue
                 elif new_w > k_target * rem + 1e-7:
                     continue
 
                 # Best Fit Heuristic: Maximize usage (linearized)
                 score = new_w + k_target * new_s
                 if score > best_score:
                     best_score = score
                     best_idx = i
 
             if best_idx != -1:
                 placement[best_idx].append(item['model'])
                 gpu_state[best_idx]['w'] += item['w']
                 gpu_state[best_idx]['s'] += item['s']
             else:
                 unplaced.append(item)
         return placement, unplaced
 
     def check_placement(k_target, attempts=10):
         # 1. Deterministic Strategies
         strategies = [
             lambda x: x['w'] + k_target * x['s'],
             lambda x: x['s'],
-            lambda x: x['w']
+            lambda x: x['w'],
+            lambda x: x['w'] / (x['s'] + 1e-9)
         ]
 
         for key_func in strategies:
             res, unplaced = solve_packing(k_target, sorted(items, key=key_func, reverse=True))
             if not unplaced: return res
 
         # 2. Randomized / Failure Driven Strategies
         if attempts > 0:
             rng = random.Random(42 + int(k_target))
             base_key = lambda x: x['w'] + k_target * x['s']
 
             # Start with a noisy sort of the base heuristic
             current_order = sorted(items, key=lambda x: base_key(x) * rng.uniform(0.85, 1.15), reverse=True)
 
             for _ in range(attempts):
                 res, unplaced = solve_packing(k_target, current_order)
                 if not unplaced: return res
 
                 # Failure-Driven: Move unplaced items to front
                 # Use item IDs to track uniqueness
                 unplaced_ids = set(id(x) for x in unplaced)
 
                 # Shuffle the unplaced ones to try different fit orders for them
                 rng.shuffle(unplaced)
 
                 placed = [x for x in current_order if id(x) not in unplaced_ids]
 
                 # New order: Unplaced (Priority) + Placed
                 current_order = unplaced + placed
 
         return None
 
     # Binary Search
     high = 1e9
     best_placement = check_placement(high, attempts=1)
 
     if best_placement is None:
         raise ValueError("Unable to place models on GPUs (insufficient total memory).")
 
     # Initial Optimization
     best_placement = optimize_placement(best_placement, iterations=100)
     high = get_placement_kvpr(best_placement)
     low = 0.0
 
     for _ in range(25):
         mid = (low + high) / 2
         res = check_placement(mid, attempts=30)
         if res is not None:
             # If feasible, optimize it immediately to see if we can tighten 'high' further
             res = optimize_placement(res, iterations=50)
             best_placement = res
             high = min(mid, get_placement_kvpr(res))
         else:
             low = mid
 
     # Final polish
     return optimize_placement(best_placement, iterations=150)
 
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")