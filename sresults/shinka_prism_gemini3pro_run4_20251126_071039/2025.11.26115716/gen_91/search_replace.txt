<NAME>
plateau_surfing_search
</NAME>

<DESCRIPTION>
1.  **Refined Binary Search**: Updates the binary search logic to tighten the upper bound `high` to the `actual_score` (real max KVPR) of the found placement, rather than just `mid`. This allows the search to converge faster to the true minimum. Also removes the "Worst Fit" heuristic from the feasibility check, as strictly tight packing (Best Fit) is preferred for feasibility validation.
2.  **Plateau Surfing Local Search**: Replaces the simple steepest descent with a two-phase local search ("Plateau Surfing").
    *   **Phase 1 (Strict Descent)**: Attempts to move or swap models from the bottleneck GPU(s) to strictly reduce the maximum KVPR.
    *   **Phase 2 (Load Balancing)**: If Phase 1 fails (stuck in local optimum), scans for moves between *any* GPUs that reduce the Sum of Squared KVPR (SSQ) without increasing the Max KVPR. This balances the load across non-bottleneck GPUs, creating slack that may allow the bottleneck to be relieved in subsequent iterations.
    *   This approach helps escape local optima where the bottleneck cannot directly offload to any other GPU due to capacity or pressure constraints, by first rearranging the other GPUs.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # ---------------------------------------------------------
    # 2. Binary Search on Target KVPR (Transformation to Bin Packing)
    # ---------------------------------------------------------
    # Problem: Minimize K such that sum(w_i)/ (C - sum(s_i)) <= K
    # Equivalent to Bin Packing with item size v_i(K) = s_i + w_i/K, bin capacity C.

    total_w = sum(m.req_rate / m.slo for m in models)
    total_s = sum(m.model_size for m in models)
    rem_global = gpu_num * GPU_MEM_SIZE - total_s

    # Theoretical lower bound: Average case fluid distribution
    if rem_global > 1e-6:
        low = total_w / rem_global
        high = best_score if best_score != float('inf') else 1000.0

        if high > low + 1e-4:
            for _ in range(20):
                mid = (low + high) / 2

                # Randomized Best Fit Decreasing Check with Failure Memory
                found_placement = None
                failed_items = set()

                # Strategy pool for sorting models
                strategies = [
                    (lambda m: m.model_size + (m.req_rate / m.slo) / mid, 'Effective'),
                    (lambda m: m.model_size, 'Physical'),
                    (lambda m: m.req_rate / m.slo, 'Weight'),
                    (lambda m: (m.req_rate / m.slo) / (m.model_size + 1e-6), 'Density'),
                    # Asymptotic pressure key: prioritize items that consume almost all memory
                    (lambda m: (m.req_rate / m.slo) / (GPU_MEM_SIZE - m.model_size + 1e-9), 'Pressure'),
                ]

                for attempt in range(40):
                    # Cycle through strategies
                    base_key, _ = strategies[attempt % len(strategies)]

                    # Adaptive Noise
                    noise = 0.0
                    if attempt >= len(strategies):
                        noise = 0.01 + (attempt * 0.002)

                    def sort_key(m):
                        val = base_key(m)
                        if noise > 0:
                            val *= random.uniform(1.0 - noise, 1.0 + noise)
                        # Failure memory boost
                        if id(m) in failed_items:
                            val += 1e9
                        return val

                    bs_models = sorted(models, key=sort_key, reverse=True)

                    # Toggle Packing Strategy: Mostly Best Fit, occasionally Worst Fit
                    # Worst Fit spreads the load, which can sometimes help fit a mix of items.
                    use_worst_fit = (attempt % 6 == 5)

                    temp_placement = {i: [] for i in range(gpu_num)}
                    gpu_w = [0.0] * gpu_num
                    gpu_s = [0.0] * gpu_num
                    possible_k = True
                    first_fail_model = None

                    for model in bs_models:
                        w = model.req_rate / model.slo
                        s = model.model_size
                        eff = s + w/mid

                        best_idx = None
                        best_metric = float('inf') if not use_worst_fit else -1.0

                        for i in range(gpu_num):
                            if gpu_s[i] + s > GPU_MEM_SIZE: continue

                            curr_eff = gpu_s[i] + gpu_w[i]/mid
                            if curr_eff + eff <= GPU_MEM_SIZE + 1e-6:
                                rem_eff = GPU_MEM_SIZE - (curr_eff + eff)

                                if not use_worst_fit:
                                    # Best Fit: Minimize remaining space
                                    if rem_eff < best_metric:
                                        best_metric = rem_eff
                                        best_idx = i
                                else:
                                    # Worst Fit: Maximize remaining space (Load Balancing)
                                    if rem_eff > best_metric:
                                        best_metric = rem_eff
                                        best_idx = i

                        if best_idx is None:
                            possible_k = False
                            first_fail_model = model
                            break

                        temp_placement[best_idx].append(model)
                        gpu_w[best_idx] += w
                        gpu_s[best_idx] += s

                    if possible_k:
                        found_placement = temp_placement
                        break
                    else:
                        if first_fail_model:
                            failed_items.add(id(first_fail_model))

                if found_placement:
                    actual_score = get_max_kvpr(found_placement)
                    if actual_score < best_score:
                        best_score = actual_score
                        best_placement = found_placement
                    high = mid
                else:
                    low = mid

    if best_placement is None:
        raise ValueError("Unable to place models on GPUs with available memory.")

    # ---------------------------------------------------------
    # 3. Local Search Refinement
    # ---------------------------------------------------------
    # Iteratively move models from the bottleneck GPU to others
    for _ in range(50):
        # Find bottleneck GPU
        max_p = -1.0
        src_gpu = -1
        gpu_stats = []

        for i in range(gpu_num):
            assigned = best_placement[i]
            w = sum(m.req_rate / m.slo for m in assigned)
            s = sum(m.model_size for m in assigned)
            rem = GPU_MEM_SIZE - s
            p = w / rem if rem > 1e-9 else float('inf')
            gpu_stats.append({'w': w, 's': s, 'p': p})
            if p > max_p:
                max_p = p
                src_gpu = i

        if src_gpu == -1 or max_p < 1e-9: break

        improved = False
        src_models = best_placement[src_gpu]

        # 1. Try MOVE (Src -> Dst)
        for m_idx, model in enumerate(src_models):
            w = model.req_rate / model.slo
            s = model.model_size

            best_dst = None
            min_dst_p = float('inf')

            for dst in range(gpu_num):
                if dst == src_gpu: continue
                if gpu_stats[dst]['s'] + s > GPU_MEM_SIZE: continue

                dst_rem = GPU_MEM_SIZE - (gpu_stats[dst]['s'] + s)
                if dst_rem <= 1e-9: continue

                dst_w = gpu_stats[dst]['w'] + w
                new_dst_p = dst_w / dst_rem

                if new_dst_p < max_p - 1e-5:
                     # Find best fit destination (lowest resulting pressure)
                     if new_dst_p < min_dst_p:
                         min_dst_p = new_dst_p
                         best_dst = dst

            if best_dst is not None:
                moved_model = src_models.pop(m_idx)
                best_placement[best_dst].append(moved_model)
                improved = True
                break

        if improved: continue

        # 2. Try SWAP (Src <-> Dst)
        for m_src_idx, m_src in enumerate(src_models):
            w_src = m_src.req_rate / m_src.slo
            s_src = m_src.model_size

            for dst in range(gpu_num):
                if dst == src_gpu: continue

                dst_models = best_placement[dst]
                for m_dst_idx, m_dst in enumerate(dst_models):
                    w_dst = m_dst.req_rate / m_dst.slo
                    s_dst = m_dst.model_size

                    # Capacity Check
                    new_src_s = gpu_stats[src_gpu]['s'] - s_src + s_dst
                    if new_src_s > GPU_MEM_SIZE: continue

                    new_dst_s = gpu_stats[dst]['s'] - s_dst + s_src
                    if new_dst_s > GPU_MEM_SIZE: continue

                    # Pressure Check
                    new_src_rem = GPU_MEM_SIZE - new_src_s
                    if new_src_rem <= 1e-9: continue
                    new_src_w = gpu_stats[src_gpu]['w'] - w_src + w_dst
                    new_src_p = new_src_w / new_src_rem

                    new_dst_rem = GPU_MEM_SIZE - new_dst_s
                    if new_dst_rem <= 1e-9: continue
                    new_dst_w = gpu_stats[dst]['w'] - w_dst + w_src
                    new_dst_p = new_dst_w / new_dst_rem

                    if max(new_src_p, new_dst_p) < max_p - 1e-5:
                        # Perform Swap
                        src_models[m_src_idx] = m_dst
                        dst_models[m_dst_idx] = m_src
                        improved = True
                        break
                if improved: break
            if improved: break

        if not improved: break

    return best_placement
=======
    # ---------------------------------------------------------
    # 2. Binary Search on Target KVPR (Transformation to Bin Packing)
    # ---------------------------------------------------------
    # Problem: Minimize K such that sum(w_i)/ (C - sum(s_i)) <= K
    # Equivalent to Bin Packing with item size v_i(K) = s_i + w_i/K, bin capacity C.

    total_w = sum(m.req_rate / m.slo for m in models)
    total_s = sum(m.model_size for m in models)
    rem_global = gpu_num * GPU_MEM_SIZE - total_s

    # Theoretical lower bound: Average case fluid distribution
    if rem_global > 1e-6:
        low = total_w / rem_global
        high = best_score if best_score != float('inf') else 1000.0

        if high > low + 1e-4:
            for _ in range(16):
                mid = (low + high) / 2.0

                found_placement = None
                failed_items = set()

                # Strategy pool for sorting models
                strategies = [
                    (lambda m: m.model_size + (m.req_rate / m.slo) / mid, 'Effective'),
                    (lambda m: m.model_size, 'Physical'),
                    (lambda m: m.req_rate / m.slo, 'Weight'),
                    (lambda m: (m.req_rate / m.slo) / (m.model_size + 1e-6), 'Density'),
                ]

                # Multiple attempts with randomization
                for attempt in range(40):
                    base_key, _ = strategies[attempt % len(strategies)]

                    noise = 0.0
                    if attempt >= len(strategies):
                         noise = 0.01 + (attempt * 0.002)

                    def sort_key(m):
                        val = base_key(m)
                        if noise > 0:
                            val *= random.uniform(1.0 - noise, 1.0 + noise)
                        if id(m) in failed_items:
                            val += 1e9
                        return val

                    bs_models = sorted(models, key=sort_key, reverse=True)

                    temp_placement = {i: [] for i in range(gpu_num)}
                    gpu_w = [0.0] * gpu_num
                    gpu_s = [0.0] * gpu_num
                    possible_k = True
                    first_fail_model = None

                    for model in bs_models:
                        w = model.req_rate / model.slo
                        s = model.model_size
                        eff = s + w/mid

                        best_idx = None
                        min_rem_eff = float('inf')

                        # Best Fit Decreasing on Effective Capacity
                        for i in range(gpu_num):
                            if gpu_s[i] + s > GPU_MEM_SIZE: continue

                            curr_eff = gpu_s[i] + gpu_w[i]/mid
                            if curr_eff + eff <= GPU_MEM_SIZE + 1e-6:
                                rem_eff = GPU_MEM_SIZE - (curr_eff + eff)
                                if rem_eff < min_rem_eff:
                                    min_rem_eff = rem_eff
                                    best_idx = i

                        if best_idx is None:
                            possible_k = False
                            first_fail_model = model
                            break

                        temp_placement[best_idx].append(model)
                        gpu_w[best_idx] += w
                        gpu_s[best_idx] += s

                    if possible_k:
                        found_placement = temp_placement
                        break
                    else:
                        if first_fail_model:
                            failed_items.add(id(first_fail_model))

                if found_placement:
                    actual_score = get_max_kvpr(found_placement)
                    if actual_score < best_score:
                        best_score = actual_score
                        best_placement = found_placement
                    # Optimization: Tighten upper bound to the actual best found so far
                    high = min(mid, actual_score)
                else:
                    low = mid

    if best_placement is None:
        raise ValueError("Unable to place models on GPUs with available memory.")

    # ---------------------------------------------------------
    # 3. Local Search Refinement (Plateau Surfing)
    # ---------------------------------------------------------
    # Two-phase optimization:
    # 1. Strict Descent: Reduce Max KVPR directly.
    # 2. Load Balancing: Reduce Sum-Squared KVPR without increasing Max KVPR.

    # Initialize stats
    gpu_stats = []
    for i in range(gpu_num):
        assigned = best_placement[i]
        w = sum(m.req_rate / m.slo for m in assigned)
        s = sum(m.model_size for m in assigned)
        rem = GPU_MEM_SIZE - s
        p = w / rem if rem > 1e-9 else (float('inf') if w > 0 else 0.0)
        gpu_stats.append({'w': w, 's': s, 'p': p})

    def get_metrics(stats):
        max_p = 0.0
        ssq = 0.0
        for g in stats:
            p = g['p']
            if p > max_p: max_p = p
            ssq += p*p
        return max_p, ssq

    curr_max, curr_ssq = get_metrics(gpu_stats)

    for _ in range(200):
        # Phase 1: Reduce Max Pressure directly
        bottlenecks = [i for i, g in enumerate(gpu_stats) if g['p'] >= curr_max - 1e-6]
        random.shuffle(bottlenecks)

        improved_max = False

        # Try Move from bottleneck
        for src in bottlenecks:
            src_models = best_placement[src]
            for idx, m in enumerate(src_models):
                w = m.req_rate / m.slo
                s = m.model_size

                # New src stats if moved
                new_src_s = gpu_stats[src]['s'] - s
                new_src_w = gpu_stats[src]['w'] - w
                new_src_rem = GPU_MEM_SIZE - new_src_s
                new_src_p = new_src_w / new_src_rem if new_src_rem > 1e-9 else float('inf')

                for dst in range(gpu_num):
                    if dst == src: continue
                    if gpu_stats[dst]['s'] + s > GPU_MEM_SIZE: continue

                    new_dst_s = gpu_stats[dst]['s'] + s
                    new_dst_w = gpu_stats[dst]['w'] + w
                    new_dst_rem = GPU_MEM_SIZE - new_dst_s
                    new_dst_p = new_dst_w / new_dst_rem if new_dst_rem > 1e-9 else float('inf')

                    if max(new_src_p, new_dst_p) < curr_max - 1e-6:
                        # Check full improvement
                        moved_model = src_models.pop(idx)
                        best_placement[dst].append(moved_model)

                        gpu_stats[src] = {'w': new_src_w, 's': new_src_s, 'p': new_src_p}
                        gpu_stats[dst] = {'w': new_dst_w, 's': new_dst_s, 'p': new_dst_p}

                        curr_max, curr_ssq = get_metrics(gpu_stats)
                        improved_max = True
                        break
                if improved_max: break
            if improved_max: break

            # Try Swap with bottleneck
            for idx, m in enumerate(src_models):
                w_src = m.req_rate / m.slo
                s_src = m.model_size

                for dst in range(gpu_num):
                    if dst == src: continue

                    for d_idx, m_dst in enumerate(best_placement[dst]):
                        w_dst = m_dst.req_rate / m_dst.slo
                        s_dst = m_dst.model_size

                        # Check capacity
                        new_src_s = gpu_stats[src]['s'] - s_src + s_dst
                        if new_src_s > GPU_MEM_SIZE: continue
                        new_dst_s = gpu_stats[dst]['s'] - s_dst + s_src
                        if new_dst_s > GPU_MEM_SIZE: continue

                        # Check pressure
                        new_src_rem = GPU_MEM_SIZE - new_src_s
                        new_src_w = gpu_stats[src]['w'] - w_src + w_dst
                        new_src_p = new_src_w/new_src_rem if new_src_rem > 1e-9 else float('inf')

                        new_dst_rem = GPU_MEM_SIZE - new_dst_s
                        new_dst_w = gpu_stats[dst]['w'] - w_dst + w_src
                        new_dst_p = new_dst_w/new_dst_rem if new_dst_rem > 1e-9 else float('inf')

                        if max(new_src_p, new_dst_p) < curr_max - 1e-6:
                            # Apply swap
                            src_models[idx] = m_dst
                            best_placement[dst][d_idx] = m

                            gpu_stats[src] = {'w': new_src_w, 's': new_src_s, 'p': new_src_p}
                            gpu_stats[dst] = {'w': new_dst_w, 's': new_dst_s, 'p': new_dst_p}

                            curr_max, curr_ssq = get_metrics(gpu_stats)
                            improved_max = True
                            break
                    if improved_max: break
                if improved_max: break
            if improved_max: break

        if improved_max: continue

        # Phase 2: Plateau Surfing (Load Balancing)
        # Find a move that reduces SSQ without increasing Max P
        improved_ssq = False
        sorted_gpus = sorted(range(gpu_num), key=lambda x: gpu_stats[x]['p'], reverse=True)

        for src in sorted_gpus:
            src_models = best_placement[src]
            for idx, m in enumerate(src_models):
                w = m.req_rate / m.slo
                s = m.model_size

                src_w = gpu_stats[src]['w']
                src_s = gpu_stats[src]['s']
                src_p = gpu_stats[src]['p']

                new_src_s = src_s - s
                new_src_w = src_w - w
                rem = GPU_MEM_SIZE - new_src_s
                new_src_p = new_src_w/rem if rem > 1e-9 else float('inf')

                for dst in range(gpu_num):
                    if dst == src: continue
                    if gpu_stats[dst]['s'] + s > GPU_MEM_SIZE: continue

                    dst_w = gpu_stats[dst]['w']
                    dst_s = gpu_stats[dst]['s']
                    dst_p = gpu_stats[dst]['p']

                    new_dst_s = dst_s + s
                    new_dst_w = dst_w + w
                    rem_d = GPU_MEM_SIZE - new_dst_s
                    new_dst_p = new_dst_w/rem_d if rem_d > 1e-9 else float('inf')

                    if max(new_src_p, new_dst_p) > curr_max + 1e-9: continue

                    delta_ssq = (new_src_p**2 + new_dst_p**2) - (src_p**2 + dst_p**2)

                    if delta_ssq < -1e-4:
                        moved_model = src_models.pop(idx)
                        best_placement[dst].append(moved_model)

                        gpu_stats[src] = {'w': new_src_w, 's': new_src_s, 'p': new_src_p}
                        gpu_stats[dst] = {'w': new_dst_w, 's': new_dst_s, 'p': new_dst_p}

                        curr_max, curr_ssq = get_metrics(gpu_stats)
                        improved_ssq = True
                        break
                if improved_ssq: break
            if improved_ssq: break

        if not improved_ssq:
            break

    return best_placement
>>>>>>> REPLACE
</DIFF>