<NAME>
fail_memory_and_swap
</NAME>

<DESCRIPTION>
1. Enhance the Binary Search loop with "Failure-Driven Prioritization". When packing fails for a target K, the model that could not be placed is recorded. In subsequent stochastic attempts, this model is prioritized (sorted earlier) to prevent it from being squeezed out by easier items. This helps resolving difficult packing constraints.
2. Upgrade Local Search to include SWAP operations. If moving a model from the bottleneck GPU is not possible (e.g. due to capacity), swapping a high-density model from the bottleneck GPU with a lower-density model from another GPU is attempted. This allows escaping local optima where simple moves are blocked.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
                # Randomized Best Fit Decreasing Check
                # Try multiple attempts with perturbed sorting keys to escape local optima
                found_placement = None

                for attempt in range(15):
                    noise = 0.0
                    if attempt > 0:
                        noise = 0.05 # 5% noise for subsequent attempts

                    def sort_key(m):
                        eff_size = m.model_size + (m.req_rate / m.slo) / mid
                        if noise > 0:
                            return eff_size * random.uniform(1.0 - noise, 1.0 + noise)
                        return eff_size

                    bs_models = sorted(models, key=sort_key, reverse=True)

                    temp_placement = {i: [] for i in range(gpu_num)}
                    gpu_w = [0.0] * gpu_num
                    gpu_s = [0.0] * gpu_num
                    possible_k = True

                    for model in bs_models:
                        w = model.req_rate / model.slo
                        s = model.model_size
                        eff = s + w/mid

                        best_idx = None
                        min_rem_eff = float('inf')

                        # Best Fit on Effective Capacity
                        for i in range(gpu_num):
                            if gpu_s[i] + s > GPU_MEM_SIZE: continue

                            curr_eff = gpu_s[i] + gpu_w[i]/mid
                            if curr_eff + eff <= GPU_MEM_SIZE + 1e-6:
                                rem_eff = GPU_MEM_SIZE - (curr_eff + eff)
                                if rem_eff < min_rem_eff:
                                    min_rem_eff = rem_eff
                                    best_idx = i

                        if best_idx is None:
                            possible_k = False
                            break

                        temp_placement[best_idx].append(model)
                        gpu_w[best_idx] += w
                        gpu_s[best_idx] += s

                    if possible_k:
                        found_placement = temp_placement
                        break
=======
                # Randomized Best Fit Decreasing Check with Failure Memory
                found_placement = None
                failed_items = set()

                for attempt in range(20):
                    noise = 0.0
                    if attempt > 0:
                        noise = 0.02 + (attempt * 0.005)

                    def sort_key(m):
                        # Base: Effective size
                        val = m.model_size + (m.req_rate / m.slo) / mid
                        # Noise perturbation
                        if noise > 0:
                            val *= random.uniform(1.0 - noise, 1.0 + noise)
                        # Memory: Prioritize items that failed previously
                        if id(m) in failed_items:
                            val += 1e6
                        return val

                    bs_models = sorted(models, key=sort_key, reverse=True)

                    temp_placement = {i: [] for i in range(gpu_num)}
                    gpu_w = [0.0] * gpu_num
                    gpu_s = [0.0] * gpu_num
                    possible_k = True
                    first_fail_model = None

                    for model in bs_models:
                        w = model.req_rate / model.slo
                        s = model.model_size
                        eff = s + w/mid

                        best_idx = None
                        min_rem_eff = float('inf')

                        # Best Fit on Effective Capacity
                        for i in range(gpu_num):
                            if gpu_s[i] + s > GPU_MEM_SIZE: continue

                            curr_eff = gpu_s[i] + gpu_w[i]/mid
                            if curr_eff + eff <= GPU_MEM_SIZE + 1e-6:
                                rem_eff = GPU_MEM_SIZE - (curr_eff + eff)
                                if rem_eff < min_rem_eff:
                                    min_rem_eff = rem_eff
                                    best_idx = i

                        if best_idx is None:
                            possible_k = False
                            first_fail_model = model
                            break

                        temp_placement[best_idx].append(model)
                        gpu_w[best_idx] += w
                        gpu_s[best_idx] += s

                    if possible_k:
                        found_placement = temp_placement
                        break
                    else:
                        if first_fail_model:
                            failed_items.add(id(first_fail_model))
>>>>>>> REPLACE
<<<<<<< SEARCH
        # Try to move one model
        for m_idx, model in enumerate(src_models):
            w = model.req_rate / model.slo
            s = model.model_size

            # Predict source pressure if moved
            src_rem = GPU_MEM_SIZE - (gpu_stats[src_gpu]['s'] - s)
            src_w = gpu_stats[src_gpu]['w'] - w
            # new_src_p = src_w / src_rem

            # We need to find a dst where new_dst_p < max_p
            best_dst = None

            for dst in range(gpu_num):
                if dst == src_gpu: continue
                if gpu_stats[dst]['s'] + s > GPU_MEM_SIZE: continue

                dst_rem = GPU_MEM_SIZE - (gpu_stats[dst]['s'] + s)
                if dst_rem < 1e-9: continue

                dst_w = gpu_stats[dst]['w'] + w
                new_dst_p = dst_w / dst_rem

                # Criteria: Reduces the global max.
                # Since src_p reduces, we just need new_dst_p < old_max_p
                if new_dst_p < max_p - 1e-5:
                     best_dst = dst
                     break

            if best_dst is not None:
                moved_model = src_models.pop(m_idx)
                best_placement[best_dst].append(moved_model)
                improved = True
                break

        if not improved: break
=======
        # 1. Try MOVE (Src -> Dst)
        for m_idx, model in enumerate(src_models):
            w = model.req_rate / model.slo
            s = model.model_size

            best_dst = None
            for dst in range(gpu_num):
                if dst == src_gpu: continue
                if gpu_stats[dst]['s'] + s > GPU_MEM_SIZE: continue

                dst_rem = GPU_MEM_SIZE - (gpu_stats[dst]['s'] + s)
                if dst_rem <= 1e-9: continue

                dst_w = gpu_stats[dst]['w'] + w
                new_dst_p = dst_w / dst_rem

                if new_dst_p < max_p - 1e-5:
                     best_dst = dst
                     break

            if best_dst is not None:
                moved_model = src_models.pop(m_idx)
                best_placement[best_dst].append(moved_model)
                improved = True
                break

        if improved: continue

        # 2. Try SWAP (Src <-> Dst)
        for m_src_idx, m_src in enumerate(src_models):
            w_src = m_src.req_rate / m_src.slo
            s_src = m_src.model_size

            for dst in range(gpu_num):
                if dst == src_gpu: continue

                dst_models = best_placement[dst]
                for m_dst_idx, m_dst in enumerate(dst_models):
                    w_dst = m_dst.req_rate / m_dst.slo
                    s_dst = m_dst.model_size

                    # Capacity Check
                    new_src_s = gpu_stats[src_gpu]['s'] - s_src + s_dst
                    if new_src_s > GPU_MEM_SIZE: continue

                    new_dst_s = gpu_stats[dst]['s'] - s_dst + s_src
                    if new_dst_s > GPU_MEM_SIZE: continue

                    # Pressure Check
                    new_src_rem = GPU_MEM_SIZE - new_src_s
                    if new_src_rem <= 1e-9: continue
                    new_src_w = gpu_stats[src_gpu]['w'] - w_src + w_dst
                    new_src_p = new_src_w / new_src_rem

                    new_dst_rem = GPU_MEM_SIZE - new_dst_s
                    if new_dst_rem <= 1e-9: continue
                    new_dst_w = gpu_stats[dst]['w'] - w_dst + w_src
                    new_dst_p = new_dst_w / new_dst_rem

                    if max(new_src_p, new_dst_p) < max_p - 1e-5:
                        # Perform Swap
                        src_models[m_src_idx] = m_dst
                        dst_models[m_dst_idx] = m_src
                        improved = True
                        break
                if improved: break
            if improved: break

        if not improved: break
>>>>>>> REPLACE
</DIFF>