--- a/original.py
+++ b/original.py
@@ -1,337 +1,358 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
+
+import random
 
 GPU_MEM_SIZE = 80  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
-
-    Uses a hybrid approach:
-    1. Generates candidate placements using:
-       a. Greedy heuristics with multiple sorting keys (Weight, Size, Product).
-       b. Binary Search on KVPR target with Best-Fit Decreasing.
-    2. Selects the best candidate.
-    3. Refines the best placement using Local Search (moves and swaps).
+    
+    Algorithm:
+    1. Calculate Theoretical Lower Bound (LB) for K.
+    2. Binary Search for optimal K between LB and High.
+       - Feasibility Check: 'Noisy Best-Fit Decreasing'.
+         - Checks if items fit in M bins with capacity constraints linearized by K.
+         - Uses randomized sorting keys (perturbations) to retry if deterministic fit fails.
+       - Updates Upper Bound aggressively using the actual max KVPR of valid solutions.
+    3. Local Search Refinement.
+       - Greedy Hill-Climbing (Moves and Swaps) to reduce the bottleneck GPU's pressure.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         A placement of models to GPUs
     """
 
-    def get_kvpr(w, s):
+    # 1. Preprocessing
+    items = []
+    total_w = 0.0
+    total_s = 0.0
+    
+    for m in models:
+        w = m.req_rate / m.slo
+        s = m.model_size
+        items.append({'model': m, 'w': w, 's': s})
+        total_w += w
+        total_s += s
+
+    # Theoretical Lower Bound: K >= Sum(w) / (M*C - Sum(s))
+    rem_global = gpu_num * GPU_MEM_SIZE - total_s
+    k_min = 0.0
+    if rem_global > 1e-9:
+        k_min = total_w / rem_global
+    elif total_w > 1e-9:
+        # If no global memory remains but we have load, it's impossible. 
+        # But we let the search handle the failure.
+        k_min = 1e9 
+
+    def get_max_kvpr(placement_list):
+        """Calculate actual max KVPR of a list-based placement."""
+        max_p = 0.0
+        for p in placement_list:
+            w = sum(m.req_rate / m.slo for m in p)
+            s = sum(m.model_size for m in p)
+            rem = GPU_MEM_SIZE - s
+            if rem <= 1e-9:
+                if w > 1e-9: return float('inf')
+                val = 0.0
+            else:
+                val = w / rem
+            if val > max_p:
+                max_p = val
+        return max_p
+
+    def pack(k_target, max_attempts=5):
+        """
+        Try to pack items into gpu_num bins such that for all j:
+        sum(w) / (C - sum(s)) <= k_target
+        Equivalently: sum(w + k_target*s) <= k_target * C
+        """
+        
+        # Helper for a single packing attempt
+        def try_order(ordered_items):
+            # Track current usage
+            # Linear Capacity = K * C
+            # Item Linear Size = w + K * s
+            # We use Best Fit Decreasing on Linear Size to minimize wasted Linear Capacity
+            
+            pl = [[] for _ in range(gpu_num)]
+            g_w = [0.0] * gpu_num
+            g_s = [0.0] * gpu_num
+            
+            for item in ordered_items:
+                w = item['w']
+                s = item['s']
+                # Cost in the linearized domain
+                cost = w + k_target * s
+                
+                best_idx = -1
+                best_residue = float('inf')
+                
+                for i in range(gpu_num):
+                    # Hard memory limit
+                    if g_s[i] + s > GPU_MEM_SIZE:
+                        continue
+                    
+                    # KVPR / Linear Capacity Limit
+                    # Current load: g_w + K * g_s
+                    # New load: (g_w + w) + K * (g_s + s)
+                    current_lin_load = g_w[i] + k_target * g_s[i]
+                    new_lin_load = current_lin_load + cost
+                    limit = k_target * GPU_MEM_SIZE
+                    
+                    if new_lin_load > limit + 1e-5:
+                        continue
+                        
+                    # Best Fit: Minimize remaining space in the linearized bin
+                    # residue = limit - new_lin_load
+                    # We want to minimize residue => maximize new_lin_load
+                    residue = limit - new_lin_load
+                    
+                    if residue < best_residue:
+                        best_residue = residue
+                        best_idx = i
+                
+                if best_idx != -1:
+                    pl[best_idx].append(item['model'])
+                    g_w[best_idx] += w
+                    g_s[best_idx] += s
+                else:
+                    return None
+            return pl
+
+        # 1. Deterministic Strategy
+        # Sort by decreasing linear cost: w + k*s
+        base_items = sorted(items, key=lambda x: x['w'] + k_target * x['s'], reverse=True)
+        res = try_order(base_items)
+        if res: return res
+        
+        # 2. Randomized Restarts (Noisy Sorting)
+        if max_attempts > 0:
+            # Seed based on k_target to ensure determinism for the same call
+            rng = random.Random(hash(k_target))
+            for _ in range(max_attempts):
+                # Perturb the sort keys by +/- 10%
+                # This keeps the general "heavy first" heuristic but changes local ordering
+                noisy_items = sorted(items, key=lambda x: (x['w'] + k_target * x['s']) * rng.uniform(0.9, 1.1), reverse=True)
+                res = try_order(noisy_items)
+                if res: return res
+                
+        return None
+
+    # 2. Binary Search
+    best_pl_list = None
+    
+    # Initial Check at High Bound
+    high = 1e9
+    # Quick check
+    if not pack(high, 0):
+        # Detailed check
+        if not pack(high, 20):
+            raise ValueError("Unable to place models on GPUs (insufficient memory).")
+    
+    # If high check passed, we have a candidate. But pack(high) creates a packing valid for K=1e9.
+    # We want to measure its actual K to tighten 'high'.
+    # Re-run pack to capture the result
+    best_pl_list = pack(high, 20)
+    high = get_max_kvpr(best_pl_list)
+    if high == float('inf'): high = 1e9
+    
+    low = k_min
+    
+    # Search Loop
+    for _ in range(25):
+        if high - low < 1e-4: break
+        mid = (low + high) / 2
+        
+        # Try to pack with K = mid
+        res = pack(mid, max_attempts=5)
+        
+        if res:
+            best_pl_list = res
+            actual_max = get_max_kvpr(res)
+            # If we found a valid packing with actual max KVPR 'actual_max',
+            # we know a solution exists for K = actual_max.
+            # And by definition actual_max <= mid (since it fit in mid).
+            # So we can safely lower high to actual_max.
+            high = min(mid, actual_max)
+        else:
+            low = mid
+
+    # Convert to dict format
+    final_pl = {i: best_pl_list[i] for i in range(gpu_num)}
+
+    # 3. Local Search Refinement
+    # Optimize the placement to further reduce the bottleneck
+    
+    # State tracking
+    states = []
+    for i in range(gpu_num):
+        ms = final_pl[i]
+        w = sum(m.req_rate/m.slo for m in ms)
+        s = sum(m.model_size for m in ms)
         rem = GPU_MEM_SIZE - s
-        if rem <= 1e-9:
-            return float('inf') if w > 1e-9 else 0.0
-        return w / rem
-
-    def evaluate_placement(placement):
-        max_p = 0.0
-        for gpu_ms in placement.values():
-            w_sum = sum(m.req_rate / m.slo for m in gpu_ms)
-            s_sum = sum(m.model_size for m in gpu_ms)
-            if s_sum > GPU_MEM_SIZE: return float('inf')
-            p = get_kvpr(w_sum, s_sum)
-            if p == float('inf'): return float('inf')
-            max_p = max(max_p, p)
-        return max_p
-
-    candidates = []
-
-    # --- Strategy 1: Greedy with various sort keys ---
-    # Try: Weight (req/slo), Size, and Product
-    sort_keys = [
-        (lambda m: m.req_rate / m.slo, "Weight"),
-        (lambda m: m.model_size, "Size"),
-        (lambda m: (m.req_rate / m.slo) * m.model_size, "Product")
-    ]
-
-    for key_func, _ in sort_keys:
-        sorted_models = sorted(models, key=key_func, reverse=True)
-        pl = {i: [] for i in range(gpu_num)}
-        gpu_state = [{'w': 0.0, 's': 0.0} for _ in range(gpu_num)]
-        possible = True
-
-        for m in sorted_models:
-            w = m.req_rate / m.slo
-            s = m.model_size
-
-            best_idx = -1
-            best_metric = float('inf')
-
-            # Place model in the GPU that results in lowest new KVPR for that GPU
-            for i in range(gpu_num):
-                new_s = gpu_state[i]['s'] + s
-                if new_s <= GPU_MEM_SIZE:
-                    new_w = gpu_state[i]['w'] + w
-                    metric = get_kvpr(new_w, new_s)
-                    if metric < best_metric:
-                        best_metric = metric
-                        best_idx = i
-
-            if best_idx != -1:
-                pl[best_idx].append(m)
-                gpu_state[best_idx]['w'] += w
-                gpu_state[best_idx]['s'] += s
-            else:
-                possible = False
-                break
-
-        if possible:
-            candidates.append(pl)
-
-    # --- Strategy 2: Binary Search with Best-Fit Decreasing ---
-    items = [{'m': m, 'w': m.req_rate/m.slo, 's': m.model_size} for m in models]
-
-    def check_bs(k_target):
-        # Sort by linear cost: w + k * s
-        sorted_items = sorted(items, key=lambda x: x['w'] + k_target * x['s'], reverse=True)
-        pl = {i: [] for i in range(gpu_num)}
-        load = [{'w': 0.0, 's': 0.0, 'lin': 0.0} for _ in range(gpu_num)]
-
-        for item in sorted_items:
-            best_idx = -1
-            best_lin = -1.0
-
-            for i in range(gpu_num):
-                new_s = load[i]['s'] + item['s']
-                if new_s > GPU_MEM_SIZE: continue
-
-                new_w = load[i]['w'] + item['w']
-                rem = GPU_MEM_SIZE - new_s
-
-                # KVPR constraint: w <= k * rem
-                if rem <= 1e-9:
-                    if new_w > 1e-9: continue
-                elif new_w > k_target * rem + 1e-7:
-                    continue
-
-                # Best Fit: Maximize current linear load (tightest fit)
-                curr_lin = load[i]['lin']
-                if curr_lin > best_lin:
-                    best_lin = curr_lin
-                    best_idx = i
-
-            if best_idx == -1: return None
-            pl[best_idx].append(item['m'])
-            load[best_idx]['s'] += item['s']
-            load[best_idx]['w'] += item['w']
-            load[best_idx]['lin'] += item['w'] + k_target * item['s']
-
-        return pl
-
-    low, high = 0.0, 1e9
-    # Initialization
-    bs_pl = check_bs(high)
-    if bs_pl:
-        high = evaluate_placement(bs_pl)
-        if high == float('inf'): high = 1e9
-
-        for _ in range(25):
-            mid = (low + high) / 2
-            res = check_bs(mid)
-            if res:
-                bs_pl = res
-                high = mid
-            else:
-                low = mid
-        candidates.append(bs_pl)
-
-    if not candidates:
-        raise ValueError("Unable to place models on GPUs (insufficient memory).")
-
-    # Select best candidate
-    best_pl = min(candidates, key=evaluate_placement)
-
-    # --- Strategy 3: Local Search Refinement ---
-    # Iteratively improve the solution by moving or swapping models from the bottleneck GPU
-
-    # Make a copy to modify
-    current_pl = {i: list(best_pl[i]) for i in best_pl}
-
-    # Track states for efficiency
-    current_state = []
-    for i in range(gpu_num):
-        w = sum(m.req_rate/m.slo for m in current_pl[i])
-        s = sum(m.model_size for m in current_pl[i])
-        current_state.append({'w': w, 's': s, 'kvpr': get_kvpr(w, s)})
-
-    current_max = max(st['kvpr'] for st in current_state)
-
-    improved = True
-    iterations = 0
-    # Limited iterations for speed
-    while improved and iterations < 150:
+        val = w/rem if rem > 1e-9 else (float('inf') if w > 1e-9 else 0.0)
+        states.append({'w': w, 's': s, 'models': ms, 'kvpr': val})
+
+    # Hill Climbing Loop
+    for _ in range(150):
+        # Identify bottleneck
+        # Sort indices by KVPR desc
+        sorted_gpus = sorted(range(gpu_num), key=lambda x: states[x]['kvpr'], reverse=True)
+        src = sorted_gpus[0]
+        curr_max = states[src]['kvpr']
+        
+        if curr_max <= 0: break
+        
         improved = False
-        iterations += 1
-
-        # Identify bottleneck GPU
-        src_idx = max(range(gpu_num), key=lambda i: current_state[i]['kvpr'])
-        src_kvpr = current_state[src_idx]['kvpr']
-
-        # Try Moving a model from Source to Destination
-        best_move = None
-        best_move_gain = 0
-
-        for m_idx, m in enumerate(current_pl[src_idx]):
+        
+        # Strategy A: Move a model from Source to another GPU
+        # We want to find a move that results in max(new_src_kvpr, new_dst_kvpr) < curr_max
+        
+        for m_idx, m in enumerate(states[src]['models']):
             m_w = m.req_rate / m.slo
             m_s = m.model_size
-
-            # Simulated removal from src
-            new_src_s = current_state[src_idx]['s'] - m_s
-            new_src_w = current_state[src_idx]['w'] - m_w
-            new_src_kvpr = get_kvpr(new_src_w, new_src_s)
-
-            for dst_idx in range(gpu_num):
-                if src_idx == dst_idx: continue
-
-                # Simulated addition to dst
-                new_dst_s = current_state[dst_idx]['s'] + m_s
-                if new_dst_s > GPU_MEM_SIZE: continue
-
-                new_dst_w = current_state[dst_idx]['w'] + m_w
-                new_dst_kvpr = get_kvpr(new_dst_w, new_dst_s)
-
-                local_max = max(new_src_kvpr, new_dst_kvpr)
-
-                if local_max < src_kvpr:
-                     if (src_kvpr - local_max) > best_move_gain:
-                         best_move_gain = src_kvpr - local_max
-                         best_move = (m_idx, dst_idx)
-
-        if best_move:
-            m_idx, dst_idx = best_move
-            m = current_pl[src_idx].pop(m_idx)
-            current_pl[dst_idx].append(m)
-
-            m_w = m.req_rate / m.slo
-            m_s = m.model_size
-
-            current_state[src_idx]['w'] -= m_w
-            current_state[src_idx]['s'] -= m_s
-            current_state[src_idx]['kvpr'] = get_kvpr(current_state[src_idx]['w'], current_state[src_idx]['s'])
-
-            current_state[dst_idx]['w'] += m_w
-            current_state[dst_idx]['s'] += m_s
-            current_state[dst_idx]['kvpr'] = get_kvpr(current_state[dst_idx]['w'], current_state[dst_idx]['s'])
-
-            current_max = max(st['kvpr'] for st in current_state)
-            improved = True
-            continue
-
-        # Try Swapping
-        best_swap = None
-        best_swap_gain = 0
-
-        for m1_idx, m1 in enumerate(current_pl[src_idx]):
+            
+            # Predict Src after removal
+            ns_s = states[src]['s'] - m_s
+            ns_w = states[src]['w'] - m_w
+            ns_rem = GPU_MEM_SIZE - ns_s
+            ns_kvpr = ns_w/ns_rem if ns_rem > 1e-9 else 0.0
+            
+            if ns_kvpr >= curr_max: continue # Source not improved enough
+            
+            for dst in sorted_gpus[1:]:
+                # Predict Dst after addition
+                nd_s = states[dst]['s'] + m_s
+                if nd_s > GPU_MEM_SIZE: continue
+                
+                nd_w = states[dst]['w'] + m_w
+                nd_rem = GPU_MEM_SIZE - nd_s
+                nd_kvpr = nd_w/nd_rem if nd_rem > 1e-9 else (float('inf') if nd_w > 1e-9 else 0.0)
+                
+                # Check if this move is globally better (locally for these 2 nodes)
+                # Since 'dst' was <= curr_max, we just need to ensure new 'dst' < curr_max
+                if nd_kvpr < curr_max:
+                    # Apply Move
+                    model = states[src]['models'].pop(m_idx)
+                    states[dst]['models'].append(model)
+                    
+                    states[src]['s'] = ns_s
+                    states[src]['w'] = ns_w
+                    states[src]['kvpr'] = ns_kvpr
+                    
+                    states[dst]['s'] = nd_s
+                    states[dst]['w'] = nd_w
+                    states[dst]['kvpr'] = nd_kvpr
+                    
+                    improved = True
+                    break
+            if improved: break
+            
+        if improved: continue
+        
+        # Strategy B: Swap a model from Source with a model from another GPU
+        for m1_idx, m1 in enumerate(states[src]['models']):
             m1_w = m1.req_rate / m1.slo
             m1_s = m1.model_size
-
-            for dst_idx in range(gpu_num):
-                if src_idx == dst_idx: continue
-
-                for m2_idx, m2 in enumerate(current_pl[dst_idx]):
+            
+            for dst in sorted_gpus[1:]:
+                # Optimization: Don't swap with highly loaded GPUs
+                if states[dst]['kvpr'] > curr_max * 0.95: continue
+                
+                for m2_idx, m2 in enumerate(states[dst]['models']):
                     m2_w = m2.req_rate / m2.slo
                     m2_s = m2.model_size
-
-                    # New Src: -m1 + m2
-                    new_src_s = current_state[src_idx]['s'] - m1_s + m2_s
-                    if new_src_s > GPU_MEM_SIZE: continue
-                    new_src_w = current_state[src_idx]['w'] - m1_w + m2_w
-                    new_src_kvpr = get_kvpr(new_src_w, new_src_s)
-
-                    # New Dst: -m2 + m1
-                    new_dst_s = current_state[dst_idx]['s'] - m2_s + m1_s
-                    if new_dst_s > GPU_MEM_SIZE: continue
-                    new_dst_w = current_state[dst_idx]['w'] - m2_w + m1_w
-                    new_dst_kvpr = get_kvpr(new_dst_w, new_dst_s)
-
-                    local_max = max(new_src_kvpr, new_dst_kvpr)
-
-                    if local_max < src_kvpr:
-                        if (src_kvpr - local_max) > best_swap_gain:
-                            best_swap_gain = src_kvpr - local_max
-                            best_swap = (m1_idx, dst_idx, m2_idx)
-
-        if best_swap:
-            m1_idx, dst_idx, m2_idx = best_swap
-
-            m1 = current_pl[src_idx][m1_idx]
-            m2 = current_pl[dst_idx][m2_idx]
-
-            # Execute swap
-            current_pl[src_idx][m1_idx] = m2
-            current_pl[dst_idx][m2_idx] = m1
-
-            # Update states
-            m1_w = m1.req_rate / m1.slo
-            m1_s = m1.model_size
-            m2_w = m2.req_rate / m2.slo
-            m2_s = m2.model_size
-
-            current_state[src_idx]['w'] += (-m1_w + m2_w)
-            current_state[src_idx]['s'] += (-m1_s + m2_s)
-            current_state[src_idx]['kvpr'] = get_kvpr(current_state[src_idx]['w'], current_state[src_idx]['s'])
-
-            current_state[dst_idx]['w'] += (-m2_w + m1_w)
-            current_state[dst_idx]['s'] += (-m2_s + m1_s)
-            current_state[dst_idx]['kvpr'] = get_kvpr(current_state[dst_idx]['w'], current_state[dst_idx]['s'])
-
-            current_max = max(st['kvpr'] for st in current_state)
-            improved = True
-
-    return current_pl
-
+                    
+                    # New Src
+                    ns_s = states[src]['s'] - m1_s + m2_s
+                    if ns_s > GPU_MEM_SIZE: continue
+                    ns_w = states[src]['w'] - m1_w + m2_w
+                    ns_rem = GPU_MEM_SIZE - ns_s
+                    ns_kvpr = ns_w/ns_rem if ns_rem > 1e-9 else (float('inf') if ns_w > 1e-9 else 0.0)
+                    
+                    if ns_kvpr >= curr_max: continue
+
+                    # New Dst
+                    nd_s = states[dst]['s'] - m2_s + m1_s
+                    if nd_s > GPU_MEM_SIZE: continue
+                    nd_w = states[dst]['w'] - m2_w + m1_w
+                    nd_rem = GPU_MEM_SIZE - nd_s
+                    nd_kvpr = nd_w/nd_rem if nd_rem > 1e-9 else (float('inf') if nd_w > 1e-9 else 0.0)
+                    
+                    if nd_kvpr < curr_max:
+                        # Apply Swap
+                        model1 = states[src]['models'][m1_idx]
+                        model2 = states[dst]['models'][m2_idx]
+                        
+                        states[src]['models'][m1_idx] = model2
+                        states[dst]['models'][m2_idx] = model1
+                        
+                        states[src]['s'] = ns_s
+                        states[src]['w'] = ns_w
+                        states[src]['kvpr'] = ns_kvpr
+                        
+                        states[dst]['s'] = nd_s
+                        states[dst]['w'] = nd_w
+                        states[dst]['kvpr'] = nd_kvpr
+                        
+                        improved = True
+                        break
+                if improved: break
+            if improved: break
+
+    return {i: states[i]['models'] for i in range(gpu_num)}
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")
