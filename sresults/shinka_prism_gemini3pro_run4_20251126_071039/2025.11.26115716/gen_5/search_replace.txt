<NAME>
improved_greedy_placement
</NAME>

<DESCRIPTION>
I propose replacing the placement logic with a greedy strategy that minimizes the *resultant* KVPR (Lookahead) rather than the *current* KVPR ratio. Minimizing the resultant KVPR directly targets the objective function (minimizing maximum KVPR).

Additionally, I refine the sorting strategy. The current program sorts by `req_rate / slo`, which performed well. I retain this as the primary key but add `model_size` as a secondary key. This ensures that among models with similar load intensity, larger models (which are harder to fit) are placed first. This combination combines the successful sorting of the current program with the more accurate placement logic of the Lookahead approach.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Greedy KVPR-minimizing placement based on Algorithm 1 (without Ï„ check)
    # 1) Sort models by r_j / s_j in descending order
    sorted_models = sorted(models, key=lambda m: (m.req_rate / m.slo), reverse=True)

    # 2) Initialize per-GPU states
    placement = {gpu_id: [] for gpu_id in range(gpu_num)}
    shared_kv = [GPU_MEM_SIZE for _ in range(gpu_num)]  # remaining memory per GPU
    weighted_req_rate = [0.0 for _ in range(gpu_num)]   # sum of r_j / s_j per GPU

    # 3) Assign each model to the GPU that minimizes current KVPR while fitting in memory
    for model in sorted_models:
        best_idx = None
        best_ratio = float('inf')

        for gpu_id in range(gpu_num):
            if model.model_size <= shared_kv[gpu_id] and shared_kv[gpu_id] > 0:
                current_ratio = weighted_req_rate[gpu_id] / shared_kv[gpu_id]
                if current_ratio < best_ratio:
                    best_ratio = current_ratio
                    best_idx = gpu_id

        # Failure: if no GPU can fit, raise an error instead of overcommitting
        if best_idx is None:
            raise ValueError(
                f"Unable to place model of size {model.model_size} GB on any GPU. "
                f"Remaining per-GPU memory: {shared_kv}"
            )

        placement[best_idx].append(model)
        weighted_req_rate[best_idx] += model.req_rate / model.slo
        shared_kv[best_idx] -= model.model_size

    return placement
=======
    # Greedy KVPR-minimizing placement with lookahead
    # 1) Sort models by req_rate/slo (primary) and size (secondary) in descending order.
    # Prioritizing high-rate models ensures they are distributed early to avoid stacking.
    # Secondary size sort helps place larger models earlier among those with similar rates.
    sorted_models = sorted(models, key=lambda m: (m.req_rate / m.slo, m.model_size), reverse=True)

    # 2) Initialize per-GPU states
    placement = {gpu_id: [] for gpu_id in range(gpu_num)}
    shared_kv = [GPU_MEM_SIZE for _ in range(gpu_num)]  # remaining memory per GPU
    weighted_req_rate = [0.0 for _ in range(gpu_num)]   # sum of r_j / s_j per GPU

    # 3) Assign each model to the GPU that minimizes the KVPR AFTER placement
    for model in sorted_models:
        best_idx = None
        best_new_kvpr = float('inf')

        for gpu_id in range(gpu_num):
            if model.model_size <= shared_kv[gpu_id]:
                # Calculate potential new state
                new_rem = shared_kv[gpu_id] - model.model_size
                new_rate = weighted_req_rate[gpu_id] + (model.req_rate / model.slo)

                # Calculate resulting KVPR (minimize new_rate / new_rem)
                if new_rem > 1e-6:
                    new_kvpr = new_rate / new_rem
                else:
                    new_kvpr = float('inf')

                # Update best if this gives a lower resulting KVPR
                if new_kvpr < best_new_kvpr:
                    best_new_kvpr = new_kvpr
                    best_idx = gpu_id

        # Failure: if no GPU can fit, raise an error instead of overcommitting
        if best_idx is None:
            raise ValueError(
                f"Unable to place model of size {model.model_size} GB on any GPU. "
                f"Remaining per-GPU memory: {shared_kv}"
            )

        placement[best_idx].append(model)
        weighted_req_rate[best_idx] += model.req_rate / model.slo
        shared_kv[best_idx] -= model.model_size

    return placement
>>>>>>> REPLACE
</DIFF>