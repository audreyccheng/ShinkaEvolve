--- a/original.py
+++ b/original.py
@@ -1,347 +1,358 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 GPU_MEM_SIZE = 80  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
-
+    
     Algorithm:
-    1. Calculate Lower Bound (LB) for K.
-    2. Binary Search for optimal K.
-       - Feasibility Check: Beam Search packing.
-         - Transforms capacity constraints to linearized form: w + K*s <= K*C.
-         - Sorts items by linearized cost.
-         - Explores packing states using Beam Search with adaptive width.
-         - Heuristic: Best Fit (Sum of Squares of linearized loads).
-    3. Steepest Descent Local Search.
-       - Iteratively applies the single best move or swap that reduces the 
-         maximum KVPR across all GPUs until convergence.
-
-    Args:
-        gpu_num: Number of GPUs
-        models: List of models to place
-
-    Returns:
-        A placement of models to GPUs
+    1. Binary Search for optimal Max-KVPR (K).
+    2. Feasibility Check (Hybrid):
+       - First, attempts fast Greedy Best-Fit packing using multiple sort keys 
+         (Linearized Cost, Size, Weight).
+       - If Greedy fails, falls back to a Beam Search (width=5) on Linearized Cost.
+         Beam search tracks load states and maximizes packing density.
+    3. Local Search (Steepest Descent):
+       - Refines the placement by iteratively finding the single best Move or Swap
+         that reduces the maximum KVPR of the system.
     """
 
-    # 1. Preprocessing and Lower Bound
+    # Preprocessing
     items = []
-    total_w = 0.0
-    total_s = 0.0
-    
     for m in models:
-        w = m.req_rate / m.slo
-        s = m.model_size
-        items.append({'model': m, 'w': w, 's': s})
-        total_w += w
-        total_s += s
-    
-    # Global capacity lower bound
-    rem_global = gpu_num * GPU_MEM_SIZE - total_s
-    lb = 0.0
-    if rem_global > 1e-9:
-        lb = total_w / rem_global
-    elif total_w > 1e-9:
-        lb = 1e9 # Impossible
-    
-    # Single item constraint lower bound
-    for x in items:
-        rem = GPU_MEM_SIZE - x['s']
-        if rem > 1e-9:
-            val = x['w'] / rem
-            if val > lb: lb = val
+        items.append({
+            'model': m,
+            'w': m.req_rate / m.slo,
+            's': m.model_size
+        })
+
+    def calc_kvpr(w, s):
+        """Calculate KVPR for a bin with weight w and size s."""
+        rem = GPU_MEM_SIZE - s
+        if rem <= 1e-9:
+            return float('inf') if w > 1e-9 else 0.0
+        return w / rem
 
     def get_max_kvpr(placement_list):
+        """Get max KVPR from a list-of-lists placement."""
         mx = 0.0
         for p in placement_list:
-            w = sum(x['w'] for x in p)
-            s = sum(x['s'] for x in p)
-            rem = GPU_MEM_SIZE - s
-            if rem <= 1e-9:
-                if w > 1e-9: return float('inf')
-                val = 0.0
-            else:
-                val = w / rem
-            mx = max(mx, val)
+            w = sum(m.req_rate / m.slo for m in p)
+            s = sum(m.model_size for m in p)
+            mx = max(mx, calc_kvpr(w, s))
         return mx
 
-    def solve_check(k_target, beam_width=4):
+    def try_pack_greedy(k_target, ordered_items):
         """
-        Check feasibility for K using Beam Search.
-        Constraint: Sum(w + K*s) <= K*C for each bin.
+        Attempt to pack items using Best Fit Decreasing logic with KVPR constraint.
+        Constraint: w + k*s <= k*C
         """
         limit = k_target * GPU_MEM_SIZE
-        
-        # Prepare and sort items by linearized cost
+        # State: w, s, items list per bin
+        bins = [{'w': 0.0, 's': 0.0, 'items': []} for _ in range(gpu_num)]
+        
+        for item in ordered_items:
+            w, s = item['w'], item['s']
+            cost = w + k_target * s
+            
+            # Immediate fail if single item violates constraint
+            if cost > limit + 1e-5: return None
+            
+            best_idx = -1
+            best_fill = -1.0
+            
+            for i in range(gpu_num):
+                b = bins[i]
+                if b['s'] + s > GPU_MEM_SIZE: continue
+                
+                # Check Linearized Constraint
+                curr_lin = b['w'] + k_target * b['s']
+                if curr_lin + cost > limit + 1e-7: continue
+                
+                # Best Fit: Pick the bin that is fullest (closest to limit)
+                # but still fits the item. This minimizes fragmentation.
+                if curr_lin > best_fill:
+                    best_fill = curr_lin
+                    best_idx = i
+            
+            if best_idx != -1:
+                bins[best_idx]['w'] += w
+                bins[best_idx]['s'] += s
+                bins[best_idx]['items'].append(item['model'])
+            else:
+                return None
+        return [b['items'] for b in bins]
+
+    def try_pack_beam(k_target, width=5):
+        """
+        Attempt to pack items using Beam Search.
+        Sorts items by Linearized Cost (w + k*s) descending.
+        """
+        limit = k_target * GPU_MEM_SIZE
+        
+        # Prepare items
         weighted = []
         for x in items:
             cost = x['w'] + k_target * x['s']
             if cost > limit + 1e-5: return None
             weighted.append((cost, x))
-        
-        # Sort descending (Best Fit Decreasing strategy)
         weighted.sort(key=lambda x: x[0], reverse=True)
         
-        # Beam State: (score, loads_tuple, placement_list)
-        # Score: Sum of squares of loads (preference for tight packing)
-        # Loads: Tuple of linearized loads
-        
+        # Beam State: (score, loads_tuple, placement_tuple)
+        # Using tuples for immutability / hashing
         start_loads = tuple([0.0] * gpu_num)
-        start_pl = tuple([[] for _ in range(gpu_num)])
+        start_pl = tuple([() for _ in range(gpu_num)])
         
         beam = [(0.0, start_loads, start_pl)]
         
         for cost, item in weighted:
             candidates = []
-            seen_signatures = set()
+            seen = set()
             
             for score, loads, pl in beam:
-                # Try placing in each bin
-                # Optimization: Duplicate load handling (Symmetry breaking)
+                # Symmetry breaking: don't try same load value multiple times
                 tried_loads = set()
                 
                 for i in range(gpu_num):
-                    current_l = loads[i]
-                    if current_l in tried_loads: continue
-                    
-                    if current_l + cost <= limit + 1e-5:
-                        tried_loads.add(current_l)
-                        
-                        new_loads_list = list(loads)
-                        new_loads_list[i] += cost
-                        
-                        # Signature for state merging: sorted loads
-                        sig = tuple(sorted(new_loads_list))
-                        if sig in seen_signatures: continue
-                        seen_signatures.add(sig)
-                        
-                        new_pl_list = list(pl)
-                        new_pl_list[i] = pl[i] + [item]
-                        
-                        # Heuristic: Maximize sum of squares (Best Fit)
-                        new_score = sum(l*l for l in new_loads_list)
-                        
-                        # Use negative score for sorting logic (if min-heap/sort default)
-                        # Here we just store score and sort desc later
-                        candidates.append((new_score, tuple(new_loads_list), tuple(new_pl_list)))
-            
-            if not candidates:
-                return None
-            
-            # Select top beam_width
+                    if loads[i] in tried_loads: continue
+                    
+                    if loads[i] + cost <= limit + 1e-5:
+                        tried_loads.add(loads[i])
+                        
+                        new_loads = list(loads)
+                        new_loads[i] += cost
+                        new_loads_t = tuple(new_loads)
+                        
+                        # Merge equivalent states
+                        sig = tuple(sorted(new_loads))
+                        if sig in seen: continue
+                        seen.add(sig)
+                        
+                        new_pl = list(pl)
+                        new_pl[i] = pl[i] + (item['model'],)
+                        
+                        # Heuristic: Sum of squares of loads (preference for full bins)
+                        new_score = sum(l*l for l in new_loads)
+                        
+                        candidates.append((new_score, new_loads_t, tuple(new_pl)))
+            
+            if not candidates: return None
+            
+            # Keep top 'width' candidates
             candidates.sort(key=lambda x: x[0], reverse=True)
-            beam = candidates[:beam_width]
-            
-        return beam[0][2]
-
-    def local_optimize(placement_list):
-        """Steepest Descent Hill Climbing"""
-        
-        # Initialize state
+            beam = candidates[:width]
+            
+        return [list(p) for p in beam[0][2]]
+
+    def solve_check(k_target):
+        # 1. Try Fast Greedy Strategies
+        strategies = [
+            lambda x: x['w'] + k_target * x['s'], # Linear Cost
+            lambda x: x['s'],                     # Size
+            lambda x: x['w']                      # Weight
+        ]
+        
+        for key in strategies:
+            ordered = sorted(items, key=key, reverse=True)
+            res = try_pack_greedy(k_target, ordered)
+            if res: return res
+            
+        # 2. Fallback to Beam Search
+        return try_pack_beam(k_target, width=5)
+
+    def local_optimize(placement):
+        """Steepest Descent Hill Climbing."""
+        # Convert to mutable state
         state = []
-        for p in placement_list:
-            w = sum(x['w'] for x in p)
-            s = sum(x['s'] for x in p)
-            rem = GPU_MEM_SIZE - s
-            val = w / rem if rem > 1e-9 else (float('inf') if w > 1e-9 else 0.0)
-            state.append({'w': w, 's': s, 'items': list(p), 'val': val})
-            
-        # Helper to calc potential kvpr
-        def calc_k(w, s):
-            rem = GPU_MEM_SIZE - s
-            if rem <= 1e-9: return float('inf') if w > 1e-9 else 0.0
-            return w / rem
-
-        for _ in range(100): # Limit iterations
-            # Identify current max and bottleneck
-            max_val = -1.0
+        for p in placement:
+            w = sum(m.req_rate / m.slo for m in p)
+            s = sum(m.model_size for m in p)
+            state.append({'w': w, 's': s, 'items': list(p)})
+            
+        for _ in range(150):
+            # Identify Bottleneck
+            max_k = -1.0
             src_idx = -1
+            gpu_k = [] # Cache Ks
+            
             for i, st in enumerate(state):
-                if st['val'] > max_val:
-                    max_val = st['val']
+                k = calc_kvpr(st['w'], st['s'])
+                gpu_k.append(k)
+                if k > max_k:
+                    max_k = k
                     src_idx = i
             
-            if max_val <= 1e-9: break
+            if max_k <= 1e-9: break
             
             src = state[src_idx]
             best_move = None
-            best_improvement = 0.0
-            
-            # 1. Try Moves (Source -> Any Dest)
+            best_imp = 0.0
+            
+            # Try Moves (Src -> Dst)
             for i, item in enumerate(src['items']):
-                # Predict new src
-                ns_w = src['w'] - item['w']
-                ns_s = src['s'] - item['s']
-                ns_val = calc_k(ns_w, ns_s)
-                
-                # Optimization: if src doesn't improve enough to matter, skip
-                # (But we want steepest descent, so we check all that improve max_val)
+                iw = item.req_rate / item.slo
+                is_ = item.model_size
+                
+                ns_w = src['w'] - iw
+                ns_s = src['s'] - is_
+                ns_k = calc_kvpr(ns_w, ns_s)
                 
                 for dst_idx in range(gpu_num):
                     if dst_idx == src_idx: continue
                     dst = state[dst_idx]
                     
-                    if dst['s'] + item['s'] > GPU_MEM_SIZE: continue
-                    
-                    nd_val = calc_k(dst['w'] + item['w'], dst['s'] + item['s'])
-                    
-                    # The new system max will be at least max(ns_val, nd_val)
-                    # We want to reduce max_val.
-                    # Improvement is roughly max_val - max(ns_val, nd_val)
-                    # Note: We assume other GPUs don't exceed max_val (they are <= max_val)
-                    
-                    new_local_max = max(ns_val, nd_val)
-                    if new_local_max < max_val:
-                        imp = max_val - new_local_max
-                        if imp > best_improvement:
-                            best_improvement = imp
+                    if dst['s'] + is_ > GPU_MEM_SIZE: continue
+                    
+                    nd_w = dst['w'] + iw
+                    nd_s = dst['s'] + is_
+                    nd_k = calc_kvpr(nd_w, nd_s)
+                    
+                    # We are improving the global max if max(ns_k, nd_k) < max_k
+                    # (Assuming no other GPU is at max_k, or we iterate until all are fixed)
+                    current_local_max = max(ns_k, nd_k)
+                    if current_local_max < max_k:
+                        imp = max_k - current_local_max
+                        if imp > best_imp:
+                            best_imp = imp
                             best_move = ('move', i, dst_idx, -1)
-            
-            # 2. Try Swaps (Source <-> Any Dest)
-            # Only perform if we haven't found a very simple move? 
-            # Or always check to find BEST? Steepest descent says check all.
-            
+                            
+            # Try Swaps (Src <-> Dst)
             for i, item1 in enumerate(src['items']):
+                iw1, is1 = item1.req_rate / item1.slo, item1.model_size
+                
                 for dst_idx in range(gpu_num):
                     if dst_idx == src_idx: continue
                     dst = state[dst_idx]
                     
-                    # Heuristic pruning
-                    if dst['val'] > max_val * 0.95: continue
+                    # Optimization: Skip if dst is also highly loaded
+                    if gpu_k[dst_idx] > max_k * 0.98: continue
                     
                     for j, item2 in enumerate(dst['items']):
-                        # Sizes
-                        ns_s = src['s'] - item1['s'] + item2['s']
-                        nd_s = dst['s'] - item2['s'] + item1['s']
+                        iw2, is2 = item2.req_rate / item2.slo, item2.model_size
+                        
+                        ns_s = src['s'] - is1 + is2
+                        nd_s = dst['s'] - is2 + is1
                         if ns_s > GPU_MEM_SIZE or nd_s > GPU_MEM_SIZE: continue
                         
-                        ns_val = calc_k(src['w'] - item1['w'] + item2['w'], ns_s)
-                        nd_val = calc_k(dst['w'] - item2['w'] + item1['w'], nd_s)
-                        
-                        new_local_max = max(ns_val, nd_val)
-                        if new_local_max < max_val:
-                            imp = max_val - new_local_max
-                            if imp > best_improvement:
-                                best_improvement = imp
+                        ns_w = src['w'] - iw1 + iw2
+                        nd_w = dst['w'] - iw2 + iw1
+                        
+                        ns_k = calc_kvpr(ns_w, ns_s)
+                        nd_k = calc_kvpr(nd_w, nd_s)
+                        
+                        current_local_max = max(ns_k, nd_k)
+                        if current_local_max < max_k:
+                            imp = max_k - current_local_max
+                            if imp > best_imp:
+                                best_imp = imp
                                 best_move = ('swap', i, dst_idx, j)
             
-            # Apply best move
             if best_move:
-                m_type, i, dst_idx, j = best_move
-                dst = state[dst_idx]
-                
-                if m_type == 'move':
+                mtype, i, d_idx, j = best_move
+                dst = state[d_idx]
+                if mtype == 'move':
                     item = src['items'].pop(i)
                     dst['items'].append(item)
                 else:
                     item1 = src['items'][i]
                     item2 = dst['items'][j]
                     src['items'][i] = item2
                     dst['items'][j] = item1
                 
-                # Update stats
+                # Update weights/sizes for modified bins
                 for b in [src, dst]:
-                    b['w'] = sum(x['w'] for x in b['items'])
-                    b['s'] = sum(x['s'] for x in b['items'])
-                    b['val'] = calc_k(b['w'], b['s'])
+                    b['w'] = sum(x.req_rate / x.slo for x in b['items'])
+                    b['s'] = sum(x.model_size for x in b['items'])
             else:
                 break
                 
-        return [p['items'] for p in state]
-
+        return [s['items'] for s in state]
+
+    # Main Execution
+    high = 1e9
+    
+    # Initial Check
+    best_placement = solve_check(high)
+    if not best_placement:
+        raise ValueError("Unable to place models on GPUs.")
+    
+    # Refine initial
+    best_placement = local_optimize(best_placement)
+    high = get_max_kvpr(best_placement)
+    low = 0.0
+    
     # Binary Search
-    high = 1e9
-    
-    # Initial Check (try simple greedy then wide beam)
-    res = solve_check(high, beam_width=1)
-    if not res:
-        res = solve_check(high, beam_width=8)
-        if not res:
-            raise ValueError("Unable to place models on GPUs.")
-    
-    # Optimize initial solution to tighten bound
-    res = local_optimize(res)
-    best_pl = res
-    high = min(high, get_max_kvpr(res))
-    low = lb
-    
     for _ in range(25):
         if high - low < 1e-4: break
         mid = (low + high) / 2
         
-        # Adaptive Beam Width: Fast check then robust check
-        r = solve_check(mid, beam_width=2)
-        if not r:
-            r = solve_check(mid, beam_width=8)
-            
-        if r:
+        res = solve_check(mid)
+        if res:
             # Found valid placement for K=mid.
-            # Can we do better? Run local optimization on this placement
-            r = local_optimize(r)
-            mx = get_max_kvpr(r)
-            
-            if mx < get_max_kvpr(best_pl):
-                best_pl = r
-                
+            # Try to optimize it further to tighten bound
+            res = local_optimize(res)
+            mx = get_max_kvpr(res)
+            
+            if mx < get_max_kvpr(best_placement):
+                best_placement = res
+            
             high = min(mid, mx)
         else:
             low = mid
             
-    # Final Optimization pass
-    best_pl = local_optimize(best_pl)
-    
-    # Format for return
-    return {i: [x['model'] for x in p] for i, p in enumerate(best_pl)}
+    # Final Optimization
+    best_placement = local_optimize(best_placement)
+    
+    return {i: best_placement[i] for i in range(gpu_num)}
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")