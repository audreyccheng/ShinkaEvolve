--- a/original.py
+++ b/original.py
@@ -1,334 +1,378 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
-GPU_MEM_SIZE = 80  # GB
+import random
+import math
+
+GPU_MEM_SIZE = 80.0  # GB
 
 def compute_model_placement(gpu_num, models):
     """
-    Compute a model placement that minimizes the maximum KVPR across all GPUs.
-    Combines Greedy Heuristics, Binary Search (Bin Packing), and Local Search.
+    Minimizes max KVPR using a hybrid approach:
+    1. Greedy Heuristics Ensemble for initialization and upper bound.
+    2. Binary Search on target Pressure K (Bin Packing transformation) with deterministic and stochastic strategies.
+    3. Local Search Refinement (Move and Swap) to optimize the bottleneck.
     """
 
-    # Helper to calculate max KVPR of a placement
-    def get_max_kvpr(placement):
+    # -------------------------------------------------------
+    # 0. Precompute model data
+    # -------------------------------------------------------
+    m_data = []
+    total_w = 0.0
+    total_s = 0.0
+    for i, m in enumerate(models):
+        w = m.req_rate / m.slo
+        s = m.model_size
+        m_data.append({'w': w, 's': s, 'obj': m, 'id': i})
+        total_w += w
+        total_s += s
+
+    # Theoretical Lower Bound
+    rem_global = gpu_num * GPU_MEM_SIZE - total_s
+    if rem_global <= 1e-9:
+        lb = float('inf') if total_w > 0 else 0.0
+    else:
+        lb = total_w / rem_global
+
+    best_placement = None
+    best_max_kvpr = float('inf')
+
+    # Helper to calculate max KVPR of a placement dict (gpu_id -> list of indices)
+    def calc_placement_kvpr(placement_map):
         max_p = 0.0
-        for assigned in placement.values():
-            w = sum(m.req_rate / m.slo for m in assigned)
-            s = sum(m.model_size for m in assigned)
+        for idxs in placement_map.values():
+            w = sum(m_data[i]['w'] for i in idxs)
+            s = sum(m_data[i]['s'] for i in idxs)
             rem = GPU_MEM_SIZE - s
             if rem <= 1e-9:
-                if w > 0: return float('inf')
-                else: continue
-            max_p = max(max_p, w / rem)
+                val = float('inf') if w > 0 else 0.0
+            else:
+                val = w / rem
+            max_p = max(max_p, val)
         return max_p
 
-    best_placement = None
-    best_score = float('inf')
-
-    # ---------------------------------------------------------
-    # 1. Greedy Heuristics Ensemble
-    # ---------------------------------------------------------
-    # Fast initial solutions to set an upper bound
+    # -------------------------------------------------------
+    # 1. Greedy Initialization (Ensemble)
+    # -------------------------------------------------------
+    # Strategies to get a good initial solution and tight upper bound
     heuristics = [
-        # (Key Function, Strategy Name)
-        # Strategy 'min_result': Place on GPU minimizing resulting KVPR
-        # Strategy 'min_current': Place on GPU minimizing current KVPR (Load Balancing)
-        (lambda m: (m.req_rate / m.slo, m.model_size), 'min_result'),
-        (lambda m: m.model_size, 'min_result'),
-        (lambda m: (m.req_rate / m.slo) / (GPU_MEM_SIZE - m.model_size + 1e-6), 'min_result'),
-        (lambda m: m.req_rate / m.slo, 'min_current'),
+        (lambda x: x['w'] + lb * x['s'], 'linear_approx'), 
+        (lambda x: x['s'], 'physical_size'),
+        (lambda x: x['w'], 'weight'),
+        (lambda x: x['w'] / (x['s'] + 1e-6), 'density'),
     ]
-
-    for key_fn, strategy in heuristics:
-        sorted_models = sorted(models, key=key_fn, reverse=True)
-        placement = {i: [] for i in range(gpu_num)}
-        gpu_w = [0.0] * gpu_num
-        gpu_s = [0.0] * gpu_num
+    
+    for key_func, _ in heuristics:
+        sorted_indices = sorted(range(len(m_data)), key=lambda i: key_func(m_data[i]), reverse=True)
+        bins = [{'w': 0.0, 's': 0.0, 'idxs': []} for _ in range(gpu_num)]
         possible = True
-
-        for model in sorted_models:
-            w = model.req_rate / model.slo
-            s = model.model_size
-            best_idx = None
+        
+        for idx in sorted_indices:
+            w = m_data[idx]['w']
+            s = m_data[idx]['s']
+            best_bin = None
             best_val = float('inf')
-
-            for i in range(gpu_num):
-                if gpu_s[i] + s > GPU_MEM_SIZE: continue
-                rem = GPU_MEM_SIZE - gpu_s[i]
-
-                if strategy == 'min_result':
-                    new_rem = rem - s
-                    if new_rem > 1e-9:
-                        val = (gpu_w[i] + w) / new_rem
-                    else:
-                        val = float('inf')
-                else: # min_current
-                    if rem > 1e-9:
-                        val = gpu_w[i] / rem
-                    else:
-                        val = float('inf')
-
+            
+            # Greedy Min-KVPR placement
+            for b_idx in range(gpu_num):
+                b = bins[b_idx]
+                if b['s'] + s > GPU_MEM_SIZE: continue
+                
+                rem = GPU_MEM_SIZE - (b['s'] + s)
+                if rem <= 1e-9:
+                    val = float('inf') if (b['w'] + w) > 0 else 0.0
+                else:
+                    val = (b['w'] + w) / rem
+                
                 if val < best_val:
                     best_val = val
-                    best_idx = i
-                elif val == best_val and best_idx is None:
-                    best_idx = i
-
-            if best_idx is None:
+                    best_bin = b_idx
+            
+            if best_bin is None:
                 possible = False
                 break
-
-            placement[best_idx].append(model)
-            gpu_w[best_idx] += w
-            gpu_s[best_idx] += s
-
+            
+            bins[best_bin]['idxs'].append(idx)
+            bins[best_bin]['w'] += w
+            bins[best_bin]['s'] += s
+        
         if possible:
-            score = get_max_kvpr(placement)
-            if score < best_score:
-                best_score = score
-                best_placement = placement
-
-    # ---------------------------------------------------------
-    # 2. Binary Search on Target KVPR (Transformation to Bin Packing)
-    # ---------------------------------------------------------
-    # Problem: Minimize K such that sum(w_i)/ (C - sum(s_i)) <= K
-    # Equivalent to Bin Packing with item size v_i(K) = s_i + w_i/K, bin capacity C.
-
-    total_w = sum(m.req_rate / m.slo for m in models)
-    total_s = sum(m.model_size for m in models)
-    rem_global = gpu_num * GPU_MEM_SIZE - total_s
-
-    # Theoretical lower bound: Average case fluid distribution
-    if rem_global > 1e-6:
-        low = total_w / rem_global
-        high = best_score if best_score != float('inf') else 1000.0
-
-        if high > low + 1e-4:
-            for _ in range(20):
-                mid = (low + high) / 2
-
-                # Check feasibility with multiple heuristics to increase success rate
-                check_keys = [
-                    lambda m: m.model_size + (m.req_rate/m.slo)/mid, # Effective Size
-                    lambda m: m.model_size,                          # Physical Size
-                    lambda m: m.req_rate/m.slo                       # Weight
-                ]
-
-                found_valid_for_mid = False
-
-                for key_fn in check_keys:
-                    bs_models = sorted(models, key=key_fn, reverse=True)
-
-                    temp_placement = {i: [] for i in range(gpu_num)}
-                    gpu_w = [0.0] * gpu_num   # Actual w
-                    gpu_s = [0.0] * gpu_num   # Actual s
-                    possible_k = True
-
-                    for model in bs_models:
-                        w = model.req_rate / model.slo
-                        s = model.model_size
-
-                        best_idx = None
-                        min_slack = float('inf')
-
-                        # Best Fit Decreasing on Effective Constraint
-                        # Constraint: w_new <= mid * rem_phys_new
-                        for i in range(gpu_num):
-                            if gpu_s[i] + s > GPU_MEM_SIZE: continue
-
-                            rem_phys_new = GPU_MEM_SIZE - (gpu_s[i] + s)
-                            # Allow slight float tolerance
-                            if rem_phys_new < 0: rem_phys_new = 0.0
-
-                            lhs = gpu_w[i] + w
-                            rhs = mid * rem_phys_new
-
-                            if lhs <= rhs + 1e-5:
-                                # Slack: unused effective capacity
-                                slack = rhs - lhs
-                                if slack < min_slack:
-                                    min_slack = slack
-                                    best_idx = i
-
-                        if best_idx is None:
-                            possible_k = False
-                            break
-
-                        temp_placement[best_idx].append(model)
-                        gpu_w[best_idx] += w
-                        gpu_s[best_idx] += s
-
-                    if possible_k:
-                        actual_score = get_max_kvpr(temp_placement)
-                        if actual_score < best_score:
-                            best_score = actual_score
-                            best_placement = temp_placement
-                        found_valid_for_mid = True
-                        break # Found a valid packing, no need to check other sorts
-
-                if found_valid_for_mid:
-                    high = mid
-                else:
-                    low = mid
+            current_map = {i: bins[i]['idxs'] for i in range(gpu_num)}
+            score = calc_placement_kvpr(current_map)
+            if score < best_max_kvpr:
+                best_max_kvpr = score
+                best_placement = current_map
+
+    # -------------------------------------------------------
+    # 2. Binary Search on Pressure K
+    # -------------------------------------------------------
+    if best_max_kvpr == float('inf'):
+        high = 5000.0
+    else:
+        high = best_max_kvpr
+    
+    low = lb
+    
+    def try_pack(target_k, randomized=False):
+        """
+        Attempts to pack models with max pressure <= target_k.
+        Uses Best Fit Decreasing on virtual item sizes.
+        """
+        strategies = []
+        if not randomized:
+            # Deterministic strategies
+            strategies = [
+                lambda x: x['w'] + target_k * x['s'],
+                lambda x: x['s'],
+                lambda x: x['w']
+            ]
+        else:
+            # Stochastic strategies with noise
+            for _ in range(8):
+                strategies.append(
+                    lambda x: (x['w'] + target_k * x['s']) * random.uniform(0.9, 1.1)
+                )
+
+        for key_func in strategies:
+            ordered_indices = sorted(range(len(m_data)), key=lambda i: key_func(m_data[i]), reverse=True)
+            
+            bins = [{'w': 0.0, 's': 0.0, 'idxs': []} for _ in range(gpu_num)]
+            possible = True
+            
+            for idx in ordered_indices:
+                w, s = m_data[idx]['w'], m_data[idx]['s']
+                best_bin = None
+                min_slack = float('inf')
+                
+                # Check for Best Fit (min slack) satisfying constraints
+                for b_idx in range(gpu_num):
+                    b = bins[b_idx]
+                    if b['s'] + s > GPU_MEM_SIZE: continue
+                    
+                    phys_rem = GPU_MEM_SIZE - (b['s'] + s)
+                    if phys_rem < 0: phys_rem = 0.0
+                    
+                    limit_w = target_k * phys_rem
+                    new_w = b['w'] + w
+                    
+                    if new_w <= limit_w + 1e-5:
+                        # Slack is remaining virtual capacity: limit_w - new_w
+                        slack = limit_w - new_w
+                        if slack < min_slack:
+                            min_slack = slack
+                            best_bin = b_idx
+                
+                if best_bin is None:
+                    possible = False
+                    break
+                
+                bins[best_bin]['idxs'].append(idx)
+                bins[best_bin]['w'] += w
+                bins[best_bin]['s'] += s
+            
+            if possible:
+                return {i: bins[i]['idxs'] for i in range(gpu_num)}
+        return None
+
+    # Perform Binary Search if optimization is possible
+    if high > low + 1e-4:
+        for _ in range(16):
+            mid = (low + high) / 2.0
+            
+            # Try deterministic then stochastic
+            sol = try_pack(mid, randomized=False)
+            if not sol:
+                sol = try_pack(mid, randomized=True)
+            
+            if sol:
+                # Valid solution found, save it and try tighter K
+                score = calc_placement_kvpr(sol)
+                if score < best_max_kvpr:
+                    best_max_kvpr = score
+                    best_placement = sol
+                high = mid
+            else:
+                low = mid
 
     if best_placement is None:
-        raise ValueError("Unable to place models on GPUs with available memory.")
-
-    # ---------------------------------------------------------
+        # Fallback (should be covered by greedy, but just in case)
+        sol = try_pack(5000.0, randomized=False)
+        if sol: 
+            best_placement = sol
+        else:
+            raise ValueError("Unable to place models on GPUs with available memory.")
+
+    # -------------------------------------------------------
     # 3. Local Search Refinement
-    # ---------------------------------------------------------
-    # Iteratively move or swap models to reduce the peak KVPR
-
-    # Cache state for faster evaluation
-    gpu_w = [0.0] * gpu_num
-    gpu_s = [0.0] * gpu_num
-    for i in range(gpu_num):
-        assigned = best_placement[i]
-        gpu_w[i] = sum(m.req_rate / m.slo for m in assigned)
-        gpu_s[i] = sum(m.model_size for m in assigned)
-
+    # -------------------------------------------------------
+    curr_map = best_placement # dict: gpu -> list of indices
+    
+    # Initialize stats
+    g_stats = []
+    for g in range(gpu_num):
+        idxs = curr_map[g]
+        w = sum(m_data[i]['w'] for i in idxs)
+        s = sum(m_data[i]['s'] for i in idxs)
+        rem = GPU_MEM_SIZE - s
+        p = w / rem if rem > 1e-9 else (float('inf') if w > 0 else 0.0)
+        g_stats.append({'w': w, 's': s, 'p': p})
+
+    # Hill Climbing Loop
     for _ in range(100):
-        # Identify bottleneck
+        # Identify bottleneck GPU
         max_p = -1.0
         src_gpu = -1
-
-        # Recalculate pressures
-        for i in range(gpu_num):
-            rem = GPU_MEM_SIZE - gpu_s[i]
-            p = gpu_w[i] / rem if rem > 1e-9 else float('inf')
-            if p > max_p:
-                max_p = p
-                src_gpu = i
-
+        for g in range(gpu_num):
+            if g_stats[g]['p'] > max_p:
+                max_p = g_stats[g]['p']
+                src_gpu = g
+        
         if src_gpu == -1 or max_p < 1e-9: break
-
+        
         improved = False
-        src_models = best_placement[src_gpu]
-
-        # 1. Try MOVE (Source -> Dest)
-        for m_idx, model in enumerate(src_models):
-            w = model.req_rate / model.slo
-            s = model.model_size
-
+        src_list = curr_map[src_gpu]
+        
+        # 3.1 Try MOVE
+        for list_idx, m_idx in enumerate(src_list):
+            m = m_data[m_idx]
+            
+            # Hypothetical Src State
+            src_rem_new = GPU_MEM_SIZE - (g_stats[src_gpu]['s'] - m['s'])
+            src_w_new = g_stats[src_gpu]['w'] - m['w']
+            src_p_new = src_w_new / src_rem_new if src_rem_new > 1e-9 else (float('inf') if src_w_new > 0 else 0.0)
+            
+            best_dst = None
+            
             for dst in range(gpu_num):
                 if dst == src_gpu: continue
-
-                if gpu_s[dst] + s > GPU_MEM_SIZE: continue
-
-                new_dst_rem = GPU_MEM_SIZE - (gpu_s[dst] + s)
-                if new_dst_rem <= 1e-9: continue
-
-                new_dst_p = (gpu_w[dst] + w) / new_dst_rem
-
-                # Move if destination doesn't become worse than current max
-                if new_dst_p < max_p - 1e-6:
-                    moved_model = src_models.pop(m_idx)
-                    best_placement[dst].append(moved_model)
-
-                    gpu_w[src_gpu] -= w
-                    gpu_s[src_gpu] -= s
-                    gpu_w[dst] += w
-                    gpu_s[dst] += s
-                    improved = True
-                    break
-            if improved: break
-
+                if g_stats[dst]['s'] + m['s'] > GPU_MEM_SIZE: continue
+                
+                dst_rem_new = GPU_MEM_SIZE - (g_stats[dst]['s'] + m['s'])
+                dst_w_new = g_stats[dst]['w'] + m['w']
+                dst_p_new = dst_w_new / dst_rem_new if dst_rem_new > 1e-9 else (float('inf') if dst_w_new > 0 else 0.0)
+                
+                # Check if this reduces the global maximum pressure (or at least the bottleneck)
+                if max(src_p_new, dst_p_new) < max_p - 1e-5:
+                    best_dst = dst
+                    break # First improvement
+            
+            if best_dst is not None:
+                # Apply Move
+                curr_map[src_gpu].pop(list_idx)
+                curr_map[best_dst].append(m_idx)
+                
+                # Update Stats
+                g_stats[src_gpu] = {'w': src_w_new, 's': GPU_MEM_SIZE - src_rem_new, 'p': src_p_new}
+                
+                # Re-calculate destination stats
+                dst_w = g_stats[best_dst]['w'] + m['w']
+                dst_rem = GPU_MEM_SIZE - (g_stats[best_dst]['s'] + m['s'])
+                dst_p = dst_w / dst_rem if dst_rem > 1e-9 else (float('inf') if dst_w > 0 else 0.0)
+                g_stats[best_dst] = {'w': dst_w, 's': GPU_MEM_SIZE - dst_rem, 'p': dst_p}
+                
+                improved = True
+                break
+        
         if improved: continue
-
-        # 2. Try SWAP (Source <-> Dest)
-        for m_idx, m_src in enumerate(src_models):
-            w_src = m_src.req_rate / m_src.slo
-            s_src = m_src.model_size
-
+        
+        # 3.2 Try SWAP
+        for s_list_idx, m_src_idx in enumerate(src_list):
+            m_src = m_data[m_src_idx]
+            
             for dst in range(gpu_num):
                 if dst == src_gpu: continue
-
-                dst_models = best_placement[dst]
-                for d_idx, m_dst in enumerate(dst_models):
-                    w_dst = m_dst.req_rate / m_dst.slo
-                    s_dst = m_dst.model_size
-
-                    new_src_s = gpu_s[src_gpu] - s_src + s_dst
-                    new_dst_s = gpu_s[dst] - s_dst + s_src
-
-                    if new_src_s > GPU_MEM_SIZE or new_dst_s > GPU_MEM_SIZE: continue
-
+                dst_list = curr_map[dst]
+                
+                for d_list_idx, m_dst_idx in enumerate(dst_list):
+                    m_dst = m_data[m_dst_idx]
+                    
+                    # Capacity Check
+                    new_src_s = g_stats[src_gpu]['s'] - m_src['s'] + m_dst['s']
+                    if new_src_s > GPU_MEM_SIZE: continue
+                    new_dst_s = g_stats[dst]['s'] - m_dst['s'] + m_src['s']
+                    if new_dst_s > GPU_MEM_SIZE: continue
+                    
+                    # Pressure Check
                     new_src_rem = GPU_MEM_SIZE - new_src_s
+                    new_src_w = g_stats[src_gpu]['w'] - m_src['w'] + m_dst['w']
+                    new_src_p = new_src_w / new_src_rem if new_src_rem > 1e-9 else float('inf')
+                    
                     new_dst_rem = GPU_MEM_SIZE - new_dst_s
-
-                    if new_src_rem <= 1e-9 or new_dst_rem <= 1e-9: continue
-
-                    new_src_p = (gpu_w[src_gpu] - w_src + w_dst) / new_src_rem
-                    new_dst_p = (gpu_w[dst] - w_dst + w_src) / new_dst_rem
-
-                    if max(new_src_p, new_dst_p) < max_p - 1e-6:
-                        mod_src = src_models.pop(m_idx)
-                        mod_dst = dst_models.pop(d_idx)
-
-                        src_models.append(mod_dst)
-                        dst_models.append(mod_src)
-
-                        gpu_w[src_gpu] = gpu_w[src_gpu] - w_src + w_dst
-                        gpu_s[src_gpu] = gpu_s[src_gpu] - s_src + s_dst
-                        gpu_w[dst] = gpu_w[dst] - w_dst + w_src
-                        gpu_s[dst] = gpu_s[dst] - s_dst + s_src
-
+                    new_dst_w = g_stats[dst]['w'] - m_dst['w'] + m_src['w']
+                    new_dst_p = new_dst_w / new_dst_rem if new_dst_rem > 1e-9 else float('inf')
+                    
+                    if max(new_src_p, new_dst_p) < max_p - 1e-5:
+                        # Apply Swap
+                        curr_map[src_gpu].pop(s_list_idx)
+                        curr_map[src_gpu].append(m_dst_idx)
+                        
+                        curr_map[dst].pop(d_list_idx)
+                        curr_map[dst].append(m_src_idx)
+                        
+                        g_stats[src_gpu] = {'w': new_src_w, 's': new_src_s, 'p': new_src_p}
+                        g_stats[dst] = {'w': new_dst_w, 's': new_dst_s, 'p': new_dst_p}
+                        
                         improved = True
                         break
                 if improved: break
             if improved: break
-
+            
         if not improved: break
 
-    return best_placement
+    # Final Result Construction
+    result = {}
+    for g, idxs in curr_map.items():
+        result[g] = [m_data[i]['obj'] for i in idxs]
+        
+    return result
 
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")