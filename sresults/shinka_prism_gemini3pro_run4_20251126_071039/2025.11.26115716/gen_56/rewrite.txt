# EVOLVE-BLOCK-START
"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""

import random
import math

GPU_MEM_SIZE = 80.0  # GB

def compute_model_placement(gpu_num, models):
    """
    Minimizes max KVPR using a hybrid approach:
    1. Greedy Heuristics Ensemble for initialization and upper bound.
    2. Binary Search on target Pressure K (Bin Packing transformation) with deterministic and stochastic strategies.
    3. Local Search Refinement (Move and Swap) to optimize the bottleneck.
    """

    # -------------------------------------------------------
    # 0. Precompute model data
    # -------------------------------------------------------
    m_data = []
    total_w = 0.0
    total_s = 0.0
    for i, m in enumerate(models):
        w = m.req_rate / m.slo
        s = m.model_size
        m_data.append({'w': w, 's': s, 'obj': m, 'id': i})
        total_w += w
        total_s += s

    # Theoretical Lower Bound
    rem_global = gpu_num * GPU_MEM_SIZE - total_s
    if rem_global <= 1e-9:
        lb = float('inf') if total_w > 0 else 0.0
    else:
        lb = total_w / rem_global

    best_placement = None
    best_max_kvpr = float('inf')

    # Helper to calculate max KVPR of a placement dict (gpu_id -> list of indices)
    def calc_placement_kvpr(placement_map):
        max_p = 0.0
        for idxs in placement_map.values():
            w = sum(m_data[i]['w'] for i in idxs)
            s = sum(m_data[i]['s'] for i in idxs)
            rem = GPU_MEM_SIZE - s
            if rem <= 1e-9:
                val = float('inf') if w > 0 else 0.0
            else:
                val = w / rem
            max_p = max(max_p, val)
        return max_p

    # -------------------------------------------------------
    # 1. Greedy Initialization (Ensemble)
    # -------------------------------------------------------
    # Strategies to get a good initial solution and tight upper bound
    heuristics = [
        (lambda x: x['w'] + lb * x['s'], 'linear_approx'), 
        (lambda x: x['s'], 'physical_size'),
        (lambda x: x['w'], 'weight'),
        (lambda x: x['w'] / (x['s'] + 1e-6), 'density'),
    ]
    
    for key_func, _ in heuristics:
        sorted_indices = sorted(range(len(m_data)), key=lambda i: key_func(m_data[i]), reverse=True)
        bins = [{'w': 0.0, 's': 0.0, 'idxs': []} for _ in range(gpu_num)]
        possible = True
        
        for idx in sorted_indices:
            w = m_data[idx]['w']
            s = m_data[idx]['s']
            best_bin = None
            best_val = float('inf')
            
            # Greedy Min-KVPR placement
            for b_idx in range(gpu_num):
                b = bins[b_idx]
                if b['s'] + s > GPU_MEM_SIZE: continue
                
                rem = GPU_MEM_SIZE - (b['s'] + s)
                if rem <= 1e-9:
                    val = float('inf') if (b['w'] + w) > 0 else 0.0
                else:
                    val = (b['w'] + w) / rem
                
                if val < best_val:
                    best_val = val
                    best_bin = b_idx
            
            if best_bin is None:
                possible = False
                break
            
            bins[best_bin]['idxs'].append(idx)
            bins[best_bin]['w'] += w
            bins[best_bin]['s'] += s
        
        if possible:
            current_map = {i: bins[i]['idxs'] for i in range(gpu_num)}
            score = calc_placement_kvpr(current_map)
            if score < best_max_kvpr:
                best_max_kvpr = score
                best_placement = current_map

    # -------------------------------------------------------
    # 2. Binary Search on Pressure K
    # -------------------------------------------------------
    if best_max_kvpr == float('inf'):
        high = 5000.0
    else:
        high = best_max_kvpr
    
    low = lb
    
    def try_pack(target_k, randomized=False):
        """
        Attempts to pack models with max pressure <= target_k.
        Uses Best Fit Decreasing on virtual item sizes.
        """
        strategies = []
        if not randomized:
            # Deterministic strategies
            strategies = [
                lambda x: x['w'] + target_k * x['s'],
                lambda x: x['s'],
                lambda x: x['w']
            ]
        else:
            # Stochastic strategies with noise
            for _ in range(8):
                strategies.append(
                    lambda x: (x['w'] + target_k * x['s']) * random.uniform(0.9, 1.1)
                )

        for key_func in strategies:
            ordered_indices = sorted(range(len(m_data)), key=lambda i: key_func(m_data[i]), reverse=True)
            
            bins = [{'w': 0.0, 's': 0.0, 'idxs': []} for _ in range(gpu_num)]
            possible = True
            
            for idx in ordered_indices:
                w, s = m_data[idx]['w'], m_data[idx]['s']
                best_bin = None
                min_slack = float('inf')
                
                # Check for Best Fit (min slack) satisfying constraints
                for b_idx in range(gpu_num):
                    b = bins[b_idx]
                    if b['s'] + s > GPU_MEM_SIZE: continue
                    
                    phys_rem = GPU_MEM_SIZE - (b['s'] + s)
                    if phys_rem < 0: phys_rem = 0.0
                    
                    limit_w = target_k * phys_rem
                    new_w = b['w'] + w
                    
                    if new_w <= limit_w + 1e-5:
                        # Slack is remaining virtual capacity: limit_w - new_w
                        slack = limit_w - new_w
                        if slack < min_slack:
                            min_slack = slack
                            best_bin = b_idx
                
                if best_bin is None:
                    possible = False
                    break
                
                bins[best_bin]['idxs'].append(idx)
                bins[best_bin]['w'] += w
                bins[best_bin]['s'] += s
            
            if possible:
                return {i: bins[i]['idxs'] for i in range(gpu_num)}
        return None

    # Perform Binary Search if optimization is possible
    if high > low + 1e-4:
        for _ in range(16):
            mid = (low + high) / 2.0
            
            # Try deterministic then stochastic
            sol = try_pack(mid, randomized=False)
            if not sol:
                sol = try_pack(mid, randomized=True)
            
            if sol:
                # Valid solution found, save it and try tighter K
                score = calc_placement_kvpr(sol)
                if score < best_max_kvpr:
                    best_max_kvpr = score
                    best_placement = sol
                high = mid
            else:
                low = mid

    if best_placement is None:
        # Fallback (should be covered by greedy, but just in case)
        sol = try_pack(5000.0, randomized=False)
        if sol: 
            best_placement = sol
        else:
            raise ValueError("Unable to place models on GPUs with available memory.")

    # -------------------------------------------------------
    # 3. Local Search Refinement
    # -------------------------------------------------------
    curr_map = best_placement # dict: gpu -> list of indices
    
    # Initialize stats
    g_stats = []
    for g in range(gpu_num):
        idxs = curr_map[g]
        w = sum(m_data[i]['w'] for i in idxs)
        s = sum(m_data[i]['s'] for i in idxs)
        rem = GPU_MEM_SIZE - s
        p = w / rem if rem > 1e-9 else (float('inf') if w > 0 else 0.0)
        g_stats.append({'w': w, 's': s, 'p': p})

    # Hill Climbing Loop
    for _ in range(100):
        # Identify bottleneck GPU
        max_p = -1.0
        src_gpu = -1
        for g in range(gpu_num):
            if g_stats[g]['p'] > max_p:
                max_p = g_stats[g]['p']
                src_gpu = g
        
        if src_gpu == -1 or max_p < 1e-9: break
        
        improved = False
        src_list = curr_map[src_gpu]
        
        # 3.1 Try MOVE
        for list_idx, m_idx in enumerate(src_list):
            m = m_data[m_idx]
            
            # Hypothetical Src State
            src_rem_new = GPU_MEM_SIZE - (g_stats[src_gpu]['s'] - m['s'])
            src_w_new = g_stats[src_gpu]['w'] - m['w']
            src_p_new = src_w_new / src_rem_new if src_rem_new > 1e-9 else (float('inf') if src_w_new > 0 else 0.0)
            
            best_dst = None
            
            for dst in range(gpu_num):
                if dst == src_gpu: continue
                if g_stats[dst]['s'] + m['s'] > GPU_MEM_SIZE: continue
                
                dst_rem_new = GPU_MEM_SIZE - (g_stats[dst]['s'] + m['s'])
                dst_w_new = g_stats[dst]['w'] + m['w']
                dst_p_new = dst_w_new / dst_rem_new if dst_rem_new > 1e-9 else (float('inf') if dst_w_new > 0 else 0.0)
                
                # Check if this reduces the global maximum pressure (or at least the bottleneck)
                if max(src_p_new, dst_p_new) < max_p - 1e-5:
                    best_dst = dst
                    break # First improvement
            
            if best_dst is not None:
                # Apply Move
                curr_map[src_gpu].pop(list_idx)
                curr_map[best_dst].append(m_idx)
                
                # Update Stats
                g_stats[src_gpu] = {'w': src_w_new, 's': GPU_MEM_SIZE - src_rem_new, 'p': src_p_new}
                
                # Re-calculate destination stats
                dst_w = g_stats[best_dst]['w'] + m['w']
                dst_rem = GPU_MEM_SIZE - (g_stats[best_dst]['s'] + m['s'])
                dst_p = dst_w / dst_rem if dst_rem > 1e-9 else (float('inf') if dst_w > 0 else 0.0)
                g_stats[best_dst] = {'w': dst_w, 's': GPU_MEM_SIZE - dst_rem, 'p': dst_p}
                
                improved = True
                break
        
        if improved: continue
        
        # 3.2 Try SWAP
        for s_list_idx, m_src_idx in enumerate(src_list):
            m_src = m_data[m_src_idx]
            
            for dst in range(gpu_num):
                if dst == src_gpu: continue
                dst_list = curr_map[dst]
                
                for d_list_idx, m_dst_idx in enumerate(dst_list):
                    m_dst = m_data[m_dst_idx]
                    
                    # Capacity Check
                    new_src_s = g_stats[src_gpu]['s'] - m_src['s'] + m_dst['s']
                    if new_src_s > GPU_MEM_SIZE: continue
                    new_dst_s = g_stats[dst]['s'] - m_dst['s'] + m_src['s']
                    if new_dst_s > GPU_MEM_SIZE: continue
                    
                    # Pressure Check
                    new_src_rem = GPU_MEM_SIZE - new_src_s
                    new_src_w = g_stats[src_gpu]['w'] - m_src['w'] + m_dst['w']
                    new_src_p = new_src_w / new_src_rem if new_src_rem > 1e-9 else float('inf')
                    
                    new_dst_rem = GPU_MEM_SIZE - new_dst_s
                    new_dst_w = g_stats[dst]['w'] - m_dst['w'] + m_src['w']
                    new_dst_p = new_dst_w / new_dst_rem if new_dst_rem > 1e-9 else float('inf')
                    
                    if max(new_src_p, new_dst_p) < max_p - 1e-5:
                        # Apply Swap
                        curr_map[src_gpu].pop(s_list_idx)
                        curr_map[src_gpu].append(m_dst_idx)
                        
                        curr_map[dst].pop(d_list_idx)
                        curr_map[dst].append(m_src_idx)
                        
                        g_stats[src_gpu] = {'w': new_src_w, 's': new_src_s, 'p': new_src_p}
                        g_stats[dst] = {'w': new_dst_w, 's': new_dst_s, 'p': new_dst_p}
                        
                        improved = True
                        break
                if improved: break
            if improved: break
            
        if not improved: break

    # Final Result Construction
    result = {}
    for g, idxs in curr_map.items():
        result[g] = [m_data[i]['obj'] for i in idxs]
        
    return result

# EVOLVE-BLOCK-END