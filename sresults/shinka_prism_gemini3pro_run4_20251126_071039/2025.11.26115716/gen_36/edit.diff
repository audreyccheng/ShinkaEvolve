--- a/original.py
+++ b/original.py
@@ -1,192 +1,320 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
-GPU_MEM_SIZE = 80  # GB
+import math
+import heapq
+
+GPU_MEM_SIZE = 80.0
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
-
-    Args:
-        gpu_num: Number of GPUs
-        models: List of models to place
-
-    Returns:
-        A placement of models to GPUs
+    
+    Method:
+    1. Binary Search on the target KVPR (K).
+    2. Feasibility check using Beam Search (Width=5):
+       - Sorts items by linearized cost (w + K*s).
+       - Maintains top W states.
+       - State representation: Sorted tuple of bin loads to break symmetry.
+       - Heuristic: Maximize sum of squared loads (Best Fit preference).
+    3. Post-verification Hill Climbing:
+       - If a solution is found for K, optimize it using move/swap operations to minimize actual KVPR.
+       - Use the optimized KVPR to tighten the binary search upper bound.
     """
 
-    # Pre-process models to extract relevant metrics: weight (req/slo) and size
+    # 1. Preprocess items
+    # We store index to track original models
     items = []
-    for m in models:
-        items.append({
-            'model': m,
-            'w': m.req_rate / m.slo,
-            's': m.model_size
-        })
-
-    def check_placement(k_target):
-        import random
+    for i, m in enumerate(models):
+        w = m.req_rate / m.slo
+        s = m.model_size
+        items.append({'w': w, 's': s, 'model': m, 'id': i})
+
+    def get_max_kvpr(placement):
+        """Calculate the actual maximum KVPR of a placement."""
+        max_k = 0.0
+        for p in placement.values():
+            w_sum = sum(m.req_rate / m.slo for m in p)
+            s_sum = sum(m.model_size for m in p)
+            rem = GPU_MEM_SIZE - s_sum
+            if rem <= 1e-9:
+                if w_sum > 1e-9: return float('inf')
+                val = 0.0
+            else:
+                val = w_sum / rem
+            if val > max_k: max_k = val
+        return max_k
+
+    def hill_climb(placement):
         """
-        Determines if it is possible to place all models such that for every GPU:
-        KVPR <= k_target.
+        Refine a valid placement to reduce max KVPR.
+        Uses greedy moves and swaps from the bottleneck GPU.
         """
-
-        def try_pack(item_order):
-            placement = {i: [] for i in range(gpu_num)}
-            gpu_state = [{'w': 0.0, 's': 0.0} for _ in range(gpu_num)]
-
-            for item in item_order:
-                best_idx = -1
-                best_fill = -1.0
-
+        # Convert to mutable structure
+        # bins: list of {'w': float, 's': float, 'models': list}
+        bins = []
+        for i in range(gpu_num):
+            m_list = list(placement[i])
+            w = sum(m.req_rate / m.slo for m in m_list)
+            s = sum(m.model_size for m in m_list)
+            bins.append({'w': w, 's': s, 'models': m_list})
+            
+        def calc_k(w, s):
+            rem = GPU_MEM_SIZE - s
+            if rem <= 1e-9: return float('inf') if w > 1e-9 else 0.0
+            return w / rem
+
+        for _ in range(50): # Limit iterations for speed
+            # Find bottleneck
+            max_k = -1.0
+            src_idx = -1
+            
+            # Recompute Ks
+            for i in range(gpu_num):
+                k = calc_k(bins[i]['w'], bins[i]['s'])
+                if k > max_k:
+                    max_k = k
+                    src_idx = i
+            
+            if max_k <= 1e-9: break
+            
+            improved = False
+            src = bins[src_idx]
+            
+            # 1. Try Move (Source -> Dest)
+            for i, m in enumerate(src['models']):
+                mw = m.req_rate / m.slo
+                ms = m.model_size
+                
+                # Check src improvement
+                ns_w = src['w'] - mw
+                ns_s = src['s'] - ms
+                ns_k = calc_k(ns_w, ns_s)
+                
+                if ns_k >= max_k - 1e-9: continue
+                
+                for dst_idx in range(gpu_num):
+                    if dst_idx == src_idx: continue
+                    dst = bins[dst_idx]
+                    
+                    if dst['s'] + ms > GPU_MEM_SIZE: continue
+                    nd_w = dst['w'] + mw
+                    nd_s = dst['s'] + ms
+                    nd_k = calc_k(nd_w, nd_s)
+                    
+                    if nd_k < max_k - 1e-9:
+                        # Apply Move
+                        src['models'].pop(i)
+                        src['w'], src['s'] = ns_w, ns_s
+                        dst['models'].append(m)
+                        dst['w'], dst['s'] = nd_w, nd_s
+                        improved = True
+                        break
+                if improved: break
+            
+            if improved: continue
+            
+            # 2. Try Swap
+            for i, m1 in enumerate(src['models']):
+                m1w = m1.req_rate / m1.slo
+                m1s = m1.model_size
+                
+                for dst_idx in range(gpu_num):
+                    if dst_idx == src_idx: continue
+                    dst = bins[dst_idx]
+                    # Optimization: Skip if dst is also high pressure
+                    if calc_k(dst['w'], dst['s']) > max_k * 0.9: continue
+
+                    for j, m2 in enumerate(dst['models']):
+                        m2w = m2.req_rate / m2.slo
+                        m2s = m2.model_size
+                        
+                        # New Source
+                        ns_s = src['s'] - m1s + m2s
+                        if ns_s > GPU_MEM_SIZE: continue
+                        ns_w = src['w'] - m1w + m2w
+                        ns_k = calc_k(ns_w, ns_s)
+                        
+                        if ns_k >= max_k - 1e-9: continue
+                        
+                        # New Dest
+                        nd_s = dst['s'] - m2s + m1s
+                        if nd_s > GPU_MEM_SIZE: continue
+                        nd_w = dst['w'] - m2w + m1w
+                        nd_k = calc_k(nd_w, nd_s)
+                        
+                        if nd_k < max_k - 1e-9:
+                            # Apply Swap
+                            src['models'][i] = m2
+                            src['w'], src['s'] = ns_w, ns_s
+                            dst['models'][j] = m1
+                            dst['w'], dst['s'] = nd_w, nd_s
+                            improved = True
+                            break
+                    if improved: break
+                if improved: break
+                
+            if not improved: break
+            
+        return {i: bins[i]['models'] for i in range(gpu_num)}
+
+    def check_feasibility_beam(k_target, beam_width=5):
+        """
+        Check if models can be placed using Beam Search.
+        Objective: Minimize K implies Maximize utilization under constraint.
+        Constraint: w + k*s <= k*C
+        """
+        limit = k_target * GPU_MEM_SIZE
+        
+        # Prepare items: sort by linearized cost
+        # w + k*s
+        weighted_items = []
+        for it in items:
+            cost = it['w'] + k_target * it['s']
+            if cost > limit + 1e-5: return None # Impossible single item
+            weighted_items.append((cost, it))
+        
+        # BFD: Sort descending
+        weighted_items.sort(key=lambda x: x[0], reverse=True)
+        
+        # State: (score, loads_tuple, placement_list_of_lists)
+        # We use tuple for loads to be hashable if needed, but here just for structure
+        # Loads track 'linearized load'
+        
+        # Initial Beam
+        # (score, [load_0, ..., load_m], [ [item_indices], ... ])
+        # Score: Sum of squares of loads (encourages imbalance/filling bins)
+        init_loads = tuple([0.0] * gpu_num)
+        init_placements = tuple([[] for _ in range(gpu_num)])
+        
+        beam = [(0.0, init_loads, init_placements)]
+        
+        for cost, it in weighted_items:
+            next_beam = []
+            # Optimization: Use set of seen load configurations (sorted) to prune duplicates
+            seen_configs = set()
+            
+            for score, loads, placements in beam:
+                # Try putting item in each unique load bin
+                # Since GPUs are identical, we only need to try one bin for each unique load value
+                tried_load_vals = set()
+                
                 for i in range(gpu_num):
-                    new_s = gpu_state[i]['s'] + item['s']
-                    if new_s > GPU_MEM_SIZE: continue
-
-                    new_w = gpu_state[i]['w'] + item['w']
-                    rem_mem = GPU_MEM_SIZE - new_s
-
-                    # KVPR Check
-                    if rem_mem <= 1e-9:
-                        if k_target > 1e12: pass
-                        elif new_w > 1e-9: continue
-                    elif new_w > k_target * rem_mem + 1e-9:
+                    curr_load = loads[i]
+                    if curr_load in tried_load_vals:
                         continue
-
-                    # Best Fit: Maximize w + k*s
-                    current_fill = new_w + k_target * new_s
-                    if current_fill > best_fill:
-                        best_fill = current_fill
-                        best_idx = i
-
-                if best_idx != -1:
-                    placement[best_idx].append(item['model'])
-                    gpu_state[best_idx]['s'] += item['s']
-                    gpu_state[best_idx]['w'] += item['w']
-                else:
-                    return None
-            return placement
-
-        # 1. Deterministic Strategies
-        # Sort keys (descending)
-        strategies = [
-            lambda x: x['w'] + k_target * x['s'],
-            lambda x: x['s'],
-            lambda x: x['w'],
-            lambda x: x['w'] / (x['s'] + 1e-9)
-        ]
-
-        for key in strategies:
-            res = try_pack(sorted(items, key=key, reverse=True))
-            if res: return res
-
-        # 2. Randomized Strategies (Shuffle + Best Fit)
-        rng = random.Random(42)
-        indices = list(range(len(items)))
-        for _ in range(50):
-            rng.shuffle(indices)
-            res = try_pack([items[i] for i in indices])
-            if res: return res
-
-        return None
-
-    # Binary Search for the Minimum Maximum KVPR (K)
-
-    # Initialization
+                    
+                    if curr_load + cost <= limit + 1e-5:
+                        tried_load_vals.add(curr_load)
+                        
+                        # Construct new state
+                        new_loads_list = list(loads)
+                        new_loads_list[i] += cost
+                        
+                        # Create canonical configuration for checking duplicates
+                        # Sort loads to detect symmetry
+                        canon_loads = sorted(new_loads_list, reverse=True)
+                        canon_tuple = tuple(canon_loads)
+                        
+                        if canon_tuple not in seen_configs:
+                            seen_configs.add(canon_tuple)
+                            
+                            # Update placement
+                            new_placements_list = list(placements)
+                            new_placements_list[i] = placements[i] + [it['model']]
+                            
+                            # Heuristic score: Sum of squares of loads
+                            new_score = sum(l*l for l in new_loads_list)
+                            
+                            next_beam.append((new_score, tuple(new_loads_list), tuple(new_placements_list)))
+            
+            if not next_beam:
+                return None
+            
+            # Prune
+            next_beam.sort(key=lambda x: x[0], reverse=True)
+            beam = next_beam[:beam_width]
+            
+        # Return best
+        best = beam[0]
+        return {i: best[2][i] for i in range(gpu_num)}
+
+    # Binary Search
     high = 1e9
-    best_placement = check_placement(high)
-
-    if best_placement is None:
-        raise ValueError("Unable to place models on GPUs (insufficient total memory).")
-
-    # Refine 'high' based on found solution
-    current_max = 0.0
-    for gpu_p in best_placement.values():
-        w_sum = sum(m.req_rate / m.slo for m in gpu_p)
-        s_sum = sum(m.model_size for m in gpu_p)
-        rem = GPU_MEM_SIZE - s_sum
-        if rem > 1e-9:
-            current_max = max(current_max, w_sum / rem)
-        elif w_sum > 0:
-            current_max = high
-
-    high = current_max
+    
+    # Initial check
+    best_placement = check_feasibility_beam(high)
+    if not best_placement:
+        raise ValueError("Insufficient memory.")
+        
+    best_placement = hill_climb(best_placement)
+    high = get_max_kvpr(best_placement)
     low = 0.0
-
-    # Binary Search Loop
-    for _ in range(30):
+    
+    # Search
+    for _ in range(25):
+        if high - low < 1e-4: break
         mid = (low + high) / 2
-        result = check_placement(mid)
-        if result is not None:
-            best_placement = result
-
-            # Refine high bound using the actual max KVPR of the valid solution
-            actual_max = 0.0
-            for gpu_p in best_placement.values():
-                w_sum = sum(m.req_rate / m.slo for m in gpu_p)
-                s_sum = sum(m.model_size for m in gpu_p)
-                rem = GPU_MEM_SIZE - s_sum
-                if rem > 1e-9:
-                    val = w_sum / rem
-                elif w_sum > 0:
-                    val = float('inf')
-                else:
-                    val = 0.0
-                if val > actual_max:
-                    actual_max = val
-
-            high = min(mid, actual_max)
+        
+        res = check_feasibility_beam(mid)
+        if res:
+            # Optimize to find true max pressure
+            res = hill_climb(res)
+            max_k = get_max_kvpr(res)
+            best_placement = res
+            high = min(mid, max_k)
         else:
             low = mid
-
+            
     return best_placement
+
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")