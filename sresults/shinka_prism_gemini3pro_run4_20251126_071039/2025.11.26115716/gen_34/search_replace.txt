<NAME>
stochastic_packing_and_swaps
</NAME>

<DESCRIPTION>
1. Add `import random` to the module.
2. Add a new greedy heuristic based on linearized KVPR estimation (w + K*s) to improve initial placement quality.
3. Enhance the Binary Search feasibility check by introducing a "Noisy Best Fit" strategy: if the deterministic packing fails for a target KVPR, the algorithm retries multiple times with slightly perturbed item sort orders. This helps escape local optima in the bin packing problem.
4. Improve the Local Search phase by adding a Swap operation (exchanging models between the bottleneck GPU and others) in addition to the existing Move operation, allowing for finer balancing of pressure.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""

GPU_MEM_SIZE = 80  # GB

def compute_model_placement(gpu_num, models):
=======
"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""

import random

GPU_MEM_SIZE = 80  # GB

def compute_model_placement(gpu_num, models):
>>>>>>> REPLACE
<<<<<<< SEARCH
    # ---------------------------------------------------------
    # 1. Greedy Heuristics Ensemble
    # ---------------------------------------------------------
    # Fast initial solutions to set an upper bound
    heuristics = [
        # (Key Function, Strategy Name)
        # Strategy 'min_result': Place on GPU minimizing resulting KVPR
        # Strategy 'min_current': Place on GPU minimizing current KVPR (Load Balancing)
        (lambda m: (m.req_rate / m.slo, m.model_size), 'min_result'),
        (lambda m: m.model_size, 'min_result'),
        (lambda m: (m.req_rate / m.slo) / (GPU_MEM_SIZE - m.model_size + 1e-6), 'min_result'),
        (lambda m: m.req_rate / m.slo, 'min_current'),
    ]

    for key_fn, strategy in heuristics:
        sorted_models = sorted(models, key=key_fn, reverse=True)
=======
    # ---------------------------------------------------------
    # 1. Greedy Heuristics Ensemble
    # ---------------------------------------------------------
    # Fast initial solutions to set an upper bound

    # Estimate K for linearized heuristic
    t_w = sum(m.req_rate / m.slo for m in models)
    t_s = sum(m.model_size for m in models)
    t_rem = gpu_num * GPU_MEM_SIZE - t_s
    k_est = t_w / t_rem if t_rem > 1e-9 else 1.0

    heuristics = [
        # (Key Function, Strategy Name)
        # Strategy 'min_result': Place on GPU minimizing resulting KVPR
        # Strategy 'min_current': Place on GPU minimizing current KVPR (Load Balancing)
        (lambda m: (m.req_rate / m.slo, m.model_size), 'min_result'),
        (lambda m: m.model_size, 'min_result'),
        (lambda m: (m.req_rate / m.slo) / (GPU_MEM_SIZE - m.model_size + 1e-6), 'min_result'),
        (lambda m: m.req_rate / m.slo, 'min_current'),
        # Linearized pressure heuristic: w + K*s
        (lambda m: (m.req_rate / m.slo) + k_est * m.model_size, 'min_result'),
    ]

    for key_fn, strategy in heuristics:
        sorted_models = sorted(models, key=key_fn, reverse=True)
>>>>>>> REPLACE
<<<<<<< SEARCH
    # ---------------------------------------------------------
    # 2. Binary Search on Target KVPR (Transformation to Bin Packing)
    # ---------------------------------------------------------
    # Problem: Minimize K such that sum(w_i)/ (C - sum(s_i)) <= K
    # Equivalent to Bin Packing with item size v_i(K) = s_i + w_i/K, bin capacity C.

    total_w = sum(m.req_rate / m.slo for m in models)
    total_s = sum(m.model_size for m in models)
    rem_global = gpu_num * GPU_MEM_SIZE - total_s

    # Theoretical lower bound: Average case fluid distribution
    if rem_global > 1e-6:
        low = total_w / rem_global
        high = best_score if best_score != float('inf') else 1000.0

        if high > low + 1e-4:
            for _ in range(20):
                mid = (low + high) / 2

                # Sort items by effective size s + w/K descending (Best Fit Decreasing)
                bs_models = sorted(models, key=lambda m: m.model_size + (m.req_rate/m.slo)/mid, reverse=True)

                temp_placement = {i: [] for i in range(gpu_num)}
                gpu_w = [0.0] * gpu_num   # Actual w
                gpu_s = [0.0] * gpu_num   # Actual s
                possible_k = True

                for model in bs_models:
                    w = model.req_rate / model.slo
                    s = model.model_size
                    eff = s + w/mid

                    best_idx = None
                    min_rem_eff = float('inf')

                    # Best Fit on Effective Capacity
                    for i in range(gpu_num):
                        # Must fit physically
                        if gpu_s[i] + s > GPU_MEM_SIZE: continue

                        # Must fit effectively: current_eff + eff <= C
                        curr_eff = gpu_s[i] + gpu_w[i]/mid

                        if curr_eff + eff <= GPU_MEM_SIZE + 1e-6:
                            rem_eff = GPU_MEM_SIZE - (curr_eff + eff)
                            # Best Fit: minimize remaining effective space
                            if rem_eff < min_rem_eff:
                                min_rem_eff = rem_eff
                                best_idx = i

                    if best_idx is None:
                        possible_k = False
                        break

                    temp_placement[best_idx].append(model)
                    gpu_w[best_idx] += w
                    gpu_s[best_idx] += s

                if possible_k:
                    # Check actual score to be sure (approximation errors)
                    actual_score = get_max_kvpr(temp_placement)
                    if actual_score < best_score:
                        best_score = actual_score
                        best_placement = temp_placement
                    # We found a valid config for K=mid, try lower
                    high = mid
                else:
                    low = mid
=======
    # ---------------------------------------------------------
    # 2. Binary Search on Target KVPR (Transformation to Bin Packing)
    # ---------------------------------------------------------
    # Problem: Minimize K such that sum(w_i)/ (C - sum(s_i)) <= K
    # Equivalent to Bin Packing with item size v_i(K) = s_i + w_i/K, bin capacity C.

    total_w = sum(m.req_rate / m.slo for m in models)
    total_s = sum(m.model_size for m in models)
    rem_global = gpu_num * GPU_MEM_SIZE - total_s

    # Theoretical lower bound: Average case fluid distribution
    if rem_global > 1e-6:
        low = total_w / rem_global
        high = best_score if best_score != float('inf') else 1000.0

        if high > low + 1e-4:
            for _ in range(20):
                mid = (low + high) / 2

                # Randomized Best Fit Decreasing Check
                # Try multiple attempts with perturbed sorting keys to escape local optima
                found_placement = None

                for attempt in range(15):
                    noise = 0.0
                    if attempt > 0:
                        noise = 0.05 # 5% noise for subsequent attempts

                    def sort_key(m):
                        eff_size = m.model_size + (m.req_rate / m.slo) / mid
                        if noise > 0:
                            return eff_size * random.uniform(1.0 - noise, 1.0 + noise)
                        return eff_size

                    bs_models = sorted(models, key=sort_key, reverse=True)

                    temp_placement = {i: [] for i in range(gpu_num)}
                    gpu_w = [0.0] * gpu_num
                    gpu_s = [0.0] * gpu_num
                    possible_k = True

                    for model in bs_models:
                        w = model.req_rate / model.slo
                        s = model.model_size
                        eff = s + w/mid

                        best_idx = None
                        min_rem_eff = float('inf')

                        # Best Fit on Effective Capacity
                        for i in range(gpu_num):
                            if gpu_s[i] + s > GPU_MEM_SIZE: continue

                            curr_eff = gpu_s[i] + gpu_w[i]/mid
                            if curr_eff + eff <= GPU_MEM_SIZE + 1e-6:
                                rem_eff = GPU_MEM_SIZE - (curr_eff + eff)
                                if rem_eff < min_rem_eff:
                                    min_rem_eff = rem_eff
                                    best_idx = i

                        if best_idx is None:
                            possible_k = False
                            break

                        temp_placement[best_idx].append(model)
                        gpu_w[best_idx] += w
                        gpu_s[best_idx] += s

                    if possible_k:
                        found_placement = temp_placement
                        break

                if found_placement:
                    actual_score = get_max_kvpr(found_placement)
                    if actual_score < best_score:
                        best_score = actual_score
                        best_placement = found_placement
                    high = mid
                else:
                    low = mid
>>>>>>> REPLACE
<<<<<<< SEARCH
    # ---------------------------------------------------------
    # 3. Local Search Refinement
    # ---------------------------------------------------------
    # Iteratively move models from the bottleneck GPU to others
    for _ in range(50):
        # Find bottleneck GPU
        max_p = -1.0
        src_gpu = -1
        gpu_stats = []

        for i in range(gpu_num):
            assigned = best_placement[i]
            w = sum(m.req_rate / m.slo for m in assigned)
            s = sum(m.model_size for m in assigned)
            rem = GPU_MEM_SIZE - s
            p = w / rem if rem > 1e-9 else float('inf')
            gpu_stats.append({'w': w, 's': s, 'p': p})
            if p > max_p:
                max_p = p
                src_gpu = i

        if src_gpu == -1 or max_p < 1e-9: break

        improved = False
        src_models = best_placement[src_gpu]

        # Try to move one model
        for m_idx, model in enumerate(src_models):
            w = model.req_rate / model.slo
            s = model.model_size

            # Predict source pressure if moved
            src_rem = GPU_MEM_SIZE - (gpu_stats[src_gpu]['s'] - s)
            src_w = gpu_stats[src_gpu]['w'] - w
            # new_src_p = src_w / src_rem

            # We need to find a dst where new_dst_p < max_p
            best_dst = None

            for dst in range(gpu_num):
                if dst == src_gpu: continue
                if gpu_stats[dst]['s'] + s > GPU_MEM_SIZE: continue

                dst_rem = GPU_MEM_SIZE - (gpu_stats[dst]['s'] + s)
                if dst_rem < 1e-9: continue

                dst_w = gpu_stats[dst]['w'] + w
                new_dst_p = dst_w / dst_