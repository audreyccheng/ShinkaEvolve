<NAME>
best_two_block_endgame_bnb_focus
</NAME>

<DESCRIPTION>
I propose three targeted improvements that directly reduce makespan by better resolving conflicts at key points while keeping runtime in check:

1) Add best_two_block memoization and use it in Or-opt moves:
   - Introduce EpisodeContext.best_two_block(seq, block, ...) to evaluate the best insertion position for a contiguous block with deterministic position sampling and caching. Replace the VND Or-opt inner loop with a call to this memoized function. This reuses evaluations across neighborhoods and iterations, leading to faster and more accurate block relocations.

2) Fix LNS reinsertion focus and ordering:
   - Previously, LNS attempted to sort removed transactions by sensitivity but used cur.index(t) after removal, which is wrong (t is no longer in cur). Add an orig_pos mapping to record each removed transaction’s original index and:
     - Sort the reinsertion pool using their original index’s sensitivity.
     - Pass focus_idx=orig_pos[t] to best_two_insertions to bias position sampling near the original region, reducing thrashing and improving rebuild quality.

3) Strengthen beam’s endgame completion with bounded BnB and a small transposition table:
   - Increase ENDGAME_K to 10 and TIME_BUDGET to 0.05s.
   - In bounded_finish’s DFS, compute and use c_partial bounds, and add a small transposition keyed by (remaining set, suffix of the prefix) storing the best partial cost seen to prune duplicates. This consistently finds exact or near-exact completions in the endgame without timeouts.

These changes improve search guidance at construction endgame, local block relocations, and LNS repair—all directly tied to conflict delay reduction and makespan minimization—while leveraging deterministic sampling/memoization to avoid redundant simulator calls.

</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    class EpisodeContext:
        def __init__(self, workload, n):
            self.workload = workload
            self.n = n
            self.cost_cache = {}  # tuple(seq) -> cost
            # best-two cache: (tuple(seq), txn, pos_sig) -> (best_cost, best_pos, second_cost)
            self.best2_cache = {}

        def eval_cost(self, seq):
            key = tuple(seq)
            c = self.cost_cache.get(key)
            if c is None:
                c = self.workload.get_opt_seq_cost(list(seq))
                self.cost_cache[key] = c
            return c

        def pos_samples(self, seq_len, cap=POS_SAMPLE_CAP, focus_idx=None, ring=RING_RADIUS, exhaustive=False):
            """
            Deterministic stratified positions:
              - Always include ends, quartiles, and median.
              - Optionally include a tight ring around focus_idx (±ring).
              - Fill interiors with a low-discrepancy sequence based on golden ratio.
            """
            total_slots = seq_len + 1
            if exhaustive or cap is None or total_slots <= cap:
                pos = list(range(total_slots))
                return pos, ('all', seq_len)  # signature for caching

            anchors = {0, seq_len, seq_len // 2, seq_len // 4, (3 * seq_len) // 4}
            if focus_idx is not None:
                for d in range(-ring, ring + 1):
                    p = focus_idx + d
                    if 0 <= p <= seq_len:
                        anchors.add(p)
            anchors = [p for p in sorted(anchors) if 0 <= p <= seq_len]
            # Low-discrepancy interiors
            interiors = [i for i in range(1, seq_len) if i not in anchors]
            need = max(0, min(cap - len(anchors), len(interiors)))
            # Use additive recurrence with golden ratio to spread deterministically
            phi = (1 + 5**0.5) / 2.0
            step = phi - 1  # ~0.618
            picks = []
            x = 0.0
            for _ in range(need):
                x = (x + step) % 1.0
                idx = int(x * len(interiors))
                picks.append(interiors[idx])
            pos = sorted(set(anchors).union(picks))
            pos_sig = tuple(pos)  # signature used in cache key
            return pos, pos_sig

        def best_two_insertions(self, seq, txn, focus_idx=None, force_all=False):
            """
            Return (best_cost, best_pos, second_cost) for inserting txn into seq.
            Use deterministic positions; reuse results across modules via (seq, txn, pos_sig) cache.
            """
            seq_len = len(seq)
            exhaustive = force_all or (seq_len <= EXHAUSTIVE_THRESHOLD)
            positions, pos_sig = self.pos_samples(seq_len, focus_idx=focus_idx, exhaustive=exhaustive)
            key = (tuple(seq), txn, pos_sig)
            cached = self.best2_cache.get(key)
            if cached is not None:
                return cached
            best_c = float('inf')
            best_p = 0
            second_c = float('inf')
            for p in positions:
                cand = seq[:]
                cand.insert(p, txn)
                c = self.eval_cost(cand)
                if c < best_c:
                    second_c = best_c
                    best_c = c
                    best_p = p
                elif c < second_c:
                    second_c = c
            if second_c == float('inf'):
                second_c = best_c
            res = (best_c, best_p, second_c)
            self.best2_cache[key] = res
            return res
=======
    class EpisodeContext:
        def __init__(self, workload, n):
            self.workload = workload
            self.n = n
            self.cost_cache = {}  # tuple(seq) -> cost
            # best-two cache: (tuple(seq), obj_key, pos_sig) -> (best_cost, best_pos, second_cost)
            # where obj_key is ('txn', txn) or ('block', tuple(block))
            self.best2_cache = {}

        def eval_cost(self, seq):
            key = tuple(seq)
            c = self.cost_cache.get(key)
            if c is None:
                c = self.workload.get_opt_seq_cost(list(seq))
                self.cost_cache[key] = c
            return c

        def pos_samples(self, seq_len, cap=POS_SAMPLE_CAP, focus_idx=None, ring=RING_RADIUS, exhaustive=False):
            """
            Deterministic stratified positions:
              - Always include ends, quartiles, and median.
              - Optionally include a tight ring around focus_idx (±ring).
              - Fill interiors with a low-discrepancy sequence based on golden ratio.
            """
            total_slots = seq_len + 1
            if exhaustive or cap is None or total_slots <= cap:
                pos = list(range(total_slots))
                return pos, ('all', seq_len)  # signature for caching

            anchors = {0, seq_len, seq_len // 2, seq_len // 4, (3 * seq_len) // 4}
            if focus_idx is not None:
                for d in range(-ring, ring + 1):
                    p = focus_idx + d
                    if 0 <= p <= seq_len:
                        anchors.add(p)
            anchors = [p for p in sorted(anchors) if 0 <= p <= seq_len]
            # Low-discrepancy interiors
            interiors = [i for i in range(1, seq_len) if i not in anchors]
            need = max(0, min(cap - len(anchors), len(interiors)))
            # Use additive recurrence with golden ratio to spread deterministically
            phi = (1 + 5**0.5) / 2.0
            step = phi - 1  # ~0.618
            picks = []
            x = 0.0
            for _ in range(need):
                x = (x + step) % 1.0
                idx = int(x * len(interiors))
                picks.append(interiors[idx])
            pos = sorted(set(anchors).union(picks))
            pos_sig = tuple(pos)  # signature used in cache key
            return pos, pos_sig

        def best_two_insertions(self, seq, txn, focus_idx=None, force_all=False):
            """
            Return (best_cost, best_pos, second_cost) for inserting txn into seq.
            Use deterministic positions; reuse results across modules via (seq, ('txn',txn), pos_sig) cache.
            """
            seq_len = len(seq)
            exhaustive = force_all or (seq_len <= EXHAUSTIVE_THRESHOLD)
            positions, pos_sig = self.pos_samples(seq_len, focus_idx=focus_idx, exhaustive=exhaustive)
            key = (tuple(seq), ('txn', txn), pos_sig)
            cached = self.best2_cache.get(key)
            if cached is not None:
                return cached
            best_c = float('inf')
            best_p = 0
            second_c = float('inf')
            for p in positions:
                cand = seq[:]
                cand.insert(p, txn)
                c = self.eval_cost(cand)
                if c < best_c:
                    second_c = best_c
                    best_c = c
                    best_p = p
                elif c < second_c:
                    second_c = c
            if second_c == float('inf'):
                second_c = best_c
            res = (best_c, best_p, second_c)
            self.best2_cache[key] = res
            return res

        def best_two_block(self, seq, block, focus_idx=None, force_all=False):
            """
            Return (best_cost, best_pos, second_cost) for inserting a contiguous block into seq.
            Shares the same deterministic position sampling and memoization.
            """
            seq_len = len(seq)
            exhaustive = force_all or (seq_len <= EXHAUSTIVE_THRESHOLD)
            positions, pos_sig = self.pos_samples(seq_len, focus_idx=focus_idx, exhaustive=exhaustive)
            block_key = ('block', tuple(block))
            key = (tuple(seq), block_key, pos_sig)
            cached = self.best2_cache.get(key)
            if cached is not None:
                return cached
            best_c = float('inf')
            best_p = 0
            second_c = float('inf')
            for p in positions:
                cand = seq[:]
                cand[p:p] = block
                c = self.eval_cost(cand)
                if c < best_c:
                    second_c = best_c
                    best_c = c
                    best_p = p
                elif c < second_c:
                    second_c = c
            if second_c == float('inf'):
                second_c = best_c
            res = (best_c, best_p, second_c)
            self.best2_cache[key] = res
            return res
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    def beam_search():
        bw = beam_width
        diversity_quota = max(1, int(diversity_quota_ratio * bw))
        ENDGAME_K = 8  # bounded enumeration when remaining <= K
        TIME_BUDGET = 0.03
=======
    def beam_search():
        bw = beam_width
        diversity_quota = max(1, int(diversity_quota_ratio * bw))
        ENDGAME_K = 10  # bounded enumeration when remaining <= K
        TIME_BUDGET = 0.05
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
        # helper: bounded finish via DFS using best-insertion at each step
        def bounded_finish(prefix, rem_set):
            start_t = time.time()

            # greedy upper bound using best-insertion
            def greedy_complete(prefix, rem_list):
                cur = list(prefix)
                rem = list(rem_list)
                while rem:
                    best_t, best_p, best_c = None, 0, float('inf')
                    for t in rem:
                        c_ins, p_ins, _ = ctx.best_two_insertions(cur, t, force_all=(len(cur) <= EXHAUSTIVE_THRESHOLD))
                        if c_ins < best_c:
                            best_c, best_t, best_p = c_ins, t, p_ins
                    cur.insert(best_p, best_t)
                    rem.remove(best_t)
                return ctx.eval_cost(cur), cur

            ub_cost, ub_seq = greedy_complete(prefix, list(rem_set))
            best_cost_local = ub_cost
            best_seq_local = ub_seq

            cache = {}

            def dfs(cur_seq, rem_fs):
                nonlocal best_cost_local, best_seq_local
                if time.time() - start_t > TIME_BUDGET:
                    return
                if not rem_fs:
                    c = ctx.eval_cost(cur_seq)
                    if c < best_cost_local:
                        best_cost_local, best_seq_local = c, list(cur_seq)
                    return
                key = (tuple(cur_seq), rem_fs)
                if cache.get(key, float('inf')) <= best_cost_local + 1e-12:
                    return
                cache[key] = best_cost_local

                # order candidates by best-insertion cost
                order = []
                for t in rem_fs:
                    c_ins, p_ins, _ = ctx.best_two_insertions(cur_seq, t, force_all=(len(cur_seq) <= EXHAUSTIVE_THRESHOLD))
                    order.append((c_ins, p_ins, t))
                order.sort(key=lambda x: x[0])

                for c_ins, p_ins, t in order:
                    if c_ins >= best_cost_local - 1e-12:
                        continue
                    nxt = cur_seq[:]
                    nxt.insert(p_ins, t)
                    dfs(nxt, frozenset(x for x in rem_fs if x != t))

            dfs(list(prefix), frozenset(rem_set))
            return best_cost_local, best_seq_local
=======
        # helper: bounded finish via DFS using best-insertion at each step
        def bounded_finish(prefix, rem_set):
            start_t = time.time()

            # greedy upper bound using best-insertion
            def greedy_complete(prefix, rem_list):
                cur = list(prefix)
                rem = list(rem_list)
                while rem:
                    best_t, best_p, best_c = None, 0, float('inf')
                    for t in rem:
                        c_ins, p_ins, _ = ctx.best_two_insertions(cur, t, force_all=(len(cur) <= EXHAUSTIVE_THRESHOLD))
                        if c_ins < best_c:
                            best_c, best_t, best_p = c_ins, t, p_ins
                    cur.insert(best_p, best_t)
                    rem.remove(best_t)
                return ctx.eval_cost(cur), cur

            ub_cost, ub_seq = greedy_complete(prefix, list(rem_set))
            best_cost_local = ub_cost
            best_seq_local = ub_seq

            # small transposition: map (remaining set, suffix of prefix) -> best partial cost seen
            tt = {}

            def dfs(cur_seq, rem_fs):
                nonlocal best_cost_local, best_seq_local
                if time.time() - start_t > TIME_BUDGET:
                    return
                # partial bound on current prefix
                c_partial = ctx.eval_cost(cur_seq)
                if c_partial >= best_cost_local - 1e-12:
                    return
                if not rem_fs:
                    if c_partial < best_cost_local:
                        best_cost_local, best_seq_local = c_partial, list(cur_seq)
                    return
                key = (rem_fs, tuple(cur_seq[-3:]) if len(cur_seq) >= 3 else tuple(cur_seq))
                prev = tt.get(key)
                if prev is not None and c_partial >= prev - 1e-12:
                    return
                # store improved partial bound
                tt[key] = c_partial if prev is None else min(prev, c_partial)

                # order candidates by best-insertion cost
                order = []
                for t in rem_fs:
                    c_ins, p_ins, _ = ctx.best_two_insertions(cur_seq, t, force_all=(len(cur_seq) <= EXHAUSTIVE_THRESHOLD))
                    order.append((c_ins, p_ins, t))
                order.sort(key=lambda x: x[0])

                for c_ins, p_ins, t in order:
                    if c_ins >= best_cost_local - 1e-12:
                        continue
                    nxt = cur_seq[:]
                    nxt.insert(p_ins, t)
                    dfs(nxt, frozenset(x for x in rem_fs if x != t))

            dfs(list(prefix), frozenset(rem_set))
            return best_cost_local, best_seq_local
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    def or_opt_pass(seq, start_cost, k, pos_cap=POS_SAMPLE_CAP):
        best_seq = list(seq)
        best_cost = start_cost
        L = len(best_seq)
        if L <= k:
            return best_seq, best_cost, False
        improved = False
        i = 0
        while i <= len(best_seq) - k:
            block = best_seq[i:i + k]
            base = best_seq[:i] + best_seq[i + k:]
            m = len(base)
            # Use deterministic positions (reuse pos signature)
            positions, _sig = ctx.pos_samples(m, cap=pos_cap, focus_idx=i, ring=2, exhaustive=(m + 1 <= EXHAUSTIVE_THRESHOLD))
            move_best_c = best_cost
            move_best_p = None
            for p in positions:
                cand = base[:]
                cand[p:p] = block
                c = ctx.eval_cost(cand)
                if c < move_best_c:
                    move_best_c = c
                    move_best_p = p
            if move_best_p is not None and move_best_c + 1e-12 < best_cost:
                new_seq = base[:]
                new_seq[move_best_p:move_best_p] = block
                best_seq = new_seq
                best_cost = move_best_c
                improved = True
                i = 0  # restart scan after structural change
            else:
                i += 1
        return best_seq, best_cost, improved
=======
    def or_opt_pass(seq, start_cost, k, pos_cap=POS_SAMPLE_CAP):
        best_seq = list(seq)
        best_cost = start_cost
        L = len(best_seq)
        if L <= k:
            return best_seq, best_cost, False
        improved = False
        i = 0
        while i <= len(best_seq) - k:
            block = best_seq[i:i + k]
            base = best_seq[:i] + best_seq[i + k:]
            # Use block-aware best-two with deterministic positions and cache reuse
            move_best_c, move_best_p, _ = ctx.best_two_block(base, block, focus_idx=i, force_all=(len(base) <= EXHAUSTIVE_THRESHOLD))
            if move_best_c + 1e-12 < best_cost:
                new_seq = base[:]
                new_seq[move_best_p:move_best_p] = block
                best_seq = new_seq
                best_cost = move_best_c
                improved = True
                i = 0  # restart scan after structural change
            else:
                i += 1
        return best_seq, best_cost, improved
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    def lns_ruin_and_repair(seq, start_cost, rounds=LNS_ROUNDS):
        best_seq = list(seq)
        best_cost = start_cost
        stagnation = 0

        for _ in range(max(1, rounds)):
            cur = list(best_seq)
            L = len(cur)
            if L <= 6:
                break

            # Decide removal size with escalation
            factor = 1.0 + (0.25 * (stagnation // 2))
            m = int(min(LNS_REMOVE_MAX, max(LNS_REMOVE_MIN, LNS_REMOVE_FRAC * L * factor)))
            m = min(m, L - 2)

            sens = sensitivity_rank(cur, P=6)
            high_sens = max(1, int(0.4 * m))
            remove_idx = set(i for _, i in sens[:high_sens])

            # Add contiguous block(s)
            remaining = m - len(remove_idx)
            blocks = 2 if stagnation >= 2 else 1
            for _b in range(blocks):
                if remaining <= 0:
                    break
                blk_len = max(2, min(remaining, max(2, L // 12)))
                start = random.randint(0, max(0, L - blk_len))
                for j in range(start, start + blk_len):
                    remove_idx.add(j)
                    if len(remove_idx) >= m:
                        break
                remaining = m - len(remove_idx)

            # Fill if short
            while len(remove_idx) < m:
                remove_idx.add(random.randint(0, L - 1))

            remove_idx = sorted(remove_idx)
            removed = [cur[i] for i in remove_idx]
            base = [cur[i] for i in range(L) if i not in remove_idx]

            # Regret-guided repair using shared best-two cache
            rebuilt = list(base)
            pool = list(removed)
            # deterministic reinsertion order: by sensitivity (higher first)
            pool_sens = {i: s for s, i in sens}
            pool.sort(key=lambda t: -pool_sens.get(cur.index(t) if t in cur else 0.0))
            while pool:
                # evaluate all when small; else subset
                exhaustive = (len(rebuilt) <= EXHAUSTIVE_THRESHOLD) or (len(pool) <= 2 * beam_width)
                cand_txns = pool if len(pool) <= 10 else random.sample(pool, 10)
                scored = []
                for t in cand_txns:
                    # optional focus: aim near middle
                    c1, p1, c2 = ctx.best_two_insertions(rebuilt, t, focus_idx=None, force_all=exhaustive)
                    scored.append((c1, c2, t, p1))
                if not scored:
                    t = pool.pop()
                    rebuilt.append(t)
                    continue
                # choose highest regret among top-3 by best cost
                scored.sort(key=lambda x: x[0])
                top = scored[:min(3, len(scored))]
                top.sort(key=lambda x: (-(x[1] - x[0]), x[0]))
                c1, c2, t_pick, p_pick = top[0]
                rebuilt.insert(p_pick, t_pick)
                pool.remove(t_pick)

            c_new = ctx.eval_cost(rebuilt)
            if c_new + 1e-12 < best_cost:
                best_cost = c_new
                best_seq = rebuilt
                stagnation = 0
                # quick micro-polish
                best_seq, best_cost = or_opt_pass(best_seq, best_cost, 1)[0:2]
            else:
                stagnation += 1

        return best_seq, best_cost
=======
    def lns_ruin_and_repair(seq, start_cost, rounds=LNS_ROUNDS):
        best_seq = list(seq)
        best_cost = start_cost
        stagnation = 0

        for _ in range(max(1, rounds)):
            cur = list(best_seq)
            L = len(cur)
            if L <= 6:
                break

            # Decide removal size with escalation
            factor = 1.0 + (0.25 * (stagnation // 2))
            m = int(min(LNS_REMOVE_MAX, max(LNS_REMOVE_MIN, LNS_REMOVE_FRAC * L * factor)))
            m = min(m, L - 2)

            sens = sensitivity_rank(cur, P=6)
            high_sens = max(1, int(0.4 * m))
            remove_idx = set(i for _, i in sens[:high_sens])

            # Add contiguous block(s)
            remaining = m - len(remove_idx)
            blocks = 2 if stagnation >= 2 else 1
            for _b in range(blocks):
                if remaining <= 0:
                    break
                blk_len = max(2, min(remaining, max(2, L // 12)))
                start = random.randint(0, max(0, L - blk_len))
                for j in range(start, start + blk_len):
                    remove_idx.add(j)
                    if len(remove_idx) >= m:
                        break
                remaining = m - len(remove_idx)

            # Fill if short
            while len(remove_idx) < m:
                remove_idx.add(random.randint(0, L - 1))

            remove_idx = sorted(remove_idx)
            removed = [cur[i] for i in remove_idx]
            base = [cur[i] for i in range(L) if i not in remove_idx]

            # Map txn -> original index for focused reinsertion and sensitivity reference
            orig_pos = {cur[i]: i for i in remove_idx}
            sens_by_idx = {i: s for s, i in sens}

            # Regret-guided repair using shared best-two cache
            rebuilt = list(base)
            pool = list(removed)
            # deterministic reinsertion order: by sensitivity at original index (higher first)
            pool.sort(key=lambda t: -sens_by_idx.get(orig_pos.get(t, 0), 0.0))
            while pool:
                # evaluate all when small; else subset
                exhaustive = (len(rebuilt) <= EXHAUSTIVE_THRESHOLD) or (len(pool) <= 2 * beam_width)
                cand_txns = pool if len(pool) <= 10 else random.sample(pool, 10)
                scored = []
                for t in cand_txns:
                    focus = min(orig_pos.get(t, len(rebuilt)), len(rebuilt))
                    c1, p1, c2 = ctx.best_two_insertions(rebuilt, t, focus_idx=focus, force_all=exhaustive)
                    scored.append((c1, c2, t, p1))
                if not scored:
                    t = pool.pop()
                    rebuilt.append(t)
                    continue
                # choose highest regret among top-3 by best cost
                scored.sort(key=lambda x: x[0])
                top = scored[:min(3, len(scored))]
                top.sort(key=lambda x: (-(x[1] - x[0]), x[0]))
                c1, c2, t_pick, p_pick = top[0]
                rebuilt.insert(p_pick, t_pick)
                pool.remove(t_pick)

            c_new = ctx.eval_cost(rebuilt)
            if c_new + 1e-12 < best_cost:
                best_cost = c_new
                best_seq = rebuilt
                stagnation = 0
                # quick micro-polish
                best_seq, best_cost = or_opt_pass(best_seq, best_cost, 1)[0:2]
            else:
                stagnation += 1

        return best_seq, best_cost
>>>>>>> REPLACE
</DIFF>