<NAME>
regret_diverse_beam_and_oropt
</NAME>

<DESCRIPTION>
I introduce two complementary improvements that directly target makespan reduction by better handling conflicts during construction and providing stronger local optimizations:

1) Regret-weighted diverse beam expansion with suffix-2 diversity:
   - For each beam state, we score all candidate next transactions using a two-step lookahead. We then keep a small set of the best per-state expansions (top-4) and compute a regret measure (difference between best and second-best).
   - During global pruning to beam width, we first keep about 70% of survivors by best score while enforcing diversity by the last-2 transaction suffix signature to avoid converging to near-identical prefixes. We then fill remaining slots with candidates that have the highest regret (opportunities) even if their score is slightly worse, again enforcing suffix diversity. This balances exploitation and exploration in a way tied to actual simulated makespan, not proxy heuristics.

2) Stronger local search via Or-opt (block reinsertions of sizes 1â€“3):
   - I upgrade local_reinsert_refine to perform Or-opt style block relocations (sizes 1, 2, and 3) with best-improving acceptance, sampling insertion positions when long. This captures larger structural improvements (e.g., moving write-heavy blocks across conflicting readers/writers) that adjacent swaps alone miss.

These changes are lightweight to integrate, compatible with the current caching and simulator API, and improve both construction quality and post-optimization depth without exploding runtime.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    def local_reinsert_refine(seq, max_passes=2, pos_sample_limit=25):
        """
        Reinsertion local search: try moving a transaction to a different position.
        Evaluates all positions for small n; otherwise samples positions.
        """
        current = list(seq)
        best_c = eval_cost(current)
        passes = 0
        while passes < max_passes:
            improved = False
            indices = list(range(len(current)))
            random.shuffle(indices)
            for i in indices:
                base = current[:]
                t = base.pop(i)
                m = len(base) + 1
                # Determine candidate positions
                if m <= pos_sample_limit + 1:
                    positions = list(range(m))
                else:
                    # Always include ends; sample interior
                    k = min(pos_sample_limit, m - 1)
                    interior = list(range(1, m - 1))
                    sampled = set(random.sample(interior, k)) if interior else set()
                    sampled.update({0, m - 1})
                    positions = sorted(sampled)
                best_local_c = float('inf')
                best_pos = None
                for pos in positions:
                    cand = base[:]
                    cand.insert(pos, t)
                    c = eval_cost(cand)
                    if c < best_local_c:
                        best_local_c = c
                        best_pos = pos
                        # Early exit on clear improvement over current best
                if best_local_c + 1e-9 < best_c and best_pos is not None:
                    base.insert(best_pos, t)
                    current, best_c = base, best_local_c
                    improved = True
                    break  # restart search from new incumbent
            passes += 1
            if not improved:
                break
        return best_c, current
=======
    def local_reinsert_refine(seq, max_passes=2, pos_sample_limit=25):
        """
        Or-opt reinsertion local search: move contiguous blocks (sizes 1..3) to the best position.
        Evaluates all positions for small n; otherwise samples interior while keeping ends.
        Best-improving acceptance with restarts per improvement.
        """
        current = list(seq)
        best_c = eval_cost(current)
        if len(current) <= 2:
            return best_c, current
        passes = 0
        while passes < max_passes:
            improved = False
            # Try larger blocks first to escape local minima
            for blk in (3, 2, 1):
                if blk > len(current):
                    continue
                indices = list(range(0, len(current) - blk + 1))
                random.shuffle(indices)
                moved = False
                for i in indices:
                    # Remove block [i : i+blk)
                    left = current[:i]
                    right = current[i + blk:]
                    base = left + right
                    block = current[i:i + blk]
                    m = len(base) + 1  # insertion slots
                    # Determine candidate positions
                    if m <= pos_sample_limit + 1:
                        positions = list(range(m))
                    else:
                        k = min(pos_sample_limit, m - 1)
                        interior = list(range(1, m - 1))
                        sampled = set(random.sample(interior, k)) if interior else set()
                        sampled.update({0, m - 1})
                        positions = sorted(sampled)
                    move_best_c = float('inf')
                    move_best_pos = None
                    for pos in positions:
                        cand = base[:]
                        cand[pos:pos] = block
                        c = eval_cost(cand)
                        if c < move_best_c:
                            move_best_c = c
                            move_best_pos = pos
                    if move_best_pos is not None and move_best_c + 1e-9 < best_c:
                        base[move_best_pos:move_best_pos] = block
                        current = base
                        best_c = move_best_c
                        improved = True
                        moved = True
                        break  # restart after an improving move
                if moved:
                    break
            passes += 1
            if not improved:
                break
        return best_c, current
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    def beam_search(beam_width, cand_per_state, lookahead_k=5, alpha=0.7):
        """
        Beam search guided by partial-prefix costs with shallow two-step lookahead.

        beam_width: number of partial sequences to keep at each depth
        cand_per_state: number of candidates to expand per beam state when remaining is large
        lookahead_k: number of second-step candidates to sample for scoring
        alpha: weight on immediate cost vs lookahead (0..1)
        """
        # Evaluate all single-transaction starters and pick top starters
        starters = []
        for t in range(n):
            c = eval_cost([t])
            starters.append((c, [t]))
        starters.sort(key=lambda x: x[0])

        init_count = min(max(beam_width * 2, beam_width), n)
        init_beam = []
        used_prefixes = set()
        for c, seq in starters[:init_count]:
            key = tuple(seq)
            if key in used_prefixes:
                continue
            used_prefixes.add(key)
            rem = tuple(x for x in range(n) if x != seq[0])
            init_beam.append((c, seq, rem))

        # Add a GRASP-style seed to diversify
        grasp_c, grasp_seq = construct_grasp_seed()
        rem = tuple(x for x in range(n) if x not in set(grasp_seq))
        if len(grasp_seq) == n:
            init_beam.append((grasp_c, grasp_seq, ()))
        else:
            init_beam.append((grasp_c, grasp_seq, rem))

        best_complete = (float('inf'), [])
        beam = init_beam

        # Progressively grow prefixes
        for depth in range(1, n + 1):
            next_candidates = []
            for cost_so_far, prefix, remaining in beam:
                rem_list = list(remaining)
                if not rem_list:
                    # Completed
                    if cost_so_far < best_complete[0]:
                        best_complete = (cost_so_far, prefix)
                    # Keep as candidate (no further expansion)
                    next_candidates.append((cost_so_far, cost_so_far, prefix, remaining))
                    continue

                # Decide candidate set size
                if len(rem_list) <= min(cand_per_state, 16):
                    expand_list = rem_list
                else:
                    k = min(cand_per_state, len(rem_list))
                    expand_list = random.sample(rem_list, k)

                # Expand each candidate and score by immediate + lookahead
                for t in expand_list:
                    new_prefix = prefix + [t]
                    c1 = eval_cost(new_prefix)
                    rem_after = [x for x in rem_list if x != t]

                    score = c1
                    if rem_after:
                        # sample second-step candidates
                        k2 = min(lookahead_k, len(rem_after))
                        second = random.sample(rem_after, k2)
                        best_c2 = float('inf')
                        for u in second:
                            c2 = eval_cost(new_prefix + [u])
                            if c2 < best_c2:
                                best_c2 = c2
                        # Combine scores
                        if best_c2 < float('inf'):
                            score = alpha * c1 + (1.0 - alpha) * best_c2

                    if len(new_prefix) == n and c1 < best_complete[0]:
                        best_complete = (c1, new_prefix)
                    next_candidates.append((score, c1, new_prefix, tuple(rem_after)))

            if not next_candidates:
                break

            # Prune to beam width by score; keep some diversity
            next_candidates.sort(key=lambda x: x[0])
            pruned = []
            seen = set()
            for score, c1, seq, rem in next_candidates:
                key = tuple(seq)
                if key in seen:
                    continue
                seen.add(key)
                pruned.append((c1, seq, rem))  # store cost_so_far as c1 for next depth
                if len(pruned) >= beam_width:
                    break

            # Inject 1-2 random survivors for diversification if space allows
            if len(pruned) < beam_width and len(next_candidates) > beam_width:
                extras = random.sample(next_candidates[beam_width:], min(2, len(next_candidates) - beam_width))
                for score, c1, seq, rem in extras:
                    key = tuple(seq)
                    if key in seen:
                        continue
                    seen.add(key)
                    pruned.append((c1, seq, rem))
                    if len(pruned) >= beam_width:
                        break

            beam = pruned

        # If we didn't complete during loop, finalize from beam candidates greedily
        for c, seq, rem in beam:
            if len(seq) == n and c < best_complete[0]:
                best_complete = (c, seq)

        if best_complete[1] and len(best_complete[1]) == n:
            return best_complete

        if beam:
            c, seq, rem = min(beam, key=lambda x: x[0])
            rem_list = list(rem)
            cur_seq = list(seq)
            while rem_list:
                best_ext = None
                best_ext_cost = float('inf')
                for t in rem_list:
                    c2 = eval_cost(cur_seq + [t])
                    if c2 < best_ext_cost:
                        best_ext_cost = c2
                        best_ext = t
                cur_seq.append(best_ext)
                rem_list.remove(best_ext)
            final_cost = eval_cost(cur_seq)
            return final_cost, cur_seq

        # Absolute fallback: identity sequence
        identity = list(range(n))
        return eval_cost(identity), identity
=======
    def beam_search(beam_width, cand_per_state, lookahead_k=5, alpha=0.7):
        """
        Beam search with two-step lookahead, suffix-2 diversity, and regret-weighted selection.

        beam_width: number of partial sequences to keep at each depth
        cand_per_state: number of candidates to expand per beam state when remaining is large
        lookahead_k: number of second-step candidates to sample for scoring
        alpha: weight on immediate cost vs lookahead (0..1)
        """
        # Evaluate all single-transaction starters and pick top starters
        starters = []
        for t in range(n):
            c = eval_cost([t])
            starters.append((c, [t]))
        starters.sort(key=lambda x: x[0])

        init_count = min(max(beam_width * 2, beam_width), n)
        init_beam = []
        used_prefixes = set()
        for c, seq in starters[:init_count]:
            key = tuple(seq)
            if key in used_prefixes:
                continue
            used_prefixes.add(key)
            rem = tuple(x for x in range(n) if x != seq[0])
            init_beam.append((c, seq, rem))

        # Add a GRASP-style seed to diversify
        grasp_c, grasp_seq = construct_grasp_seed()
        rem = tuple(x for x in range(n) if x not in set(grasp_seq))
        if len(grasp_seq) == n:
            init_beam.append((grasp_c, grasp_seq, ()))
        else:
            init_beam.append((grasp_c, grasp_seq, rem))

        best_complete = (float('inf'), [])
        beam = init_beam

        # Helper for suffix-2 signature
        def suffix_sig(seq):
            if not seq:
                return ()
            if len(seq) == 1:
                return (seq[-1],)
            return (seq[-2], seq[-1])

        # Progressively grow prefixes
        for depth in range(1, n + 1):
            next_candidates = []  # entries: (score, regret, c1, seq, rem, sig)
            for cost_so_far, prefix, remaining in beam:
                rem_list = list(remaining)
                if not rem_list:
                    # Completed
                    if cost_so_far < best_complete[0]:
                        best_complete = (cost_so_far, prefix)
                    # Keep as candidate (no further expansion)
                    next_candidates.append((cost_so_far, 0.0, cost_so_far, prefix, remaining, suffix_sig(prefix)))
                    continue

                # Decide candidate set size (endgame widening)
                if len(rem_list) <= min(cand_per_state, 16):
                    expand_list = rem_list
                else:
                    k = min(cand_per_state, len(rem_list))
                    expand_list = random.sample(rem_list, k)

                # Expand and score by immediate + lookahead
                local = []
                for t in expand_list:
                    new_prefix = prefix + [t]
                    c1 = eval_cost(new_prefix)
                    rem_after = [x for x in rem_list if x != t]

                    score = c1
                    if rem_after:
                        # Sample second-step candidates; in endgame, evaluate all
                        k2 = len(rem_after) if len(rem_after) <= 6 else min(lookahead_k, len(rem_after))
                        second = rem_after if k2 == len(rem_after) else random.sample(rem_after, k2)
                        best_c2 = float('inf')
                        for u in second:
                            c2 = eval_cost(new_prefix + [u])
                            if c2 < best_c2:
                                best_c2 = c2
                        score = alpha * c1 + (1.0 - alpha) * best_c2

                    if len(new_prefix) == n and c1 < best_complete[0]:
                        best_complete = (c1, new_prefix)
                    local.append((score, c1, new_prefix, tuple(rem_after)))

                if not local:
                    continue
                local.sort(key=lambda x: x[0])
                # compute regret for the best local choice
                best_score = local[0][0]
                second_score = local[1][0] if len(local) > 1 else best_score
                regret = max(0.0, second_score - best_score)
                # Keep a few of the best local moves for global selection
                keep_local = local[:min(4, len(local))]
                for idx, (score, c1, seq_cand, rem_cand) in enumerate(keep_local):
                    reg = regret if idx == 0 else 0.0
                    next_candidates.append((score, reg, c1, seq_cand, rem_cand, suffix_sig(seq_cand)))

            if not next_candidates:
                break

            # Primary selection by score with suffix-2 diversity
            next_candidates.sort(key=lambda x: x[0])
            pruned = []
            used_seq = set()
            seen_suffix = set()
            primary_target = max(1, int(0.7 * beam_width))
            for score, reg, c1, seq_cand, rem_cand, sig in next_candidates:
                key = tuple(seq_cand)
                if sig in seen_suffix or key in used_seq:
                    continue
                pruned.append((c1, seq_cand, rem_cand))
                used_seq.add(key)
                seen_suffix.add(sig)
                if len(pruned) >= primary_target:
                    break

            # Regret-boosted fill with diversity
            if len(pruned) < beam_width:
                remaining = [x for x in next_candidates if tuple(x[3]) not in used_seq]
                remaining.sort(key=lambda x: (-x[1], x[0]))  # high regret first, then better score
                for score, reg, c1, seq_cand, rem_cand, sig in remaining:
                    key = tuple(seq_cand)
                    if key in used_seq or sig in seen_suffix:
                        continue
                    pruned.append((c1, seq_cand, rem_cand))
                    used_seq.add(key)
                    seen_suffix.add(sig)
                    if len(pruned) >= beam_width:
                        break

            # If still short, fill by best score regardless of suffix
            if len(pruned) < beam_width:
                for score, reg, c1, seq_cand, rem_cand, sig in next_candidates:
                    key = tuple(seq_cand)
                    if key in used_seq:
                        continue
                    pruned.append((c1, seq_cand, rem_cand))
                    used_seq.add(key)
                    if len(pruned) >= beam_width:
                        break

            beam = pruned

        # If we didn't complete during loop, finalize from beam candidates greedily
        for c, seq, rem in beam:
            if len(seq) == n and c < best_complete[0]:
                best_complete = (c, seq)

        if best_complete[1] and len(best_complete[1]) == n:
            return best_complete

        if beam:
            c, seq, rem = min(beam, key=lambda x: x[0])
            rem_list = list(rem)
            cur_seq = list(seq)
            while rem_list:
                best_ext = None
                best_ext_cost = float('inf')
                for t in rem_list:
                    c2 = eval_cost(cur_seq + [t])
                    if c2 < best_ext_cost:
                        best_ext_cost = c2
                        best_ext = t
                cur_seq.append(best_ext)
                rem_list.remove(best_ext)
            final_cost = eval_cost(cur_seq)
            return final_cost, cur_seq

        # Absolute fallback: identity sequence
        identity = list(range(n))
        return eval_cost(identity), identity
>>>>>>> REPLACE
</DIFF>