<NAME>
adaptive_beam_vnd_plus
</NAME>

<DESCRIPTION>
I introduce two focused upgrades that target makespan directly via evaluation-driven search:

1) Adaptive beam lookahead and diversity: inside beam_search, I compute a per-state “second-step regret span” and adapt the lookahead blend. If the spread among second-step costs is high, I reduce myopia by using a 0.5/0.5 mix between immediate and lookahead; otherwise I use a greedier 0.8/0.2 mix. I also widen local candidate retention in the endgame (remaining <= 2*beam_width) and adjust layer diversity by reducing the primary score quota to 60% to allow more regret-diverse expansions. This avoids getting stuck in locally good but globally poor orderings.

2) Stronger VND neighborhoods: I add sampled non-adjacent pair swaps and sampled segment reversals (2-opt-like) after reinsertion and adjacent swaps. Both use cost evaluations, accept improvements, and reset search on success. Finally, I add a quick polish pass (reinsertion + adjacent) to consolidate gains. These moves directly reduce conflict chains between read/write operations across transactions.

Together, these changes increase search breadth when the landscape is deceptive, and deepen local refinement to escape plateaus—leading to lower makespan schedules without large runtime overhead.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        for depth in range(1, n + 1):
            next_candidates = []  # entries: (score, regret, c1, seq, rem, sig)
            for cost_so_far, prefix, remaining in beam:
=======
        for depth in range(1, n + 1):
            next_candidates = []  # entries: (score, regret, c1, seq, rem, sig)
            min_remaining = n
            for cost_so_far, prefix, remaining in beam:
>>>>>>> REPLACE

<<<<<<< SEARCH
                # Expand and score by immediate + lookahead
                local = []
                for t in expand_list:
                    new_prefix = prefix + [t]
                    c1 = eval_cost(new_prefix)
                    rem_after = [x for x in rem_list if x != t]

                    score = c1
                    if rem_after:
                        # Sample second-step candidates; in endgame, evaluate all
                        k2 = len(rem_after) if len(rem_after) <= 6 else min(lookahead_k, len(rem_after))
                        second = rem_after if k2 == len(rem_after) else random.sample(rem_after, k2)
                        best_c2 = float('inf')
                        for u in second:
                            c2 = eval_cost(new_prefix + [u])
                            if c2 < best_c2:
                                best_c2 = c2
                        score = alpha * c1 + (1.0 - alpha) * best_c2

                    if len(new_prefix) == n and c1 < best_complete[0]:
                        best_complete = (c1, new_prefix)
                    local.append((score, c1, new_prefix, tuple(rem_after)))
=======
                # Expand and score by immediate + adaptive lookahead
                local = []
                # Collect raw expansion stats first to compute regret span-driven alpha
                raw_moves = []  # (c1, best_c2, span, new_prefix, rem_after)
                for t in expand_list:
                    new_prefix = prefix + [t]
                    c1 = eval_cost(new_prefix)
                    rem_after = [x for x in rem_list if x != t]

                    best_c2 = c1
                    span = 0.0
                    if rem_after:
                        # Sample second-step candidates; in endgame, evaluate all
                        k2 = len(rem_after) if len(rem_after) <= 6 else min(lookahead_k, len(rem_after))
                        second = rem_after if k2 == len(rem_after) else random.sample(rem_after, k2)
                        second_costs = []
                        best_c2 = float('inf')
                        for u in second:
                            c2 = eval_cost(new_prefix + [u])
                            second_costs.append(c2)
                            if c2 < best_c2:
                                best_c2 = c2
                        if second_costs:
                            span = max(second_costs) - min(second_costs)

                    if len(new_prefix) == n and c1 < best_complete[0]:
                        best_complete = (c1, new_prefix)
                    raw_moves.append((c1, best_c2, span, new_prefix, tuple(rem_after)))

                # Compute median span to adapt alpha within this state
                if raw_moves:
                    spans = sorted([s for _, _, s, _, _ in raw_moves])
                    mid = len(spans) // 2
                    median_span = spans[mid] if spans else 0.0
                    # Build local scored list with adaptive mixing
                    for c1, best_c2, span, new_prefix, rem_after in raw_moves:
                        # If high dispersion among second-step costs, rely more on lookahead
                        if span > median_span:
                            a = 0.5
                        else:
                            a = 0.8
                        score = a * c1 + (1.0 - a) * best_c2
                        local.append((score, c1, new_prefix, rem_after))
>>>>>>> REPLACE

<<<<<<< SEARCH
                keep_local = local[:min(4, len(local))]
=======
                # Widen retention in endgame for this state
                keep_k = 6 if len(rem_list) <= 2 * beam_width else 4
                keep_local = local[:min(keep_k, len(local))]
>>>>>>> REPLACE

<<<<<<< SEARCH
            next_candidates.sort(key=lambda x: x[0])
            pruned = []
            used_seq = set()
            seen_suffix = set()
            primary_target = max(1, int(0.7 * beam_width))
=======
            next_candidates.sort(key=lambda x: x[0])
            pruned = []
            used_seq = set()
            seen_suffix = set()
            endgame = (min_remaining <= 2 * beam_width)
            primary_target = max(1, int((0.6 if endgame else 0.7) * beam_width))
>>>>>>> REPLACE

<<<<<<< SEARCH
        return best_c, current

    def construct_grasp_seed():
=======
        return best_c, current

    def local_pair_swaps_sampled(seq, curr_cost, tries=100):
        """
        Sampled non-adjacent pair swaps; accept improving swaps and reset attempts on improvement.
        """
        best_seq = list(seq)
        best_cost = curr_cost
        n_local = len(best_seq)
        if n_local <= 3:
            return best_cost, best_seq
        attempts = 0
        while attempts < tries:
            i = random.randint(0, n_local - 1)
            j = random.randint(0, n_local - 1)
            if i == j or abs(i - j) <= 1:
                attempts += 1
                continue
            cand = best_seq[:]
            cand[i], cand[j] = cand[j], cand[i]
            c = eval_cost(cand)
            if c < best_cost:
                best_cost = c
                best_seq = cand
                attempts = 0  # reset on improvement
            else:
                attempts += 1
        return best_cost, best_seq

    def local_segment_reversals_sampled(seq, curr_cost, tries=80):
        """
        Sampled 2-opt-like segment reversals; accept improving reversals and reset on improvement.
        """
        best_seq = list(seq)
        best_cost = curr_cost
        n_local = len(best_seq)
        if n_local <= 4:
            return best_cost, best_seq
        attempts = 0
        while attempts < tries:
            i = random.randint(0, n_local - 2)
            j = random.randint(i + 1, min(n_local - 1, i + 10))  # cap reversal length
            if j - i < 2:
                attempts += 1
                continue
            cand = best_seq[:]
            cand[i:j + 1] = reversed(cand[i:j + 1])
            c = eval_cost(cand)
            if c < best_cost:
                best_cost = c
                best_seq = cand
                attempts = 0
            else:
                attempts += 1
        return best_cost, best_seq

    def construct_grasp_seed():
>>>>>>> REPLACE

<<<<<<< SEARCH
        c, seq = beam_search(beam_width=beam_width, cand_per_state=cand_per_state, lookahead_k=5, alpha=0.7)

        # Strong local refinements: reinsertion first, then adjacent swaps
        c1, s1 = local_reinsert_refine(seq, max_passes=2, pos_sample_limit=25)
        c2, s2 = local_adjacent_refine(s1, max_passes=2)

        if c2 < c:
            c, seq = c2, s2
        elif c1 < c:
            c, seq = c1, s1
=======
        c, seq = beam_search(beam_width=beam_width, cand_per_state=cand_per_state, lookahead_k=5, alpha=0.7)

        # Strong local refinements: reinsertion -> adjacent swaps -> sampled pair swaps -> segment reversals
        c1, s1 = local_reinsert_refine(seq, max_passes=2, pos_sample_limit=25)
        c2, s2 = local_adjacent_refine(s1, max_passes=2)
        c3, s3 = local_pair_swaps_sampled(s2, c2, tries=max(60, n // 2))
        c4, s4 = local_segment_reversals_sampled(s3, c3, tries=max(50, n // 2))
        # Quick polish
        c5, s5 = local_reinsert_refine(s4, max_passes=1, pos_sample_limit=20)
        c6, s6 = local_adjacent_refine(s5, max_passes=1)

        # Adopt best among refined variants
        best_local_cost = c
        best_local_seq = seq
        for cand_cost, cand_seq in [(c1, s1), (c2, s2), (c3, s3), (c4, s4), (c5, s5), (c6, s6)]:
            if cand_cost < best_local_cost:
                best_local_cost, best_local_seq = cand_cost, cand_seq
        c, seq = best_local_cost, best_local_seq
>>>>>>> REPLACE

</DIFF>