<NAME>
regret_insertion_completion_and_suffix_best
</NAME>

<DESCRIPTION>
I propose two targeted improvements to reduce makespan:

1) Regret-guided insertion for beam completion: The current greedy completion simply appends the remaining transactions at the end based on single-step cost. This often traps the schedule into poor endgame placements. I replace this with a regret-guided best-insertion procedure (using best-two insertion evaluations with a local cache), which inserts each remaining transaction at its best position and selects among candidates by a blend of cost and regret. This yields much stronger completions that better respect conflict structure, reducing the need for large local search corrections and improving final makespan.

2) Smarter suffix diversity in the beam: The current beam enforces a hard suffix-2 uniqueness, which can prematurely prune better expansions that share a suffix. I replace this with a suffix_best map tracking the best c1 (prefix cost) seen for each suffix. We still limit duplicates but allow a candidate with the same suffix if it is strictly better than the previously seen one. This recovers strong candidates that would otherwise be excluded while preserving diversity.

Additionally, I slightly strengthen VND neighborhoods by allowing the sampled non-adjacent swaps and 2-opt passes to use their entire attempt budgets (rather than min(tries, n)), enabling more thorough improvement without excessive overhead (tries are already bounded).

Together, these changes directly improve schedule quality by better endgame placement and avoiding over-aggressive pruning while keeping runtime practical through caching and bounded sampling.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
            next_pool = []
            layer_seen = set()
            suffix_seen = set()  # suffix diversity on last-2
            # Collect regret dispersion to adjust lookahead blending
            layer_regrets = []
=======
            next_pool = []
            layer_seen = set()
            suffix_best = {}  # suffix-2 -> best c1 tracked for allowing strictly better duplicates
            # Collect regret dispersion to adjust lookahead blending
            layer_regrets = []
>>>>>>> REPLACE

<<<<<<< SEARCH
                    # Suffix diversity: last-2 signature
                    if len(new_seq) >= 2:
                        sig = (new_seq[-2], new_seq[-1])
                    else:
                        sig = (None, new_seq[-1])

                    key = tuple(new_seq)
                    if key in layer_seen:
                        continue
                    if sig in suffix_seen:
                        continue

                    layer_seen.add(key)
                    suffix_seen.add(sig)
                    next_pool.append((c1, new_seq, new_rem))
=======
                    # Suffix diversity: last-2 signature
                    if len(new_seq) >= 2:
                        sig = (new_seq[-2], new_seq[-1])
                    else:
                        sig = (None, new_seq[-1])

                    key = tuple(new_seq)
                    if key in layer_seen:
                        continue
                    # Allow one per suffix unless a strictly better c1 appears; admit if new or better
                    prev_best = suffix_best.get(sig)
                    if prev_best is not None and c1 >= prev_best - 1e-9:
                        continue

                    layer_seen.add(key)
                    suffix_best[sig] = c1 if prev_best is None or c1 < prev_best else prev_best
                    next_pool.append((c1, new_seq, new_rem))
>>>>>>> REPLACE

<<<<<<< SEARCH
        # Greedy completion from best partial
        if beam:
            c, seq, rem = min(beam, key=lambda x: x[0])
            cur = list(seq)
            rem_list = list(rem)
            while rem_list:
                best_t = None
                best_c = float('inf')
                for t in rem_list:
                    c2 = eval_cost(cur + [t])
                    if c2 < best_c:
                        best_c = c2
                        best_t = t
                cur.append(best_t)
                rem_list.remove(best_t)
            return eval_cost(cur), cur
=======
        # Greedy completion from best partial using regret-guided best insertion
        if beam:
            c, seq, rem = min(beam, key=lambda x: x[0])
            seq_partial = list(seq)
            remaining = list(rem)
            re_cache = make_best_two_cache()
            while remaining:
                k_t = min(len(remaining), max(6, cand_txn_sample))
                cand_txns = remaining if len(remaining) <= k_t else random.sample(remaining, k_t)
                scored = []
                for t in cand_txns:
                    (c1, p1), (c2, p2) = best_two_insertions(seq_partial, t, re_cache)
                    regret = (c2 - c1) if c2 < float('inf') else 0.0
                    scored.append((c1, regret, t, p1))
                if not scored:
                    t = remaining.pop()
                    seq_partial.append(t)
                    continue
                # Build small pool by best cost and highest regret
                scored.sort(key=lambda x: x[0])
                rcl_cost = scored[:min(3, len(scored))]
                scored.sort(key=lambda x: (-x[1], x[0]))
                rcl_regret = scored[:min(2, len(scored))]
                pool = list({(c1, rg, t, p) for (c1, rg, t, p) in (rcl_cost + rcl_regret)})
                # Choose the best from the pool by cost, tie-break by regret
                pool.sort(key=lambda x: (x[0], -x[1]))
                chosen_c, chosen_rg, chosen_t, chosen_p = pool[0]
                seq_partial.insert(chosen_p, chosen_t)
                remaining.remove(chosen_t)
            final_cost = eval_cost(seq_partial)
            return final_cost, seq_partial
>>>>>>> REPLACE

<<<<<<< SEARCH
            for _ in range(min(tries, ncur)):
                i = random.randint(0, ncur - 1)
                j = random.randint(0, ncur - 1)
                if i == j or abs(i - j) <= 1:
                    continue
                cand = best_seq[:]
                cand[i], cand[j] = cand[j], cand[i]
                c = eval_cost(cand)
                delta = best_cost - c
                if delta > best_delta:
                    best_delta = delta
                    best_ij = (i, j, cand, c)
=======
            for _ in range(tries):
                i = random.randint(0, ncur - 1)
                j = random.randint(0, ncur - 1)
                if i == j or abs(i - j) <= 1:
                    continue
                cand = best_seq[:]
                cand[i], cand[j] = cand[j], cand[i]
                c = eval_cost(cand)
                delta = best_cost - c
                if delta > best_delta:
                    best_delta = delta
                    best_ij = (i, j, cand, c)
>>>>>>> REPLACE

<<<<<<< SEARCH
            for _ in range(min(tries, ncur)):
                i = random.randint(0, ncur - 3)
                j = random.randint(i + 2, ncur - 1)
                cand = best_seq[:i] + list(reversed(best_seq[i:j + 1])) + best_seq[j + 1:]
                c = eval_cost(cand)
                delta = best_cost - c
                if delta > best_delta:
                    best_delta = delta
                    best_pair = (i, j, cand, c)
=======
            for _ in range(tries):
                i = random.randint(0, ncur - 3)
                j = random.randint(i + 2, ncur - 1)
                cand = best_seq[:i] + list(reversed(best_seq[i:j + 1])) + best_seq[j + 1:]
                c = eval_cost(cand)
                delta = best_cost - c
                if delta > best_delta:
                    best_delta = delta
                    best_pair = (i, j, cand, c)
>>>>>>> REPLACE

</DIFF>