<NAME>
beam_and_local_search
</NAME>
<DESCRIPTION>
I replace the single-start, random-sampling greedy builder with a beam search that evaluates the true makespan cost for partial sequences and keeps multiple best partial schedules at each step. This reduces getting stuck in poor local choices because we consider several candidate frontiers in parallel.

To further improve the final schedule, I add a lightweight local improvement phase (adjacent swap hill climbing) that uses the exact cost function to refine the beamâ€™s best sequence. I also add a simple memoization cache for partial sequence cost evaluations to avoid redundant calls to workload.get_opt_seq_cost.

Key improvements:
- Deterministic initialization via evaluating all single-transaction starts and seeding the beam with the best few.
- Beam expansion that either evaluates all remaining options (when small) or samples a bounded subset to control runtime.
- Diversity control in the beam by keeping the best extension for distinct last transactions.
- Post-processing adjacent-swap hill climbing for fine-grained improvements.
- Uses num_seqs to set beam width; no reliance on heuristics like transaction length.

This approach stays faithful to minimizing true makespan, scales reasonably to 100 transactions, and typically yields lower makespans than the original random-greedy sampling.
</DESCRIPTION>
<DIFF>
<<<<<<< SEARCH
def get_best_schedule(workload, num_seqs):
    """
    Get optimal schedule using greedy cost sampling strategy.

    Args:
        workload: Workload object containing transaction data
        num_seqs: Number of sequences to sample for greedy selection

    Returns:
        Tuple of (lowest makespan, corresponding schedule)
    """
    def get_greedy_cost_sampled(num_samples, sample_rate):
        # greedy with random starting point
        start_txn = random.randint(0, workload.num_txns - 1)
        txn_seq = [start_txn]
        remaining_txns = [x for x in range(0, workload.num_txns)]
        remaining_txns.remove(start_txn)
        running_cost = workload.txns[start_txn][0][3]

        for i in range(0, workload.num_txns - 1):
            min_cost = 100000  # MAX
            min_relative_cost = 10
            min_txn = -1
            holdout_txns = []
            done = False
            key_maps = []

            sample = random.random()
            if sample > sample_rate:
                idx = random.randint(0, len(remaining_txns) - 1)
                t = remaining_txns[idx]
                txn_seq.append(t)
                remaining_txns.pop(idx)
                continue

            for j in range(0, num_samples):
                idx = 0
                if len(remaining_txns) > 1:
                    idx = random.randint(0, len(remaining_txns) - 1)
                else:
                    done = True
                t = remaining_txns[idx]
                holdout_txns.append(remaining_txns.pop(idx))

                txn_len = workload.txns[t][0][3]
                test_seq = txn_seq.copy()
                test_seq.append(t)
                cost = 0
                cost = workload.get_opt_seq_cost(test_seq)
                if cost < min_cost:
                    min_cost = cost
                    min_txn = t
                if done:
                    break
            assert(min_txn != -1)
            running_cost = min_cost
            txn_seq.append(min_txn)
            holdout_txns.remove(min_txn)
            remaining_txns.extend(holdout_txns)

        assert len(set(txn_seq)) == workload.num_txns

        overall_cost = workload.get_opt_seq_cost(txn_seq)
        return overall_cost, txn_seq

    return get_greedy_cost_sampled(10, 1.0)
=======
def get_best_schedule(workload, num_seqs):
    """
    Find a low-makespan schedule using beam search with memoized cost
    evaluation and a local adjacent-swap refinement pass.

    Args:
        workload: Workload object containing transaction data
        num_seqs: Controls beam width (diversity of partial schedules)

    Returns:
        Tuple of (lowest makespan, corresponding schedule)
    """
    n = workload.num_txns
    all_txns = list(range(n))

    # Simple memoization to avoid repeated cost computations for the same prefix
    cost_cache = {}
    def seq_cost(seq):
        key = tuple(seq)
        if key in cost_cache:
            return cost_cache[key]
        c = workload.get_opt_seq_cost(seq)
        cost_cache[key] = c
        return c

    # Beam parameters derived from num_seqs to control breadth and runtime
    beam_width = max(3, min(8, int(num_seqs)))  # keep a few strong candidates
    expand_all_threshold = 24  # when remaining txns are few, evaluate all
    sample_k = 24  # otherwise, sample a bounded subset for expansion

    # Initialize beam with the best singletons (evaluate exact cost for each start)
    init_candidates = []
    for t in all_txns:
        c = seq_cost([t])
        rem = set(all_txns)
        rem.remove(t)
        init_candidates.append((c, [t], rem))
    init_candidates.sort(key=lambda x: x[0])
    beam = init_candidates[:beam_width]

    # Beam search: grow sequences while keeping top-k by exact cost
    for _step in range(1, n):
        new_beam = []
        for cost_so_far, seq, rem in beam:
            if not rem:
                new_beam.append((cost_so_far, seq, rem))
                continue

            rem_list = list(rem)
            if len(rem_list) <= expand_all_threshold:
                candidates = rem_list
            else:
                candidates = random.sample(rem_list, min(sample_k, len(rem_list)))

            for t in candidates:
                new_seq = seq + [t]
                c = seq_cost(new_seq)
                new_rem = rem.copy()
                new_rem.remove(t)
                new_beam.append((c, new_seq, new_rem))

        # Select next beam: best unique endings to encourage diversity
        new_beam.sort(key=lambda x: x[0])
        next_beam = []
        seen_ends = set()
        for c, s, r in new_beam:
            end = s[-1]
            if end in seen_ends:
                continue
            seen_ends.add(end)
            next_beam.append((c, s, r))
            if len(next_beam) >= beam_width:
                break

        if not next_beam:
            # Fallback in degenerate cases
            next_beam = new_beam[:beam_width]

        beam = next_beam

    # Best complete sequence from the beam
    best_cost, best_seq, _ = min(beam, key=lambda x: x[0])

    # Local refinement: adjacent swap hill-climbing with first-improvement
    improved = True
    while improved:
        improved = False
        for i in range(n - 1):
            candidate = best_seq.copy()
            candidate[i], candidate[i + 1] = candidate[i + 1], candidate[i]
            c = seq_cost(candidate)
            if c < best_cost:
                best_cost = c
                best_seq = candidate
                improved = True
                break  # restart scan to capture cascading improvements

    return best_cost, best_seq
>>>>>>> REPLACE

</DIFF>