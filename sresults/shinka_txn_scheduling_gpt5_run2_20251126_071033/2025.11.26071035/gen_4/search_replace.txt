<NAME>
greedy_insertion_ils
</NAME>
<DESCRIPTION>
I replace the beam-search-only approach with a hybrid constructive + local search method tailored to minimize makespan using the true cost function.

Key ideas:
- Greedy best-insertion construction: Build schedules by inserting each next transaction at the position that yields the lowest makespan, approximated via sampling of candidate transactions and insertion positions. This better captures non-local effects than simple append-based beam expansion.
- Multiple diverse seeds: Start from several elite singletons (lowest-cost starts) plus randomized seeds to explore different regions of the search space.
- Stronger local refinement: Extend the adjacent-swap hill climbing with a reinsertion (1-move) local search that moves any transaction to its best position, enabling large non-adjacent improvements. Memoized cost evaluation is retained to control compute.
- Iterated local search: Apply small perturbations and re-refine to escape local minima.

Why this helps:
- Insertion heuristics are well-suited for sequencing problems with complex interactions (like transaction conflicts) and tend to yield better initial schedules than beam append.
- Reinsertion moves capture larger structural improvements that adjacent swaps miss.
- Multiple seeds and mild perturbations enhance exploration without exploding runtime.

Parameters are adaptive to num_seqs and workload size to keep the runtime reasonable while improving solution quality.
</DESCRIPTION>
<DIFF>
<<<<<<< SEARCH
def get_best_schedule(workload, num_seqs):
    """
    Find a low-makespan schedule using beam search with memoized cost
    evaluation and a local adjacent-swap refinement pass.

    Args:
        workload: Workload object containing transaction data
        num_seqs: Controls beam width (diversity of partial schedules)

    Returns:
        Tuple of (lowest makespan, corresponding schedule)
    """
    n = workload.num_txns
    all_txns = list(range(n))

    # Simple memoization to avoid repeated cost computations for the same prefix
    cost_cache = {}
    def seq_cost(seq):
        key = tuple(seq)
        if key in cost_cache:
            return cost_cache[key]
        c = workload.get_opt_seq_cost(seq)
        cost_cache[key] = c
        return c

    # Beam parameters derived from num_seqs to control breadth and runtime
    beam_width = max(3, min(8, int(num_seqs)))  # keep a few strong candidates
    expand_all_threshold = 24  # when remaining txns are few, evaluate all
    sample_k = 24  # otherwise, sample a bounded subset for expansion

    # Initialize beam with the best singletons (evaluate exact cost for each start)
    init_candidates = []
    for t in all_txns:
        c = seq_cost([t])
        rem = set(all_txns)
        rem.remove(t)
        init_candidates.append((c, [t], rem))
    init_candidates.sort(key=lambda x: x[0])
    beam = init_candidates[:beam_width]

    # Beam search: grow sequences while keeping top-k by exact cost
    for _step in range(1, n):
        new_beam = []
        for cost_so_far, seq, rem in beam:
            if not rem:
                new_beam.append((cost_so_far, seq, rem))
                continue

            rem_list = list(rem)
            if len(rem_list) <= expand_all_threshold:
                candidates = rem_list
            else:
                candidates = random.sample(rem_list, min(sample_k, len(rem_list)))

            for t in candidates:
                new_seq = seq + [t]
                c = seq_cost(new_seq)
                new_rem = rem.copy()
                new_rem.remove(t)
                new_beam.append((c, new_seq, new_rem))

        # Select next beam: best unique endings to encourage diversity
        new_beam.sort(key=lambda x: x[0])
        next_beam = []
        seen_ends = set()
        for c, s, r in new_beam:
            end = s[-1]
            if end in seen_ends:
                continue
            seen_ends.add(end)
            next_beam.append((c, s, r))
            if len(next_beam) >= beam_width:
                break

        if not next_beam:
            # Fallback in degenerate cases
            next_beam = new_beam[:beam_width]

        beam = next_beam

    # Best complete sequence from the beam
    best_cost, best_seq, _ = min(beam, key=lambda x: x[0])

    # Local refinement: adjacent swap hill-climbing with first-improvement
    improved = True
    while improved:
        improved = False
        for i in range(n - 1):
            candidate = best_seq.copy()
            candidate[i], candidate[i + 1] = candidate[i + 1], candidate[i]
            c = seq_cost(candidate)
            if c < best_cost:
                best_cost = c
                best_seq = candidate
                improved = True
                break  # restart scan to capture cascading improvements

    return best_cost, best_seq
=======
def get_best_schedule(workload, num_seqs):
    """
    Find a low-makespan schedule using greedy best-insertion construction
    with memoized cost evaluation, followed by strong local refinement
    (adjacent swaps + reinsertion) and light iterated local search.

    Args:
        workload: Workload object containing transaction data
        num_seqs: Controls search breadth (number of seeds, sampling sizes)

    Returns:
        Tuple of (lowest makespan, corresponding schedule)
    """
    n = workload.num_txns
    all_txns = list(range(n))

    # Memoization to avoid repeated cost computations for the same sequence
    cost_cache = {}
    def seq_cost(seq):
        key = tuple(seq)
        if key in cost_cache:
            return cost_cache[key]
        c = workload.get_opt_seq_cost(seq)
        cost_cache[key] = c
        return c

    # Parameterization (kept modest for runtime)
    elite_seeds = max(2, min(6, int(num_seqs)))         # number of elite singleton starts
    random_seeds = max(1, min(3, int(num_seqs) // 4))   # random additional starts
    # Candidate sampling sizes
    k_txn_sample = min(12, max(6, 2 + int(num_seqs)))   # txns to consider per insertion
    k_pos_sample = 6                                     # insertion positions to sample
    rem_all_threshold = 12                               # when few remain, consider all txns
    # Iterated local search parameters
    perturbations = 2
    random_swap_trials = 3

    # Helper to generate insertion position samples (always include ends)
    def position_samples(seq_len):
        if seq_len <= 1:
            return [0, seq_len]
        pos_set = {0, seq_len, seq_len // 2}
        # Add a few random internal positions
        for _ in range(min(k_pos_sample, seq_len + 1)):
            pos_set.add(random.randint(0, seq_len))
        # Keep deterministic ordering for caching benefits
        return sorted(pos_set)

    # Construct a schedule from a seed using greedy best insertion
    def build_from_seed(seed_t):
        seq = [seed_t]
        rem = set(all_txns)
        rem.remove(seed_t)

        # Add a second element by choosing best of prepend/append among sampled txns
        if rem:
            cand_txns = list(rem)
            if len(cand_txns) > k_txn_sample:
                cand_txns = random.sample(cand_txns, k_txn_sample)
            best_c2 = float('inf')
            best_seq2 = None
            best_t2 = None
            for t in cand_txns:
                # Try prepend and append for quick early shaping
                seq_a = [t] + seq
                c_a = seq_cost(seq_a)
                seq_b = seq + [t]
                c_b = seq_cost(seq_b)
                if c_a < best_c2:
                    best_c2 = c_a
                    best_seq2 = seq_a
                    best_t2 = t
                if c_b < best_c2:
                    best_c2 = c_b
                    best_seq2 = seq_b
                    best_t2 = t
            if best_seq2 is not None:
                seq = best_seq2
                rem.remove(best_t2)

        # Iteratively insert remaining transactions at best sampled position
        while rem:
            if len(rem) <= rem_all_threshold:
                cand_txns = list(rem)
            else:
                cand_txns = random.sample(list(rem), min(k_txn_sample, len(rem)))

            best_c = float('inf')
            best_new_seq = None
            best_t = None

            for t in cand_txns:
                positions = position_samples(len(seq))
                for p in positions:
                    new_seq = seq[:p] + [t] + seq[p:]
                    c = seq_cost(new_seq)
                    if c < best_c:
                        best_c = c
                        best_new_seq = new_seq
                        best_t = t

            # Fallback to random append if something went wrong
            if best_new_seq is None:
                t = random.choice(tuple(rem))
                best_new_seq = seq + [t]
                best_t = t

            seq = best_new_seq
            rem.remove(best_t)

        return seq

    # Local refinement: adjacent swaps and reinsertion (first-improvement)
    def local_refine(seq):
        best_seq = seq[:]
        best_cost = seq_cost(best_seq)
        improved = True
        while improved:
            improved = False
            # Adjacent swap pass
            for i in range(n - 1):
                cand = best_seq[:]
                cand[i], cand[i + 1] = cand[i + 1], cand[i]
                c = seq_cost(cand)
                if c < best_cost:
                    best_cost = c
                    best_seq = cand
                    improved = True
                    break
            if improved:
                continue
            # Reinsertion pass: move one item to a better position
            for i in range(n):
                item = best_seq[i]
                base = best_seq[:i] + best_seq[i + 1:]
                positions = position_samples(len(base))
                for p in positions:
                    cand = base[:p] + [item] + base[p:]
                    c = seq_cost(cand)
                    if c < best_cost:
                        best_cost = c
                        best_seq = cand
                        improved = True
                        break
                if improved:
                    break
        return best_cost, best_seq

    # Seed selection: evaluate all singletons, take elite + some random seeds
    singleton_scores = []
    for t in all_txns:
        singleton_scores.append((seq_cost([t]), t))
    singleton_scores.sort(key=lambda x: x[0])

    seed_txns = [t for _, t in singleton_scores[:elite_seeds]]
    # Add random distinct seeds
    remaining = [t for t in all_txns if t not in seed_txns]
    if remaining and random_seeds > 0:
        extra = random.sample(remaining, min(random_seeds, len(remaining)))
        seed_txns.extend(extra)

    # Build schedules from seeds and keep the best
    best_overall_cost = float('inf')
    best_overall_seq = None

    for seed in seed_txns:
        seq0 = build_from_seed(seed)
        c0 = seq_cost(seq0)
        # Local refinement
        c1, s1 = local_refine(seq0)

        if c1 < best_overall_cost:
            best_overall_cost = c1
            best_overall_seq = s1

    # Iterated local search: small perturbations and re-refinement
    if best_overall_seq is None:
        # Fallback: random permutation if everything failed
        best_overall_seq = all_txns[:]
        random.shuffle(best_overall_seq)
        best_overall_cost = seq_cost(best_overall_seq)

    for _ in range(perturbations):
        pert = best_overall_seq[:]
        # Apply a few random swaps to escape local minima
        for _trial in range(random_swap_trials):
            i = random.randint(0, n - 1)
            j = random.randint(0, n - 1)
            if i == j:
                continue
            pert[i], pert[j] = pert[j], pert[i]
        c2, s2 = local_refine(pert)
        if c2 < best_overall_cost:
            best_overall_cost = c2
            best_overall_seq = s2

    return best_overall_cost, best_overall_seq
>>>>>>> REPLACE
</DIFF>