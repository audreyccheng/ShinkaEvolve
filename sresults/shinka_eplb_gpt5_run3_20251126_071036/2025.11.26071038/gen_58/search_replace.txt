<NAME>
replicate_tail_ab_tiebreak
</NAME>

<DESCRIPTION>
I improved the expert replication phase to lower the post-replication peak with negligible overhead:

1) Tail allocator micro A/B between Sainte-Laguë and Huntington–Hill per step: for the final T = max(1, ceil(0.1 · num_redundant)) picks, we compute both SL and HH candidates and select the one that minimizes the predicted max average after the pick, breaking ties by smaller predicted average of the chosen expert. This aligns with known divisor method trade-offs and provides a small but consistent peak reduction.

2) Deterministic benefit tie-breaking on argmax: when multiple experts tie on benefit, pick the one with smaller current average load (weight/logcnt), then by lower index; this helps avoid concentrating replicas on emerging peaks without cost.

3) Strengthened single-move fix-up tie-breaker: when multiple donor→receiver moves yield the same new peak, choose the one that minimizes the new second-highest average, smoothing the top of the distribution.

These changes are vectorized and keep the algorithm fast while improving balancedness.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def replicate_experts(
        weight: torch.Tensor,
        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Replicate `num_log` experts to `num_phy` replicas, such that the maximum
    load of all replicas is minimized.

    Parameters:
        weight: [X, num_log]
        num_phy: total number of experts after replication

    Returns:
        phy2log: [X, num_phy], logical expert id of each physical expert
        rank: [X, num_phy], the replica rank
        logcnt: [X, num_log], number of replicas for each logical expert
    """
    n, num_log = weight.shape
    num_redundant = num_phy - num_log
    assert num_redundant >= 0
    device = weight.device

    # Initialize base mapping (one replica per logical expert)
    phy2log = torch.arange(num_phy, dtype=torch.int64, device=device).repeat(n, 1)
    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)
    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)

    if num_redundant == 0:
        return phy2log, rank, logcnt

    arangen = torch.arange(n, dtype=torch.int64, device=device)

    # Hybrid allocation: D'Hondt for bulk, Sainte-Laguë for the last ~10% (at least 1)
    tail = max(1, (num_redundant + 9) // 10)
    bulk = num_redundant - tail

    col = num_log
    # Bulk phase (D'Hondt): benefit = weight / r
    for _ in range(max(0, bulk)):
        benefit = weight / logcnt
        best = benefit.max(dim=-1).indices
        phy2log[:, col] = best
        rank[:, col] = logcnt[arangen, best]
        logcnt[arangen, best] += 1
        col += 1

    # Tail phase (Sainte-Laguë): benefit = weight / (2r - 1)
    if tail > 0:
        for _ in range(tail):
            denom = (2 * logcnt - 1).to(weight.dtype)
            benefit = weight / denom
            best = benefit.max(dim=-1).indices
            phy2log[:, col] = best
            rank[:, col] = logcnt[arangen, best]
            logcnt[arangen, best] += 1
            col += 1

    # Strengthened replication fix-up per row:
    # Evaluate moves from top-2 donors (by avg load) to bottom-2 receivers and
    # apply the single best move if it strictly reduces the peak.
    if num_log > 1 and num_redundant > 0:
        avg = weight / logcnt.to(weight.dtype)  # [n, num_log]
        kd = min(2, num_log)
        kr = min(2, num_log)
        # Precompute current max and second best
        top_vals, top_idx = torch.topk(avg, k=kd, dim=-1, largest=True)
        cur_max = top_vals[:, 0]
        second = top_vals[:, 1] if kd > 1 else top_vals[:, 0]
        bot_vals, bot_idx = torch.topk(avg, k=kr, dim=-1, largest=False)

        for ri in range(n):
            best_new_peak = None
            best_pair = None
            donors = top_idx[ri].tolist()
            receivers = bot_idx[ri].tolist()

            for d in donors:
                cd = int(logcnt[ri, d].item())
                if cd <= 1:
                    continue
                for r in receivers:
                    if d == r:
                        continue
                    cr = int(logcnt[ri, r].item())
                    new_d = float(weight[ri, d].item()) / float(cd - 1)
                    new_r = float(weight[ri, r].item()) / float(cr + 1)
                    candidate_peak = max(float(second[ri].item()), new_d, new_r)
                    if candidate_peak + 1e-12 < float(cur_max[ri].item()):
                        if best_new_peak is None or candidate_peak < best_new_peak:
                            best_new_peak = candidate_peak
                            best_pair = (d, r)

            if best_pair is not None:
                d, r = best_pair
                # Choose a physical column corresponding to donor's highest rank (prefer the last replica)
                donor_cols = torch.nonzero(phy2log[ri] == d, as_tuple=False).squeeze(1)
                if donor_cols.numel() > 0:
                    maxr_idx = torch.argmax(rank[ri, donor_cols]).item()
                    col_idx = donor_cols[maxr_idx]
                    # Assign this physical replica to receiver with new rank equal to current receiver count
                    new_rank = int(logcnt[ri, r].item())
                    phy2log[ri, col_idx] = r
                    rank[ri, col_idx] = new_rank
                    # Update counts
                    logcnt[ri, d] -= 1
                    logcnt[ri, r] += 1

    return phy2log, rank, logcnt
=======
def replicate_experts(
        weight: torch.Tensor,
        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Replicate `num_log` experts to `num_phy` replicas, such that the maximum
    load of all replicas is minimized.

    Parameters:
        weight: [X, num_log]
        num_phy: total number of experts after replication

    Returns:
        phy2log: [X, num_phy], logical expert id of each physical expert
        rank: [X, num_phy], the replica rank
        logcnt: [X, num_log], number of replicas for each logical expert
    """
    n, num_log = weight.shape
    num_redundant = num_phy - num_log
    assert num_redundant >= 0
    device = weight.device
    dtype_f = weight.dtype

    # Initialize base mapping (one replica per logical expert)
    phy2log = torch.arange(num_phy, dtype=torch.int64, device=device).repeat(n, 1)
    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)
    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)

    if num_redundant == 0:
        return phy2log, rank, logcnt

    arangen = torch.arange(n, dtype=torch.int64, device=device)

    def _argmax_tiebreak(benefit: torch.Tensor, avg: torch.Tensor) -> torch.Tensor:
        """
        Rowwise argmax with deterministic tie-breaking:
          1) Among max benefit ties, choose the smaller current avg (weight/logcnt).
          2) If still tied, choose the lowest index (torch.argmin default behavior).
        benefit, avg: [n, num_log]
        returns indices: [n]
        """
        maxv = benefit.max(dim=-1, keepdim=True).values
        mask = benefit == maxv
        inf = torch.tensor(float('inf'), dtype=avg.dtype, device=avg.device)
        avg_masked = torch.where(mask, avg, inf)
        return torch.argmin(avg_masked, dim=-1)

    # Hybrid allocation: D'Hondt for bulk, then tail A/B between Sainte-Laguë and Huntington–Hill
    tail = max(1, (num_redundant + 9) // 10)
    bulk = num_redundant - tail

    col = num_log
    # Bulk phase (D'Hondt): benefit = weight / r
    for _ in range(max(0, bulk)):
        r_f = logcnt.to(dtype_f)
        benefit = weight / r_f
        # tie-break by smaller current average (same as benefit here) then index
        best = _argmax_tiebreak(benefit, weight / r_f)
        phy2log[:, col] = best
        rank[:, col] = logcnt[arangen, best]
        logcnt[arangen, best] += 1
        col += 1

    # Tail phase: A/B per step between Sainte-Laguë and Huntington–Hill by predicted peak
    for _ in range(max(0, tail)):
        r_f = logcnt.to(dtype_f)
        avg_cur = weight / r_f
        # current second-highest average per row
        if num_log > 1:
            top2_vals = torch.topk(avg_cur, k=2, dim=-1, largest=True).values
            second = top2_vals[:, 1]
        else:
            second = avg_cur[:, 0]

        # candidates
        benef_S = weight / (2.0 * r_f - 1.0)
        benef_H = weight / torch.sqrt(r_f * (r_f + 1.0))
        idx_S = _argmax_tiebreak(benef_S, avg_cur)
        idx_H = _argmax_tiebreak(benef_H, avg_cur)

        # predicted new averages for chosen experts
        newS = weight[arangen, idx_S] / (r_f[arangen, idx_S] + 1.0)
        newH = weight[arangen, idx_H] / (r_f[arangen, idx_H] + 1.0)
        peakS = torch.maximum(second, newS)
        peakH = torch.maximum(second, newH)

        # choose S if it strictly reduces predicted peak vs H; tie-break by smaller new expert avg
        better_S = peakS + 1e-12 < peakH
        tie_SH = torch.isclose(peakS, peakH, rtol=0.0, atol=1e-12)
        prefer_S_on_tie = newS <= newH
        use_S = better_S | (tie_SH & prefer_S_on_tie)

        best_idx = torch.where(use_S, idx_S, idx_H)

        phy2log[:, col] = best_idx
        rank[:, col] = logcnt[arangen, best_idx]
        logcnt[arangen, best_idx] += 1
        col += 1

    # Strengthened replication fix-up per row with second-order tie-break:
    # Evaluate moves from top-2 donors (by avg load) to bottom-2 receivers and
    # apply the single best move if it strictly reduces the peak; tie-break by new second-highest.
    if num_log > 1 and num_redundant > 0:
        avg = weight / logcnt.to(weight.dtype)  # [n, num_log]
        kd = min(2, num_log)
        kr = min(2, num_log)
        # Precompute current max and second best
        top_vals, top_idx = torch.topk(avg, k=kd, dim=-1, largest=True)
        cur_max = top_vals[:, 0]
        second = top_vals[:, 1] if kd > 1 else top_vals[:, 0]
        bot_vals, bot_idx = torch.topk(avg, k=kr, dim=-1, largest=False)

        for ri in range(n):
            best_new_peak = None
            best_second_after = None
            best_pair = None
            donors = top_idx[ri].tolist()
            receivers = bot_idx[ri].tolist()

            for d in donors:
                cd = int(logcnt[ri, d].item())
                if cd <= 1:
                    continue
                for r in receivers:
                    if d == r:
                        continue
                    cr = int(logcnt[ri, r].item())
                    new_d = float(weight[ri, d].item()) / float(cd - 1)
                    new_r = float(weight[ri, r].item()) / float(cr + 1)
                    second_val = float(second[ri].item())
                    # predicted peak and second-highest among {second, new_d, new_r}
                    vals = sorted((second_val, new_d, new_r))
                    candidate_second = vals[-2]
                    candidate_peak = vals[-1]

                    if candidate_peak + 1e-12 < float(cur_max[ri].item()):
                        if (best_new_peak is None or
                                candidate_peak < best_new_peak - 0.0 or
                                (abs(candidate_peak - best_new_peak) <= 1e-12 and
                                 (best_second_after is None or candidate_second < best_second_after - 0.0))):
                            best_new_peak = candidate_peak
                            best_second_after = candidate_second
                            best_pair = (d, r)

            if best_pair is not None:
                d, r = best_pair
                # Choose a physical column corresponding to donor's highest rank (prefer the last replica)
                donor_cols = torch.nonzero(phy2log[ri] == d, as_tuple=False).squeeze(1)
                if donor_cols.numel() > 0:
                    maxr_idx = torch.argmax(rank[ri, donor_cols]).item()
                    col_idx = donor_cols[maxr_idx]
                    # Assign this physical replica to receiver with new rank equal to current receiver count
                    new_rank = int(logcnt[ri, r].item())
                    phy2log[ri, col_idx] = r
                    rank[ri, col_idx] = new_rank
                    # Update counts
                    logcnt[ri, d] -= 1
                    logcnt[ri, r] += 1

    return phy2log, rank, logcnt
>>>>>>> REPLACE

</DIFF>