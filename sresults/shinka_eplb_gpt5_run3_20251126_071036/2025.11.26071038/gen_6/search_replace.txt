<NAME>
single_swap_refinement
</NAME>

<DESCRIPTION>
Introduce a lightweight single-swap refinement pass inside balanced_packing to reduce the maximum pack imbalance after the initial greedy assignment. For each layer, we identify the heaviest and lightest packs and perform an optimal swap of one item between them (using a searchsorted-based O(k log k) method) if it reduces the imbalance. This preserves equal item counts per pack and adds negligible overhead while improving balancedness. The rest of the algorithm remains unchanged.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=weight.device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    indices = weight.float().sort(-1, descending=True).indices.cpu()
    pack_index = torch.full_like(weight,
                                 fill_value=-1,
                                 dtype=torch.int64,
                                 device="cpu")
    rank_in_pack = torch.full_like(pack_index, fill_value=-1)
    for i in range(num_layers):
        pack_weights = [0] * num_packs
        pack_items = [0] * num_packs
        for group in indices[i]:
            pack = min(
                (i
                 for i in range(num_packs) if pack_items[i] < groups_per_pack),
                key=pack_weights.__getitem__,
            )
            assert pack_items[pack] < groups_per_pack
            pack_index[i, group] = pack
            rank_in_pack[i, group] = pack_items[pack]
            pack_weights[pack] += weight[i, group]
            pack_items[pack] += 1
    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=weight.device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    indices = weight.float().sort(-1, descending=True).indices.cpu()
    pack_index = torch.full_like(weight,
                                 fill_value=-1,
                                 dtype=torch.int64,
                                 device="cpu")
    rank_in_pack = torch.full_like(pack_index, fill_value=-1)
    for i in range(num_layers):
        pack_weights = [0] * num_packs
        pack_items = [0] * num_packs
        for group in indices[i]:
            pack = min(
                (i
                 for i in range(num_packs) if pack_items[i] < groups_per_pack),
                key=pack_weights.__getitem__,
            )
            assert pack_items[pack] < groups_per_pack
            pack_index[i, group] = pack
            rank_in_pack[i, group] = pack_items[pack]
            pack_weights[pack] += weight[i, group]
            pack_items[pack] += 1

    # Lightweight single-swap refinement per layer to reduce max imbalance
    if groups_per_pack > 1:
        for i in range(num_layers):
            packs = pack_index[i]  # [num_groups]
            w = weight[i].cpu()
            # Compute pack loads
            pack_w = torch.zeros(num_packs, dtype=w.dtype)
            pack_w.scatter_add_(0, packs, w)
            h = int(torch.argmax(pack_w))
            l = int(torch.argmin(pack_w))
            delta = float(pack_w[h] - pack_w[l])
            if delta <= 0:
                continue

            heavy_mask = packs == h
            light_mask = packs == l
            heavy_idx = torch.nonzero(heavy_mask, as_tuple=False).squeeze(1)
            light_idx = torch.nonzero(light_mask, as_tuple=False).squeeze(1)
            if heavy_idx.numel() == 0 or light_idx.numel() == 0:
                continue

            hw = w[heavy_idx]
            lw = w[light_idx]
            lw_sorted, lw_perm = torch.sort(lw)  # ascending
            # For each heavy item, find light item closest to target = hw - delta/2
            target = hw - (delta / 2.0)
            pos = torch.searchsorted(lw_sorted, target)
            if lw_sorted.numel() == 0:
                continue
            pos = torch.clamp(pos, 0, lw_sorted.numel() - 1)
            # Consider neighbors pos and pos-1 for best approximation
            cand_pos = torch.stack([pos, torch.clamp(pos - 1, 0, lw_sorted.numel() - 1)], dim=1)
            cand_lw = lw_sorted[cand_pos]  # [H, 2]
            resid = (delta - 2.0 * (hw.unsqueeze(1) - cand_lw)).abs()
            best_flat = int(torch.argmin(resid).item())
            best_h_index = best_flat // 2
            best_option = best_flat % 2
            j_sorted_idx = int(cand_pos[best_h_index, best_option].item())

            wi = float(hw[best_h_index].item())
            wj = float(lw_sorted[j_sorted_idx].item())
            new_delta = abs(delta - 2.0 * (wi - wj))
            # Apply swap only if it strictly improves imbalance
            if new_delta < delta - 1e-9:
                hi = heavy_idx[best_h_index]
                lj = light_idx[lw_perm[j_sorted_idx]]
                pack_index[i, hi] = l
                pack_index[i, lj] = h
                # Reassign ranks within affected packs to keep 0..groups_per_pack-1
                for p in (h, l):
                    mask = pack_index[i] == p
                    idx = torch.nonzero(mask, as_tuple=False).squeeze(1)
                    if idx.numel() == 0:
                        continue
                    # Stable by previous rank order
                    prev_rank = rank_in_pack[i, idx]
                    order = torch.argsort(prev_rank)
                    new_ranks = torch.arange(order.numel(), dtype=torch.int64)
                    rank_in_pack[i, idx[order]] = new_ranks

    return pack_index, rank_in_pack
>>>>>>> REPLACE

</DIFF>