<NAME>
hybrid_tail_waterfill
</NAME>

<DESCRIPTION>
I introduce a peak-aware, hybrid tail allocation in the replica count solver to reduce the maximum per-replica load without materially impacting runtime. The current _waterfill_counts_row does a binary search for a threshold and then fills remaining extras greedily by repeatedly adding to the largest w_i / c_i in large k-chunks. This is very fast but can leave small peak suboptimalities due to discrete rounding.

The change keeps the same fast bulk filling for most extras but uses an adaptive, per-row hybrid rule for the last T allocations (T = max(1, round(0.1 · (target_total − num_log))), capped by remaining extras). At each of these final steps, it evaluates the D’Hondt pick (argmax w/(c+1)) and Sainte–Laguë pick (argmax w/(2c+1)), and commits the choice that yields the lower predicted new peak average, tie-breaking by a proxy for the new second-highest average and slightly favoring heavier experts when equal. This targets the critical tail where discrete effects matter most and should improve balancedness with negligible overhead.

No interfaces change; everything remains CPU/torch-friendly with careful handling of dtypes and edge cases (e.g., num_log == 1). The rest of the algorithm and packing stages are unmodified, preserving speed.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def _waterfill_counts_row(w: torch.Tensor, target_total: int) -> torch.Tensor:
    """
    Compute integer replica counts c_i >= 1 that approximately minimize max_i w_i / c_i
    subject to sum c_i == target_total using water-filling + greedy fill.
    w: 1D float tensor (CPU)
    """
    num_log = w.numel()
    assert target_total >= num_log
    if target_total == num_log:
        return torch.ones(num_log, dtype=torch.int64, device=w.device)

    maxw = float(w.max().item()) if num_log > 0 else 0.0
    # All-zero quick path: distribute evenly
    if maxw == 0.0:
        base = target_total // num_log
        rem = target_total % num_log
        counts = torch.full((num_log,), base, dtype=torch.int64, device=w.device)
        if base == 0:
            counts[:] = 1
            extras = target_total - num_log
            if extras > 0:
                counts[:extras] += 1
        else:
            if rem > 0:
                counts[:rem] += 1
        return counts

    lo = 0.0
    hi = max(maxw, 1.0)
    for _ in range(40):
        mid = 0.5 * (lo + hi)
        c = torch.ceil(w / mid).to(torch.int64)
        c = torch.maximum(c, torch.ones_like(c))
        s = int(c.sum().item())
        if s <= target_total:
            hi = mid
        else:
            lo = mid
    counts = torch.ceil(w / hi).to(torch.int64)
    counts = torch.maximum(counts, torch.ones_like(counts))
    s = int(counts.sum().item())
    extras = target_total - s
    while extras > 0:
        k = min(extras, num_log)
        score = w / counts.to(w.dtype)
        idx = torch.argsort(score, descending=True)[:k]
        counts[idx] += 1
        extras -= k
    return counts
=======
def _waterfill_counts_row(w: torch.Tensor, target_total: int) -> torch.Tensor:
    """
    Compute integer replica counts c_i >= 1 that approximately minimize max_i w_i / c_i
    subject to sum c_i == target_total using water-filling + greedy fill.
    w: 1D float tensor (CPU)
    """
    num_log = w.numel()
    assert target_total >= num_log
    if target_total == num_log:
        return torch.ones(num_log, dtype=torch.int64, device=w.device)

    maxw = float(w.max().item()) if num_log > 0 else 0.0
    # All-zero quick path: distribute evenly
    if maxw == 0.0:
        base = target_total // num_log
        rem = target_total % num_log
        counts = torch.full((num_log,), base, dtype=torch.int64, device=w.device)
        if base == 0:
            counts[:] = 1
            extras = target_total - num_log
            if extras > 0:
                counts[:extras] += 1
        else:
            if rem > 0:
                counts[:rem] += 1
        return counts

    lo = 0.0
    hi = max(maxw, 1.0)
    for _ in range(40):
        mid = 0.5 * (lo + hi)
        c = torch.ceil(w / mid).to(torch.int64)
        c = torch.maximum(c, torch.ones_like(c))
        s = int(c.sum().item())
        if s <= target_total:
            hi = mid
        else:
            lo = mid
    counts = torch.ceil(w / hi).to(torch.int64)
    counts = torch.maximum(counts, torch.ones_like(counts))
    s = int(counts.sum().item())

    # Peak-aware hybrid tail allocation:
    # - Do a fast bulk D'Hondt-like fill for the first extras - T seats
    # - For the last T seats, choose between D’Hondt and Sainte–Laguë picks
    #   by predicting the new peak average and selecting the lower one.
    extras = target_total - s
    if extras <= 0:
        return counts

    # Tail length: 10% of total redundant seats, capped by remaining extras
    total_redundant = target_total - num_log
    tail_T = max(1, int(round(0.1 * max(0, total_redundant))))
    tail = min(tail_T, extras)
    bulk = extras - tail

    # Fast bulk: add in k-chunks by highest w / c
    while bulk > 0:
        k = min(bulk, num_log)
        score = w / counts.to(w.dtype)
        idx = torch.argsort(score, descending=True)[:k]
        counts[idx] += 1
        bulk -= k

    # Recompute remaining as tail
    remaining = target_total - int(counts.sum().item())
    if remaining <= 0:
        return counts

    # Tail: hybrid D’Hondt vs Sainte–Laguë, peak-aware
    for _ in range(remaining):
        if num_log == 1:
            counts[0] += 1
            continue

        cfloat = counts.to(w.dtype)
        avg = w / torch.clamp(cfloat, min=1)

        # D’Hondt and Sainte–Laguë candidate indices
        d_scores = w / (cfloat + 1.0)
        s_scores = w / (2.0 * cfloat + 1.0)
        d_idx = int(torch.argmax(d_scores).item())
        s_idx = int(torch.argmax(s_scores).item())

        # Current top-2 averages to quickly get "others' max"
        topk_vals, topk_idx = torch.topk(avg, k=min(2, num_log))
        cur_max_val = float(topk_vals[0].item())
        cur_max_idx = int(topk_idx[0].item())
        cur_second_val = float(topk_vals[1].item()) if topk_vals.numel() >= 2 else float("-inf")

        best_idx = None
        best_key = None  # (new_peak, proxy_second, -expert_weight) to bias fixes to heavy experts on ties
        for cand in {d_idx, s_idx}:
            new_avg_cand = float((w[cand] / (cfloat[cand] + 1.0)).item())
            others_max = cur_second_val if cand == cur_max_idx else cur_max_val
            new_peak = max(others_max, new_avg_cand)
            proxy_second = others_max  # good proxy for the new second-highest avg
            tie_bias = -float(w[cand].item())
            key = (new_peak, proxy_second, tie_bias)
            if best_key is None or key < best_key:
                best_key = key
                best_idx = cand

        counts[best_idx] += 1

    return counts
>>>>>>> REPLACE

</DIFF>