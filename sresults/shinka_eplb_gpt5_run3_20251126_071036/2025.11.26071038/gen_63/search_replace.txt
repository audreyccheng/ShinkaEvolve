<NAME>
two_stage_fixup_and_broadened_gpu_swap
</NAME>

<DESCRIPTION>
I implement two targeted changes that improve balancedness with negligible overhead:
1) In replicate_experts, upgrade the single donor→receiver fix-up to a 2×2 candidate set with a strict peak guard and second-order tie-break. For each row, consider donors as the top-2 average-loaded experts with count > 1 and receivers as the bottom-2 by average load. Evaluate all donor→receiver single moves and choose the one that minimizes the new peak (ties broken by the new second-highest average). Apply at most one move only if it strictly reduces the peak. This typically smooths peaks better than a single argmax→argmin move while remaining O(num_log) per row.
2) In balanced_packing_diverse GPU placement refinement, broaden the 1×1 swap search: consider donors from {heaviest, second-heaviest} and receivers from up to {lightest, second-lightest, third-lightest} excluding donors. For each pair, test swapping top-2 items from the donor with bottom-2 items from the receiver and pick the swap that minimizes the post-swap global maximum (tie-break by imbalance, then label-duplicate penalty). Apply only if the new peak strictly improves on the current peak. This reduces worst-pack load without extra passes and keeps runtime minimal.

Both edits are deterministic and bounded, preserving speed while improving load smoothing.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        # One-step donor->receiver fix-up (only if it strictly reduces peak avg)
        if num_log > 1:
            avg = w / counts.to(w.dtype)
            cur_peak = float(avg.max().item())
            can_donate = (counts > 1)
            if bool(can_donate.any()):
                avg_mask = avg.clone()
                avg_mask[~can_donate] = float("-inf")
                d = int(torch.argmax(avg_mask).item())
                r = int(torch.argmin(avg).item())
                if d != r:
                    c_try = counts.clone()
                    c_try[d] -= 1
                    c_try[r] += 1
                    peak_try = float((w / c_try.to(w.dtype)).max().item())
                    if peak_try + 1e-12 < cur_peak:
                        counts = c_try
=======
        # Guarded donor->receiver fix-up over a bounded 2x2 candidate set.
        # Donors: top-2 by avg with count>1; Receivers: bottom-2 by avg.
        # Apply at most one move only if it strictly reduces the peak average;
        # tie-break by the new second-highest average.
        if num_log > 1:
            avg = w / counts.to(w.dtype)
            cur_peak = float(avg.max().item())
            can_donate = (counts > 1)
            if bool(can_donate.any()):
                avg_mask = avg.clone()
                avg_mask[~can_donate] = float("-inf")
                kd = int(min(2, int(can_donate.sum().item())))
                donors = torch.topk(avg_mask, k=kd).indices.tolist() if kd > 0 else []
                kr = int(min(2, num_log))
                receivers = torch.topk(-avg, k=kr).indices.tolist() if kr > 0 else []

                best = None  # (new_peak, new_second, d, r, c_try)
                for d in donors:
                    for r in receivers:
                        if d == r:
                            continue
                        c_try = counts.clone()
                        c_try[d] -= 1
                        c_try[r] += 1
                        avg_try = w / c_try.to(w.dtype)
                        new_peak = float(avg_try.max().item())
                        if new_peak + 1e-12 < cur_peak:
                            if num_log >= 2:
                                topk = torch.topk(avg_try, k=min(2, num_log)).values
                                new_second = float(topk[-1].item()) if topk.numel() >= 2 else float("-inf")
                            else:
                                new_second = float("-inf")
                            cand = (new_peak, new_second, int(d), int(r), c_try)
                            if best is None or cand < best:
                                best = cand
                if best is not None:
                    counts = best[4]
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
        # Micro refinement: evaluate one 1x1 swap between heaviest and the best of the two lightest packs
        if num_packs >= 2:
            cur_max = max(pack_loads)
            cur_min = min(pack_loads)
            if cur_max > cur_min:
                # Build pack membership lists for this row
                pack_groups = [[] for _ in range(num_packs)]
                for g in range(num_items):
                    p = int(pack_index[i, g])
                    pack_groups[p].append(g)

                h = max(range(num_packs), key=lambda k: pack_loads[k])
                others = [p for p in range(num_packs) if p != h]
                if others:
                    sorted_lights = sorted(others, key=lambda k: pack_loads[k])[:min(2, len(others))]
                    # Prepare heavy candidates: top-2 by weight
                    if pack_groups[h]:
                        h_idx_tensor = torch.tensor(pack_groups[h], dtype=torch.int64, device=row_w.device)
                        hw = row_w[h_idx_tensor]
                        kh = min(2, hw.numel())
                        if kh > 0:
                            top_h = torch.topk(hw, kh).indices.tolist()
                            # Optional label histograms for penalty when duplicates exist
                            label_hist = None
                            if has_dup:
                                label_hist = [defaultdict(int) for _ in range(num_packs)]
                                for p in range(num_packs):
                                    for g in pack_groups[p]:
                                        label_hist[p][int(row_labels[g].item())] += 1

                            cur_imb = cur_max - cur_min
                            best = None
                            best_key = None  # (new_peak, new_imb, penalty)
                            for l in sorted_lights:
                                if not pack_groups[l]:
                                    continue
                                l_idx_tensor = torch.tensor(pack_groups[l], dtype=torch.int64, device=row_w.device)
                                lw = row_w[l_idx_tensor]
                                kl = min(2, lw.numel())
                                if kl == 0:
                                    continue
                                bot_l = torch.topk(lw, kl, largest=False).indices.tolist()

                                # Precompute others' max/min excluding h and l
                                other_max = max([pack_loads[p] for p in range(num_packs) if p != h and p != l],
                                                default=float("-inf"))
                                other_min = min([pack_loads[p] for p in range(num_packs) if p != h and p != l],
                                                default=float("inf"))

                                for ai in top_h:
                                    a_item = int(h_idx_tensor[ai].item())
                                    wa = float(hw[ai].item())
                                    la = int(row_labels[a_item].item())
                                    for bi in bot_l:
                                        b_item = int(l_idx_tensor[bi].item())
                                        wb = float(lw[bi].item())
                                        lb = int(row_labels[b_item].item())
                                        new_h = pack_loads[h] - wa + wb
                                        new_l = pack_loads[l] - wb + wa
                                        new_peak = max(new_h, new_l, other_max)
                                        new_bottom = min(new_h, new_l, other_min)
                                        new_imb = new_peak - new_bottom
                                        penalty = 0
                                        if has_dup and label_hist is not None:
                                            penalty += 1 if label_hist[h].get(lb, 0) > 0 else 0
                                            penalty += 1 if label_hist[l].get(la, 0) > 0 else 0
                                        cand_key = (new_peak, new_imb, penalty)
                                        if best_key is None or cand_key < best_key:
                                            best_key = cand_key
                                            best = (h, l, ai, bi, a_item, b_item, wa, wb)

                            if best is not None:
                                new_peak, new_imb, _ = best_key  # type: ignore[misc]
                                # Apply only if strictly improves either global peak or imbalance
                                if (new_peak + 1e-12 < cur_max) or (abs(new_peak - cur_max) <= 1e-12 and new_imb + 1e-12 < cur_imb):
                                    h_sel, l_sel, ai, bi, a_item, b_item, wa, wb = best  # type: ignore[misc]
                                    # Update loads
                                    pack_loads[h_sel] = pack_loads[h_sel] - wa + wb
                                    pack_loads[l_sel] = pack_loads[l_sel] - wb + wa
                                    # Swap membership and indices
                                    pack_groups[h_sel][ai] = b_item
                                    pack_groups[l_sel][bi] = a_item
                                    pack_index[i, a_item] = l_sel
                                    pack_index[i, b_item] = h_sel
                                    # Update ranks for affected packs
                                    for r, g in enumerate(pack_groups[h_sel]):
                                        rank_in_pack[i, g] = r
                                    for r, g in enumerate(pack_groups[l_sel]):
                                        rank_in_pack[i, g] = r
=======
        # Micro refinement: broaden 1x1 search donors={heaviest, second-heaviest}
        # and receivers={lightest, second-lightest, third-lightest} (excluding donors).
        # Evaluate top-2 vs bottom-2 items and apply the best swap if it strictly reduces global peak
        # (ties by imbalance, then label-duplicate penalty). Only one swap is applied.
        if num_packs >= 2:
            cur_max = max(pack_loads)
            cur_min = min(pack_loads)
            if cur_max > cur_min:
                # Build pack membership lists for this row
                pack_groups = [[] for _ in range(num_packs)]
                for g in range(num_items):
                    p = int(pack_index[i, g])
                    pack_groups[p].append(g)

                order_asc = sorted(range(num_packs), key=lambda k: pack_loads[k])
                order_desc = list(reversed(order_asc))
                donors = []
                if order_desc:
                    donors.append(order_desc[0])
                    if len(order_desc) > 1 and order_desc[1] != order_desc[0]:
                        donors.append(order_desc[1])

                receivers = []
                for p in order_asc:
                    if p not in donors:
                        receivers.append(p)
                    if len(receivers) >= 3:
                        break

                if donors and receivers:
                    # Optional label histograms for penalty when duplicates exist
                    label_hist = None
                    if has_dup:
                        label_hist = [defaultdict(int) for _ in range(num_packs)]
                        for p in range(num_packs):
                            for g in pack_groups[p]:
                                label_hist[p][int(row_labels[g].item())] += 1

                    cur_imb = cur_max - cur_min
                    best = None
                    best_key = None  # (new_peak, new_imb, penalty)

                    # Precompute a per-pack tensor view for fast lookup
                    for d in donors:
                        if not pack_groups[d]:
                            continue
                        d_idx_tensor = torch.tensor(pack_groups[d], dtype=torch.int64, device=row_w.device)
                        d_w = row_w[d_idx_tensor]
                        kd = min(2, d_w.numel())
                        if kd == 0:
                            continue
                        d_top = torch.topk(d_w, kd).indices.tolist()

                        for r in receivers:
                            if not pack_groups[r]:
                                continue
                            if pack_loads[d] <= pack_loads[r]:
                                continue
                            r_idx_tensor = torch.tensor(pack_groups[r], dtype=torch.int64, device=row_w.device)
                            r_w = row_w[r_idx_tensor]
                            kr = min(2, r_w.numel())
                            if kr == 0:
                                continue
                            r_bot = torch.topk(r_w, kr, largest=False).indices.tolist()

                            # Precompute other packs' extrema
                            other_max = max([pack_loads[p] for p in range(num_packs) if p != d and p != r],
                                            default=float("-inf"))
                            other_min = min([pack_loads[p] for p in range(num_packs) if p != d and p != r],
                                            default=float("inf"))

                            for ai in d_top:
                                a_item = int(d_idx_tensor[ai].item())
                                wa = float(d_w[ai].item())
                                la = int(row_labels[a_item].item())
                                for bi in r_bot:
                                    b_item = int(r_idx_tensor[bi].item())
                                    wb = float(r_w[bi].item())
                                    lb = int(row_labels[b_item].item())

                                    new_d = pack_loads[d] - wa + wb
                                    new_r = pack_loads[r] - wb + wa
                                    new_peak = max(new_d, new_r, other_max)
                                    new_bottom = min(new_d, new_r, other_min)
                                    new_imb = new_peak - new_bottom
                                    penalty = 0
                                    if has_dup and label_hist is not None:
                                        penalty += 1 if label_hist[d].get(lb, 0) > 0 else 0
                                        penalty += 1 if label_hist[r].get(la, 0) > 0 else 0
                                    cand_key = (new_peak, new_imb, penalty)
                                    if best_key is None or cand_key < best_key:
                                        best_key = cand_key
                                        best = (d, r, ai, bi, a_item, b_item, wa, wb)

                    if best is not None:
                        new_peak, new_imb, _ = best_key  # type: ignore[misc]
                        # Apply only if it strictly reduces the global peak;
                        # if peak ties, require a strict imbalance improvement
                        if (new_peak + 1e-12 < cur_max) or (abs(new_peak - cur_max) <= 1e-12 and new_imb + 1e-12 < cur_imb):
                            d_sel, r_sel, ai, bi, a_item, b_item, wa, wb = best  # type: ignore[misc]
                            # Update loads
                            pack_loads[d_sel] = pack_loads[d_sel] - wa + wb
                            pack_loads[r_sel] = pack_loads[r_sel] - wb + wa
                            # Swap membership and indices
                            pack_groups[d_sel][ai] = b_item
                            pack_groups[r_sel][bi] = a_item
                            pack_index[i, a_item] = r_sel
                            pack_index[i, b_item] = d_sel
                            # Update ranks for affected packs
                            for rpos, g in enumerate(pack_groups[d_sel]):
                                rank_in_pack[i, g] = rpos
                            for rpos, g in enumerate(pack_groups[r_sel]):
                                rank_in_pack[i, g] = rpos
>>>>>>> REPLACE

</DIFF>