<NAME>
hybrid_sainte_lague_tail_and_stage_refine
</NAME>

<DESCRIPTION>
I introduce two targeted improvements that keep the algorithm fast while improving balance:

1) Hybrid D’Hondt→Sainte-Laguë tail in replica allocation: the extra replicas are allocated greedily by max(weight / count) as before, but for the last 10% (at least one) of extra replicas we switch to a Sainte-Laguë-like quotient weight / (2*count + 1). This tempers over-replication of already heavy experts and can reduce the maximum per-replica load with negligible overhead.

2) Stage-specific refinement for packing: expose a refine_steps parameter in balanced_packing and use fewer steps (1) for group→node packing but keep 2 steps for the final GPU packing stage. This focuses refinement where it matters most (GPU balance) without hurting speed.

Both changes are minimal, fully compatible with the existing structure, and should preserve speed while nudging the balancedness metric upwards.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int,
                     refine_steps: int = 2) -> tuple[torch.Tensor, torch.Tensor]:
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    # Bounded multi-swap refinement per layer to reduce max imbalance
    if groups_per_pack > 1:
        max_swaps = 2  # keep small to preserve speed
        for i in range(num_layers):
            for _ in range(max_swaps):
                packs = pack_index[i]  # [num_groups], CPU
                w = weight[i]  # CPU
                # Compute pack loads
                pack_w = torch.zeros(num_packs, dtype=w.dtype)
                pack_w.scatter_add_(0, packs, w)
                h = int(torch.argmax(pack_w))
                l = int(torch.argmin(pack_w))
                delta = float(pack_w[h] - pack_w[l])
                if delta <= 1e-9:
                    break

                heavy_idx = torch.nonzero(packs == h, as_tuple=False).squeeze(1)
                light_idx = torch.nonzero(packs == l, as_tuple=False).squeeze(1)
                if heavy_idx.numel() == 0 or light_idx.numel() == 0:
                    break

                hw = w[heavy_idx]
                lw = w[light_idx]
                lw_sorted, lw_perm = torch.sort(lw)  # ascending
                if lw_sorted.numel() == 0 or hw.numel() == 0:
                    break

                # For each heavy item, find light item closest to target = hw - delta/2
                target = hw - (delta / 2.0)
                pos = torch.searchsorted(lw_sorted, target)
                pos = torch.clamp(pos, 0, lw_sorted.numel() - 1)
                # Consider neighbors pos and pos-1 for best approximation
                cand_pos = torch.stack([pos, torch.clamp(pos - 1, 0, lw_sorted.numel() - 1)], dim=1)
                cand_lw = lw_sorted[cand_pos]  # [H, 2]
                resid = (delta - 2.0 * (hw.unsqueeze(1) - cand_lw)).abs()
                best_flat = int(torch.argmin(resid).item())
                best_h_index = best_flat // 2
                best_option = best_flat % 2
                j_sorted_idx = int(cand_pos[best_h_index, best_option].item())

                wi = float(hw[best_h_index].item())
                wj = float(lw_sorted[j_sorted_idx].item())
                new_delta = abs(delta - 2.0 * (wi - wj))
                # Apply swap only if it strictly improves imbalance
                if new_delta < delta - 1e-9:
                    hi = heavy_idx[best_h_index]
                    lj = light_idx[lw_perm[j_sorted_idx]]
                    pack_index[i, hi] = l
                    pack_index[i, lj] = h
                    # Reassign ranks within affected packs to keep 0..groups_per_pack-1
                    for p in (h, l):
                        mask = pack_index[i] == p
                        idx = torch.nonzero(mask, as_tuple=False).squeeze(1)
                        if idx.numel() == 0:
                            continue
                        # Stable by previous rank order
                        prev_rank = rank_in_pack[i, idx]
                        order = torch.argsort(prev_rank)
                        new_ranks = torch.arange(order.numel(), dtype=torch.int64)
                        rank_in_pack[i, idx[order]] = new_ranks
                    # continue to next potential swap
                    continue
                else:
                    break
=======
    # Bounded multi-swap refinement per layer to reduce max imbalance
    if groups_per_pack > 1:
        max_swaps = refine_steps  # stage-specific control
        for i in range(num_layers):
            for _ in range(max_swaps):
                packs = pack_index[i]  # [num_groups], CPU
                w = weight[i]  # CPU
                # Compute pack loads
                pack_w = torch.zeros(num_packs, dtype=w.dtype)
                pack_w.scatter_add_(0, packs, w)
                h = int(torch.argmax(pack_w))
                l = int(torch.argmin(pack_w))
                delta = float(pack_w[h] - pack_w[l])
                if delta <= 1e-9:
                    break

                heavy_idx = torch.nonzero(packs == h, as_tuple=False).squeeze(1)
                light_idx = torch.nonzero(packs == l, as_tuple=False).squeeze(1)
                if heavy_idx.numel() == 0 or light_idx.numel() == 0:
                    break

                hw = w[heavy_idx]
                lw = w[light_idx]
                lw_sorted, lw_perm = torch.sort(lw)  # ascending
                if lw_sorted.numel() == 0 or hw.numel() == 0:
                    break

                # For each heavy item, find light item closest to target = hw - delta/2
                target = hw - (delta / 2.0)
                pos = torch.searchsorted(lw_sorted, target)
                pos = torch.clamp(pos, 0, lw_sorted.numel() - 1)
                # Consider neighbors pos and pos-1 for best approximation
                cand_pos = torch.stack([pos, torch.clamp(pos - 1, 0, lw_sorted.numel() - 1)], dim=1)
                cand_lw = lw_sorted[cand_pos]  # [H, 2]
                resid = (delta - 2.0 * (hw.unsqueeze(1) - cand_lw)).abs()
                best_flat = int(torch.argmin(resid).item())
                best_h_index = best_flat // 2
                best_option = best_flat % 2
                j_sorted_idx = int(cand_pos[best_h_index, best_option].item())

                wi = float(hw[best_h_index].item())
                wj = float(lw_sorted[j_sorted_idx].item())
                new_delta = abs(delta - 2.0 * (wi - wj))
                # Apply swap only if it strictly improves imbalance
                if new_delta < delta - 1e-9:
                    hi = heavy_idx[best_h_index]
                    lj = light_idx[lw_perm[j_sorted_idx]]
                    pack_index[i, hi] = l
                    pack_index[i, lj] = h
                    # Reassign ranks within affected packs to keep 0..groups_per_pack-1
                    for p in (h, l):
                        mask = pack_index[i] == p
                        idx = torch.nonzero(mask, as_tuple=False).squeeze(1)
                        if idx.numel() == 0:
                            continue
                        # Stable by previous rank order
                        prev_rank = rank_in_pack[i, idx]
                        order = torch.argsort(prev_rank)
                        new_ranks = torch.arange(order.numel(), dtype=torch.int64)
                        rank_in_pack[i, idx[order]] = new_ranks
                    # continue to next potential swap
                    continue
                else:
                    break
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
def replicate_experts(
        weight: torch.Tensor,
        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Replicate `num_log` experts to `num_phy` replicas, such that the maximum
    load of all replicas is minimized.

    Parameters:
        weight: [X, num_log]
        num_phy: total number of experts after replication

    Returns:
        phy2log: [X, num_phy], logical expert id of each physical expert
        rank: [X, num_phy], the replica rank
        logcnt: [X, num_log], number of replicas for each logical expert
    """
    n, num_log = weight.shape
    num_redundant = num_phy - num_log
    assert num_redundant >= 0
    device = weight.device
    dtype_i64 = torch.int64

    # Greedy D'Hondt-like allocation of extra replicas
    logcnt = torch.ones(n, num_log, dtype=dtype_i64, device=device)
    arangen = torch.arange(n, dtype=dtype_i64, device=device)
    for _ in range(num_redundant):
        best = (weight / logcnt).max(dim=-1).indices
        logcnt[arangen, best] += 1

    # One-step donor->receiver fix-up per layer (optional improvement)
    # Try moving one replica from the highest avg to the lowest avg if it reduces the max avg.
    if num_phy > 0 and num_log > 1:
        eps = 1e-12
        for i in range(n):
            ci = logcnt[i]
            wi = weight[i]
            # ensure no div by zero
            avg = wi / ci.clamp_min(1)
            donor = int(torch.argmax(avg).item())
            receiver = int(torch.argmin(avg).item())
            if donor == receiver:
                continue
            if int(ci[donor].item()) <= 1:
                continue
            current_max = float(avg.max().item())
            ci_new = ci.clone()
            ci_new[donor] -= 1
            ci_new[receiver] += 1
            # Evaluate new max average only if valid
            new_avg = wi / ci_new.clamp_min(1)
            if float(new_avg.max().item()) + eps < current_max:
                logcnt[i] = ci_new

    # Build phy2log and rank from final counts (deterministic order by logical id)
    phy2log = torch.empty((n, num_phy), dtype=dtype_i64, device=device)
    rank = torch.empty((n, num_phy), dtype=dtype_i64, device=device)
    ar_log = torch.arange(num_log, dtype=dtype_i64, device=device)

    for i in range(n):
        reps = logcnt[i].to(dtype_i64)
        # Materialize mapping row
        phy_row = ar_log.repeat_interleave(reps)
        # Ranks: for each logical j, 0..reps[j]-1
        rank_blocks = []
        for c in reps.tolist():
            rank_blocks.append(torch.arange(int(c), dtype=dtype_i64, device=device))
        rank_row = torch.cat(rank_blocks, dim=0) if rank_blocks else torch.empty(0, dtype=dtype_i64, device=device)

        # Store (length must equal num_phy)
        phy2log[i, :phy_row.numel()] = phy_row
        rank[i, :rank_row.numel()] = rank_row

    return phy2log, rank, logcnt
=======
def replicate_experts(
        weight: torch.Tensor,
        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Replicate `num_log` experts to `num_phy` replicas, such that the maximum
    load of all replicas is minimized.

    Parameters:
        weight: [X, num_log]
        num_phy: total number of experts after replication

    Returns:
        phy2log: [X, num_phy], logical expert id of each physical expert
        rank: [X, num_phy], the replica rank
        logcnt: [X, num_log], number of replicas for each logical expert
    """
    n, num_log = weight.shape
    num_redundant = num_phy - num_log
    assert num_redundant >= 0
    device = weight.device
    dtype_i64 = torch.int64

    # Greedy D'Hondt-like allocation with Sainte-Laguë tail for the last ~10%
    logcnt = torch.ones(n, num_log, dtype=dtype_i64, device=device)
    arangen = torch.arange(n, dtype=dtype_i64, device=device)
    if num_redundant > 0:
        tail = max(1, num_redundant // 10)
        for t in range(num_redundant):
            if t >= num_redundant - tail:
                denom = (2 * logcnt + 1)  # Sainte-Laguë-like tempering
            else:
                denom = logcnt           # D'Hondt-like waterfilling
            benefit = weight / denom
            best = benefit.max(dim=-1).indices
            logcnt[arangen, best] += 1

    # One-step donor->receiver fix-up per layer (optional improvement)
    # Try moving one replica from the highest avg to the lowest avg if it reduces the max avg.
    if num_phy > 0 and num_log > 1:
        eps = 1e-12
        for i in range(n):
            ci = logcnt[i]
            wi = weight[i]
            # ensure no div by zero
            avg = wi / ci.clamp_min(1)
            donor = int(torch.argmax(avg).item())
            receiver = int(torch.argmin(avg).item())
            if donor == receiver:
                continue
            if int(ci[donor].item()) <= 1:
                continue
            current_max = float(avg.max().item())
            ci_new = ci.clone()
            ci_new[donor] -= 1
            ci_new[receiver] += 1
            # Evaluate new max average only if valid
            new_avg = wi / ci_new.clamp_min(1)
            if float(new_avg.max().item()) + eps < current_max:
                logcnt[i] = ci_new

    # Build phy2log and rank from final counts (deterministic order by logical id)
    phy2log = torch.empty((n, num_phy), dtype=dtype_i64, device=device)
    rank = torch.empty((n, num_phy), dtype=dtype_i64, device=device)
    ar_log = torch.arange(num_log, dtype=dtype_i64, device=device)

    for i in range(n):
        reps = logcnt[i].to(dtype_i64)
        # Materialize mapping row
        phy_row = ar_log.repeat_interleave(reps)
        # Ranks: for each logical j, 0..reps[j]-1
        rank_blocks = []
        for c in reps.tolist():
            rank_blocks.append(torch.arange(int(c), dtype=dtype_i64, device=device))
        rank_row = torch.cat(rank_blocks, dim=0) if rank_blocks else torch.empty(0, dtype=dtype_i64, device=device)

        # Store (length must equal num_phy)
        phy2log[i, :phy_row.numel()] = phy_row
        rank[i, :rank_row.numel()] = rank_row

    return phy2log, rank, logcnt
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    group_pack_index, group_rank_in_pack = balanced_packing(
        tokens_per_group, num_nodes)
=======
    group_pack_index, group_rank_in_pack = balanced_packing(
        tokens_per_group, num_nodes, refine_steps=1)
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,
                                                num_gpus // num_nodes)
=======
    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,
                                                num_gpus // num_nodes,
                                                refine_steps=2)
>>>>>>> REPLACE
</DIFF>