<NAME>
peak_aware_tail_and_fixup_tiebreaks
</NAME>

<DESCRIPTION>
This edit improves both load balancing quality and keeps runtime efficiency by:
1) Making the water-filling tail allocation peak-aware with a stronger tie-break: at each adaptive step we now select between D’Hondt and Sainte–Laguë using a tuple objective (new_peak, new_second_peak, receiver_count, -new_avg), preferring to reduce the global and secondary peak while spreading replicas across more experts.
2) Strengthening the donor→receiver fix-up in replicate_experts_waterfill: among top-2 donors and bottom-2 receivers, we now minimize (new_peak, new_second_peak, donor_post_move_avg) and still apply up to 2 moves only when the global peak strictly reduces. This yields better peak smoothing without increasing complexity.
3) Making GPU pack refinement steps adaptive with an extra pass only when the residual imbalance warrants it: 1/2/3 refinement steps based on relative imbalance thresholds (<=2%, 2–12%, >12%) to target difficult rows while preserving speed.

These targeted changes align with the potential recommendations and aim to lift balancedness without hurting the speed score.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Greedy water-filling with peak-aware hybrid tail
    extras = target_total - s
    if extras > 0:
        # Bulk D'Hondt (w/c) fill except for a small adaptive tail
        num_redundant = max(0, target_total - num_log)
        tail = min(extras, max(1, int(round(0.1 * num_redundant))))
        bulk = extras - tail

        # Fast batched fill for bulk
        while bulk > 0:
            k = min(bulk, num_log)
            scores = w / counts.to(w.dtype)
            topk_idx = torch.argsort(scores, descending=True)[:k]
            counts[topk_idx] += 1
            bulk -= k

        # Adaptive tail: choose between D'Hondt and Sainte–Laguë at each step
        for _ in range(tail):
            counts_f = counts.to(w.dtype)
            avg = w / counts_f
            if num_log >= 2:
                vals, idxs = torch.topk(avg, k=2)
                peak1_val = float(vals[0].item())
                peak1_idx = int(idxs[0].item())
                peak2_val = float(vals[1].item())
            else:
                peak1_val = float(avg[0].item())
                peak1_idx = 0
                peak2_val = float("-inf")

            # D'Hondt pick (argmax w/c == argmax avg)
            d_pick = int(torch.argmax(avg).item())
            # Sainte–Laguë pick (argmax w/(2c+1))
            denom = (2 * counts + 1).to(w.dtype)
            s_scores = w / denom
            s_pick = int(torch.argmax(s_scores).item())

            best_choice = None
            best_key = None  # (new_peak, new_second, -new_avg_chosen)

            for pick in (d_pick, s_pick):
                if best_choice is not None and pick == best_choice:
                    continue
                new_avg_chosen = float((w[pick] / (counts_f[pick] + 1.0)).item())
                # Predict new peak and second-highest after this assignment
                if pick == peak1_idx:
                    new_peak = max(new_avg_chosen, peak2_val)
                    new_second = min(new_avg_chosen, peak2_val)
                else:
                    new_peak = max(peak1_val, new_avg_chosen)
                    new_second = (peak1_val if new_avg_chosen >= peak1_val else max(new_avg_chosen, peak2_val))
                key = (new_peak, new_second, -new_avg_chosen)
                if best_key is None or key < best_key:
                    best_key = key
                    best_choice = pick

            counts[best_choice] += 1

    return counts
=======
    # Greedy water-filling with peak-aware hybrid tail
    extras = target_total - s
    if extras > 0:
        # Bulk D'Hondt (w/c) fill except for a small adaptive tail
        num_redundant = max(0, target_total - num_log)
        tail = min(extras, max(1, int(round(0.1 * num_redundant))))
        bulk = extras - tail

        # Fast batched fill for bulk (use topk for efficiency)
        while bulk > 0:
            k = min(bulk, num_log)
            scores = w / counts.to(w.dtype)
            topk_idx = torch.topk(scores, k=k, largest=True).indices
            counts[topk_idx] += 1
            bulk -= k

        # Adaptive tail: choose between D'Hondt and Sainte–Laguë at each step,
        # minimizing (new_peak, new_second_peak, receiver_count, -new_avg_chosen)
        for _ in range(tail):
            counts_f = counts.to(w.dtype)
            avg = w / counts_f
            if num_log >= 2:
                vals, idxs = torch.topk(avg, k=2)
                peak1_val = float(vals[0].item())
                peak1_idx = int(idxs[0].item())
                peak2_val = float(vals[1].item())
            else:
                peak1_val = float(avg[0].item())
                peak1_idx = 0
                peak2_val = float("-inf")

            # D'Hondt pick (argmax w/c == argmax avg)
            d_pick = int(torch.argmax(avg).item())
            # Sainte–Laguë pick (argmax w/(2c+1))
            denom = (2 * counts + 1).to(w.dtype)
            s_scores = w / denom
            s_pick = int(torch.argmax(s_scores).item())

            best_choice = None
            best_key = None  # (new_peak, new_second, receiver_cnt, -new_avg)

            for pick in (d_pick, s_pick):
                if best_choice is not None and pick == best_choice:
                    continue
                new_avg_chosen = float((w[pick] / (counts_f[pick] + 1.0)).item())
                # Predict new peak and second-highest after this assignment
                if pick == peak1_idx:
                    new_peak = max(new_avg_chosen, peak2_val)
                    new_second = min(new_avg_chosen, peak2_val)
                else:
                    new_peak = max(peak1_val, new_avg_chosen)
                    new_second = (peak1_val if new_avg_chosen >= peak1_val else max(new_avg_chosen, peak2_val))
                receiver_cnt = int(counts[pick].item())
                key = (new_peak, new_second, receiver_cnt, -new_avg_chosen)
                if best_key is None or key < best_key:
                    best_key = key
                    best_choice = pick

            counts[best_choice] += 1

    return counts
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
                best_pair = None
                best_peak = cur_peak
                for d in donors:
                    for r in receivers:
                        if d == r:
                            continue
                        c_try = counts.clone()
                        c_try[d] -= 1
                        c_try[r] += 1
                        avg_try = w / c_try.to(w.dtype)
                        peak = float(avg_try.max().item())
                        if peak + 1e-9 < best_peak:
                            best_peak = peak
                            best_pair = (d, r)

                if best_pair is None:
                    break
                d, r = best_pair
                counts[d] -= 1
                counts[r] += 1
=======
                # Peak-aware selection with stronger tie-breakers:
                # minimize (new_peak, new_second_peak, donor_post_move_avg)
                best_pair = None
                best_key = None
                eps = 1e-9
                for d in donors:
                    for r in receivers:
                        if d == r or counts[d] <= 1:
                            continue
                        c_try = counts.clone()
                        c_try[d] -= 1
                        c_try[r] += 1
                        avg_try = w / c_try.to(w.dtype)
                        new_peak = float(avg_try.max().item())
                        if new_peak + eps >= cur_peak:
                            continue
                        # second-highest after the move
                        k2 = min(2, num_log)
                        top2_vals = torch.topk(avg_try, k=k2).values
                        if top2_vals.numel() >= 2:
                            new_second = float(top2_vals[1].item())
                        else:
                            new_second = float("-inf")
                        donor_post = float((w[d] / float(counts[d] - 1)).item()) if counts[d] > 1 else float("inf")
                        cand_key = (new_peak, new_second, donor_post)
                        if best_key is None or cand_key < best_key:
                            best_key = cand_key
                            best_pair = (d, r)

                if best_pair is None:
                    break
                d, r = best_pair
                counts[d] -= 1
                counts[r] += 1
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
            # Adaptive depth: two passes only when imbalance is high
            mean_load = sum(loads) / max(1, num_packs)
            delta = max(loads) - min(loads)
            refine_steps = 2 if (mean_load > 0 and (delta / max(mean_load, 1e-12) > 0.12)) else 1
            for _ in range(refine_steps):
                changed = _refine_once()
                if not changed:
                    break
=======
            # Adaptive depth: more passes only when residual imbalance is notable
            mean_load = sum(loads) / max(1, num_packs)
            delta = max(loads) - min(loads)
            rel = (delta / max(mean_load, 1e-12)) if mean_load > 0 else 0.0
            if rel > 0.12:
                refine_steps = 3
            elif rel > 0.02:
                refine_steps = 2
            else:
                refine_steps = 1
            for _ in range(refine_steps):
                changed = _refine_once()
                if not changed:
                    break
>>>>>>> REPLACE

</DIFF>