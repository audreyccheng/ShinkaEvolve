<NAME>
diverse_pack_projload_and_incremental_ratio
</NAME>

<DESCRIPTION>
This edit improves load balancing with minimal overhead by:
1) Using projected load as a tie-breaker in the diversity-aware packing (balanced_packing_diverse), which better anticipates the impact of each assignment on the maximum load per pack. It also improves the no-duplicate fast path to use projected load.
2) Adding a strictly bounded, lightweight 2-opt refinement to balanced_packing, performing up to four beneficial swaps between the heaviest and lightest packs to reduce imbalance without noticeable runtime cost.
3) Optimizing replicate_experts by maintaining an incremental ratio array and updating only the affected entries per iteration, replacing repeated division and max operations. This maintains the same greedy policy while reducing computational overhead.

Together, these changes aim to increase balancedness while preserving speed.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=weight.device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    indices = weight.float().sort(-1, descending=True).indices.cpu()
    pack_index = torch.full_like(weight,
                                 fill_value=-1,
                                 dtype=torch.int64,
                                 device="cpu")
    rank_in_pack = torch.full_like(pack_index, fill_value=-1)
    for i in range(num_layers):
        pack_weights = [0] * num_packs
        pack_items = [0] * num_packs
        for group in indices[i]:
            pack = min(
                (i
                 for i in range(num_packs) if pack_items[i] < groups_per_pack),
                key=pack_weights.__getitem__,
            )
            assert pack_items[pack] < groups_per_pack
            pack_index[i, group] = pack
            rank_in_pack[i, group] = pack_items[pack]
            pack_weights[pack] += weight[i, group]
            pack_items[pack] += 1
    return pack_index, rank_in_pack
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_groups = weight.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weight.size(-1),
                                  dtype=torch.int64,
                                  device=weight.device).expand(weight.shape)
        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
        return pack_index, rank_in_pack

    indices = weight.float().sort(-1, descending=True).indices.cpu()
    pack_index = torch.full_like(weight,
                                 fill_value=-1,
                                 dtype=torch.int64,
                                 device="cpu")
    rank_in_pack = torch.full_like(pack_index, fill_value=-1)
    for i in range(num_layers):
        pack_weights = [0.0] * num_packs
        pack_items = [0] * num_packs
        # Greedy assignment by current load
        for group in indices[i]:
            pack = min(
                (pi for pi in range(num_packs)
                 if pack_items[pi] < groups_per_pack),
                key=lambda k: pack_weights[k],
            )
            pack_index[i, group] = pack
            rank_in_pack[i, group] = pack_items[pack]
            pack_weights[pack] += float(weight[i, group].item())
            pack_items[pack] += 1

        # Lightweight local refinement: swap between heaviest and lightest packs
        # Perform up to 4 swaps to reduce max-min imbalance
        max_refine = min(4, groups_per_pack)
        if max_refine > 0:
            wi = weight[i].cpu()
            # Build pack -> groups mapping
            pack_groups = [[] for _ in range(num_packs)]
            for g in range(num_groups):
                p = int(pack_index[i, g])
                pack_groups[p].append(g)
            # Recompute precise pack weights
            pack_weights_f = [0.0] * num_packs
            for p in range(num_packs):
                if pack_groups[p]:
                    pack_weights_f[p] = float(wi[pack_groups[p]].sum().item())
            # Try a few targeted swaps
            for _ in range(max_refine):
                h = max(range(num_packs), key=lambda k: pack_weights_f[k])
                l = min(range(num_packs), key=lambda k: pack_weights_f[k])
                diff = pack_weights_f[h] - pack_weights_f[l]
                if diff <= 0 or not pack_groups[h] or not pack_groups[l]:
                    break
                h_weights = wi[pack_groups[h]]
                l_weights = wi[pack_groups[l]]
                a_idx = int(torch.argmax(h_weights).item())
                b_idx = int(torch.argmin(l_weights).item())
                a = pack_groups[h][a_idx]
                b = pack_groups[l][b_idx]
                wa = float(h_weights[a_idx].item())
                wb = float(l_weights[b_idx].item())
                if wa <= wb:
                    break
                # Swap
                pack_groups[h][a_idx] = b
                pack_groups[l][b_idx] = a
                pack_index[i, a] = l
                pack_index[i, b] = h
                pack_weights_f[h] += (wb - wa)
                pack_weights_f[l] += (wa - wb)
            # Recompute ranks after refinement
            for p in range(num_packs):
                for r, g in enumerate(pack_groups[p]):
                    rank_in_pack[i, g] = r
    return pack_index, rank_in_pack
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def balanced_packing_diverse(weight: torch.Tensor,
                             label: torch.Tensor,
                             num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Balanced packing with diversity: prefer spreading items with the same label
    across different packs to reduce hotspotting when replicas of the same
    logical expert are placed on the same GPU.

    Parameters:
        weight: [X, n], the weight of each item
        label:  [X, n], an integer label for each item (e.g., logical expert id)
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_items = weight.shape
    assert num_items % num_packs == 0
    items_per_pack = num_items // num_packs

    # Trivial case: one item per pack, fallback to standard balanced packing
    if items_per_pack == 1 or num_packs == 1:
        return balanced_packing(weight, num_packs)

    # Ensure CPU tensors for Python-side loops
    device = weight.device
    w = weight.float()
    labels = label.to(dtype=torch.int64, device=device)

    # Pre-sort indices by descending weights per row
    sorted_indices = w.sort(dim=-1, descending=True).indices

    pack_index = torch.full_like(weight,
                                 fill_value=-1,
                                 dtype=torch.int64,
                                 device=device)
    rank_in_pack = torch.full_like(pack_index, fill_value=-1)

    for i in range(num_layers):
        row_sorted = sorted_indices[i].tolist()
        row_w = w[i]
        row_labels = labels[i]

        # Quick duplicate check to avoid overhead when not needed
        seen = set()
        has_dup = False
        for idx in range(num_items):
            l = int(row_labels[idx].item())
            if l in seen:
                has_dup = True
                break
            seen.add(l)

        if not has_dup:
            # No duplicate labels; standard greedy suffices and is faster
            pack_loads = [0.0] * num_packs
            pack_counts = [0] * num_packs
            for group in row_sorted:
                # choose among packs with capacity the one with min load
                best_p = min(
                    (p for p in range(num_packs) if pack_counts[p] < items_per_pack),
                    key=lambda p: pack_loads[p],
                )
                pack_index[i, group] = best_p
                rank_in_pack[i, group] = pack_counts[best_p]
                pack_counts[best_p] += 1
                pack_loads[best_p] += float(row_w[group].item())
            continue

        # Diversity-aware greedy
        pack_loads = [0.0] * num_packs
        pack_counts = [0] * num_packs
        label_counts = [defaultdict(int) for _ in range(num_packs)]

        for group in row_sorted:
            lab = int(row_labels[group].item())
            # Select pack preferring fewer repetitions of label, then lower load, then fewer items
            best_p = None
            best_key = None
            for p in range(num_packs):
                if pack_counts[p] >= items_per_pack:
                    continue
                rep = label_counts[p].get(lab, 0)
                key = (rep, pack_loads[p], pack_counts[p])
                if best_key is None or key < best_key:
                    best_key = key
                    best_p = p
            # Assign
            pack_index[i, group] = best_p
            rank_in_pack[i, group] = pack_counts[best_p]
            pack_counts[best_p] += 1
            pack_loads[best_p] += float(row_w[group].item())
            label_counts[best_p][lab] += 1

    return pack_index, rank_in_pack
=======
def balanced_packing_diverse(weight: torch.Tensor,
                             label: torch.Tensor,
                             num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Balanced packing with diversity: prefer spreading items with the same label
    across different packs to reduce hotspotting when replicas of the same
    logical expert are placed on the same GPU.

    Parameters:
        weight: [X, n], the weight of each item
        label:  [X, n], an integer label for each item (e.g., logical expert id)
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_items = weight.shape
    assert num_items % num_packs == 0
    items_per_pack = num_items // num_packs

    # Trivial case: one item per pack, fallback to standard balanced packing
    if items_per_pack == 1 or num_packs == 1:
        return balanced_packing(weight, num_packs)

    device = weight.device
    w = weight.float()
    labels = label.to(dtype=torch.int64, device=device)

    # Pre-sort indices by descending weights per row
    sorted_indices = w.sort(dim=-1, descending=True).indices

    pack_index = torch.full_like(weight,
                                 fill_value=-1,
                                 dtype=torch.int64,
                                 device=device)
    rank_in_pack = torch.full_like(pack_index, fill_value=-1)

    for i in range(num_layers):
        row_sorted = sorted_indices[i].tolist()
        row_w = w[i]
        row_labels = labels[i]

        # Fast duplicate check via unique
        unique_labels = torch.unique(row_labels, sorted=False)
        has_dup = (unique_labels.numel() != num_items)

        if not has_dup:
            # No duplicate labels; use projected load minimization
            pack_loads = [0.0] * num_packs
            pack_counts = [0] * num_packs
            for group in row_sorted:
                wv = float(row_w[group].item())
                best_p = None
                best_key = None
                for p in range(num_packs):
                    if pack_counts[p] >= items_per_pack:
                        continue
                    key = (pack_loads[p] + wv, pack_counts[p])
                    if best_key is None or key < best_key:
                        best_key = key
                        best_p = p
                pack_index[i, group] = best_p
                rank_in_pack[i, group] = pack_counts[best_p]
                pack_counts[best_p] += 1
                pack_loads[best_p] += wv
            continue

        # Diversity-aware greedy with projected load
        pack_loads = [0.0] * num_packs
        pack_counts = [0] * num_packs
        label_counts = [defaultdict(int) for _ in range(num_packs)]

        for group in row_sorted:
            lab = int(row_labels[group].item())
            wv = float(row_w[group].item())
            best_p = None
            best_key = None
            for p in range(num_packs):
                if pack_counts[p] >= items_per_pack:
                    continue
                rep = label_counts[p].get(lab, 0)
                # Prefer fewer repeats; then minimize projected load; then fewer items
                key = (rep, pack_loads[p] + wv, pack_counts[p])
                if best_key is None or key < best_key:
                    best_key = key
                    best_p = p
            pack_index[i, group] = best_p
            rank_in_pack[i, group] = pack_counts[best_p]
            pack_counts[best_p] += 1
            pack_loads[best_p] += wv
            label_counts[best_p][lab] += 1

    return pack_index, rank_in_pack
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def replicate_experts(
        weight: torch.Tensor,
        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Replicate `num_log` experts to `num_phy` replicas, such that the maximum
    load of all replicas is minimized.

    Parameters:
        weight: [X, num_log]
        num_phy: total number of experts after replication

    Returns:
        phy2log: [X, num_phy], logical expert id of each physical expert
        rank: [X, num_phy], the replica rank
        logcnt: [X, num_log], number of replicas for each logical expert
    """
    n, num_log = weight.shape
    num_redundant = num_phy - num_log
    assert num_redundant >= 0
    device = weight.device
    phy2log = torch.arange(num_phy, dtype=torch.int64,
                           device=device).repeat(n, 1)
    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)
    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)
    arangen = torch.arange(n, dtype=torch.int64, device=device)
    for i in range(num_log, num_phy):
        redundant_indices = (weight / logcnt).max(dim=-1).indices
        phy2log[:, i] = redundant_indices
        rank[:, i] = logcnt[arangen, redundant_indices]
        logcnt[arangen, redundant_indices] += 1
    return phy2log, rank, logcnt
=======
def replicate_experts(
        weight: torch.Tensor,
        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Replicate `num_log` experts to `num_phy` replicas, such that the maximum
    load of all replicas is minimized.

    Parameters:
        weight: [X, num_log]
        num_phy: total number of experts after replication

    Returns:
        phy2log: [X, num_phy], logical expert id of each physical expert
        rank: [X, num_phy], the replica rank
        logcnt: [X, num_log], number of replicas for each logical expert
    """
    n, num_log = weight.shape
    num_redundant = num_phy - num_log
    assert num_redundant >= 0
    device = weight.device
    phy2log = torch.arange(num_phy, dtype=torch.int64,
                           device=device).repeat(n, 1)
    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)
    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)
    arangen = torch.arange(n, dtype=torch.int64, device=device)

    # Maintain an incremental ratio to avoid recomputing divisions each step
    ratio = weight / logcnt
    for i in range(num_log, num_phy):
        redundant_indices = ratio.argmax(dim=-1)
        phy2log[:, i] = redundant_indices
        rank[:, i] = logcnt[arangen, redundant_indices]
        logcnt[arangen, redundant_indices] += 1
        # Update only the affected positions in ratio
        ratio[arangen, redundant_indices] = weight[arangen, redundant_indices] / \
            logcnt[arangen, redundant_indices]
    return phy2log, rank, logcnt
>>>>>>> REPLACE

</DIFF>