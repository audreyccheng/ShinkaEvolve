<NAME>
hybrid_tail_ab_and_2x2_refine
</NAME>

<DESCRIPTION>
This edit improves load balancing with minimal overhead by:
1) Making the expert replication tail adaptive per row and per step with a D’Hondt vs. Sainte–Laguë A/B choice. For each of the last tail allocations, we compute both candidates, predict the post-pick peak using a max-except-c trick, and choose the one minimizing new_peak with a deterministic tie-breaker favoring the lower receiver count. This targets peak reduction more directly than a fixed Sainte–Laguë tail.
2) Expanding the guarded replication fix-up to permit up to 2 moves per row when both strictly reduce the peak, keeping the donor/receiver candidate set tiny (top-2 vs. bottom-2).
3) Adding a bounded 2x2 fallback swap in the balanced_packing refinement when the 1x1 swap stalls. We evaluate the top-2 heaviest vs. bottom-2 lightest items and commit only if it further reduces the max pack delta. This preserves speed by operating on O(1) candidates.

All changes are local, deterministic, and maintain the previous CPU-friendly control flow, preserving speed while improving balancedness.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Bounded refinement between heaviest and lightest pack
    if groups_per_pack > 1 and refine_steps > 0:
        max_swaps = int(refine_steps)
        for i in range(num_layers):
            for _ in range(max_swaps):
                packs = pack_index[i]  # [num_groups], CPU
                w = weight[i]          # CPU
                # Compute pack loads
                pack_w = torch.zeros(num_packs, dtype=w.dtype)
                pack_w.scatter_add_(0, packs, w)
                h = int(torch.argmax(pack_w))
                l = int(torch.argmin(pack_w))
                delta = float(pack_w[h] - pack_w[l])
                if delta <= 1e-9:
                    break

                heavy_idx = torch.nonzero(packs == h, as_tuple=False).squeeze(1)
                light_idx = torch.nonzero(packs == l, as_tuple=False).squeeze(1)
                if heavy_idx.numel() == 0 or light_idx.numel() == 0:
                    break

                hw = w[heavy_idx]
                lw = w[light_idx]
                lw_sorted, lw_perm = torch.sort(lw)  # ascending
                if lw_sorted.numel() == 0 or hw.numel() == 0:
                    break

                # For each heavy item, find light item closest to target = hw - delta/2
                target = hw - (delta / 2.0)
                pos = torch.searchsorted(lw_sorted, target)
                pos = torch.clamp(pos, 0, lw_sorted.numel() - 1)
                # Consider neighbors pos and pos-1 for best approximation
                cand_pos = torch.stack([pos, torch.clamp(pos - 1, 0, lw_sorted.numel() - 1)], dim=1)
                cand_lw = lw_sorted[cand_pos]  # [H, 2]
                resid = (delta - 2.0 * (hw.unsqueeze(1) - cand_lw)).abs()
                best_flat = int(torch.argmin(resid).item())
                best_h_index = best_flat // 2
                best_option = best_flat % 2
                j_sorted_idx = int(cand_pos[best_h_index, best_option].item())

                wi = float(hw[best_h_index].item())
                wj = float(lw_sorted[j_sorted_idx].item())
                new_delta = abs(delta - 2.0 * (wi - wj))
                # Apply swap only if it strictly improves imbalance
                if new_delta < delta - 1e-9:
                    hi = heavy_idx[best_h_index]
                    lj = light_idx[lw_perm[j_sorted_idx]]
                    pack_index[i, hi] = l
                    pack_index[i, lj] = h
                    # Reassign ranks within affected packs to keep 0..groups_per_pack-1
                    for p in (h, l):
                        mask = pack_index[i] == p
                        idx = torch.nonzero(mask, as_tuple=False).squeeze(1)
                        if idx.numel() == 0:
                            continue
                        # stable ranks by previous rank order
                        prev_rank = rank_in_pack[i, idx]
                        order = torch.argsort(prev_rank)
                        rank_in_pack[i, idx[order]] = torch.arange(order.numel(), dtype=torch.int64)
                    continue
                else:
                    break

    return pack_index, rank_in_pack
=======
    # Bounded refinement between heaviest and lightest pack
    if groups_per_pack > 1 and refine_steps > 0:
        max_swaps = int(refine_steps)
        for i in range(num_layers):
            for _ in range(max_swaps):
                packs = pack_index[i]  # [num_groups], CPU
                w = weight[i]          # CPU
                # Compute pack loads
                pack_w = torch.zeros(num_packs, dtype=w.dtype)
                pack_w.scatter_add_(0, packs, w)
                h = int(torch.argmax(pack_w))
                l = int(torch.argmin(pack_w))
                delta = float(pack_w[h] - pack_w[l])
                if delta <= 1e-9:
                    break

                heavy_idx = torch.nonzero(packs == h, as_tuple=False).squeeze(1)
                light_idx = torch.nonzero(packs == l, as_tuple=False).squeeze(1)
                if heavy_idx.numel() == 0 or light_idx.numel() == 0:
                    break

                hw = w[heavy_idx]
                lw = w[light_idx]
                lw_sorted, lw_perm = torch.sort(lw)  # ascending
                if lw_sorted.numel() == 0 or hw.numel() == 0:
                    break

                # For each heavy item, find light item closest to target = hw - delta/2
                target = hw - (delta / 2.0)
                pos = torch.searchsorted(lw_sorted, target)
                pos = torch.clamp(pos, 0, lw_sorted.numel() - 1)
                # Consider neighbors pos and pos-1 for best approximation
                cand_pos = torch.stack([pos, torch.clamp(pos - 1, 0, lw_sorted.numel() - 1)], dim=1)
                cand_lw = lw_sorted[cand_pos]  # [H, 2]
                resid = (delta - 2.0 * (hw.unsqueeze(1) - cand_lw)).abs()
                best_flat = int(torch.argmin(resid).item())
                best_h_index = best_flat // 2
                best_option = best_flat % 2
                j_sorted_idx = int(cand_pos[best_h_index, best_option].item())

                wi = float(hw[best_h_index].item())
                wj = float(lw_sorted[j_sorted_idx].item())
                new_delta = abs(delta - 2.0 * (wi - wj))
                improved = new_delta < delta - 1e-9
                if improved:
                    # Commit best 1x1 swap
                    hi = heavy_idx[best_h_index]
                    lj = light_idx[lw_perm[j_sorted_idx]]
                    pack_index[i, hi] = l
                    pack_index[i, lj] = h
                    # Reassign ranks within affected packs to keep 0..groups_per_pack-1
                    for p in (h, l):
                        mask = pack_index[i] == p
                        idx = torch.nonzero(mask, as_tuple=False).squeeze(1)
                        if idx.numel() == 0:
                            continue
                        prev_rank = rank_in_pack[i, idx]
                        order = torch.argsort(prev_rank)
                        rank_in_pack[i, idx[order]] = torch.arange(order.numel(), dtype=torch.int64)
                    continue
                else:
                    # 2x2 fallback: evaluate top-2 heavy vs bottom-2 light
                    kh = min(2, hw.numel())
                    kl = min(2, lw_sorted.numel())
                    if kh >= 2 and kl >= 2:
                        # top-2 heavy
                        top2_hw_vals, top2_hw_pos = torch.topk(hw, k=2, largest=True)
                        # bottom-2 light: use sorted ascending
                        bot2_lw_vals = lw_sorted[:2]
                        delta22 = abs(delta - 2.0 * float((top2_hw_vals.sum() - bot2_lw_vals.sum()).item()))
                        if delta22 < delta - 1e-9:
                            # Map local positions to global indices and commit both swaps
                            hi1 = heavy_idx[top2_hw_pos[0]]
                            hi2 = heavy_idx[top2_hw_pos[1]]
                            # bot2 indices in original light order via lw_perm
                            lj1 = light_idx[lw_perm[0]]
                            lj2 = light_idx[lw_perm[1]]
                            # perform two swaps
                            pack_index[i, hi1] = l
                            pack_index[i, lj1] = h
                            pack_index[i, hi2] = l
                            pack_index[i, lj2] = h
                            # Reassign ranks for affected packs
                            for p in (h, l):
                                mask = pack_index[i] == p
                                idx = torch.nonzero(mask, as_tuple=False).squeeze(1)
                                if idx.numel() == 0:
                                    continue
                                prev_rank = rank_in_pack[i, idx]
                                order = torch.argsort(prev_rank)
                                rank_in_pack[i, idx[order]] = torch.arange(order.numel(), dtype=torch.int64)
                            continue
                    # No beneficial swap found; early exit
                    break

    return pack_index, rank_in_pack
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
    # Tail phase (Sainte-Laguë): benefit = weight / (2r - 1)
    if tail > 0:
        for _ in range(tail):
            denom = (2 * logcnt - 1).to(weight.dtype)
            benefit = weight / denom
            best = benefit.max(dim=-1).indices
            phy2log[:, col] = best
            rank[:, col] = logcnt[arangen, best]
            logcnt[arangen, best] += 1
            col += 1
=======
    # Tail phase: per-step A/B between D'Hondt and Sainte-Laguë; choose the one
    # minimizing the predicted post-pick peak per row (lexicographic tie-break on receiver count).
    if tail > 0:
        for _ in range(tail):
            r_f = logcnt.to(weight.dtype)
            avg = weight / r_f
            # current top-2 stats
            k2 = 2 if num_log >= 2 else 1
            top2_vals, top2_idx = torch.topk(avg, k=k2, dim=-1, largest=True)
            cur_max_idx = top2_idx[:, 0]
            cur_max = top2_vals[:, 0]
            cur_second = top2_vals[:, 1] if k2 == 2 else top2_vals[:, 0]

            # candidate winners under D'Hondt and Sainte-Laguë
            idx_D = (weight / r_f).argmax(dim=-1)
            denom_S = (2.0 * r_f - 1.0)
            idx_S = (weight / denom_S).argmax(dim=-1)

            # predicted peaks if allocating to each
            # For candidate c: new avg at c becomes w_c / (r_c + 1); others unchanged.
            wD = weight[arangen, idx_D]
            wS = weight[arangen, idx_S]
            cD_cnt = r_f[arangen, idx_D]
            cS_cnt = r_f[arangen, idx_S]
            newD = wD / (cD_cnt + 1.0)
            newS = wS / (cS_cnt + 1.0)

            # max_except_c is cur_second if c is current argmax else cur_max
            isDmax = (idx_D == cur_max_idx)
            isSmax = (idx_S == cur_max_idx)
            max_except_D = torch.where(isDmax, cur_second, cur_max)
            max_except_S = torch.where(isSmax, cur_second, cur_max)
            peakD = torch.maximum(max_except_D, newD)
            peakS = torch.maximum(max_except_S, newS)

            # choose better; on tie prefer lower receiver count (before increment), then lower index
            better = peakD < peakS
            tie = peakD == peakS
            cnt_tie_pref = cD_cnt < cS_cnt
            idx_tie_pref = idx_D < idx_S
            choose_D = better | (tie & (cnt_tie_pref | ((cD_cnt == cS_cnt) & idx_tie_pref)))
            chosen = torch.where(choose_D, idx_D, idx_S)

            phy2log[:, col] = chosen
            rank[:, col] = logcnt[arangen, chosen]
            logcnt[arangen, chosen] += 1
            col += 1
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
    # Strengthened replication fix-up per row:
    # Evaluate donors = top-2 by avg load and receivers = bottom-2, choose the single best
    # donor->receiver move if it strictly lowers the peak average.
    if num_log > 1 and num_redundant > 0:
        avg = weight / logcnt.to(weight.dtype)  # [n, num_log]
        kdon = min(2, num_log)
        krec = min(2, num_log)
        top_vals, top_idx = torch.topk(avg, k=kdon, dim=-1, largest=True)
        bot_vals, bot_idx = torch.topk(avg, k=krec, dim=-1, largest=False)
        cur_max = top_vals[:, 0]
        second = top_vals[:, 1] if kdon > 1 else top_vals[:, 0]

        for ri in range(n):
            best_new_peak = None
            best_pair = None
            donors = top_idx[ri].tolist()
            receivers = bot_idx[ri].tolist()

            for d in donors:
                cd = int(logcnt[ri, d].item())
                if cd <= 1:
                    continue
                for r in receivers:
                    if d == r:
                        continue
                    cr = int(logcnt[ri, r].item())
                    new_d = float(weight[ri, d].item()) / float(cd - 1)
                    new_r = float(weight[ri, r].item()) / float(cr + 1)
                    candidate_peak = max(float(second[ri].item()), new_d, new_r)
                    if candidate_peak + 1e-12 < float(cur_max[ri].item()):
                        if best_new_peak is None or candidate_peak < best_new_peak:
                            best_new_peak = candidate_peak
                            best_pair = (d, r)

            if best_pair is not None:
                d, r = best_pair
                # Choose a physical column corresponding to donor's highest rank (prefer the last replica)
                donor_cols = torch.nonzero(phy2log[ri] == d, as_tuple=False).squeeze(1)
                if donor_cols.numel() == 0:
                    continue
                # Among donor cols, pick the one with max rank
                maxr_idx = torch.argmax(rank[ri, donor_cols]).item()
                col_idx = donor_cols[maxr_idx]

                # Assign this physical replica to receiver with new rank equal to current receiver count
                new_rank = int(logcnt[ri, r].item())
                phy2log[ri, col_idx] = r
                rank[ri, col_idx] = new_rank

                # Update counts
                logcnt[ri, d] -= 1
                logcnt[ri, r] += 1
=======
    # Strengthened replication fix-up per row:
    # Evaluate donors=top-2 by avg and receivers=bottom-2; allow up to 2 moves
    # if both strictly reduce the peak. Keep O(1) candidates and deterministic column choice.
    if num_log > 1 and num_redundant > 0:
        def best_move_for_row(ri: int):
            avg_row = weight[ri] / logcnt[ri].to(weight.dtype)
            kdon = min(2, num_log)
            krec = min(2, num_log)
            top_vals, top_idx = torch.topk(avg_row, k=kdon, largest=True)
            bot_vals, bot_idx = torch.topk(avg_row, k=krec, largest=False)
            cur_max = float(top_vals[0].item())
            second = float((top_vals[1].item() if kdon > 1 else top_vals[0].item()))
            best_pair = None
            best_peak = None
            for d in top_idx.tolist():
                cd = int(logcnt[ri, d].item())
                if cd <= 1:
                    continue
                for r in bot_idx.tolist():
                    if r == d:
                        continue
                    cr = int(logcnt[ri, r].item())
                    new_d = float(weight[ri, d].item()) / float(cd - 1)
                    new_r = float(weight[ri, r].item()) / float(cr + 1)
                    cand_peak = max(second, new_d, new_r)
                    if cand_peak + 1e-12 < cur_max:
                        if best_peak is None or cand_peak < best_peak:
                            best_peak = cand_peak
                            best_pair = (d, r)
            return best_pair

        for ri in range(n):
            # First move (if any)
            mv = best_move_for_row(ri)
            applied = False
            if mv is not None:
                d, r = mv
                donor_cols = torch.nonzero(phy2log[ri] == d, as_tuple=False).squeeze(1)
                if donor_cols.numel() > 0:
                    maxr_idx = torch.argmax(rank[ri, donor_cols]).item()
                    col_idx = donor_cols[maxr_idx]
                    new_rank = int(logcnt[ri, r].item())
                    phy2log[ri, col_idx] = r
                    rank[ri, col_idx] = new_rank
                    logcnt[ri, d] -= 1
                    logcnt[ri, r] += 1
                    applied = True

            # Optionally attempt a second move after recomputing with updated counts
            if applied:
                mv2 = best_move_for_row(ri)
                if mv2 is not None:
                    d2, r2 = mv2
                    donor_cols2 = torch.nonzero(phy2log[ri] == d2, as_tuple=False).squeeze(1)
                    if donor_cols2.numel() > 0:
                        maxr_idx2 = torch.argmax(rank[ri, donor_cols2]).item()
                        col_idx2 = donor_cols2[maxr_idx2]
                        new_rank2 = int(logcnt[ri, r2].item())
                        phy2log[ri, col_idx2] = r2
                        rank[ri, col_idx2] = new_rank2
                        logcnt[ri, d2] -= 1
                        logcnt[ri, r2] += 1
>>>>>>> REPLACE

</DIFF>