<NAME>
peak_aware_tail_and_two_step_fixup
</NAME>

<DESCRIPTION>
This edit improves the expert replication stage, which strongly influences global load balance. Two targeted changes are applied to replicate_experts:

1) Peak-aware tail allocation: Instead of always using Sainte–Laguë for the final ~10% replicas, at each tail step the algorithm now evaluates both D’Hondt and Sainte–Laguë choices per row and selects the one that minimizes the predicted new peak average (with a tie-break by the smaller post-allocation average). This preserves the fast D’Hondt bulk while making the tail explicitly peak-focused, improving balancedness with negligible extra cost.

2) Guarded two-step replication fix-up: The post-allocation fix-up still searches donor→receiver moves among top-2 (donors) × bottom-2 (receivers). Now it can apply a second move for a row only if the first move yields a shallow improvement (<10%). This maintains speed while unlocking additional peak reductions in hard cases.

Both changes are lightweight and deterministic, and they keep the function signatures and overall flow intact. They target where balancing gains matter most without impacting the packing stages or introducing large computational overheads.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def replicate_experts(
        weight: torch.Tensor,
        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Replicate `num_log` experts to `num_phy` replicas, such that the maximum
    load of all replicas is minimized.

    Parameters:
        weight: [X, num_log]
        num_phy: total number of experts after replication

    Returns:
        phy2log: [X, num_phy], logical expert id of each physical expert
        rank: [X, num_phy], the replica rank
        logcnt: [X, num_log], number of replicas for each logical expert
    """
    n, num_log = weight.shape
    num_redundant = num_phy - num_log
    assert num_redundant >= 0
    device = weight.device

    # Initialize base mapping (one replica per logical expert)
    phy2log = torch.arange(num_phy, dtype=torch.int64, device=device).repeat(n, 1)
    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)
    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)

    if num_redundant == 0:
        return phy2log, rank, logcnt

    arangen = torch.arange(n, dtype=torch.int64, device=device)

    # Hybrid allocation: D'Hondt for bulk, Sainte-Laguë for the last ~10% (at least 1)
    tail = max(1, (num_redundant + 9) // 10)
    bulk = num_redundant - tail

    col = num_log
    # Bulk phase (D'Hondt): benefit = weight / r
    for _ in range(bulk):
        benefit = weight / logcnt
        best = benefit.max(dim=-1).indices
        phy2log[:, col] = best
        rank[:, col] = logcnt[arangen, best]
        logcnt[arangen, best] += 1
        col += 1

    # Tail phase (Sainte-Laguë): benefit = weight / (2r - 1)
    if tail > 0:
        for _ in range(tail):
            denom = (2 * logcnt - 1).to(weight.dtype)
            benefit = weight / denom
            best = benefit.max(dim=-1).indices
            phy2log[:, col] = best
            rank[:, col] = logcnt[arangen, best]
            logcnt[arangen, best] += 1
            col += 1

    # Strengthened replication fix-up per row:
    # Evaluate moves from top-2 donors (by avg load) to bottom-2 receivers and
    # apply the single best move if it strictly reduces the peak.
    if num_log > 1 and num_redundant > 0:
        avg = weight / logcnt.to(weight.dtype)  # [n, num_log]
        kdon = min(2, num_log)
        krec = min(2, num_log)
        top2_vals, top2_idx = torch.topk(avg, k=kdon, dim=-1, largest=True)
        bot2_vals, bot2_idx = torch.topk(avg, k=krec, dim=-1, largest=False)
        cur_max = avg.max(dim=-1).values
        argmax_idx = avg.argmax(dim=-1)

        rows = torch.arange(n, dtype=torch.int64, device=device)
        for ri in rows.tolist():
            best_new_peak = None
            best_pair = None

            donors = top2_idx[ri].tolist()
            receivers = bot2_idx[ri].tolist()

            for d in donors:
                cd = int(logcnt[ri, d].item())
                if cd <= 1:
                    continue
                for r in receivers:
                    if d == r:
                        continue
                    cr = int(logcnt[ri, r].item())

                    # Baseline peak ignoring donor if donor is current max
                    baseline_other = float(cur_max[ri].item())
                    if d == int(argmax_idx[ri].item()):
                        # second-best under current configuration
                        baseline_other = float(torch.topk(avg[ri], k=2, largest=True).values[1].item())

                    new_d = float(weight[ri, d].item()) / float(cd - 1)
                    new_r = float(weight[ri, r].item()) / float(cr + 1)
                    candidate_peak = max(baseline_other, new_d, new_r)

                    if candidate_peak + 1e-12 < float(cur_max[ri].item()):
                        if best_new_peak is None or candidate_peak < best_new_peak:
                            best_new_peak = candidate_peak
                            best_pair = (d, r)

            if best_pair is not None:
                d, r = best_pair
                # Choose a physical column corresponding to donor's highest rank (prefer the last replica)
                donor_cols = torch.nonzero(phy2log[ri] == d, as_tuple=False).squeeze(1)
                if donor_cols.numel() == 0:
                    continue
                maxr_idx = torch.argmax(rank[ri, donor_cols]).item()
                col_idx = donor_cols[maxr_idx]

                # Assign this physical replica to receiver with new rank equal to current receiver count
                new_rank = int(logcnt[ri, r].item())
                phy2log[ri, col_idx] = r
                rank[ri, col_idx] = new_rank

                # Update counts
                logcnt[ri, d] -= 1
                logcnt[ri, r] += 1

    return phy2log, rank, logcnt
=======
def replicate_experts(
        weight: torch.Tensor,
        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Replicate `num_log` experts to `num_phy` replicas, minimizing the maximum
    per-replica average load with a hybrid rule:
      - Bulk: D'Hondt (weight / r)
      - Tail: per-step peak-aware choice between D'Hondt and Sainte–Laguë
    Followed by a small guarded fix-up (up to 2 moves on shallow improvement).
    """
    n, num_log = weight.shape
    num_redundant = num_phy - num_log
    assert num_redundant >= 0
    device = weight.device
    dtype_i64 = torch.int64
    dtype_f = weight.dtype

    # Initialize base mapping (one replica per logical expert)
    phy2log = torch.arange(num_phy, dtype=dtype_i64, device=device).repeat(n, 1)
    rank = torch.zeros(n, num_phy, dtype=dtype_i64, device=device)
    logcnt = torch.ones(n, num_log, dtype=dtype_i64, device=device)

    if num_redundant == 0:
        return phy2log, rank, logcnt

    arangen = torch.arange(n, dtype=dtype_i64, device=device)

    # Hybrid allocation: D'Hondt for bulk, then peak-aware chooser at the tail (~10%)
    tail = max(1, (num_redundant + 9) // 10)
    bulk = num_redundant - tail

    col = num_log
    # Bulk phase (D'Hondt): benefit = weight / r
    for _ in range(max(0, bulk)):
        r_f = logcnt.to(dtype_f)
        benefit = weight / r_f
        best = benefit.argmax(dim=-1)
        phy2log[:, col] = best
        rank[:, col] = logcnt[arangen, best]
        logcnt[arangen, best] += 1
        col += 1

    # Tail phase: at each step pick the rule (D or S) that minimizes predicted peak
    for _ in range(max(0, tail)):
        r_f = logcnt.to(dtype_f)
        avg_cur = weight / r_f
        if num_log > 1:
            top2_vals = torch.topk(avg_cur, k=2, dim=-1, largest=True).values
            second = top2_vals[:, 1]
        else:
            second = avg_cur[:, 0]

        benef_D = weight / r_f
        benef_S = weight / (2.0 * r_f - 1.0)
        idx_D = benef_D.argmax(dim=-1)
        idx_S = benef_S.argmax(dim=-1)

        newD = weight[arangen, idx_D] / (r_f[arangen, idx_D] + 1.0)
        newS = weight[arangen, idx_S] / (r_f[arangen, idx_S] + 1.0)
        peakD = torch.maximum(second, newD)
        peakS = torch.maximum(second, newS)

        # Choose the smaller predicted peak; tie-break by smaller post-allocation avg
        better_S = peakS + 1e-12 < peakD
        tie_SD = torch.isclose(peakS, peakD, rtol=0.0, atol=1e-12)
        prefer_S_on_tie = newS <= newD
        use_S = better_S | (tie_SD & prefer_S_on_tie)

        best_idx = torch.where(use_S, idx_S, idx_D)
        phy2log[:, col] = best_idx
        rank[:, col] = logcnt[arangen, best_idx]
        logcnt[arangen, best_idx] += 1
        col += 1

    # Strengthened replication fix-up per row:
    # Evaluate moves from top-2 donors (by avg load) to bottom-2 receivers and
    # apply the single best move if it strictly reduces the peak.
    # If the first move yields shallow improvement (<10%), try at most one more move.
    if num_log > 1 and num_redundant > 0:
        def best_move_for_row(ri: int) -> tuple[bool, int, int, float, float]:
            """
            Returns (improves, donor_id, receiver_id, old_peak, new_peak_if_moved)
            """
            r_fi = logcnt[ri].to(dtype_f)
            avgi = weight[ri] / r_fi
            kdon = min(2, num_log)
            krec = min(2, num_log)
            top_vals, top_idx = torch.topk(avgi, k=kdon, largest=True)
            bot_vals, bot_idx = torch.topk(avgi, k=krec, largest=False)
            cur_max = float(top_vals[0].item())
            second = float((top_vals[1].item() if kdon > 1 else top_vals[0].item()))
            best_pair = None
            best_new_peak = None

            for di in range(kdon):
                d = int(top_idx[di].item())
                cd = int(logcnt[ri, d].item())
                if cd <= 1:
                    continue
                for rj in range(krec):
                    r = int(bot_idx[rj].item())
                    if d == r:
                        continue
                    cr = int(logcnt[ri, r].item())
                    new_d = float(weight[ri, d].item()) / float(cd - 1)
                    new_r = float(weight[ri, r].item()) / float(cr + 1)
                    candidate_peak = max(second, new_d, new_r)
                    if candidate_peak + 1e-12 < cur_max:
                        if best_new_peak is None or candidate_peak < best_new_peak:
                            best_new_peak = candidate_peak
                            best_pair = (d, r)

            if best_pair is None:
                return (False, -1, -1, cur_max, cur_max)
            return (True, best_pair[0], best_pair[1], cur_max, best_new_peak if best_new_peak is not None else cur_max)

        rows = torch.arange(n, dtype=dtype_i64, device=device).tolist()
        for ri in rows:
            improved, d, r, old_peak, pred_new_peak = best_move_for_row(ri)
            if not improved:
                continue

            # choose a donor physical column with highest rank (prefer last replica)
            donor_cols = torch.nonzero(phy2log[ri] == d, as_tuple=False).squeeze(1)
            if donor_cols.numel() == 0:
                continue
            maxr_idx = torch.argmax(rank[ri, donor_cols]).item()
            col_idx = donor_cols[maxr_idx]

            # Apply the move
            new_rank = int(logcnt[ri, r].item())
            phy2log[ri, col_idx] = r
            rank[ri, col_idx] = new_rank
            logcnt[ri, d] -= 1
            logcnt[ri, r] += 1

            # If shallow improvement (<10%), allow one more attempt
            shallow = (old_peak - pred_new_peak) <= 0.10 * max(old_peak, 1e-12)
            if shallow:
                improved2, d2, r2, old_peak2, pred_new_peak2 = best_move_for_row(ri)
                if improved2:
                    donor_cols2 = torch.nonzero(phy2log[ri] == d2, as_tuple=False).squeeze(1)
                    if donor_cols2.numel() > 0:
                        maxr_idx2 = torch.argmax(rank[ri, donor_cols2]).item()
                        col_idx2 = donor_cols2[maxr_idx2]
                        new_rank2 = int(logcnt[ri, r2].item())
                        phy2log[ri, col_idx2] = r2
                        rank[ri, col_idx2] = new_rank2
                        logcnt[ri, d2] -= 1
                        logcnt[ri, r2] += 1

    return phy2log, rank, logcnt
>>>>>>> REPLACE

</DIFF>