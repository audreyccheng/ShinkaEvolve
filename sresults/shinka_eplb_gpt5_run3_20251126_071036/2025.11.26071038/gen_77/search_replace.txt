<NAME>
adaptive_tail_and_fixup
</NAME>

<DESCRIPTION>
I improve the expert replication phase in two ways while keeping computation cheap:

1) Adaptive, peak-aware tail allocation: Keep D’Hondt for the bulk but, for the last ~10% extra replicas, choose per row at each step between D’Hondt and Sainte–Laguë by predicting the new peak average after the assignment. This directly targets reducing the maximum per-replica average where it matters and is implemented in a vectorized way.

2) Strengthened one-move fix-up: Instead of a single move from the current max to the global min, consider donors from the top-2 highest averages and receivers from the bottom-2 lowest averages, then apply the single best move that strictly reduces the peak (with correct handling of the second-highest baseline). This is still O(1) candidates per row and adds negligible overhead.

These changes increase balancedness without sacrificing speed, addressing the NP-hardness with a fast, targeted heuristic.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def replicate_experts(
        weight: torch.Tensor,
        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Replicate `num_log` experts to `num_phy` replicas, such that the maximum
    load of all replicas is minimized.

    Parameters:
        weight: [X, num_log]
        num_phy: total number of experts after replication

    Returns:
        phy2log: [X, num_phy], logical expert id of each physical expert
        rank: [X, num_phy], the replica rank
        logcnt: [X, num_log], number of replicas for each logical expert
    """
    n, num_log = weight.shape
    num_redundant = num_phy - num_log
    assert num_redundant >= 0
    device = weight.device

    # Initialize base mapping (one replica per logical expert)
    phy2log = torch.arange(num_phy, dtype=torch.int64, device=device).repeat(n, 1)
    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)
    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)

    if num_redundant == 0:
        return phy2log, rank, logcnt

    arangen = torch.arange(n, dtype=torch.int64, device=device)

    # Hybrid allocation: D'Hondt for bulk, Sainte-Laguë for the last ~10% (at least 1)
    tail = max(1, (num_redundant + 9) // 10)
    bulk = num_redundant - tail

    col = num_log
    # Bulk phase (D'Hondt): benefit = weight / r
    for _ in range(bulk):
        benefit = weight / logcnt
        best = benefit.max(dim=-1).indices
        phy2log[:, col] = best
        rank[:, col] = logcnt[arangen, best]
        logcnt[arangen, best] += 1
        col += 1

    # Tail phase (Sainte-Laguë): benefit = weight / (2r - 1)
    # Note: r starts at 1, so (2r - 1) >= 1
    if tail > 0:
        for _ in range(tail):
            denom = (2 * logcnt - 1).to(weight.dtype)
            benefit = weight / denom
            best = benefit.max(dim=-1).indices
            phy2log[:, col] = best
            rank[:, col] = logcnt[arangen, best]
            logcnt[arangen, best] += 1
            col += 1

    # One-step replication fix-up per row:
    # Move one replica from the heaviest-per-replica expert to the lightest if it strictly
    # reduces the global maximum average load.
    if num_log > 1 and num_redundant > 0:
        avg = weight / logcnt.to(weight.dtype)  # [n, num_log]
        # top-2 to account for ties at the maximum
        top2_vals, top2_idx = torch.topk(avg, k=2, dim=-1)
        cur_max = top2_vals[:, 0]
        second = top2_vals[:, 1]
        donor = top2_idx[:, 0]
        receiver = torch.argmin(avg, dim=-1)

        cd = logcnt[arangen, donor]  # donor counts
        cr = logcnt[arangen, receiver]  # receiver counts
        # Valid only if donor != receiver and donor has at least 2 replicas
        valid = (donor != receiver) & (cd > 1)

        # Compute new peak after moving 1 replica
        new_d = weight[arangen, donor] / (cd.to(weight.dtype) - 1)
        new_r = weight[arangen, receiver] / (cr.to(weight.dtype) + 1)
        new_peak = torch.maximum(second, torch.maximum(new_d, new_r))
        improve = valid & (new_peak + 1e-12 < cur_max)

        rows = torch.nonzero(improve, as_tuple=False).squeeze(1)
        if rows.numel() > 0:
            for ri in rows.tolist():
                d = int(donor[ri].item())
                r = int(receiver[ri].item())
                # Choose a physical column corresponding to donor's highest rank (prefer the last replica)
                donor_cols = torch.nonzero(phy2log[ri] == d, as_tuple=False).squeeze(1)
                if donor_cols.numel() == 0:
                    continue
                # Among donor cols, pick the one with max rank
                maxr_idx = torch.argmax(rank[ri, donor_cols]).item()
                col_idx = donor_cols[maxr_idx]

                # Assign this physical replica to receiver with new rank equal to current receiver count
                new_rank = int(logcnt[ri, r].item())
                phy2log[ri, col_idx] = r
                rank[ri, col_idx] = new_rank

                # Update counts
                logcnt[ri, d] -= 1
                logcnt[ri, r] += 1

    return phy2log, rank, logcnt
=======
def replicate_experts(
        weight: torch.Tensor,
        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Replicate `num_log` experts to `num_phy` replicas, such that the maximum
    load of all replicas is minimized.

    Parameters:
        weight: [X, num_log]
        num_phy: total number of experts after replication

    Returns:
        phy2log: [X, num_phy], logical expert id of each physical expert
        rank: [X, num_phy], the replica rank
        logcnt: [X, num_log], number of replicas for each logical expert
    """
    n, num_log = weight.shape
    num_redundant = num_phy - num_log
    assert num_redundant >= 0
    device = weight.device
    dtype_f = weight.dtype

    # Initialize base mapping (one replica per logical expert)
    phy2log = torch.arange(num_phy, dtype=torch.int64, device=device).repeat(n, 1)
    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)
    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)

    if num_redundant == 0:
        return phy2log, rank, logcnt

    arangen = torch.arange(n, dtype=torch.int64, device=device)

    # Hybrid allocation: D'Hondt for bulk, then adaptive peak-aware tail per row.
    tail = max(1, (num_redundant + 9) // 10)
    bulk = num_redundant - tail

    col = num_log
    # Bulk phase (D'Hondt): benefit = weight / r
    for _ in range(bulk):
        r_f = logcnt.to(dtype_f)
        benefit = weight / r_f
        best = benefit.argmax(dim=-1)
        phy2log[:, col] = best
        rank[:, col] = logcnt[arangen, best]
        logcnt[arangen, best] += 1
        col += 1

    # Tail phase: at each step choose between D'Hondt and Sainte-Laguë by predicting the new peak
    eps = 1e-12
    for _ in range(max(0, tail)):
        r_f = logcnt.to(dtype_f)
        avg_now = weight / r_f
        if num_log > 1:
            top2 = torch.topk(avg_now, k=2, dim=-1, largest=True).values
            second = top2[:, 1]
        else:
            second = avg_now[:, 0]

        # Candidate indices under D'Hondt and Sainte-Laguë
        idx_D = (weight / r_f).argmax(dim=-1)
        idx_S = (weight / (2.0 * r_f - 1.0)).argmax(dim=-1)

        # Predicted new averages for selected experts after adding one replica
        newD = weight[arangen, idx_D] / (r_f[arangen, idx_D] + 1.0)
        newS = weight[arangen, idx_S] / (r_f[arangen, idx_S] + 1.0)

        peakD = torch.maximum(second, newD)
        peakS = torch.maximum(second, newS)
        secD = torch.minimum(second, newD)
        secS = torch.minimum(second, newS)

        choose_D = peakD + eps < peakS
        tie = (torch.abs(peakD - peakS) <= eps)
        tie_break_D = secD <= secS
        pick_D = choose_D | (tie & tie_break_D)

        best_idx = torch.where(pick_D, idx_D, idx_S)

        phy2log[:, col] = best_idx
        rank[:, col] = logcnt[arangen, best_idx]
        logcnt[arangen, best_idx] += 1
        col += 1

    # Strengthened one-move replication fix-up per row:
    # Evaluate donors from top-2 avg and receivers from bottom-2 avg; apply best improving move.
    if num_log > 1 and num_redundant > 0:
        avg = weight / logcnt.to(dtype_f)
        cur_max_vals, argmax_idx = avg.max(dim=-1)
        k = min(2, num_log)
        top_vals, top_idx = torch.topk(avg, k=k, dim=-1, largest=True)
        bot_vals, bot_idx = torch.topk(avg, k=k, dim=-1, largest=False)

        rows = torch.arange(n, dtype=torch.int64, device=device).tolist()
        for ri in rows:
            cur_max = float(cur_max_vals[ri].item())
            second = float((top_vals[ri, 1].item() if k > 1 else top_vals[ri, 0].item()))
            best_pair = None
            best_peak = None

            donors = top_idx[ri].tolist()
            receivers = bot_idx[ri].tolist()
            for d in donors:
                cd = int(logcnt[ri, d].item())
                if cd <= 1:
                    continue
                for r in receivers:
                    if d == r:
                        continue
                    cr = int(logcnt[ri, r].item())
                    # baseline "other" peak if donor is current max is the second-best
                    baseline_other = second if d == int(argmax_idx[ri].item()) else cur_max
                    new_d = float(weight[ri, d].item()) / float(cd - 1)
                    new_r = float(weight[ri, r].item()) / float(cr + 1)
                    candidate_peak = max(baseline_other, new_d, new_r)
                    if candidate_peak + 1e-12 < cur_max:
                        if best_peak is None or candidate_peak < best_peak:
                            best_peak = candidate_peak
                            best_pair = (d, r)

            if best_pair is not None:
                d, r = best_pair
                donor_cols = torch.nonzero(phy2log[ri] == d, as_tuple=False).squeeze(1)
                if donor_cols.numel() == 0:
                    continue
                maxr_idx = torch.argmax(rank[ri, donor_cols]).item()
                col_idx = donor_cols[maxr_idx]
                new_rank = int(logcnt[ri, r].item())
                phy2log[ri, col_idx] = r
                rank[ri, col_idx] = new_rank
                logcnt[ri, d] -= 1
                logcnt[ri, r] += 1

    return phy2log, rank, logcnt
>>>>>>> REPLACE

</DIFF>