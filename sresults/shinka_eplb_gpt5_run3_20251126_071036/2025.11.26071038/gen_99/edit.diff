--- a/original.py
+++ b/original.py
@@ -1,417 +1,479 @@
 # EVOLVE-BLOCK-START
 """
 Expert parallelism load balancer (EPLB) for vLLM.
 
 This module implements the core rearrangement algorithm.
 
 The rearrangement algorithm is adapted from
 [DeepSeek EPLB](https://github.com/deepseek-ai/eplb).
 
 Please find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example
 on how the EPLB algorithm works.
 """
 
 import torch
 
 
 def balanced_packing(weight: torch.Tensor,
                      num_packs: int,
                      refine_steps: int = 1) -> tuple[torch.Tensor, torch.Tensor]:
     """
     Pack n weighted objects to m packs, such that each bin contains exactly
     n/m objects and the weights of all packs are as balanced as possible.
 
     Parameters:
         weight: [X, n], the weight of each item
         num_packs: number of packs
         refine_steps: small bounded number of refinement swaps per layer
 
     Returns:
         pack_index: [X, n], the pack index of each item
         rank_in_pack: [X, n], the rank of the item in the pack
     """
     num_layers, num_groups = weight.shape
     assert num_groups % num_packs == 0
     groups_per_pack = num_groups // num_packs
 
     if groups_per_pack == 1:
         pack_index = torch.arange(weight.size(-1),
                                   dtype=torch.int64,
                                   device=weight.device).expand(weight.shape)
         rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
         return pack_index, rank_in_pack
 
     indices = weight.float().sort(-1, descending=True).indices.cpu()
     pack_index = torch.full_like(weight,
                                  fill_value=-1,
                                  dtype=torch.int64,
                                  device="cpu")
     rank_in_pack = torch.full_like(pack_index, fill_value=-1)
     for i in range(num_layers):
         pack_weights = [0] * num_packs
         pack_items = [0] * num_packs
         for group in indices[i]:
             pack = min(
                 (i
                  for i in range(num_packs) if pack_items[i] < groups_per_pack),
                 key=pack_weights.__getitem__,
             )
             assert pack_items[pack] < groups_per_pack
             pack_index[i, group] = pack
             rank_in_pack[i, group] = pack_items[pack]
             pack_weights[pack] += weight[i, group]
             pack_items[pack] += 1
 
     # Bounded multi-swap refinement per layer to reduce max imbalance
     if groups_per_pack > 1 and refine_steps > 0:
-        max_swaps = int(refine_steps)  # keep small to preserve speed
+        base_swaps = int(refine_steps)  # keep small to preserve speed
         for i in range(num_layers):
-            for _ in range(max_swaps):
+            # Compute initial imbalance to adapt steps for this layer (cheap, once)
+            packs0 = pack_index[i]  # [num_groups], CPU
+            w0 = weight[i]  # CPU
+            pack_w0 = torch.zeros(num_packs, dtype=w0.dtype)
+            pack_w0.scatter_add_(0, packs0, w0)
+            delta0 = float((pack_w0.max() - pack_w0.min()).item())
+            mean0 = float(pack_w0.mean().item())
+            extra = 1 if (mean0 > 0.0 and delta0 / mean0 > 0.03) else 0
+            steps_this_layer = base_swaps + extra
+
+            for _ in range(steps_this_layer):
                 packs = pack_index[i]  # [num_groups], CPU
                 w = weight[i]  # CPU
                 # Compute pack loads
                 pack_w = torch.zeros(num_packs, dtype=w.dtype)
                 pack_w.scatter_add_(0, packs, w)
                 h = int(torch.argmax(pack_w))
                 l = int(torch.argmin(pack_w))
                 delta = float(pack_w[h] - pack_w[l])
                 if delta <= 1e-9:
                     break
 
                 heavy_idx = torch.nonzero(packs == h, as_tuple=False).squeeze(1)
                 light_idx = torch.nonzero(packs == l, as_tuple=False).squeeze(1)
                 if heavy_idx.numel() == 0 or light_idx.numel() == 0:
                     break
 
                 hw = w[heavy_idx]
                 lw = w[light_idx]
                 lw_sorted, lw_perm = torch.sort(lw)  # ascending
                 if lw_sorted.numel() == 0 or hw.numel() == 0:
                     break
 
                 # For each heavy item, find light item closest to target = hw - delta/2
                 target = hw - (delta / 2.0)
                 pos = torch.searchsorted(lw_sorted, target)
                 pos = torch.clamp(pos, 0, lw_sorted.numel() - 1)
                 # Consider neighbors pos and pos-1 for best approximation
                 cand_pos = torch.stack([pos, torch.clamp(pos - 1, 0, lw_sorted.numel() - 1)], dim=1)
                 cand_lw = lw_sorted[cand_pos]  # [H, 2]
                 resid = (delta - 2.0 * (hw.unsqueeze(1) - cand_lw)).abs()
                 best_flat = int(torch.argmin(resid).item())
                 best_h_index = best_flat // 2
                 best_option = best_flat % 2
                 j_sorted_idx = int(cand_pos[best_h_index, best_option].item())
 
                 wi = float(hw[best_h_index].item())
                 wj = float(lw_sorted[j_sorted_idx].item())
                 new_delta = abs(delta - 2.0 * (wi - wj))
                 # Apply swap only if it strictly improves imbalance
                 if new_delta < delta - 1e-9:
                     hi = heavy_idx[best_h_index]
                     lj = light_idx[lw_perm[j_sorted_idx]]
                     pack_index[i, hi] = l
                     pack_index[i, lj] = h
                     # Reassign ranks within affected packs to keep 0..groups_per_pack-1
                     for p in (h, l):
                         mask = pack_index[i] == p
                         idx = torch.nonzero(mask, as_tuple=False).squeeze(1)
                         if idx.numel() == 0:
                             continue
                         # Stable by previous rank order
                         prev_rank = rank_in_pack[i, idx]
                         order = torch.argsort(prev_rank)
                         new_ranks = torch.arange(order.numel(), dtype=torch.int64)
                         rank_in_pack[i, idx[order]] = new_ranks
                     # continue to next potential swap
                     continue
                 else:
                     break
 
     return pack_index, rank_in_pack
 
 
 def replicate_experts(
         weight: torch.Tensor,
         num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Replicate `num_log` experts to `num_phy` replicas, such that the maximum
     load of all replicas is minimized.
 
     Parameters:
         weight: [X, num_log]
         num_phy: total number of experts after replication
 
     Returns:
         phy2log: [X, num_phy], logical expert id of each physical expert
         rank: [X, num_phy], the replica rank
         logcnt: [X, num_log], number of replicas for each logical expert
     """
     n, num_log = weight.shape
     num_redundant = num_phy - num_log
     assert num_redundant >= 0
     device = weight.device
     dtype_f = weight.dtype
 
     # Initialize base mapping (one replica per logical expert)
     phy2log = torch.arange(num_phy, dtype=torch.int64, device=device).repeat(n, 1)
     rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)
     logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)
 
     if num_redundant == 0:
         return phy2log, rank, logcnt
 
     arangen = torch.arange(n, dtype=torch.int64, device=device)
 
     # Hybrid allocation: D'Hondt for bulk, then adaptive peak-aware tail per row.
     tail = max(1, (num_redundant + 9) // 10)
     bulk = num_redundant - tail
 
     col = num_log
     # Bulk phase (D'Hondt): benefit = weight / r
     for _ in range(bulk):
         r_f = logcnt.to(dtype_f)
         benefit = weight / r_f
         best = benefit.argmax(dim=-1)
         phy2log[:, col] = best
         rank[:, col] = logcnt[arangen, best]
         logcnt[arangen, best] += 1
         col += 1
 
     # Tail phase: at each step choose between D'Hondt and Sainte-Laguë by predicting the new peak
     eps = 1e-12
     for _ in range(max(0, tail)):
         r_f = logcnt.to(dtype_f)
         avg_now = weight / r_f
         if num_log > 1:
             top2 = torch.topk(avg_now, k=2, dim=-1, largest=True).values
             second = top2[:, 1]
         else:
             second = avg_now[:, 0]
 
         # Candidate indices under D'Hondt and Sainte-Laguë
         idx_D = (weight / r_f).argmax(dim=-1)
         idx_S = (weight / (2.0 * r_f - 1.0)).argmax(dim=-1)
 
         # Predicted new averages for selected experts after adding one replica
         newD = weight[arangen, idx_D] / (r_f[arangen, idx_D] + 1.0)
         newS = weight[arangen, idx_S] / (r_f[arangen, idx_S] + 1.0)
 
         peakD = torch.maximum(second, newD)
         peakS = torch.maximum(second, newS)
+        # proxy for second peak tie-breaker
         secD = torch.minimum(second, newD)
         secS = torch.minimum(second, newS)
+        # receiver counts for final tie-breaking
+        crD = logcnt[arangen, idx_D]
+        crS = logcnt[arangen, idx_S]
 
         choose_D = peakD + eps < peakS
-        tie = (torch.abs(peakD - peakS) <= eps)
-        tie_break_D = secD <= secS
-        pick_D = choose_D | (tie & tie_break_D)
+        tie_peak = torch.isclose(peakD, peakS, rtol=0.0, atol=eps)
+        better_secD = secD + eps < secS
+        tie_sec = torch.isclose(secD, secS, rtol=0.0, atol=eps)
+        prefer_cntD = crD <= crS
+        pick_D = choose_D | (tie_peak & (better_secD | (tie_sec & prefer_cntD)))
 
         best_idx = torch.where(pick_D, idx_D, idx_S)
 
         phy2log[:, col] = best_idx
         rank[:, col] = logcnt[arangen, best_idx]
         logcnt[arangen, best_idx] += 1
         col += 1
 
-    # Strengthened one-move replication fix-up per row:
-    # Evaluate donors from top-2 avg and receivers from bottom-2 avg; apply best improving move.
+    # Strengthened replication fix-up per row (up to 2 moves when first improvement is shallow):
+    # Evaluate donors from top-2 avg and receivers from bottom-2 avg; apply the best improving move.
     if num_log > 1 and num_redundant > 0:
         avg = weight / logcnt.to(dtype_f)
         cur_max_vals, argmax_idx = avg.max(dim=-1)
         k = min(2, num_log)
         top_vals, top_idx = torch.topk(avg, k=k, dim=-1, largest=True)
         bot_vals, bot_idx = torch.topk(avg, k=k, dim=-1, largest=False)
 
         rows = torch.arange(n, dtype=torch.int64, device=device).tolist()
         for ri in rows:
             cur_max = float(cur_max_vals[ri].item())
             second = float((top_vals[ri, 1].item() if k > 1 else top_vals[ri, 0].item()))
             best_pair = None
             best_peak = None
 
             donors = top_idx[ri].tolist()
             receivers = bot_idx[ri].tolist()
             for d in donors:
                 cd = int(logcnt[ri, d].item())
                 if cd <= 1:
                     continue
                 for r in receivers:
                     if d == r:
                         continue
                     cr = int(logcnt[ri, r].item())
                     # baseline "other" peak if donor is current max is the second-best
                     baseline_other = second if d == int(argmax_idx[ri].item()) else cur_max
                     new_d = float(weight[ri, d].item()) / float(cd - 1)
                     new_r = float(weight[ri, r].item()) / float(cr + 1)
                     candidate_peak = max(baseline_other, new_d, new_r)
                     if candidate_peak + 1e-12 < cur_max:
                         if best_peak is None or candidate_peak < best_peak:
                             best_peak = candidate_peak
                             best_pair = (d, r)
 
             if best_pair is not None:
+                # Apply first move
                 d, r = best_pair
                 donor_cols = torch.nonzero(phy2log[ri] == d, as_tuple=False).squeeze(1)
                 if donor_cols.numel() == 0:
                     continue
                 maxr_idx = torch.argmax(rank[ri, donor_cols]).item()
                 col_idx = donor_cols[maxr_idx]
                 new_rank = int(logcnt[ri, r].item())
                 phy2log[ri, col_idx] = r
                 rank[ri, col_idx] = new_rank
                 logcnt[ri, d] -= 1
                 logcnt[ri, r] += 1
+
+                # Optional second move if improvement is shallow (<15%)
+                avg_after = weight[ri] / logcnt[ri].to(dtype_f)
+                new_peak_val = float(avg_after.max().item())
+                improve_ratio = (cur_max - new_peak_val) / max(cur_max, 1e-12)
+                if improve_ratio < 0.15:
+                    # Recompute tiny candidate set and try one more best move
+                    avg2 = avg_after
+                    cur_max2 = float(avg2.max().item())
+                    if num_log > 1:
+                        tvals2, tidx2 = torch.topk(avg2, k=k, largest=True)
+                        bvals2, bidx2 = torch.topk(avg2, k=k, largest=False)
+                        second2 = float((tvals2[1].item() if k > 1 else tvals2[0].item()))
+                    else:
+                        tidx2 = torch.arange(num_log, device=device)
+                        bidx2 = torch.arange(num_log, device=device)
+                        second2 = cur_max2
+                    best_pair2 = None
+                    best_peak2 = None
+                    for d2 in tidx2.tolist():
+                        cd2 = int(logcnt[ri, d2].item())
+                        if cd2 <= 1:
+                            continue
+                        for r2 in bidx2.tolist():
+                            if d2 == r2:
+                                continue
+                            cr2 = int(logcnt[ri, r2].item())
+                            new_d2 = float(weight[ri, d2].item()) / float(cd2 - 1)
+                            new_r2 = float(weight[ri, r2].item()) / float(cr2 + 1)
+                            cand_peak2 = max(second2 if d2 == int(avg2.argmax().item()) else cur_max2, new_d2, new_r2)
+                            if cand_peak2 + 1e-12 < cur_max2:
+                                if best_peak2 is None or cand_peak2 < best_peak2:
+                                    best_peak2 = cand_peak2
+                                    best_pair2 = (d2, r2)
+                    if best_pair2 is not None:
+                        d2, r2 = best_pair2
+                        donor_cols2 = torch.nonzero(phy2log[ri] == d2, as_tuple=False).squeeze(1)
+                        if donor_cols2.numel() > 0:
+                            maxr_idx2 = torch.argmax(rank[ri, donor_cols2]).item()
+                            col_idx2 = donor_cols2[maxr_idx2]
+                            new_rank2 = int(logcnt[ri, r2].item())
+                            phy2log[ri, col_idx2] = r2
+                            rank[ri, col_idx2] = new_rank2
+                            logcnt[ri, d2] -= 1
+                            logcnt[ri, r2] += 1
 
     return phy2log, rank, logcnt
 
 
 def rebalance_experts_hierarchical(
     weight: torch.Tensor,
     num_physical_experts: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ):
     """
     Parameters:
         weight: [num_moe_layers, num_logical_experts]
         num_physical_experts: number of physical experts after replication
         num_groups: number of expert groups
         num_nodes: number of server nodes, where the intra-node network
         (e.g, NVLink) is faster
         num_gpus: number of GPUs, must be a multiple of `num_nodes`
 
     Returns:
         physical_to_logical_map: [num_moe_layers, num_physical_experts]
         logical_to_physical_map: [num_moe_layers, num_logical_experts, X]
         logical_count: [num_moe_layers, num_logical_experts]
     """
     num_layers, num_logical_experts = weight.shape
     assert num_logical_experts % num_groups == 0
     group_size = num_logical_experts // num_groups
     assert num_groups % num_nodes == 0
     groups_per_node = num_groups // num_nodes
     assert num_gpus % num_nodes == 0
     assert num_physical_experts % num_gpus == 0
     phy_experts_per_gpu = num_physical_experts // num_gpus
 
     def inverse(perm: torch.Tensor) -> torch.Tensor:
         inv = torch.empty_like(perm)
         inv.scatter_(
             1,
             perm,
             torch.arange(perm.size(1), dtype=torch.int64,
                          device=perm.device).expand(perm.shape),
         )
         return inv
 
     # Step 1: pack groups to nodes
     tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)
     group_pack_index, group_rank_in_pack = balanced_packing(
         tokens_per_group, num_nodes)
     log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *
                  group_size).unsqueeze(-1) +
                 torch.arange(group_size,
                              dtype=torch.int64,
                              device=group_pack_index.device)).flatten(-2)
     mlog2log = inverse(log2mlog)
 
     # Step 2: construct redundant experts within nodes
     # [num_layers * num_nodes, num_logical_experts // num_nodes]
     tokens_per_mlog = weight.gather(-1, mlog2log).view(
         -1, num_logical_experts // num_nodes)
     phy2mlog, phyrank, mlogcnt = replicate_experts(
         tokens_per_mlog, num_physical_experts // num_nodes)
 
     # Step 3: pack physical_experts to GPUs
     # [num_layers * num_nodes, num_physical_experts // num_nodes]
     tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
     pack_index, rank_in_pack = balanced_packing(tokens_per_phy,
                                                 num_gpus // num_nodes,
                                                 refine_steps=2)
     phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
     pphy2phy = inverse(phy2pphy)
 
     pphy2mlog = phy2mlog.gather(
         -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]
     pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(
         0,
         num_logical_experts,
         num_logical_experts // num_nodes,
         device=group_pack_index.device,
     ).view(1, -1, 1)).flatten(-2)
     pphy2log = mlog2log.gather(-1, pphy2mlog)
     pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)
     logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)
     return pphy2log, pphyrank, logcnt
 
 
 def rebalance_experts(
     weight: torch.Tensor,
     num_replicas: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Entry point for expert-parallelism load balancer.
 
     Parameters:
         weight: [layers, num_logical_experts], the load statistics for all
             logical experts
         num_replicas: number of physical experts, must be a multiple of
             `num_gpus`
         num_groups: number of expert groups
         num_nodes: number of server nodes, where the intra-node network
             (e.g, NVLink) is faster
         num_gpus: number of GPUs, must be a multiple of `num_nodes`
 
     Returns:
         physical_to_logical_map: [layers, num_replicas], the expert index of
             each replica
         logical_to_physical_map: [layers, num_logical_experts, X], the replica
             indices for each expert
         expert_count: [layers, num_logical_experts], number of physical
             replicas for each logical expert
     """
     num_layers, num_logical_experts = weight.shape
     weight = weight.float().cpu()
     if num_groups % num_nodes == 0:
         # use hierarchical load-balance policy
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, num_groups, num_nodes, num_gpus)
     else:
         # use global load-balance policy
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, 1, 1, num_gpus)
     num_redundant_experts = num_replicas - num_logical_experts
     maxlogcnt = num_redundant_experts + 1
     log2phy: torch.Tensor = torch.full(
         (num_layers, num_logical_experts, maxlogcnt),
         -1,
         dtype=torch.int64,
         device=logcnt.device,
     )
     log2phy.view(num_layers, -1).scatter_(
         -1,
         phy2log * maxlogcnt + phyrank,
         torch.arange(num_replicas, dtype=torch.int64,
                      device=log2phy.device).expand(num_layers, -1),
     )
     return phy2log, log2phy, logcnt
 
 
 # EVOLVE-BLOCK-END
 
 
 # This part remains fixed (not evolved)
 def run_eplb(weight: torch.Tensor, num_replicas: int, num_groups: int,
              num_nodes: int, num_gpus: int):
     """Run the expert parallelism load balancer"""
     phy2log, log2phy, logcnt = rebalance_experts(
         weight, num_replicas, num_groups, num_nodes, num_gpus
     )
     return phy2log, log2phy, logcnt
 
 
 __all__ = ["rebalance_experts", "run_eplb"]