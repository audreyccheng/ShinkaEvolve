# INDIVIDUAL PROGRAM SUMMARIES

The following are summaries of individual programs evaluated since the last meta update:

**Program Name: Hierarchical Expert Load Balancer for vLLM**
- **Implementation**: Implements a greedy, hierarchical rearrangement using PyTorch: groups are packed to nodes, experts replicated within nodes, and physical experts packed to GPUs via a balanced_packing heuristic, with permutations managed by scatter/gather and an inverse mapping. Computation is performed primarily on CPU (weight.float().cpu()) with per-layer greedy choices and fixed replica counts used to build phy→log, log→phy, and expert count maps.
- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.0 over 5 workloads).
- **Feedback**: The approach is very fast due to simple greedy packing and CPU-based tensor ops, but yields only moderate load balance. Hierarchical constraints and per-layer greedy decisions (e.g., weight/logcnt replication and group-first packing) likely limit balancedness compared to more global or optimal assignments.
**Program Identifier:** Generation 0 - Patch Name initial_program - Correct Program: True

**Program Name: Hierarchical EPLB with diversity-aware packing**
- **Implementation**: Implements a three-stage hierarchical strategy: groups→nodes via balanced packing, per-node “water-filling” replication, and diversity-aware greedy assignment of replicas to GPUs that minimizes label collocation while equalizing per-pack load; permutation inverses and scatter build physical↔logical maps. Trivial/fast paths (e.g., one item per pack, no-duplicate labels) and CPU-friendly loops via list conversions keep overhead low.
- **Performance**: Combined score 0.66 (balancedness 0.3111, speed 1.00 across 5 workloads); all validations pass.
- **Feedback**: The diversity-aware step reduces hotspotting of identical labels, but the equal-items-per-pack constraint and greedy choices yield only moderate load balance. Efficient early exits and lightweight loops drive the top speed score.
**Program Identifier:** Generation 1 - Patch Name diversity_aware_gpu_packing - Correct Program: True

**Program Name: Hierarchical Expert Parallelism Load Balancer**
- **Implementation**: Uses greedy sort-based bin packing per layer with a small local 2-opt-style refinement between heaviest/lightest packs. Replication is selected via argmax(weight/logcnt) within nodes, then replicas are packed to GPUs; final mappings are built via gather/scatter and inverse permutations, operating on CPU tensors for simplicity.
- **Performance**: Combined score 0.65 (balancedness 0.306805, speed 1.00 across 5 workloads); all validations passed.
- **Feedback**: Excellent speed shows the greedy + limited refinement is lightweight, but moderate balancedness indicates room for stronger global balancing. More refinement iterations or smarter swap selection could improve balance; hierarchical policy is used when groups align with nodes, otherwise a global fallback is applied.
**Program Identifier:** Generation 2 - Patch Name local_swap_refine_in_packing - Correct Program: True

**Program Name: Hierarchical Serpentine Expert Load Balancer**
- **Implementation**: Uses vectorized serpentine (snake) packing per layer to evenly assign sorted experts to packs, and a d'Hondt-like greedy replicator that allocates extra replicas by weight/count. A hierarchical pipeline packs groups to nodes, replicates within nodes, then packs physical experts to GPUs using amortized loads, implemented with PyTorch scatter/gather and permutation inverses (with a fast path when groups_per_pack==1).
- **Performance**: Combined score 0.63 (balancedness 0.254285, speed 1.00; 5 workloads).
- **Feedback**: Fully vectorized design achieves maximal speed but modest balancedness, suggesting the greedy replication plus snake packing underperform under skewed loads. Correctness is solid (passes all tests); balancedness may improve with local swap/refinement or more global optimization.
**Program Identifier:** Generation 3 - Patch Name eplb_snake_pipeline - Correct Program: True

**Program Name: Hierarchical EPLB with Diversity-Aware Packing**
- **Implementation**: Uses a hierarchical three-stage pipeline: (1) greedy group-to-node packing with up to 4 heaviest/lightest swaps, (2) per-node expert replication via an incremental ratio (weight/count) greedy update, and (3) GPU placement with diversity-aware packing that prioritizes label spread then projected load. Mapping inverses and placements are built via scatter/gather for efficiency, with selective CPU sorting and minimal refinement to keep runtime low.
- **Performance**: Combined score 0.65 (balancedness 0.3068, speed 1.00 over 5 workloads).
- **Feedback**: The implementation is correct and passes all validation tests. It prioritizes speed—greedy choices and limited local refinement yield maximal runtime performance but only moderate balance; diversity-aware packing helps reduce hotspotting, though the capped refinements likely limit balancedness gains.
**Program Identifier:** Generation 4 - Patch Name diverse_pack_projload_and_incremental_ratio - Correct Program: True

**Program Name: Hierarchical EPLB with diversity-aware packing**
- **Implementation**: Uses a hierarchical strategy: groups are packed to nodes via balanced_packing, logical experts are replicated with Hamilton’s method (largest remainder) in replicate_experts, and physical experts are assigned to GPUs using a diversity-aware greedy packer that spreads identical labels to reduce hotspotting. Mappings are built with vectorized torch ops (scatter/inverse), and the top-level casts weights to CPU for consistent, fast execution.
- **Performance**: Combined score 0.64 (balancedness 0.2897, speed 1.0 across 5 workloads); all validation tests pass.
- **Feedback**: The approach is very fast due to simple greedy policies and vectorized construction of mappings, but balancedness is modest, indicating the greedy packing and proportional replication may under-serve heavy experts or miss global optimality. Consider stronger global balancing heuristics or improved tie-breaking beyond label diversity to raise balancedness without sacrificing speed.
**Program Identifier:** Generation 5 - Patch Name proportional_quota_replication - Correct Program: True

**Program Name: Hierarchical EPLB with greedy packing and swap refinement**
- **Implementation**: Implements a hierarchical load balancer: packs logical expert groups to nodes, greedily replicates experts within nodes, then balances physical experts across GPUs. The core balanced_packing uses per-layer greedy assignment with fixed per-pack item counts and a lightweight single-swap refinement; replicate_experts greedily allocates by current load (weight/logcnt), and mappings are built via scatter/gather operations.
- **Performance**: Combined score 0.66 (balancedness 0.3111, speed 1.0 over 5 workloads).
- **Feedback**: The approach is very fast due to simple greedy passes and minimal refinement, but achieves only moderate balance. Single-swap local improvement and equal-cardinality packing likely cap balancedness; nevertheless, the solution is correct and passes all validation tests.
**Program Identifier:** Generation 6 - Patch Name single_swap_refinement - Correct Program: True

**Program Name: Hierarchical EPLB with Greedy Packing and D’Hondt Replication**
- **Implementation**: Uses a capacity-constrained greedy packer (sort-by-weight, place into lightest pack) with a single heaviest↔lightest swap refinement and deterministic in-pack ranks; replica allocation is a vectorized D’Hondt-style water-filling. Hierarchical mapping packs groups to nodes and GPUs via permutation ops (including a scatter-based inverse), with replication done per node and final mappings produced by gather/scatter.
- **Performance**: Combined score 0.66 (balancedness 0.3111, speed 1.00 across 5 workloads).
- **Feedback**: The lightweight greedy packing favors speed but leaves noticeable imbalance; increasing refinement steps or applying smarter swap/refinement strategies could raise balancedness with modest cost. Vectorized allocation and permutation-based layout keep the approach simple and fast, and all validation tests pass.
**Program Identifier:** Generation 7 - Patch Name moe_eplb_modular_refine - Correct Program: True

**Program Name: Hierarchical EPLB with Greedy Packing and Swap Refinement**
- **Implementation**: Uses per-layer greedy balanced_packing (descending sort with capacity-constrained assignment) plus up to two bounded heavy-light swaps to reduce imbalance while reassigning ranks. Applies a hierarchical strategy: pack groups to nodes, greedily replicate experts via weight/logcnt, then pack physical experts to GPUs; mappings are constructed via gather/scatter and inverse permutations.
- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.00 across 5 workloads); passes all validation tests.
- **Feedback**: Excellent speed from lightweight greedy steps and limited refinement, but modest balancedness suggests under-optimization of load distribution. More aggressive or global refinement could improve balance at the expense of speed.
**Program Identifier:** Generation 8 - Patch Name multi_swap_refinement_balanced_packing - Correct Program: True

**Program Name: RR-WF Packing with Hare Replication**
- **Implementation**: Implements Round-Robin Waterfilling packing to assign items to packs in capacity rounds, with a lightweight post-swap refinement; uses Hare (Hamilton) largest-remainder to allocate replicas proportionally, then a hierarchical pipeline (group→node packing, intra-node replication, GPU packing) built with vectorized torch gathers/scatters and an inverse-permutation utility. The entry rebalance forces weight to float().cpu() and selects hierarchical when groups % nodes == 0, otherwise a global fallback.
- **Performance**: Combined score 0.64 (balancedness 0.289706, speed 1.000000 over 5 workloads); program is correct and passes all validation tests.
- **Feedback**: Excellent speed stems from simple per-round vectorized ops, stable sorting, and minimal refinement; per-row loops (remainders/topk, final swap) are lightweight. Balance is improved over naive strategies but remains moderate, suggesting headroom beyond the single-pass RR-WF plus optional one-swap refinement.
**Program Identifier:** Generation 9 - Patch Name rr_waterfill_hare - Correct Program: True

**Program Name: Hierarchical Water-Fill + Diverse Packing EPLB**
- **Implementation**: Implements a three-stage hierarchical balancer: capacity-aware LPT group-to-node packing, intra-node water-filling replica allocation via binary search with greedy extras, and GPU placement using a diversity-aware heap that minimizes (label repeats, load, fill). It builds contiguous phy2log/rank with repeat_interleave, inverts mappings where needed, and scatters logical→physical indices; falls back to a single-node policy when groups aren’t node-aligned.
- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000) across 5 workloads; all validations passed.
- **Feedback**: The approach is very fast due to lightweight CPU sorts, vectorized gathers/scatters, and simple greedy heuristics, but achieves only moderate balance. Balancedness likely suffers from the greedy capacity-constrained packers and per-node isolation; tuning the packing objective or more global optimization could improve load balance.
**Program Identifier:** Generation 10 - Patch Name waterfill_diverse_heap - Correct Program: True

**Program Name: Hierarchical Diverse Expert Load Balancer**
- **Implementation**: Implements hierarchical packing of logical experts to nodes and GPUs: greedy capacity-constrained packing with K=2 heavy–light swaps, diversity-aware packing to spread identical labels, and a hybrid D’Hondt→Sainte-Laguë replication strategy. Uses vectorized gathers/scatters and invert-permutation helpers to build physical↔logical maps and replica ranks.
- **Performance**: Combined score 0.65 (balancedness 0.304918, speed 1.00 over 5 workloads).
- **Feedback**: All tests passed; packing and replication are fast, yielding top speed but only moderate balance. The limited K=2 refinement and local (per-node) swaps likely cap balancedness despite diversity-aware placement.
**Program Identifier:** Generation 11 - Patch Name k2_swap_tail_hybrid - Correct Program: True

**Program Name: Hierarchical Water-Filling EPLB**
- **Implementation**: Implements a hierarchical policy: packs expert groups to nodes with capacity-aware LPT, performs per-node water-filling replication with a one-step donor→receiver fix, then assigns to GPUs via a diversity-aware heap plus a k=2 swap refinement. Builds physical→logical and logical→physical maps via scatter/gather with contiguous replica ranks.
- **Performance**: Combined score 0.66 (balancedness 0.311, speed 1.00 over 5 workloads); passes all validation tests.
- **Feedback**: Runtime is excellent, indicating low overhead from the greedy/heap steps and CPU tensor ops. Balancedness is modest, suggesting the single-step fix and limited 2-opt swap cap equalization, leaving room for deeper refinement if needed.
**Program Identifier:** Generation 12 - Patch Name k2_swap_tail_fixup - Correct Program: True

**Program Name: Hierarchical EPLB with greedy packing**
- **Implementation**: Implements a hierarchical expert-parallel balancer: groups are greedily packed to nodes with a bounded multi-swap refinement (max_swaps=2), replicas are allocated per layer via a D’Hondt-like scheme with a one-step donor→receiver fix-up, and physical experts are then packed to GPUs; mappings use gather/scatter with inverse permutations for determinism. Packing operates on CPU tensors with stable rank reassignment to maintain per-pack ordering.
- **Performance**: Combined score 0.66 (balancedness 0.311, speed 1.000 over 5 workloads).
- **Feedback**: Excellent speed stems from simple greedy heuristics and tightly bounded refinement, but balancedness is moderate, likely limited by the two-swap bound and single-step replica fix-up. Hierarchical placement benefits topology-aware locality when groups are node-divisible; otherwise it falls back to a global policy.
**Program Identifier:** Generation 13 - Patch Name replication_fixup_one_step - Correct Program: True

**Program Name: Hierarchical EPLB with Greedy Packing and Replication**
- **Implementation**: Uses per-layer sorted greedy balanced_packing with exact items-per-pack and a bounded (max 2) multi-swap refinement to reduce imbalance. Replication applies a D’Hondt-like greedy allocation plus a one-step donor→receiver fix-up; a hierarchical pipeline packs groups to nodes, replicates within nodes, then packs physical experts to GPUs with inverse maps and stable ranks.
- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000 over 5 workloads); passes all validation tests.
- **Feedback**: Excellent speed but moderate balancedness, likely limited by the small swap budget and single-step replication correction. The hierarchical policy preserves locality and determinism; increasing refinement steps could improve balance at some runtime cost.
**Program Identifier:** Generation 14 - Patch Name one_step_replication_fixup - Correct Program: True

**Program Name: EPLB Hybrid Hierarchical Balancer**
- **Implementation**: Implements hierarchical expert balancing with a hybrid D’Hondt→Sainte-Laguë replica apportionment (plus a one-step fix-up), greedy capacity-constrained packing with k=2 best-improvement swaps, and a diversity-aware GPU assignment tie-breaker. Uses permutation inversion utilities and staged refinement (1 step for groups→nodes, 2 steps for physical→GPU packing).
- **Performance**: Combined score 0.0; fails all validation tests.
- **Feedback**: The replica fix-up moves a seat from the highest-average “donor” to the lowest-average “receiver,” which worsens imbalance (direction should be receiver→donor), likely breaking correctness. Additionally, layout reshaping/gather assumes block-contiguous permutations and the forced .cpu() cast may violate expected device semantics, contributing to failures.
**Program Identifier:** Generation 15 - Patch Name hydra_k2_diversity_apportion - Correct Program: False

**Program Name: Hierarchical EPLB with Greedy Packing**
- **Implementation**: Uses capacity-constrained greedy packing over sorted weights with bounded multi-swap refinement; hierarchical grouping packs groups→nodes and replicas→GPUs via inverse-permutation mappings. Replication applies a D’Hondt allocation with a Sainte-Laguë tail and a single donor→receiver fix-up, with deterministic rank reassignment.
- **Performance**: Combined score 0.65; balancedness_score 0.304918, speed_score 1.000000 across 5 workloads.
- **Feedback**: The limited refinement (refine_steps=1–2) and one-step fix-up prioritize speed but leave noticeable imbalance. Increasing swap iterations or applying richer local search could improve balancedness at some runtime cost; all validation tests pass.
**Program Identifier:** Generation 16 - Patch Name hybrid_sainte_lague_tail_and_stage_refine - Correct Program: True

**Program Name: Hierarchical Expert Parallel Load Balancer**
- **Implementation**: Implements a hierarchical EPLB: packs expert groups to nodes via greedy balanced_packing with bounded multi-swap refinement, replicates experts using a hybrid D’Hondt (bulk) + Sainte-Laguë (tail) allocator plus a one-step donor→receiver fix-up, then packs physical replicas to GPUs. Uses CPU-based sorting/scatter and stable rank reassignment, with inverse mappings for logical/physical indices.
- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.000000 over 5 workloads); all validation tests passed.
- **Feedback**: Excellent speed indicates the greedy, bounded-swap design and CPU-side ops keep overhead low. Balancedness is moderate, likely constrained by the single-step replica fix-up and small refine_steps; allowing more refinement or multi-move adjustments could improve load balance.
**Program Identifier:** Generation 17 - Patch Name hybrid_sainte_lague_tail_and_stage_specific_refine - Correct Program: True

**Program Name: Hierarchical Diversity-Aware EPLB**
- **Implementation**: Performs hierarchical load balancing: packs expert groups to nodes with greedy balanced packing, replicates experts per node by max normalized load, then assigns replicas to GPUs using diversity-aware greedy packing with a single k=2 best-improvement swap; mappings use vectorized gather/scatter for efficiency. Includes a fast path when labels are unique and tie-breakers to reduce label duplication per pack.
- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.00 over 5 workloads).
- **Feedback**: Passes all validation tests; the diversity-aware greedy plus minimal refinement achieves excellent speed but only moderate balance. More aggressive refinement (multi-swap or iterative improvement) could raise balancedness, though current design already reduces hotspotting from replica co-location.
**Program Identifier:** Generation 18 - Patch Name k2_micro_swap_diverse - Correct Program: True

**Program Name: Hierarchical EPLB with greedy packing and hybrid apportionment**
- **Implementation**: Implements a hierarchical load balancer: packs groups to nodes, replicates experts within nodes using a hybrid D’Hondt (bulk) + Sainte-Laguë (tail) apportionment with a one-step fix-up, then assigns physical experts to GPUs via balanced packing. The packing is a sorted greedy placement with capacity constraints plus bounded local multi-swap refinement; mappings are constructed with gather/scatter and inverse permutations.
- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.0 over 5 workloads).
- **Feedback**: Correct and passes all validation tests. Excellent speed due to bounded refinement and lightweight apportionment, while moderate balancedness indicates potential gains from higher refine_steps or multi-iteration replication adjustments.
**Program Identifier:** Generation 19 - Patch Name refine_packing_and_hybrid_replication - Correct Program: True

# GLOBAL INSIGHTS SCRATCHPAD

The following are global insights about optimization approaches and their effectiveness:

## Successful Algorithmic Patterns
- Bounded heavy↔light swap refinement on top of capacity-constrained greedy packing consistently lifts balancedness from the mid-0.30s to ≈0.311–0.312 without hurting speed. The current best program (Combined 0.66, balancedness 0.311848, speed 1.00) uses refine_steps=1 for group→node packing and refine_steps=2 for physical→GPU packing; similar strategies in “Hierarchical Water-Filling EPLB” (Gen12, 0.66, ~0.311) and “Hierarchical EPLB with Greedy Packing and Replication” (Gen14, 0.66, 0.311077) confirm the benefit.
- Hybrid D’Hondt (bulk) → Sainte-Laguë (tail) replication with a one-step donor→receiver fix-up is effective. The current best and “Hierarchical Expert Parallel Load Balancer” (Gen17, 0.66, 0.311848) as well as “...hybrid apportionment” (Gen19, 0.66, 0.311848) share this pattern, beating variants that don’t include a tail phase or correction step.
- Hierarchical pipeline (group→node packing, intra-node replication, GPU packing) is a reliable foundation across top performers. Programs using this structure, including the current best, repeatedly achieve 0.66 combined with balancedness ≈0.311–0.312 and speed 1.00 (e.g., Gen10, Gen12–14, Gen17–19).
- Diversity-aware tie-breaking during GPU packing can match top scores but not surpass them. “Hierarchical Water-Fill + Diverse Packing EPLB” (Gen10, 0.66, 0.311077) and “Hierarchical Diversity-Aware EPLB” (Gen18, 0.66, 0.311077) show that label-spread heuristics help avoid hotspotting without improving the balancedness metric beyond the ~0.311 ceiling.

## Ineffective Approaches
- Mis-specified replication fix-up logic and device semantics break correctness. “EPLB Hybrid Hierarchical Balancer” (Gen15) scored 0.0 due to moving a seat in the wrong direction in the fix-up and assuming block-contiguous layout plus a forced .cpu() cast, leading to validation failures.
- Limited or overly local refinement caps balancedness around ~0.305. “Hierarchical Diverse Expert Load Balancer” (Gen11, 0.65, 0.304918) and “Hierarchical EPLB with Greedy Packing” (Gen16, 0.65, 0.304918) each apply only K=2 swaps or local per-node swaps, which the feedback notes likely cap balancedness despite keeping speed at 1.00.

## Implementation Insights
- What makes the current best program effective:
  - Balanced_packing combines sorted greedy placement with a tiny number of heavy↔light swaps driven by pack load deltas. It computes pack loads via scatter_add, selects swap candidates using searchsorted over sorted light-pack items, and only applies a swap if it strictly reduces the heavy–light delta. Ranks within affected packs are reassigned deterministically, preserving dense 0..capacity-1 ordering.
  - Replication uses a hybrid apportionment: D’Hondt for the bulk (benefit = weight / r) and Sainte-Laguë in the tail (benefit = weight / (2r − 1)), followed by a one-step donor→receiver fix-up that is guarded by a global-maximum check (new_peak < current_max) and feasibility (donor count > 1). This avoids the regression seen in Gen15’s incorrect fix-up.
  - Hierarchical mapping is built with gather/scatter and a scatter-based inverse permutation utility, all on CPU tensors for determinism and speed. The program uses a chain of gathers to map physical→logical indices (pphy→mlog→log) and constructs logical→physical via a scatter with replica ranks, which keeps runtime saturated at 1.00.
- Consistent coding patterns across high performers:
  - CPU-centric tensor ops (weight.float().cpu) with vectorized sort/gather/scatter deliver uniform top speed (1.00) while enabling deterministic mappings. This appears in the current best, Gen17, and Gen19 (all 0.66, 0.311848).
  - Bounded refinement per stage (refine_steps=1–2) yields the best balance/speed trade-off. Programs with such bounds cluster at 0.66 (e.g., Gen12–14, Gen17–19), whereas similar approaches with fewer or more localized swaps land at 0.65 (Gen11, Gen16).

## Performance Analysis
- Two clusters emerge with identical speed (1.00): a top cluster at balancedness ≈0.311–0.312 (Combined 0.66) and a lower cluster at ≈0.305 (Combined 0.65). The current best (0.311848) sits at the high end, matching “Hierarchical Expert Parallel Load Balancer” (Gen17) and “...hybrid apportionment” (Gen19), both at 0.311848.
- The slight edge from 0.311077 to 0.311848 aligns with stronger final-stage refinement. The current best uses refine_steps=2 when packing physical experts to GPUs; programs with diversity-aware placement but minimal refinement (Gen10, Gen18 at 0.311077) do not surpass this.
- All correct programs achieve speed 1.00, so improvements are driven solely by balancedness. Adding tiny, targeted refinement (one or two swaps) and a guarded one-step replication fix-up correlates with the ~0.006–0.007 balancedness gain over variants with limited or purely local swaps (Gen11, Gen16 at 0.304918).
- Hierarchical designs dominate among top scorers. Whether the GPU placement is diversity-aware (Gen10, Gen18) or purely balance-driven with small swap refinements (current best, Gen17, Gen19), the hierarchical structure plus hybrid replication sustains the highest balancedness without sacrificing speed.

# META RECOMMENDATIONS

The following are actionable recommendations for the next program generations:

1. Upgrade the refinement swap to a bounded k-candidate best-improvement step. In balanced_packing, take top-2 heaviest items from the heaviest pack and bottom-2 lightest from the lightest pack (via torch.topk on masked indices) and evaluate all 4 pair swaps; apply the single swap that yields the smallest new_delta = |delta - 2*(wi - wj)| if it strictly improves. Keep refine_steps at 1 for group→node and 2 for physical→GPU to retain 1.00 speed while extracting the extra ~0.0005–0.001 balancedness seen from stronger final-stage refinement.

2. Make refinement adaptive: allow a second swap only when the first improvement is small. After the first swap in an iteration, recompute delta; if new_delta > 0.8 * old_delta (i.e., <20% gain), attempt one more swap within the same layer; otherwise break early. Apply this only in the physical→GPU packing stage (refine_steps=2 thresholded), preserving speed but capturing occasional extra gains beyond the current best.

3. Strengthen the replication fix-up with a single best move chosen from top-2 donors and bottom-2 receivers. After the D’Hondt→Sainte-Laguë allocation, compute avg = weight/logcnt, take donors = topk(avg, k=2) and receivers = bottom-2; evaluate moving one replica for all valid donor→receiver pairs (donor count > 1), pick the move that minimizes the new peak, and apply it only if new_peak < cur_max. Still perform at most one move per row and reassign the donor’s highest-rank column, preserving correctness and the proven speed profile.

4. Use an adaptive tail apportionment and test Huntington–Hill for the final replicas. Set tail = max(1, round(num_redundant * min(0.15, max(0.05, cv/2)))) where cv = std(weight)/mean(weight) per row; use D’Hondt for the bulk and for the tail either Sainte-Laguë (current) or Huntington–Hill (benefit = weight / sqrt(r*(r+1))) and pick the better of the two in a lightweight A/B per row (deterministic). This keeps the bulk behavior that works while giving skewed layers a stronger dampening on peak averages.

5. Add a diversity-aware tie-breaker only under near-ties in GPU packing. When selecting the next pack in the greedy step, if the top-2 candidate packs’ loads differ by < epsilon (e.g., 1e-6 * mean item weight), prefer the pack with fewer replicas of the same logical expert; implement by a tiny penalty load’ = load + lambda * same_expert_count with lambda scaled to be negligible outside ties. Keep the existing refine_steps=2 so the final micro-swaps still drive the main balance gains.