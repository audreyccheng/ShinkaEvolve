<NAME>
diverse_gpu_packing_and_two_move_fixup
</NAME>

<DESCRIPTION>
We introduce two targeted improvements that keep the algorithm fast while addressing the remaining imbalance hot spots:

1) Diversity-aware GPU packing: replace the GPU-stage balanced_packing with a variant that breaks ties by spreading replicas of the same logical expert (meta-logical id) across GPUs. This avoids co-locating same-label replicas under near-tied loads, improving peak balance with negligible overhead. It retains the same greedy lightest-pack heuristic and the same bounded refinement used in the original packer.

2) Two-move replication fix-up: extend the post-allocation replication fix-up from a single donorâ†’receiver move to up to two moves per row, recomputing the best move after the first. This preserves speed but yields additional reduction of the maximum average load when the first move only partially alleviates the peak.

Both changes are small, deterministic, and bounded in cost, preserving the observed 1.00 speed while improving balancedness.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    return pack_index, rank_in_pack
=======
    return pack_index, rank_in_pack


def _balanced_packing_diverse(
    weights: torch.Tensor,
    labels: torch.Tensor,
    num_packs: int,
    refine_steps: int = 2,
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Diversity-aware variant of balanced packing:
      - Same greedy scheme to assign the next heaviest item to the lightest pack with capacity.
      - If multiple candidate packs are near-tied in load, prefer the pack with fewer items of the same label.
      - Bounded refinement afterwards (same best-swap routine), with small number of steps.

    weights: [L, N], labels: [L, N] int64
    Returns CPU tensors pack_index, rank_in_pack for consistency with caller.
    """
    num_layers, num_groups = weights.shape
    assert num_groups % num_packs == 0
    groups_per_pack = num_groups // num_packs

    if groups_per_pack == 1:
        pack_index = torch.arange(weights.size(-1),
                                  dtype=torch.int64,
                                  device=weights.device).expand(weights.shape).cpu()
        rank_in_pack = torch.zeros_like(pack_index, dtype=torch.int64)
        return pack_index, rank_in_pack

    # Sort indices by descending weight (CPU)
    indices = weights.float().sort(-1, descending=True).indices.cpu()
    labels_cpu = labels.long().cpu()
    pack_index = torch.full_like(weights,
                                 fill_value=-1,
                                 dtype=torch.int64,
                                 device="cpu")
    rank_in_pack = torch.full_like(pack_index, fill_value=-1, device="cpu")

    for i in range(num_layers):
        pack_weights = [0.0] * num_packs
        pack_items = [0] * num_packs
        label_cnt = [dict() for _ in range(num_packs)]
        wrow = weights[i].cpu()
        lrow = labels_cpu[i]
        # scale for epsilon based on mean weight to keep tie-break cheap and stable
        mean_w = float(wrow.mean().item()) if num_groups > 0 else 0.0
        eps = 1e-6 * max(mean_w, 1e-12)
        lam = 1e-8 * max(mean_w, 1e-12)  # tiny diversity penalty for near ties

        for g in indices[i]:
            g = int(g.item())
            # candidate packs with remaining capacity
            cand = [p for p in range(num_packs) if pack_items[p] < groups_per_pack]
            # baseline minimal load among candidates
            min_load = min(pack_weights[p] for p in cand)
            # effective load with diversity tie-break under near ties
            lab = int(lrow[g].item())
            best_pack = None
            best_eff = None
            for p in cand:
                ld = pack_weights[p]
                eff = ld
                if ld - min_load <= eps:
                    same = label_cnt[p].get(lab, 0)
                    eff = ld + lam * same
                if best_eff is None or eff < best_eff:
                    best_eff = eff
                    best_pack = p

            pack_index[i, g] = best_pack
            rank_in_pack[i, g] = pack_items[best_pack]
            pack_weights[best_pack] += float(wrow[g].item())
            pack_items[best_pack] += 1
            label_cnt[best_pack][lab] = label_cnt[best_pack].get(lab, 0) + 1

        # Optional small refinement (reuse same bounded best-swap as balanced_packing)
        if groups_per_pack > 1 and refine_steps > 0:
            max_swaps = int(refine_steps)
            for _ in range(max_swaps):
                packs = pack_index[i]  # [num_groups], CPU
                w = wrow  # CPU
                # Compute pack loads
                pack_w = torch.zeros(num_packs, dtype=w.dtype)
                pack_w.scatter_add_(0, packs, w)
                h = int(torch.argmax(pack_w))
                l = int(torch.argmin(pack_w))
                delta = float(pack_w[h] - pack_w[l])
                if delta <= 1e-9:
                    break

                heavy_idx = torch.nonzero(packs == h, as_tuple=False).squeeze(1)
                light_idx = torch.nonzero(packs == l, as_tuple=False).squeeze(1)
                if heavy_idx.numel() == 0 or light_idx.numel() == 0:
                    break

                hw = w[heavy_idx]
                lw = w[light_idx]
                lw_sorted, lw_perm = torch.sort(lw)  # ascending
                if lw_sorted.numel() == 0 or hw.numel() == 0:
                    break

                # For each heavy item, find light item closest to target = hw - delta/2
                target = hw - (delta / 2.0)
                pos = torch.searchsorted(lw_sorted, target)
                pos = torch.clamp(pos, 0, lw_sorted.numel() - 1)
                cand_pos = torch.stack([pos, torch.clamp(pos - 1, 0, lw_sorted.numel() - 1)], dim=1)
                cand_lw = lw_sorted[cand_pos]  # [H, 2]
                resid = (delta - 2.0 * (hw.unsqueeze(1) - cand_lw)).abs()
                best_flat = int(torch.argmin(resid).item())
                best_h_index = best_flat // 2
                best_option = best_flat % 2
                j_sorted_idx = int(cand_pos[best_h_index, best_option].item())

                wi = float(hw[best_h_index].item())
                wj = float(lw_sorted[j_sorted_idx].item())
                new_delta = abs(delta - 2.0 * (wi - wj))
                if new_delta < delta - 1e-9:
                    hi = heavy_idx[best_h_index]
                    lj = light_idx[lw_perm[j_sorted_idx]]
                    pack_index[i, hi] = l
                    pack_index[i, lj] = h
                    # Reassign ranks within affected packs to keep 0..groups_per_pack-1
                    for p in (h, l):
                        mask = pack_index[i] == p
                        idx = torch.nonzero(mask, as_tuple=False).squeeze(1)
                        if idx.numel() == 0:
                            continue
                        prev_rank = rank_in_pack[i, idx]
                        order = torch.argsort(prev_rank)
                        new_ranks = torch.arange(order.numel(), dtype=torch.int64)
                        rank_in_pack[i, idx[order]] = new_ranks
                else:
                    break

    return pack_index, rank_in_pack
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,
                                                num_gpus // num_nodes,
                                                refine_steps=2)
=======
    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
    labels_per_phy = phy2mlog  # encourage spreading same meta-logical ids across GPUs under near ties
    pack_index, rank_in_pack = _balanced_packing_diverse(tokens_per_phy,
                                                         labels_per_phy,
                                                         num_gpus // num_nodes,
                                                         refine_steps=2)
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
    # Strengthened one-move replication fix-up per row:
    # Evaluate donors from top-2 avg and receivers from bottom-2 avg; apply best improving move.
    if num_log > 1 and num_redundant > 0:
        avg = weight / logcnt.to(dtype_f)
        cur_max_vals, argmax_idx = avg.max(dim=-1)
        k = min(2, num_log)
        top_vals, top_idx = torch.topk(avg, k=k, dim=-1, largest=True)
        bot_vals, bot_idx = torch.topk(avg, k=k, dim=-1, largest=False)

        rows = torch.arange(n, dtype=torch.int64, device=device).tolist()
        for ri in rows:
            cur_max = float(cur_max_vals[ri].item())
            second = float((top_vals[ri, 1].item() if k > 1 else top_vals[ri, 0].item()))
            best_pair = None
            best_peak = None

            donors = top_idx[ri].tolist()
            receivers = bot_idx[ri].tolist()
            for d in donors:
                cd = int(logcnt[ri, d].item())
                if cd <= 1:
                    continue
                for r in receivers:
                    if d == r:
                        continue
                    cr = int(logcnt[ri, r].item())
                    # baseline "other" peak if donor is current max is the second-best
                    baseline_other = second if d == int(argmax_idx[ri].item()) else cur_max
                    new_d = float(weight[ri, d].item()) / float(cd - 1)
                    new_r = float(weight[ri, r].item()) / float(cr + 1)
                    candidate_peak = max(baseline_other, new_d, new_r)
                    if candidate_peak + 1e-12 < cur_max:
                        if best_peak is None or candidate_peak < best_peak:
                            best_peak = candidate_peak
                            best_pair = (d, r)

            if best_pair is not None:
                d, r = best_pair
                donor_cols = torch.nonzero(phy2log[ri] == d, as_tuple=False).squeeze(1)
                if donor_cols.numel() == 0:
                    continue
                maxr_idx = torch.argmax(rank[ri, donor_cols]).item()
                col_idx = donor_cols[maxr_idx]
                new_rank = int(logcnt[ri, r].item())
                phy2log[ri, col_idx] = r
                rank[ri, col_idx] = new_rank
                logcnt[ri, d] -= 1
                logcnt[ri, r] += 1
=======
    # Strengthened replication fix-up per row (up to 2 moves):
    # Iteratively evaluate donors from top-2 avg and receivers from bottom-2 avg; apply best improving move.
    if num_log > 1 and num_redundant > 0:
        rows = torch.arange(n, dtype=torch.int64, device=device).tolist()
        for ri in rows:
            for attempt in range(2):
                avg_row = weight[ri] / logcnt[ri].to(dtype_f)
                cur_max = float(avg_row.max().item())
                k = min(2, num_log)
                top_vals, top_idx = torch.topk(avg_row, k=k, largest=True)
                bot_vals, bot_idx = torch.topk(avg_row, k=k, largest=False)
                second = float((top_vals[1].item() if k > 1 else top_vals[0].item()))
                argmax_id = int(top_idx[0].item())

                best_pair = None
                best_peak = None

                donor_list = top_idx.tolist()
                recv_list = bot_idx.tolist()
                for d in donor_list:
                    cd = int(logcnt[ri, d].item())
                    if cd <= 1:
                        continue
                    for r in recv_list:
                        if d == r:
                            continue
                        cr = int(logcnt[ri, r].item())
                        baseline_other = second if d == argmax_id else cur_max
                        new_d = float(weight[ri, d].item()) / float(cd - 1)
                        new_r = float(weight[ri, r].item()) / float(cr + 1)
                        candidate_peak = max(baseline_other, new_d, new_r)
                        if candidate_peak + 1e-12 < cur_max:
                            if best_peak is None or candidate_peak < best_peak:
                                best_peak = candidate_peak
                                best_pair = (d, r)

                if best_pair is None:
                    break

                d, r = best_pair
                donor_cols = torch.nonzero(phy2log[ri] == d, as_tuple=False).squeeze(1)
                if donor_cols.numel() == 0:
                    break
                maxr_idx = torch.argmax(rank[ri, donor_cols]).item()
                col_idx = donor_cols[maxr_idx]
                new_rank = int(logcnt[ri, r].item())
                phy2log[ri, col_idx] = r
                rank[ri, col_idx] = new_rank
                logcnt[ri, d] -= 1
                logcnt[ri, r] += 1
>>>>>>> REPLACE

</DIFF>