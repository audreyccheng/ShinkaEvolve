# EVOLVE-BLOCK-START
"""
Expert parallelism load balancer (EPLB) for vLLM.

This module implements a novel rearrangement algorithm.

Key ideas:
- Round-Robin Waterfilling (RR-WF) packing: in each capacity round, assign the
  next heaviest items to the currently lightest packs.
- Hare largest-remainder (Hamilton) replica allocation: proportional counts in
  one shot, ensuring 1 replica per logical expert, then distributing extras by
  fractional remainders.

Both are designed to be fast and to improve balance in practice.
"""

import torch


class _Ops:
    @staticmethod
    def inverse(perm: torch.Tensor) -> torch.Tensor:
        """
        Compute inverse permutation row-wise.

        perm: [L, N], a permutation for each row.
        returns inv_perm where inv_perm[g, perm[g, i]] = i
        """
        L, N = perm.shape
        inv = torch.empty_like(perm)
        inv.scatter_(1, perm, torch.arange(N, dtype=torch.int64, device=perm.device).expand(L, -1))
        return inv


class RRWaterfillPacker:
    """
    Round-Robin Waterfilling packer:
      - Sort items by weight descending.
      - For each round r (0..capacity-1), assign the next M heaviest items to
        the current M lightest packs (per layer).
      - This couples "heavy items" with "light packs" in each round, improving
        balance over simple greedy or static serpentine patterns.
    """

    @staticmethod
    def pack(weight: torch.Tensor, num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Pack n weighted objects to m packs, such that each bin contains exactly
        n/m objects and the weights of all packs are as balanced as possible.

        Parameters:
            weight: [L, N], weights
            num_packs: M, number of packs

        Returns:
            pack_index: [L, N], pack id in [0, M)
            rank_in_pack: [L, N], rank in each pack [0, N/M)
        """
        L, N = weight.shape
        assert N % num_packs == 0
        capacity = N // num_packs
        device = weight.device
        dtype_i64 = torch.int64

        # Fast path: one item per pack
        if capacity == 1:
            pack_index = torch.arange(N, dtype=dtype_i64, device=device).expand(L, -1).contiguous()
            rank_in_pack = torch.zeros((L, N), dtype=dtype_i64, device=device)
            return pack_index, rank_in_pack

        # Sort indices by weight descending per layer
        sorted_idx = torch.argsort(weight, dim=-1, descending=True)

        # Outputs
        pack_index = torch.full((L, N), -1, dtype=dtype_i64, device=device)
        rank_in_pack = torch.full((L, N), -1, dtype=dtype_i64, device=device)
        loads = torch.zeros((L, num_packs), dtype=weight.dtype, device=device)

        arL = torch.arange(L, dtype=dtype_i64, device=device).unsqueeze(1)

        # Assign in rounds: in each round, fill one slot per pack
        for r in range(capacity):
            block = sorted_idx[:, r * num_packs:(r + 1) * num_packs]  # [L, M], next M heaviest items
            # Current lightest packs per layer (ascending load)
            pack_order = torch.argsort(loads, dim=-1, descending=False, stable=True)  # [L, M]

            # Assign block[:, j] -> pack_order[:, j]
            pack_index[arL, block] = pack_order
            rank_in_pack[arL, block] = r

            # Update loads
            loads.scatter_add_(dim=1, index=pack_order, src=weight[arL, block])

        # Lightweight single refinement: swap one pair between heaviest and lightest packs if it helps
        # (kept minimal to preserve speed)
        # Note: purely optional; RR-WF already provides a strong baseline.
        for li in range(L):
            w = weight[li]
            packs = pack_index[li]
            pack_w = torch.zeros(num_packs, dtype=w.dtype, device=device)
            pack_w.scatter_add_(0, packs, w)
            p_max = int(torch.argmax(pack_w))
            p_min = int(torch.argmin(pack_w))
            delta = float(pack_w[p_max] - pack_w[p_min])
            if delta <= 0:
                continue
            max_mask = (packs == p_max)
            min_mask = (packs == p_min)
            if not (max_mask.any() and min_mask.any()):
                continue
            max_items = torch.nonzero(max_mask, as_tuple=False).flatten()
            min_items = torch.nonzero(min_mask, as_tuple=False).flatten()
            # Choose heaviest from max, lightest from min
            a_idx = int(max_items[torch.argmax(w[max_items])].item())
            b_idx = int(min_items[torch.argmin(w[min_items])].item())
            wa = float(w[a_idx].item())
            wb = float(w[b_idx].item())
            new_delta = abs(delta - 2.0 * (wa - wb))
            if new_delta + 1e-6 < delta:
                # swap packs
                pa = int(packs[a_idx].item())
                pb = int(packs[b_idx].item())
                pack_index[li, a_idx] = pb
                pack_index[li, b_idx] = pa
                # ranks: recompute ranks within these two packs
                for p in (pa, pb):
                    mask = (pack_index[li] == p)
                    ids = torch.nonzero(mask, as_tuple=False).flatten()
                    # keep deterministic order by original indices
                    ids_sorted = torch.sort(ids).values
                    rank_in_pack[li, ids_sorted] = torch.arange(ids_sorted.numel(), dtype=dtype_i64, device=device)

        return pack_index, rank_in_pack


def balanced_packing(weight: torch.Tensor, num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Public API: Round-Robin Waterfilling packing.
    """
    return RRWaterfillPacker.pack(weight, num_packs)


class HareReplicator:
    """
    Replica allocator using Hare largest remainder (Hamilton apportionment):
      - Ensure one replica per logical expert.
      - Distribute extra replicas proportionally to weights using floors of the
        ideal shares and assign remaining by largest remainders.
    This directly targets equal per-replica load and is very fast.
    """

    @staticmethod
    def allocate(weight: torch.Tensor, num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Parameters:
            weight: [X, num_log]
            num_phy: total number of experts after replication

        Returns:
            phy2log: [X, num_phy], logical expert id of each physical expert
            rank:    [X, num_phy], the replica rank
            logcnt:  [X, num_log], number of replicas for each logical expert
        """
        X, num_log = weight.shape
        device = weight.device
        dtype_i64 = torch.int64

        num_extra = num_phy - num_log
        assert num_extra >= 0

        # Start with one replica per expert
        counts = torch.ones((X, num_log), dtype=dtype_i64, device=device)

        if num_extra > 0:
            # Proportional shares for extras
            sumw = weight.sum(dim=1, keepdim=True)
            denom = torch.clamp(sumw, min=1e-12)
            shares = (weight / denom) * float(num_extra)  # [X, num_log], float

            base = torch.floor(shares).to(dtype_i64)
            counts = counts + base
            remain = num_extra - base.sum(dim=1)  # [X]

            # Distribute remaining by largest remainders
            rema = shares - base.to(weight.dtype)  # [X, num_log]
            if int(remain.max().item()) > 0:
                for i in range(X):
                    r = int(remain[i].item())
                    if r <= 0:
                        continue
                    # top-r fractional remainders
                    top_idx = torch.topk(rema[i], k=r, largest=True, sorted=False).indices
                    counts[i, top_idx] += 1

        # Build phy2log and rank tensors per row
        phy2log = torch.empty((X, num_phy), dtype=dtype_i64, device=device)
        rank = torch.empty((X, num_phy), dtype=dtype_i64, device=device)

        ar_log = torch.arange(num_log, dtype=dtype_i64, device=device)
        for i in range(X):
            ci = counts[i]
            # Repeat logical ids by counts
            reps = ci.to(dtype_i64)
            phy_row = ar_log.repeat_interleave(reps)
            # Replica ranks are 0..count-1 for each expert, concatenated in order
            rank_blocks = []
            for c in reps.tolist():
                rank_blocks.append(torch.arange(int(c), dtype=dtype_i64, device=device))
            if len(rank_blocks) > 0:
                rank_row = torch.cat(rank_blocks, dim=0)
            else:
                rank_row = torch.empty(0, dtype=dtype_i64, device=device)
            # Store
            phy2log[i, :phy_row.numel()] = phy_row
            rank[i, :rank_row.numel()] = rank_row

        return phy2log, rank, counts


def replicate_experts(
        weight: torch.Tensor,
        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Backward-compatible wrapper for replica allocation.
    """
    return HareReplicator.allocate(weight, num_phy)


def rebalance_experts_hierarchical(
    weight: torch.Tensor,
    num_physical_experts: int,
    num_groups: int,
    num_nodes: int,
    num_gpus: int,
):
    """
    Parameters:
        weight: [num_moe_layers, num_logical_experts]
        num_physical_experts: number of physical experts after replication
        num_groups: number of expert groups
        num_nodes: number of server nodes, where the intra-node network
        (e.g, NVLink) is faster
        num_gpus: number of GPUs, must be a multiple of `num_nodes`

    Returns:
        physical_to_logical_map: [num_moe_layers, num_physical_experts]
        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]
        logical_count: [num_moe_layers, num_logical_experts]
    """
    L, num_logical_experts = weight.shape
    assert num_logical_experts % num_groups == 0
    group_size = num_logical_experts // num_groups
    assert num_groups % num_nodes == 0
    groups_per_node = num_groups // num_nodes
    assert num_gpus % num_nodes == 0
    assert num_physical_experts % num_gpus == 0
    phy_experts_per_gpu = num_physical_experts // num_gpus

    # Step 1: pack groups to nodes with RR-WF on group totals
    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)  # [L, num_groups]
    group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)

    # Compute permutation logical -> meta-logical (layout contiguous within node)
    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1)
                + torch.arange(group_size, dtype=torch.int64, device=weight.device)).flatten(-2)
    mlog2log = _Ops.inverse(log2mlog)

    # Step 2: replicate experts within nodes using Hare largest remainder
    tokens_per_mlog = weight.gather(-1, mlog2log).view(-1, num_logical_experts // num_nodes)
    phy2mlog, phyrank, mlogcnt = replicate_experts(tokens_per_mlog, num_physical_experts // num_nodes)

    # Step 3: pack physical experts to GPUs (still within nodes)
    # Each physical expert inherits the amortized token load of its logical
    tokens_per_phy = (tokens_per_mlog / mlogcnt.clamp_min(1)).gather(-1, phy2mlog)
    pack_index, rank_in_pack = balanced_packing(tokens_per_phy, num_gpus // num_nodes)
    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
    pphy2phy = _Ops.inverse(phy2pphy)

    # Map back to global logical ids
    pphy2mlog = phy2mlog.gather(-1, pphy2phy)  # [L*num_nodes, num_physical_experts/num_nodes]
    pphy2mlog = (pphy2mlog.view(L, num_nodes, -1)
                 + torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=weight.device).view(1, -1, 1)
                 ).flatten(-2)
    pphy2log = mlog2log.gather(-1, pphy2mlog)
    pphyrank = phyrank.gather(-1, pphy2phy).view(L, -1)
    logcnt = mlogcnt.view(L, -1).gather(-1, log2mlog)
    return pphy2log, pphyrank, logcnt


def rebalance_experts(
    weight: torch.Tensor,
    num_replicas: int,
    num_groups: int,
    num_nodes: int,
    num_gpus: int,
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Entry point for expert-parallelism load balancer.

    Parameters:
        weight: [layers, num_logical_experts], the load statistics for all
            logical experts
        num_replicas: number of physical experts, must be a multiple of
            `num_gpus`
        num_groups: number of expert groups
        num_nodes: number of server nodes, where the intra-node network
            (e.g, NVLink) is faster
        num_gpus: number of GPUs, must be a multiple of `num_nodes`

    Returns:
        physical_to_logical_map: [layers, num_replicas], the expert index of
            each replica
        logical_to_physical_map: [layers, num_logical_experts, X], the replica
            indices for each expert
        expert_count: [layers, num_logical_experts], number of physical
            replicas for each logical expert
    """
    num_layers, num_logical_experts = weight.shape
    weight = weight.float().cpu()
    if num_groups % num_nodes == 0:
        # hierarchical policy
        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
            weight, num_replicas, num_groups, num_nodes, num_gpus
        )
    else:
        # global policy
        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
            weight, num_replicas, 1, 1, num_gpus
        )

    num_redundant_experts = num_replicas - num_logical_experts
    maxlogcnt = num_redundant_experts + 1
    log2phy: torch.Tensor = torch.full(
        (num_layers, num_logical_experts, maxlogcnt),
        -1,
        dtype=torch.int64,
        device=logcnt.device,
    )
    # Scatter physical ids into per-logical replica slots
    log2phy.view(num_layers, -1).scatter_(
        -1,
        phy2log * maxlogcnt + phyrank,
        torch.arange(num_replicas, dtype=torch.int64, device=log2phy.device).expand(num_layers, -1),
    )
    return phy2log, log2phy, logcnt


# EVOLVE-BLOCK-END


# This part remains fixed (not evolved)
def run_eplb(weight: torch.Tensor, num_replicas: int, num_groups: int,
             num_nodes: int, num_gpus: int):
    """Run the expert parallelism load balancer"""
    phy2log, log2phy, logcnt = rebalance_experts(
        weight, num_replicas, num_groups, num_nodes, num_gpus
    )
    return phy2log, log2phy, logcnt


__all__ = ["rebalance_experts", "run_eplb"]