<NAME>
adaptive_tail_and_lpt_tiebreak
</NAME>

<DESCRIPTION>
Two targeted improvements to increase balancedness without impacting speed:

1) Make the water-filling hybrid tail adaptive to the observed imbalance. Instead of a fixed 10% of redundant replicas, compute the tail length per row using the ratio of the current peak average to the mean average: T = min(extras, max(1, round(0.1 · num_redundant · clamp(cur_peak/mean_avg − 1, 0, 1)))). This focuses Sainte–Laguë vs D’Hondt micro-choices only when imbalance is high, improving peak smoothing while keeping the bulk fill fast.

2) Add a deterministic tie-break by pack occupancy in the group-to-node LPT packing. When two packs have nearly equal loads, assign to the one with fewer items. This reduces early saturation of packs and typically lowers the final max load with negligible overhead.

These changes should improve the balancedness score while preserving the previous 1.0 speed score.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    for i in range(num_layers):
        pack_loads = [0.0] * num_packs
        pack_counts = [0] * num_packs
        for g in indices[i].tolist():
            # choose pack with capacity and minimal load
            best_p = None
            best_load = None
            for p in range(num_packs):
                if pack_counts[p] >= groups_per_pack:
                    continue
                if best_load is None or pack_loads[p] < best_load:
                    best_load = pack_loads[p]
                    best_p = p
            pack_index[i, g] = best_p
            rank_in_pack[i, g] = pack_counts[best_p]
            pack_counts[best_p] += 1
            pack_loads[best_p] += float(weight[i, g].item())
    return pack_index, rank_in_pack
=======
    for i in range(num_layers):
        pack_loads = [0.0] * num_packs
        pack_counts = [0] * num_packs
        for g in indices[i].tolist():
            # choose pack with capacity and minimal load; tie-break by fewer items
            best_p = None
            best_load = None
            best_cnt = None
            for p in range(num_packs):
                if pack_counts[p] >= groups_per_pack:
                    continue
                if (best_load is None or pack_loads[p] < best_load or
                    (abs(pack_loads[p] - (best_load if best_load is not None else 0.0)) <= 1e-12 and
                     (best_cnt is None or pack_counts[p] < best_cnt))):
                    best_p = p
                    best_load = pack_loads[p]
                    best_cnt = pack_counts[p]
            pack_index[i, g] = best_p
            rank_in_pack[i, g] = pack_counts[best_p]
            pack_counts[best_p] += 1
            pack_loads[best_p] += float(weight[i, g].item())
    return pack_index, rank_in_pack
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
    # Greedy water-filling with peak-aware hybrid tail
    extras = target_total - s
    if extras > 0:
        # Bulk D'Hondt (w/c) fill except for a small adaptive tail
        num_redundant = max(0, target_total - num_log)
        tail = min(extras, max(1, int(round(0.1 * num_redundant))))
        bulk = extras - tail

        # Fast batched fill for bulk (use topk for efficiency)
        while bulk > 0:
            k = min(bulk, num_log)
            scores = w / counts.to(w.dtype)
            topk_idx = torch.topk(scores, k=k, largest=True).indices
            counts[topk_idx] += 1
            bulk -= k

        # Adaptive tail: choose between D'Hondt and Sainte–Laguë at each step,
        # minimizing (new_peak, new_second_peak, receiver_count, -new_avg_chosen)
        for _ in range(tail):
            counts_f = counts.to(w.dtype)
            avg = w / counts_f
            if num_log >= 2:
                vals, idxs = torch.topk(avg, k=2)
                peak1_val = float(vals[0].item())
                peak1_idx = int(idxs[0].item())
                peak2_val = float(vals[1].item())
            else:
                peak1_val = float(avg[0].item())
                peak1_idx = 0
                peak2_val = float("-inf")

            # D'Hondt pick (argmax w/c == argmax avg)
            d_pick = int(torch.argmax(avg).item())
            # Sainte–Laguë pick (argmax w/(2c+1))
            denom = (2 * counts + 1).to(w.dtype)
            s_scores = w / denom
            s_pick = int(torch.argmax(s_scores).item())

            best_choice = None
            best_key = None  # (new_peak, new_second, receiver_cnt, -new_avg)

            for pick in (d_pick, s_pick):
                if best_choice is not None and pick == best_choice:
                    continue
                new_avg_chosen = float((w[pick] / (counts_f[pick] + 1.0)).item())
                # Predict new peak and second-highest after this assignment
                if pick == peak1_idx:
                    new_peak = max(new_avg_chosen, peak2_val)
                    new_second = min(new_avg_chosen, peak2_val)
                else:
                    new_peak = max(peak1_val, new_avg_chosen)
                    new_second = (peak1_val if new_avg_chosen >= peak1_val else max(new_avg_chosen, peak2_val))
                receiver_cnt = int(counts[pick].item())
                key = (new_peak, new_second, receiver_cnt, -new_avg_chosen)
                if best_key is None or key < best_key:
                    best_key = key
                    best_choice = pick

            counts[best_choice] += 1

    return counts
=======
    # Greedy water-filling with peak-aware hybrid tail
    extras = target_total - s
    if extras > 0:
        # Bulk D'Hondt (w/c) fill except for a small adaptive tail sized by current imbalance
        num_redundant = max(0, target_total - num_log)
        counts_f = counts.to(w.dtype)
        avg = w / counts_f
        mean_avg = float((w.sum().item() / max(1, target_total)))
        cur_peak = float(avg.max().item())
        ratio = (cur_peak / max(mean_avg, 1e-12)) if mean_avg > 0 else 0.0
        ratio = max(0.0, min(1.0, ratio - 1.0))  # clamp(cur_peak/mean - 1, 0, 1)
        tail = min(extras, max(1, int(round(0.1 * num_redundant * ratio))))
        bulk = extras - tail

        # Fast batched fill for bulk (use topk for efficiency)
        while bulk > 0:
            k = min(bulk, num_log)
            scores = w / counts.to(w.dtype)
            topk_idx = torch.topk(scores, k=k, largest=True).indices
            counts[topk_idx] += 1
            bulk -= k

        # Adaptive tail: choose between D'Hondt and Sainte–Laguë at each step,
        # minimizing (new_peak, new_second_peak, receiver_count, -new_avg_chosen)
        for _ in range(tail):
            counts_f = counts.to(w.dtype)
            avg = w / counts_f
            if num_log >= 2:
                vals, idxs = torch.topk(avg, k=2)
                peak1_val = float(vals[0].item())
                peak1_idx = int(idxs[0].item())
                peak2_val = float(vals[1].item())
            else:
                peak1_val = float(avg[0].item())
                peak1_idx = 0
                peak2_val = float("-inf")

            # D'Hondt pick (argmax w/c == argmax avg)
            d_pick = int(torch.argmax(avg).item())
            # Sainte–Laguë pick (argmax w/(2c+1))
            denom = (2 * counts + 1).to(w.dtype)
            s_scores = w / denom
            s_pick = int(torch.argmax(s_scores).item())

            best_choice = None
            best_key = None  # (new_peak, new_second, receiver_cnt, -new_avg)

            for pick in (d_pick, s_pick):
                if best_choice is not None and pick == best_choice:
                    continue
                new_avg_chosen = float((w[pick] / (counts_f[pick] + 1.0)).item())
                # Predict new peak and second-highest after this assignment
                if pick == peak1_idx:
                    new_peak = max(new_avg_chosen, peak2_val)
                    new_second = min(new_avg_chosen, peak2_val)
                else:
                    new_peak = max(peak1_val, new_avg_chosen)
                    new_second = (peak1_val if new_avg_chosen >= peak1_val else max(new_avg_chosen, peak2_val))
                receiver_cnt = int(counts[pick].item())
                key = (new_peak, new_second, receiver_cnt, -new_avg_chosen)
                if best_key is None or key < best_key:
                    best_key = key
                    best_choice = pick

            counts[best_choice] += 1

    return counts
>>>>>>> REPLACE

</DIFF>