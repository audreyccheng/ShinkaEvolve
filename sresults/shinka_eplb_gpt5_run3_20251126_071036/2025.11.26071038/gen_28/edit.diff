--- a/original.py
+++ b/original.py
@@ -1,387 +1,353 @@
 # EVOLVE-BLOCK-START
 """
 Expert parallelism load balancer (EPLB) for vLLM.
 
 This module implements the core rearrangement algorithm.
 
 The rearrangement algorithm is adapted from
 [DeepSeek EPLB](https://github.com/deepseek-ai/eplb).
 
 Please find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example
 on how the EPLB algorithm works.
 """
 
 import torch
 
 
+class PermOps:
+    @staticmethod
+    def inverse(perm: torch.Tensor) -> torch.Tensor:
+        """
+        Compute inverse permutation row-wise.
+
+        perm: [L, N], a permutation for each row.
+        returns inv_perm where inv_perm[g, perm[g, i]] = i
+        """
+        L, N = perm.shape
+        inv = torch.empty_like(perm)
+        inv.scatter_(1, perm, torch.arange(N, dtype=torch.int64, device=perm.device).expand(L, -1))
+        return inv
+
+
+def _stripe_pack_single_row(w: torch.Tensor, num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
+    """
+    Zig-zag stripe packing for a single row:
+      - Sort indices by descending weight.
+      - Partition into capacity layers of size num_packs.
+      - Assign layer k in ascending pack order if k even, descending if k odd.
+    Returns pack_index_row, rank_in_pack_row (both shape [N], int64).
+    """
+    N = w.numel()
+    assert N % num_packs == 0
+    capacity = N // num_packs
+    device = w.device
+    dtype_i64 = torch.int64
+
+    if capacity == 1:
+        # identity mapping: item i -> pack i
+        pack_index = torch.arange(N, dtype=dtype_i64, device=device)
+        rank_in_pack = torch.zeros(N, dtype=dtype_i64, device=device)
+        return pack_index, rank_in_pack
+
+    order = torch.argsort(w, descending=True)
+    pack_index = torch.empty(N, dtype=dtype_i64, device=device)
+    rank_in_pack = torch.empty(N, dtype=dtype_i64, device=device)
+
+    for k in range(capacity):
+        start = k * num_packs
+        layer = order[start:start + num_packs]
+        if k % 2 == 1:
+            layer = torch.flip(layer, dims=[0])  # reverse for zig-zag
+        # assign: j-th element in layer -> pack j, rank k
+        # layer[j] is original index of item
+        # packs go 0..num_packs-1
+        pack_ids = torch.arange(num_packs, dtype=dtype_i64, device=device)
+        pack_index[layer] = pack_ids
+        rank_in_pack[layer] = k
+
+    return pack_index, rank_in_pack
+
+
 def balanced_packing(weight: torch.Tensor,
                      num_packs: int,
                      refine_steps: int = 1) -> tuple[torch.Tensor, torch.Tensor]:
     """
-    Pack n weighted objects to m packs, such that each bin contains exactly
-    n/m objects and the weights of all packs are as balanced as possible.
+    Balanced packing using zig-zag stripe assignment with exact capacity per pack.
 
     Parameters:
         weight: [X, n], the weight of each item
         num_packs: number of packs
-        refine_steps: small bounded number of refinement swaps per layer
+        refine_steps: kept for API compatibility (ignored)
 
     Returns:
         pack_index: [X, n], the pack index of each item
-        rank_in_pack: [X, n], the rank of the item in the pack
-    """
-    num_layers, num_groups = weight.shape
-    assert num_groups % num_packs == 0
-    groups_per_pack = num_groups // num_packs
-
-    if groups_per_pack == 1:
-        pack_index = torch.arange(weight.size(-1),
-                                  dtype=torch.int64,
-                                  device=weight.device).expand(weight.shape)
-        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)
-        return pack_index, rank_in_pack
-
-    indices = weight.float().sort(-1, descending=True).indices.cpu()
-    pack_index = torch.full_like(weight,
-                                 fill_value=-1,
-                                 dtype=torch.int64,
-                                 device="cpu")
-    rank_in_pack = torch.full_like(pack_index, fill_value=-1)
-    for i in range(num_layers):
-        pack_weights = [0] * num_packs
-        pack_items = [0] * num_packs
-        for group in indices[i]:
-            pack = min(
-                (i
-                 for i in range(num_packs) if pack_items[i] < groups_per_pack),
-                key=pack_weights.__getitem__,
-            )
-            assert pack_items[pack] < groups_per_pack
-            pack_index[i, group] = pack
-            rank_in_pack[i, group] = pack_items[pack]
-            pack_weights[pack] += weight[i, group]
-            pack_items[pack] += 1
-
-    # Bounded multi-swap refinement per layer to reduce max imbalance
-    if groups_per_pack > 1 and refine_steps > 0:
-        max_swaps = int(refine_steps)  # keep small to preserve speed
-        for i in range(num_layers):
-            for _ in range(max_swaps):
-                packs = pack_index[i]  # [num_groups], CPU
-                w = weight[i]  # CPU
-                # Compute pack loads
-                pack_w = torch.zeros(num_packs, dtype=w.dtype)
-                pack_w.scatter_add_(0, packs, w)
-                h = int(torch.argmax(pack_w))
-                l = int(torch.argmin(pack_w))
-                delta = float(pack_w[h] - pack_w[l])
-                if delta <= 1e-9:
-                    break
-
-                heavy_idx = torch.nonzero(packs == h, as_tuple=False).squeeze(1)
-                light_idx = torch.nonzero(packs == l, as_tuple=False).squeeze(1)
-                if heavy_idx.numel() == 0 or light_idx.numel() == 0:
-                    break
-
-                hw = w[heavy_idx]
-                lw = w[light_idx]
-                lw_sorted, lw_perm = torch.sort(lw)  # ascending
-                if lw_sorted.numel() == 0 or hw.numel() == 0:
-                    break
-
-                # For each heavy item, find light item closest to target = hw - delta/2
-                target = hw - (delta / 2.0)
-                pos = torch.searchsorted(lw_sorted, target)
-                pos = torch.clamp(pos, 0, lw_sorted.numel() - 1)
-                # Consider neighbors pos and pos-1 for best approximation
-                cand_pos = torch.stack([pos, torch.clamp(pos - 1, 0, lw_sorted.numel() - 1)], dim=1)
-                cand_lw = lw_sorted[cand_pos]  # [H, 2]
-                resid = (delta - 2.0 * (hw.unsqueeze(1) - cand_lw)).abs()
-                best_flat = int(torch.argmin(resid).item())
-                best_h_index = best_flat // 2
-                best_option = best_flat % 2
-                j_sorted_idx = int(cand_pos[best_h_index, best_option].item())
-
-                wi = float(hw[best_h_index].item())
-                wj = float(lw_sorted[j_sorted_idx].item())
-                new_delta = abs(delta - 2.0 * (wi - wj))
-                # Apply swap only if it strictly improves imbalance
-                if new_delta < delta - 1e-9:
-                    hi = heavy_idx[best_h_index]
-                    lj = light_idx[lw_perm[j_sorted_idx]]
-                    pack_index[i, hi] = l
-                    pack_index[i, lj] = h
-                    # Reassign ranks within affected packs to keep 0..groups_per_pack-1
-                    for p in (h, l):
-                        mask = pack_index[i] == p
-                        idx = torch.nonzero(mask, as_tuple=False).squeeze(1)
-                        if idx.numel() == 0:
-                            continue
-                        # Stable by previous rank order
-                        prev_rank = rank_in_pack[i, idx]
-                        order = torch.argsort(prev_rank)
-                        new_ranks = torch.arange(order.numel(), dtype=torch.int64)
-                        rank_in_pack[i, idx[order]] = new_ranks
-                    # continue to next potential swap
-                    continue
-                else:
-                    break
+        rank_in_pack: [X, n], the rank of the item in the pack (0..n/m-1)
+    """
+    L, N = weight.shape
+    assert N % num_packs == 0, "Number of items must be divisible by num_packs."
+
+    # operate on CPU or current device directly (weight already moved to CPU by caller)
+    pack_index = torch.empty((L, N), dtype=torch.int64, device=weight.device)
+    rank_in_pack = torch.empty((L, N), dtype=torch.int64, device=weight.device)
+
+    # Stripe pack each row independently
+    for li in range(L):
+        pidx, rnk = _stripe_pack_single_row(weight[li], num_packs)
+        pack_index[li] = pidx
+        rank_in_pack[li] = rnk
 
     return pack_index, rank_in_pack
+
+
+def _waterfill_counts_single_row(w: torch.Tensor, total: int) -> torch.Tensor:
+    """
+    Water-filling replica allocation for a single row.
+    Given weights w: [M] and total replicas 'total' (>= M), find integer counts c >= 1
+    that minimize the maximum average load max_i w_i / c_i via:
+      - Solve continuous r_i = max(1, w_i / T) with sum r_i = total by binary search on T.
+      - Floor r_i to integers and distribute remaining replicas by largest marginal benefit
+        Δ_i = w_i/floor_i − w_i/(floor_i + 1).
+
+    Returns c: [M] int64 with sum(c) == total.
+    """
+    M = w.numel()
+    assert total >= M, "Total replicas must be at least number of logical experts."
+    device = w.device
+    dtype_f = w.dtype
+    dtype_i64 = torch.int64
+
+    if total == M:
+        return torch.ones(M, dtype=dtype_i64, device=device)
+
+    max_w = torch.max(w).item() if M > 0 else 0.0
+    # Binary search for T in (0, max_w], ensure S(lo) >= total and S(hi) <= total
+    lo = 0.0
+    hi = max(max_w, 1.0)
+    # Iterate fixed steps for robustness
+    for _ in range(50):
+        mid = 0.5 * (lo + hi)
+        mid_safe = max(mid, 1e-12)
+        r = torch.maximum(torch.ones(M, dtype=dtype_f, device=device), w / mid_safe)
+        S = float(r.sum().item())
+        if S > total:
+            lo = mid
+        else:
+            hi = mid
+    T = hi
+    rstar = torch.maximum(torch.ones(M, dtype=dtype_f, device=device), w / max(T, 1e-12))
+    floor_c = torch.floor(rstar).to(dtype_i64)
+    # Ensure at least 1
+    floor_c = torch.clamp(floor_c, min=1)
+
+    used = int(floor_c.sum().item())
+    remain = total - used
+    if remain > 0:
+        # Marginal peak reduction by incrementing count by +1
+        floor_f = floor_c.to(dtype_f)
+        benefit = w / floor_f - w / (floor_f + 1.0)
+        # In rare cases where benefit has ties or zeros, topk is deterministic enough
+        k = min(remain, M)
+        # Use topk selection and then add 1 to those indices; if remain > M we do multiple passes
+        # Handle remain possibly > M by repeating full increments
+        full_rounds, last = divmod(remain, M)
+        if full_rounds > 0:
+            floor_c += full_rounds
+        if last > 0:
+            top_vals, top_idx = torch.topk(benefit, k=last, largest=True)
+            floor_c[top_idx] += 1
+    # No case for remain < 0 should occur due to floor <= rstar and sum(rstar)=total
+    return floor_c
 
 
 def replicate_experts(
         weight: torch.Tensor,
         num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Replicate `num_log` experts to `num_phy` replicas, such that the maximum
-    load of all replicas is minimized.
+    load of all replicas is minimized (water-filling + optimal rounding).
 
     Parameters:
         weight: [X, num_log]
         num_phy: total number of experts after replication
 
     Returns:
         phy2log: [X, num_phy], logical expert id of each physical expert
         rank: [X, num_phy], the replica rank
         logcnt: [X, num_log], number of replicas for each logical expert
     """
     n, num_log = weight.shape
-    num_redundant = num_phy - num_log
-    assert num_redundant >= 0
+    num_extra = num_phy - num_log
+    assert num_extra >= 0
     device = weight.device
-
-    # Initialize base mapping (one replica per logical expert)
-    phy2log = torch.arange(num_phy, dtype=torch.int64, device=device).repeat(n, 1)
-    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)
-    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)
-
-    if num_redundant == 0:
+    dtype_i64 = torch.int64
+
+    # Base arrays
+    phy2log = torch.empty((n, num_phy), dtype=dtype_i64, device=device)
+    rank = torch.empty((n, num_phy), dtype=dtype_i64, device=device)
+    logcnt = torch.empty((n, num_log), dtype=dtype_i64, device=device)
+
+    if num_extra == 0:
+        # One-to-one
+        base = torch.arange(num_log, dtype=dtype_i64, device=device).unsqueeze(0).expand(n, -1)
+        phy2log[:, :num_log] = base
+        rank[:, :num_log] = 0
+        logcnt.fill_(1)
         return phy2log, rank, logcnt
 
-    arangen = torch.arange(n, dtype=torch.int64, device=device)
-
-    # Hybrid allocation: D'Hondt for bulk, Sainte-Laguë for the last ~10% (at least 1)
-    tail = max(1, (num_redundant + 9) // 10)
-    bulk = num_redundant - tail
-
-    col = num_log
-    # Bulk phase (D'Hondt): benefit = weight / r
-    for _ in range(bulk):
-        benefit = weight / logcnt
-        best = benefit.max(dim=-1).indices
-        phy2log[:, col] = best
-        rank[:, col] = logcnt[arangen, best]
-        logcnt[arangen, best] += 1
-        col += 1
-
-    # Tail phase (Sainte-Laguë): benefit = weight / (2r - 1)
-    # Note: r starts at 1, so (2r - 1) >= 1
-    if tail > 0:
-        for _ in range(tail):
-            denom = (2 * logcnt - 1).to(weight.dtype)
-            benefit = weight / denom
-            best = benefit.max(dim=-1).indices
-            phy2log[:, col] = best
-            rank[:, col] = logcnt[arangen, best]
-            logcnt[arangen, best] += 1
-            col += 1
-
-    # One-step replication fix-up per row:
-    # Move one replica from the heaviest-per-replica expert to the lightest if it strictly
-    # reduces the global maximum average load.
-    if num_log > 1 and num_redundant > 0:
-        avg = weight / logcnt.to(weight.dtype)  # [n, num_log]
-        # top-2 to account for ties at the maximum
-        top2_vals, top2_idx = torch.topk(avg, k=2, dim=-1)
-        cur_max = top2_vals[:, 0]
-        second = top2_vals[:, 1]
-        donor = top2_idx[:, 0]
-        receiver = torch.argmin(avg, dim=-1)
-
-        cd = logcnt[arangen, donor]  # donor counts
-        cr = logcnt[arangen, receiver]  # receiver counts
-        # Valid only if donor != receiver and donor has at least 2 replicas
-        valid = (donor != receiver) & (cd > 1)
-
-        # Compute new peak after moving 1 replica
-        new_d = weight[arangen, donor] / (cd.to(weight.dtype) - 1)
-        new_r = weight[arangen, receiver] / (cr.to(weight.dtype) + 1)
-        new_peak = torch.maximum(second, torch.maximum(new_d, new_r))
-        improve = valid & (new_peak + 1e-12 < cur_max)
-
-        rows = torch.nonzero(improve, as_tuple=False).squeeze(1)
-        if rows.numel() > 0:
-            for ri in rows.tolist():
-                d = int(donor[ri].item())
-                r = int(receiver[ri].item())
-                # Choose a physical column corresponding to donor's highest rank (prefer the last replica)
-                donor_cols = torch.nonzero(phy2log[ri] == d, as_tuple=False).squeeze(1)
-                if donor_cols.numel() == 0:
-                    continue
-                # Among donor cols, pick the one with max rank
-                maxr_idx = torch.argmax(rank[ri, donor_cols]).item()
-                col_idx = donor_cols[maxr_idx]
-
-                # Assign this physical replica to receiver with new rank equal to current receiver count
-                new_rank = int(logcnt[ri, r].item())
-                phy2log[ri, col_idx] = r
-                rank[ri, col_idx] = new_rank
-
-                # Update counts
-                logcnt[ri, d] -= 1
-                logcnt[ri, r] += 1
+    # Compute counts per row via water-filling
+    for ri in range(n):
+        wrow = weight[ri]
+        cnt = _waterfill_counts_single_row(wrow, num_phy)
+        logcnt[ri] = cnt
+
+        # Build mapping columns deterministically: enumerate experts by id and emit cnt copies
+        cols = []
+        ranks = []
+        for eid in range(num_log):
+            c = int(cnt[eid].item())
+            if c <= 0:
+                continue
+            cols.extend([eid] * c)
+            ranks.extend(list(range(c)))
+        cols_t = torch.tensor(cols, dtype=dtype_i64, device=device)
+        ranks_t = torch.tensor(ranks, dtype=dtype_i64, device=device)
+        # Ensure exact length num_phy
+        assert cols_t.numel() == num_phy
+        phy2log[ri] = cols_t
+        rank[ri] = ranks_t
 
     return phy2log, rank, logcnt
 
 
 def rebalance_experts_hierarchical(
     weight: torch.Tensor,
     num_physical_experts: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ):
     """
     Parameters:
         weight: [num_moe_layers, num_logical_experts]
         num_physical_experts: number of physical experts after replication
         num_groups: number of expert groups
         num_nodes: number of server nodes, where the intra-node network
         (e.g, NVLink) is faster
         num_gpus: number of GPUs, must be a multiple of `num_nodes`
 
     Returns:
         physical_to_logical_map: [num_moe_layers, num_physical_experts]
         logical_to_physical_map: [num_moe_layers, num_logical_experts, X]
         logical_count: [num_moe_layers, num_logical_experts]
     """
-    num_layers, num_logical_experts = weight.shape
+    L, num_logical_experts = weight.shape
     assert num_logical_experts % num_groups == 0
     group_size = num_logical_experts // num_groups
     assert num_groups % num_nodes == 0
     groups_per_node = num_groups // num_nodes
     assert num_gpus % num_nodes == 0
     assert num_physical_experts % num_gpus == 0
     phy_experts_per_gpu = num_physical_experts // num_gpus
 
-    def inverse(perm: torch.Tensor) -> torch.Tensor:
-        inv = torch.empty_like(perm)
-        inv.scatter_(
-            1,
-            perm,
-            torch.arange(perm.size(1), dtype=torch.int64,
-                         device=perm.device).expand(perm.shape),
-        )
-        return inv
-
-    # Step 1: pack groups to nodes
-    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)
-    group_pack_index, group_rank_in_pack = balanced_packing(
-        tokens_per_group, num_nodes)
-    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *
-                 group_size).unsqueeze(-1) +
-                torch.arange(group_size,
-                             dtype=torch.int64,
-                             device=group_pack_index.device)).flatten(-2)
-    mlog2log = inverse(log2mlog)
-
-    # Step 2: construct redundant experts within nodes
-    # [num_layers * num_nodes, num_logical_experts // num_nodes]
-    tokens_per_mlog = weight.gather(-1, mlog2log).view(
-        -1, num_logical_experts // num_nodes)
-    phy2mlog, phyrank, mlogcnt = replicate_experts(
-        tokens_per_mlog, num_physical_experts // num_nodes)
-
-    # Step 3: pack physical_experts to GPUs
-    # [num_layers * num_nodes, num_physical_experts // num_nodes]
+    # Step 1: pack groups to nodes with stripe packing on total tokens per group
+    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)  # [L, num_groups]
+    group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes, refine_steps=0)
+
+    # Compute permutation logical -> meta-logical (contiguous within node)
+    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +
+                torch.arange(group_size, dtype=torch.int64, device=weight.device)).flatten(-2)
+    mlog2log = PermOps.inverse(log2mlog)
+
+    # Step 2: replicate experts within nodes based on local loads (water-filling)
+    tokens_per_mlog = weight.gather(-1, mlog2log).view(-1, num_logical_experts // num_nodes)
+    phy2mlog, phyrank, mlogcnt = replicate_experts(tokens_per_mlog, num_physical_experts // num_nodes)
+
+    # Step 3: pack physical experts to GPUs (within nodes) using stripe packing
+    # Build per-replica average load tensor
     tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
-    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,
-                                                num_gpus // num_nodes,
-                                                refine_steps=2)
+    pack_index, rank_in_pack = balanced_packing(tokens_per_phy, num_gpus // num_nodes, refine_steps=0)
     phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
-    pphy2phy = inverse(phy2pphy)
-
-    pphy2mlog = phy2mlog.gather(
-        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]
-    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(
-        0,
-        num_logical_experts,
-        num_logical_experts // num_nodes,
-        device=group_pack_index.device,
-    ).view(1, -1, 1)).flatten(-2)
+    pphy2phy = PermOps.inverse(phy2pphy)
+
+    # Map back to global logical ids
+    pphy2mlog = phy2mlog.gather(-1, pphy2phy)  # [L*num_nodes, num_physical_experts/num_nodes]
+    pphy2mlog = (pphy2mlog.view(L, num_nodes, -1) +
+                 torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=weight.device).view(1, -1, 1)
+                 ).flatten(-2)
     pphy2log = mlog2log.gather(-1, pphy2mlog)
-    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)
-    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)
+    pphyrank = phyrank.gather(-1, pphy2phy).view(L, -1)
+    logcnt = mlogcnt.view(L, -1).gather(-1, log2mlog)
     return pphy2log, pphyrank, logcnt
 
 
 def rebalance_experts(
     weight: torch.Tensor,
     num_replicas: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Entry point for expert-parallelism load balancer.
 
     Parameters:
         weight: [layers, num_logical_experts], the load statistics for all
             logical experts
         num_replicas: number of physical experts, must be a multiple of
             `num_gpus`
         num_groups: number of expert groups
         num_nodes: number of server nodes, where the intra-node network
             (e.g, NVLink) is faster
         num_gpus: number of GPUs, must be a multiple of `num_nodes`
 
     Returns:
         physical_to_logical_map: [layers, num_replicas], the expert index of
             each replica
         logical_to_physical_map: [layers, num_logical_experts, X], the replica
             indices for each expert
         expert_count: [layers, num_logical_experts], number of physical
             replicas for each logical expert
     """
     num_layers, num_logical_experts = weight.shape
     weight = weight.float().cpu()
     if num_groups % num_nodes == 0:
         # use hierarchical load-balance policy
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, num_groups, num_nodes, num_gpus)
     else:
-        # use global load-balance policy
+        # use global load-balance policy (treat as a single group across nodes)
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
             weight, num_replicas, 1, 1, num_gpus)
+
     num_redundant_experts = num_replicas - num_logical_experts
     maxlogcnt = num_redundant_experts + 1
     log2phy: torch.Tensor = torch.full(
         (num_layers, num_logical_experts, maxlogcnt),
         -1,
         dtype=torch.int64,
         device=logcnt.device,
     )
+    # Scatter physical ids into per-logical replica slots
     log2phy.view(num_layers, -1).scatter_(
         -1,
         phy2log * maxlogcnt + phyrank,
         torch.arange(num_replicas, dtype=torch.int64,
                      device=log2phy.device).expand(num_layers, -1),
     )
     return phy2log, log2phy, logcnt
 
 
 # EVOLVE-BLOCK-END
 
 
 # This part remains fixed (not evolved)
 def run_eplb(weight: torch.Tensor, num_replicas: int, num_groups: int,
              num_nodes: int, num_gpus: int):
     """Run the expert parallelism load balancer"""
     phy2log, log2phy, logcnt = rebalance_experts(
         weight, num_replicas, num_groups, num_nodes, num_gpus
     )
     return phy2log, log2phy, logcnt
 
 
 __all__ = ["rebalance_experts", "run_eplb"]