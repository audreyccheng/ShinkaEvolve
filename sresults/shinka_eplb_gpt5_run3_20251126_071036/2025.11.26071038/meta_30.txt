# INDIVIDUAL PROGRAM SUMMARIES

The following are summaries of individual programs evaluated since the last meta update:

**Program Name: Hierarchical Expert Load Balancer for vLLM**
- **Implementation**: Implements a greedy, hierarchical rearrangement using PyTorch: groups are packed to nodes, experts replicated within nodes, and physical experts packed to GPUs via a balanced_packing heuristic, with permutations managed by scatter/gather and an inverse mapping. Computation is performed primarily on CPU (weight.float().cpu()) with per-layer greedy choices and fixed replica counts used to build phy→log, log→phy, and expert count maps.
- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.0 over 5 workloads).
- **Feedback**: The approach is very fast due to simple greedy packing and CPU-based tensor ops, but yields only moderate load balance. Hierarchical constraints and per-layer greedy decisions (e.g., weight/logcnt replication and group-first packing) likely limit balancedness compared to more global or optimal assignments.
**Program Identifier:** Generation 0 - Patch Name initial_program - Correct Program: True

**Program Name: Hierarchical EPLB with diversity-aware packing**
- **Implementation**: Implements a three-stage hierarchical strategy: groups→nodes via balanced packing, per-node “water-filling” replication, and diversity-aware greedy assignment of replicas to GPUs that minimizes label collocation while equalizing per-pack load; permutation inverses and scatter build physical↔logical maps. Trivial/fast paths (e.g., one item per pack, no-duplicate labels) and CPU-friendly loops via list conversions keep overhead low.
- **Performance**: Combined score 0.66 (balancedness 0.3111, speed 1.00 across 5 workloads); all validations pass.
- **Feedback**: The diversity-aware step reduces hotspotting of identical labels, but the equal-items-per-pack constraint and greedy choices yield only moderate load balance. Efficient early exits and lightweight loops drive the top speed score.
**Program Identifier:** Generation 1 - Patch Name diversity_aware_gpu_packing - Correct Program: True

**Program Name: Hierarchical Expert Parallelism Load Balancer**
- **Implementation**: Uses greedy sort-based bin packing per layer with a small local 2-opt-style refinement between heaviest/lightest packs. Replication is selected via argmax(weight/logcnt) within nodes, then replicas are packed to GPUs; final mappings are built via gather/scatter and inverse permutations, operating on CPU tensors for simplicity.
- **Performance**: Combined score 0.65 (balancedness 0.306805, speed 1.00 across 5 workloads); all validations passed.
- **Feedback**: Excellent speed shows the greedy + limited refinement is lightweight, but moderate balancedness indicates room for stronger global balancing. More refinement iterations or smarter swap selection could improve balance; hierarchical policy is used when groups align with nodes, otherwise a global fallback is applied.
**Program Identifier:** Generation 2 - Patch Name local_swap_refine_in_packing - Correct Program: True

**Program Name: Hierarchical Serpentine Expert Load Balancer**
- **Implementation**: Uses vectorized serpentine (snake) packing per layer to evenly assign sorted experts to packs, and a d'Hondt-like greedy replicator that allocates extra replicas by weight/count. A hierarchical pipeline packs groups to nodes, replicates within nodes, then packs physical experts to GPUs using amortized loads, implemented with PyTorch scatter/gather and permutation inverses (with a fast path when groups_per_pack==1).
- **Performance**: Combined score 0.63 (balancedness 0.254285, speed 1.00; 5 workloads).
- **Feedback**: Fully vectorized design achieves maximal speed but modest balancedness, suggesting the greedy replication plus snake packing underperform under skewed loads. Correctness is solid (passes all tests); balancedness may improve with local swap/refinement or more global optimization.
**Program Identifier:** Generation 3 - Patch Name eplb_snake_pipeline - Correct Program: True

**Program Name: Hierarchical EPLB with Diversity-Aware Packing**
- **Implementation**: Uses a hierarchical three-stage pipeline: (1) greedy group-to-node packing with up to 4 heaviest/lightest swaps, (2) per-node expert replication via an incremental ratio (weight/count) greedy update, and (3) GPU placement with diversity-aware packing that prioritizes label spread then projected load. Mapping inverses and placements are built via scatter/gather for efficiency, with selective CPU sorting and minimal refinement to keep runtime low.
- **Performance**: Combined score 0.65 (balancedness 0.3068, speed 1.00 over 5 workloads).
- **Feedback**: The implementation is correct and passes all validation tests. It prioritizes speed—greedy choices and limited local refinement yield maximal runtime performance but only moderate balance; diversity-aware packing helps reduce hotspotting, though the capped refinements likely limit balancedness gains.
**Program Identifier:** Generation 4 - Patch Name diverse_pack_projload_and_incremental_ratio - Correct Program: True

**Program Name: Hierarchical EPLB with diversity-aware packing**
- **Implementation**: Uses a hierarchical strategy: groups are packed to nodes via balanced_packing, logical experts are replicated with Hamilton’s method (largest remainder) in replicate_experts, and physical experts are assigned to GPUs using a diversity-aware greedy packer that spreads identical labels to reduce hotspotting. Mappings are built with vectorized torch ops (scatter/inverse), and the top-level casts weights to CPU for consistent, fast execution.
- **Performance**: Combined score 0.64 (balancedness 0.2897, speed 1.0 across 5 workloads); all validation tests pass.
- **Feedback**: The approach is very fast due to simple greedy policies and vectorized construction of mappings, but balancedness is modest, indicating the greedy packing and proportional replication may under-serve heavy experts or miss global optimality. Consider stronger global balancing heuristics or improved tie-breaking beyond label diversity to raise balancedness without sacrificing speed.
**Program Identifier:** Generation 5 - Patch Name proportional_quota_replication - Correct Program: True

**Program Name: Hierarchical EPLB with greedy packing and swap refinement**
- **Implementation**: Implements a hierarchical load balancer: packs logical expert groups to nodes, greedily replicates experts within nodes, then balances physical experts across GPUs. The core balanced_packing uses per-layer greedy assignment with fixed per-pack item counts and a lightweight single-swap refinement; replicate_experts greedily allocates by current load (weight/logcnt), and mappings are built via scatter/gather operations.
- **Performance**: Combined score 0.66 (balancedness 0.3111, speed 1.0 over 5 workloads).
- **Feedback**: The approach is very fast due to simple greedy passes and minimal refinement, but achieves only moderate balance. Single-swap local improvement and equal-cardinality packing likely cap balancedness; nevertheless, the solution is correct and passes all validation tests.
**Program Identifier:** Generation 6 - Patch Name single_swap_refinement - Correct Program: True

**Program Name: Hierarchical EPLB with Greedy Packing and D’Hondt Replication**
- **Implementation**: Uses a capacity-constrained greedy packer (sort-by-weight, place into lightest pack) with a single heaviest↔lightest swap refinement and deterministic in-pack ranks; replica allocation is a vectorized D’Hondt-style water-filling. Hierarchical mapping packs groups to nodes and GPUs via permutation ops (including a scatter-based inverse), with replication done per node and final mappings produced by gather/scatter.
- **Performance**: Combined score 0.66 (balancedness 0.3111, speed 1.00 across 5 workloads).
- **Feedback**: The lightweight greedy packing favors speed but leaves noticeable imbalance; increasing refinement steps or applying smarter swap/refinement strategies could raise balancedness with modest cost. Vectorized allocation and permutation-based layout keep the approach simple and fast, and all validation tests pass.
**Program Identifier:** Generation 7 - Patch Name moe_eplb_modular_refine - Correct Program: True

**Program Name: Hierarchical EPLB with Greedy Packing and Swap Refinement**
- **Implementation**: Uses per-layer greedy balanced_packing (descending sort with capacity-constrained assignment) plus up to two bounded heavy-light swaps to reduce imbalance while reassigning ranks. Applies a hierarchical strategy: pack groups to nodes, greedily replicate experts via weight/logcnt, then pack physical experts to GPUs; mappings are constructed via gather/scatter and inverse permutations.
- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.00 across 5 workloads); passes all validation tests.
- **Feedback**: Excellent speed from lightweight greedy steps and limited refinement, but modest balancedness suggests under-optimization of load distribution. More aggressive or global refinement could improve balance at the expense of speed.
**Program Identifier:** Generation 8 - Patch Name multi_swap_refinement_balanced_packing - Correct Program: True

**Program Name: RR-WF Packing with Hare Replication**
- **Implementation**: Implements Round-Robin Waterfilling packing to assign items to packs in capacity rounds, with a lightweight post-swap refinement; uses Hare (Hamilton) largest-remainder to allocate replicas proportionally, then a hierarchical pipeline (group→node packing, intra-node replication, GPU packing) built with vectorized torch gathers/scatters and an inverse-permutation utility. The entry rebalance forces weight to float().cpu() and selects hierarchical when groups % nodes == 0, otherwise a global fallback.
- **Performance**: Combined score 0.64 (balancedness 0.289706, speed 1.000000 over 5 workloads); program is correct and passes all validation tests.
- **Feedback**: Excellent speed stems from simple per-round vectorized ops, stable sorting, and minimal refinement; per-row loops (remainders/topk, final swap) are lightweight. Balance is improved over naive strategies but remains moderate, suggesting headroom beyond the single-pass RR-WF plus optional one-swap refinement.
**Program Identifier:** Generation 9 - Patch Name rr_waterfill_hare - Correct Program: True

**Program Name: Hierarchical Water-Fill + Diverse Packing EPLB**
- **Implementation**: Implements a three-stage hierarchical balancer: capacity-aware LPT group-to-node packing, intra-node water-filling replica allocation via binary search with greedy extras, and GPU placement using a diversity-aware heap that minimizes (label repeats, load, fill). It builds contiguous phy2log/rank with repeat_interleave, inverts mappings where needed, and scatters logical→physical indices; falls back to a single-node policy when groups aren’t node-aligned.
- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000) across 5 workloads; all validations passed.
- **Feedback**: The approach is very fast due to lightweight CPU sorts, vectorized gathers/scatters, and simple greedy heuristics, but achieves only moderate balance. Balancedness likely suffers from the greedy capacity-constrained packers and per-node isolation; tuning the packing objective or more global optimization could improve load balance.
**Program Identifier:** Generation 10 - Patch Name waterfill_diverse_heap - Correct Program: True

**Program Name: Hierarchical Diverse Expert Load Balancer**
- **Implementation**: Implements hierarchical packing of logical experts to nodes and GPUs: greedy capacity-constrained packing with K=2 heavy–light swaps, diversity-aware packing to spread identical labels, and a hybrid D’Hondt→Sainte-Laguë replication strategy. Uses vectorized gathers/scatters and invert-permutation helpers to build physical↔logical maps and replica ranks.
- **Performance**: Combined score 0.65 (balancedness 0.304918, speed 1.00 over 5 workloads).
- **Feedback**: All tests passed; packing and replication are fast, yielding top speed but only moderate balance. The limited K=2 refinement and local (per-node) swaps likely cap balancedness despite diversity-aware placement.
**Program Identifier:** Generation 11 - Patch Name k2_swap_tail_hybrid - Correct Program: True

**Program Name: Hierarchical Water-Filling EPLB**
- **Implementation**: Implements a hierarchical policy: packs expert groups to nodes with capacity-aware LPT, performs per-node water-filling replication with a one-step donor→receiver fix, then assigns to GPUs via a diversity-aware heap plus a k=2 swap refinement. Builds physical→logical and logical→physical maps via scatter/gather with contiguous replica ranks.
- **Performance**: Combined score 0.66 (balancedness 0.311, speed 1.00 over 5 workloads); passes all validation tests.
- **Feedback**: Runtime is excellent, indicating low overhead from the greedy/heap steps and CPU tensor ops. Balancedness is modest, suggesting the single-step fix and limited 2-opt swap cap equalization, leaving room for deeper refinement if needed.
**Program Identifier:** Generation 12 - Patch Name k2_swap_tail_fixup - Correct Program: True

**Program Name: Hierarchical EPLB with greedy packing**
- **Implementation**: Implements a hierarchical expert-parallel balancer: groups are greedily packed to nodes with a bounded multi-swap refinement (max_swaps=2), replicas are allocated per layer via a D’Hondt-like scheme with a one-step donor→receiver fix-up, and physical experts are then packed to GPUs; mappings use gather/scatter with inverse permutations for determinism. Packing operates on CPU tensors with stable rank reassignment to maintain per-pack ordering.
- **Performance**: Combined score 0.66 (balancedness 0.311, speed 1.000 over 5 workloads).
- **Feedback**: Excellent speed stems from simple greedy heuristics and tightly bounded refinement, but balancedness is moderate, likely limited by the two-swap bound and single-step replica fix-up. Hierarchical placement benefits topology-aware locality when groups are node-divisible; otherwise it falls back to a global policy.
**Program Identifier:** Generation 13 - Patch Name replication_fixup_one_step - Correct Program: True

**Program Name: Hierarchical EPLB with Greedy Packing and Replication**
- **Implementation**: Uses per-layer sorted greedy balanced_packing with exact items-per-pack and a bounded (max 2) multi-swap refinement to reduce imbalance. Replication applies a D’Hondt-like greedy allocation plus a one-step donor→receiver fix-up; a hierarchical pipeline packs groups to nodes, replicates within nodes, then packs physical experts to GPUs with inverse maps and stable ranks.
- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000 over 5 workloads); passes all validation tests.
- **Feedback**: Excellent speed but moderate balancedness, likely limited by the small swap budget and single-step replication correction. The hierarchical policy preserves locality and determinism; increasing refinement steps could improve balance at some runtime cost.
**Program Identifier:** Generation 14 - Patch Name one_step_replication_fixup - Correct Program: True

**Program Name: EPLB Hybrid Hierarchical Balancer**
- **Implementation**: Implements hierarchical expert balancing with a hybrid D’Hondt→Sainte-Laguë replica apportionment (plus a one-step fix-up), greedy capacity-constrained packing with k=2 best-improvement swaps, and a diversity-aware GPU assignment tie-breaker. Uses permutation inversion utilities and staged refinement (1 step for groups→nodes, 2 steps for physical→GPU packing).
- **Performance**: Combined score 0.0; fails all validation tests.
- **Feedback**: The replica fix-up moves a seat from the highest-average “donor” to the lowest-average “receiver,” which worsens imbalance (direction should be receiver→donor), likely breaking correctness. Additionally, layout reshaping/gather assumes block-contiguous permutations and the forced .cpu() cast may violate expected device semantics, contributing to failures.
**Program Identifier:** Generation 15 - Patch Name hydra_k2_diversity_apportion - Correct Program: False

**Program Name: Hierarchical EPLB with Greedy Packing**
- **Implementation**: Uses capacity-constrained greedy packing over sorted weights with bounded multi-swap refinement; hierarchical grouping packs groups→nodes and replicas→GPUs via inverse-permutation mappings. Replication applies a D’Hondt allocation with a Sainte-Laguë tail and a single donor→receiver fix-up, with deterministic rank reassignment.
- **Performance**: Combined score 0.65; balancedness_score 0.304918, speed_score 1.000000 across 5 workloads.
- **Feedback**: The limited refinement (refine_steps=1–2) and one-step fix-up prioritize speed but leave noticeable imbalance. Increasing swap iterations or applying richer local search could improve balancedness at some runtime cost; all validation tests pass.
**Program Identifier:** Generation 16 - Patch Name hybrid_sainte_lague_tail_and_stage_refine - Correct Program: True

**Program Name: Hierarchical Expert Parallel Load Balancer**
- **Implementation**: Implements a hierarchical EPLB: packs expert groups to nodes via greedy balanced_packing with bounded multi-swap refinement, replicates experts using a hybrid D’Hondt (bulk) + Sainte-Laguë (tail) allocator plus a one-step donor→receiver fix-up, then packs physical replicas to GPUs. Uses CPU-based sorting/scatter and stable rank reassignment, with inverse mappings for logical/physical indices.
- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.000000 over 5 workloads); all validation tests passed.
- **Feedback**: Excellent speed indicates the greedy, bounded-swap design and CPU-side ops keep overhead low. Balancedness is moderate, likely constrained by the single-step replica fix-up and small refine_steps; allowing more refinement or multi-move adjustments could improve load balance.
**Program Identifier:** Generation 17 - Patch Name hybrid_sainte_lague_tail_and_stage_specific_refine - Correct Program: True

**Program Name: Hierarchical Diversity-Aware EPLB**
- **Implementation**: Performs hierarchical load balancing: packs expert groups to nodes with greedy balanced packing, replicates experts per node by max normalized load, then assigns replicas to GPUs using diversity-aware greedy packing with a single k=2 best-improvement swap; mappings use vectorized gather/scatter for efficiency. Includes a fast path when labels are unique and tie-breakers to reduce label duplication per pack.
- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.00 over 5 workloads).
- **Feedback**: Passes all validation tests; the diversity-aware greedy plus minimal refinement achieves excellent speed but only moderate balance. More aggressive refinement (multi-swap or iterative improvement) could raise balancedness, though current design already reduces hotspotting from replica co-location.
**Program Identifier:** Generation 18 - Patch Name k2_micro_swap_diverse - Correct Program: True

**Program Name: Hierarchical EPLB with greedy packing and hybrid apportionment**
- **Implementation**: Implements a hierarchical load balancer: packs groups to nodes, replicates experts within nodes using a hybrid D’Hondt (bulk) + Sainte-Laguë (tail) apportionment with a one-step fix-up, then assigns physical experts to GPUs via balanced packing. The packing is a sorted greedy placement with capacity constraints plus bounded local multi-swap refinement; mappings are constructed with gather/scatter and inverse permutations.
- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.0 over 5 workloads).
- **Feedback**: Correct and passes all validation tests. Excellent speed due to bounded refinement and lightweight apportionment, while moderate balancedness indicates potential gains from higher refine_steps or multi-iteration replication adjustments.
**Program Identifier:** Generation 19 - Patch Name refine_packing_and_hybrid_replication - Correct Program: True

**Program Name: Hierarchical Expert Load Balancer with Hybrid Replication**
- Implementation: Greedy capacity-constrained packing with per-layer targeted heavy-light swaps (searchsorted-based) determines group/node/GPU placement; a hierarchical layout uses permutation inverses to keep groups contiguous and then packs per-GPU experts after replication. Replication employs hybrid apportionment (D’Hondt bulk + Sainte-Laguë tail) with a one-step donor→receiver fix-up; deterministic intra-pack ranks and vectorized torch ops are used throughout.
- Performance: Combined score 0.66 (balancedness 0.311848, speed 1.000, 5 workloads).
- Feedback: Extremely fast but only moderately balanced; limited refinement steps and the single-step replication fix-up likely cap balancedness. Increasing refine_steps or enabling multi-swap/multi-move adjustments could improve balance at some cost in speed.
**Program Identifier:** Generation 20 - Patch Name moe_eplb_crossover_capacityperm - Correct Program: True

**Program Name: Hierarchical Water-Filling Expert Load Balancer**
- **Implementation**: Uses hierarchical staging: capacity-aware LPT to pack groups to nodes, water-filling replication per node with a top-2 donor/bottom-2 receiver fix-up, and diversity-aware heap GPU packing with a k=2 swap refinement. Physical↔logical maps and ranks are built via repeat_interleave and scatter/gather with exact per-pack capacities.
- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000; 5 workloads).
- **Feedback**: Correct and fast, but balancedness is only moderate, suggesting the greedy packing and limited local refinements reduce peaks but don’t fully equalize load. Stronger global adjustments could further improve balance without sacrificing speed.
**Program Identifier:** Generation 21 - Patch Name top2_replica_fix_and_tieaware_pack - Correct Program: True

**Program Name: Hierarchical EPLB with greedy packing and quota replication**
- **Implementation**: Uses a capacity-constrained greedy packer with bounded k=2 best-improvement swaps and an adaptive second swap; replication allocates via a D’Hondt bulk phase followed by per-row Sainte-Laguë vs Huntington–Hill selection and a single-move donor→receiver fix-up. Hierarchical policy packs groups to nodes and physical experts to GPUs, with fast permutation inverses and scatter-based map construction and deterministic in-pack ranking.
- **Performance**: Combined score 0.65 (balancedness 0.306257, speed 1.000000 across 5 workloads).
- **Feedback**: Minimal refinement and simple scans preserve peak speed but cap balance quality; adaptive tail sizing (via coefficient of variation) and hierarchical staging improve robustness. The implementation is correct and passes all validation tests.
**Program Identifier:** Generation 22 - Patch Name eplb_k2swap_hybrid_apportion - Correct Program: True

**Program Name: Hierarchical EPLB with Hybrid Replication**
- **Implementation**: Greedy capacity-constrained packing with deterministic in-pack ranking and a bounded k=2 heaviest-lightest swap refinement, combined with hierarchical node/group/GPU placement. Replication uses a hybrid D’Hondt (bulk) + Sainte-Laguë (tail) apportionment with a single best-improving donor→receiver fix-up to reduce peak average load.
- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.0 across 5 workloads); all validations pass.
- **Feedback**: The algorithm is very fast due to greedy packing and limited local refinement, but balancedness is modest, suggesting the k=2 swap and single fix-up move under-correct skew. Increasing refine_steps, considering larger swap candidate sets, or multiple fix-up iterations could improve balance at some cost to speed.
**Program Identifier:** Generation 23 - Patch Name adaptive_refine_and_replication_fixup_k2 - Correct Program: True

**Program Name: Hierarchical EPLB with k-swap refinement**
- **Implementation**: Implements a hierarchical three-stage balancer: groups-to-nodes via greedy packing plus bounded k=2 best-improvement swaps, intra-node expert replication using apportionment (D’Hondt bulk + Sainte-Laguë/Huntington–Hill tail) with a one-move fix-up, then GPU packing with a diversity-aware tie-break; deterministic ranking and inverse-permutation utilities included. Core loops run on CPU for control flow, with small-k refinement and optional adaptive second swap for improved local balance.
- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.000000 over 5 workloads); all validations pass.
- **Feedback**: Achieves maximal speed due to lightweight CPU-driven greedy packing and limited refinement, but balancedness is modest. Increasing refine_steps/k, strengthening swap search, or more aggressive diversity penalties could improve balance at some runtime cost.
**Program Identifier:** Generation 24 - Patch Name dhh_diverse_k2_refine - Correct Program: True

**Program Name: EPLB: Hierarchical Hybrid Load Balancer**
- **Implementation**: Uses a CPU-friendly greedy balanced_packing with per-layer sort, capacity-aware assignment, and bounded heaviest-lightest refinement swaps; replicate_experts applies a hybrid apportionment (D’Hondt bulk + Sainte-Laguë tail) plus a one-step donor→receiver fix-up. Hierarchical rebalancing packs groups to nodes, replicates within nodes, then packs replicas to GPUs, with permutation inverses and rank-aware log2phy scatter for final maps.
- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.0 over 5 workloads); all validation tests pass.
- **Feedback**: Excellent speed stems from greedy CPU paths and limited refinement, while balancedness is modest likely due to small refine_steps and single-step replication fix-up. The hierarchical policy favors locality and stability but may under-balance globally in some distributions.
**Program Identifier:** Generation 25 - Patch Name moe_eplb_hybrid_refine - Correct Program: True

**Program Name: Hierarchical Expert Parallelism Load Balancer (EPLB)**
- **Implementation**: Staged pipeline: GroupPacker does LPT-style greedy packing with a bounded k=2 best-improvement swap; ReplicaAllocator apportions replicas via D’Hondt bulk then A/B between Sainte-Laguë and Huntington–Hill with a single donor→receiver correction; GpuPlacer uses greedy projected-load assignment with a diversity-aware tie-breaker and adaptive k=2 refinement. Hierarchical mapping packs groups to nodes, allocates within nodes, then places to GPUs, with a global fallback when grouping doesn’t align with nodes.
- **Performance**: Combined score 0.65 (balancedness 0.299, speed 1.000) over 5 workloads; all validations passed.
- **Feedback**: Perfect speed shows the heuristics are lightweight and implementation efficient, while modest balancedness suggests the greedy choices and single bounded swaps leave residual imbalance. The apportionment tail improves peak per-replica averages and the diversity tie-breaker helps spread replicas, but limited refinement likely caps balancing quality.
**Program Identifier:** Generation 26 - Patch Name eplb_pipeline_apportion_refine - Correct Program: True

**Program Name: Hierarchical Water-Filling Expert Load Balancer**
- **Implementation**: Uses a hierarchical 3-step pipeline: capacity-aware LPT packing of groups to nodes, per-(layer,node) water-filling replication with a small donor→receiver fix-up, and diversity-aware heap packing to GPUs with a unique-label fast path and a single 2-opt swap; mappings are built via vectorized torch gather/scatter with contiguous ranks per expert. Assumes exact per-pack capacity and enforces at least one replica per logical expert.
- **Performance**: Combined score 0.66 (balancedness 0.3111, speed 1.00 over 5 workloads); all validation tests passed.
- **Feedback**: The approach is fast due to simple sorts, limited local refinements, and lightweight CPU tensor ops, but balancedness is modest, likely bounded by the greedy packing and shallow (k≤2) fix-ups. Deeper refinement or broader swap searches could improve balance at a small runtime cost.
**Program Identifier:** Generation 27 - Patch Name moe_eplb_waterfill_diverse_refine - Correct Program: True

**Program Name: Hierarchical EPLB with zig-zag packing and water-filling**
- **Implementation**: Implements a hierarchical balancer: groups are stripe-packed (zig-zag) to nodes, experts replicated in-node via water-filling (binary search + greedy rounding), and replicas stripe-packed to GPUs using per-replica average load; mappings are built with scatter/gather and permutation inverses for deterministic phy2log/log2phy. Runs on CPU tensors, enforces exact per-pack capacity, and ignores refine_steps in packing for simplicity.
- **Performance**: Combined score 0.66 (balancedness 0.310534, speed 1.000000) over 5 workloads; all validations pass.
- **Feedback**: Extremely fast but only moderately balanced; the single-pass stripe packing and batched rounding in water-filling (full-round increments before top-k) likely cap balance quality. Iterative refinement or stricter marginal-benefit allocation for remaining replicas could improve balancedness without large runtime penalties.
**Program Identifier:** Generation 28 - Patch Name waterfill_stripe_moe - Correct Program: True

**Program Name: Hierarchical Expert Load Balancer with K-swap Refinement**
- **Implementation**: Greedy capacity-constrained packing per row with bounded k=2 best-improvement swaps, plus a diversity-aware tie-break for near-ties. Hierarchical pipeline: pack groups to nodes, replicate via vectorized minimax with a one-move fix-up, then pack replicas to GPUs; outputs deterministic ranks and mappings.
- **Performance**: Combined score 0.65 (balancedness 0.308909, speed 1.000000 over 5 workloads).
- **Feedback**: Excellent speed comes from lightweight CPU control flow and vectorized allocation steps, but the small k and swap budget bias toward speed over balance, resulting in modest balancedness. Increasing refine_steps/k or strengthening the post-allocation fix-up could improve balance; the very small diversity penalty likely has limited impact.
**Program Identifier:** Generation 29 - Patch Name moe_eplb_minimax_replicator - Correct Program: True

# GLOBAL INSIGHTS SCRATCHPAD

The following are global insights about optimization approaches and their effectiveness:

## Successful Algorithmic Patterns
- Hybrid apportionment replication with a guarded one-step fix-up consistently ties for the top balancedness. Programs using D’Hondt (bulk) → Sainte-Laguë (tail) plus a donor→receiver move only when it strictly reduces the global max average load all reach balancedness ≈0.311848 at speed 1.00: 
  - Current best: Hierarchical Expert Load Balancer with Hybrid Replication (Gen20, 0.66, 0.311848, 1.00)
  - Hierarchical EPLB with Hybrid Replication (Gen23, 0.66, 0.311848, 1.00)
  - Hierarchical EPLB with k-swap refinement (Gen24, 0.66, 0.311848, 1.00)
  - EPLB: Hierarchical Hybrid Load Balancer (Gen25, 0.66, 0.311848, 1.00)
- Capacity-constrained greedy packing with tiny, targeted heavy↔light swaps lifts balance without hurting speed. The current best pairs refine_steps=1 for group→node packing and refine_steps=2 for physical→GPU packing and uses a searchsorted-based best-swap selection to reduce the heavy–light delta; similar bounded refinements in Gen23–25 match the top score.
- CPU-centric, deterministic mapping via gather/scatter and permutation inverses maintains maximal speed. The current best builds hierarchical maps with PermOps.inverse and scatter-based log2phy while keeping all tensor ops on CPU, mirroring other top performers’ speed 1.00.
- Hierarchical pipeline (group→node, replicate within node, then GPU packing) is a stable foundation. All top-scoring programs retain this structure, achieving 0.66 combined with balancedness in the 0.3118 range.

## Ineffective Approaches
- Water-filling replication with small local fix-ups underperforms the hybrid apportionment tail. Both “Hierarchical Water-Filling Expert Load Balancer” variants (Gen21 and Gen27) reach only ~0.3111 balancedness (0.66 combined), trailing the hybrid apportionment programs at 0.311848.
- Stripe (zig-zag) packing and batched rounding for water-filling caps balance. “Hierarchical EPLB with zig-zag packing and water-filling” (Gen28) hits 0.310534 balancedness (0.66 combined), below the top cluster, consistent with feedback that single-pass stripe and batched rounding limit equalization.
- Quota/minimax-style replication tails and minimal refinement produce a lower cluster. 
  - “Hierarchical EPLB with greedy packing and quota replication” (Gen22) lands at 0.306257 balancedness (0.65 combined) despite hybrid-like apportionment, likely due to simpler scans and fewer/less effective refinements.
  - “Hierarchical Expert Load Balancer with K-swap Refinement” (Gen29) with a vectorized minimax replicator scores 0.308909 (0.65 combined), indicating the minimax allocator and tiny diversity penalty don’t close the gap.
- A more complex staged pipeline with A/B tail (Sainte-Laguë vs Huntington–Hill) and global fallback regresses further. “Hierarchical Expert Parallelism Load Balancer (EPLB)” (Gen26) drops to 0.299 balancedness (0.65 combined) even though validations pass, suggesting the heuristic mix (A/B tail, global fallback) and limited swaps left substantial residual imbalance.
- From prior insights: incorrect replication fix-up direction and device/layout assumptions break correctness (Gen15 scored 0.0), underscoring the need for guarded improvement checks and consistent CPU tensor semantics.

## Implementation Insights
- What makes the current best effective:
  - Targeted swap selection: For the heaviest and lightest packs, searchsorted over the light-pack items finds the swap minimizing the new heavy–light delta, and the swap is applied only if it strictly improves the delta; pack loads are updated incrementally. This precision likely explains the 0.311848 edge over similar but less targeted swaps (e.g., water-filling variants at ~0.3111).
  - Replication: A fixed ≈10% Sainte-Laguë tail after D’Hondt bulk, followed by a one-step donor→receiver fix-up guarded by new_peak < current_max and feasibility (donor count > 1). This guard avoids regressions seen in prior incorrect fix-ups and matches all other top-scoring hybrids.
  - Deterministic, CPU-based mappings: PermOps.inverse, gather/scatter chains, and deterministic in-pack ranks ensure exact capacities and reproducible layouts while maintaining speed 1.00.
  - Bounded per-stage refinement: refine_steps=1 (groups→nodes) and refine_steps=2 (replicas→GPUs) deliver the best balance/speed trade-off without introducing expensive global searches.
- Concrete contrasts:
  - Fixed ~10% tail outperforms dynamic/adaptive tails or A/B per-row selections used in Gen22 and Gen26, which correlate with lower balancedness (0.3063 and 0.299).
  - Diversity-aware tie-breaking alone (Gen21, Gen27) matches but does not exceed ~0.3111; the current best’s small-k swap refinement at the GPU stage pushes to 0.311848.

## Performance Analysis
- With speed fixed at 1.00 across correct runs, balancedness differentiates outcomes:
  - Top cluster: 0.311848 balancedness (Gen20, Gen23–25; combined 0.66).
  - Near-top: ~0.3111 balancedness for water-filling variants (Gen21, Gen27; combined 0.66).
  - Middle: 0.310534 with stripe/zig-zag and batched rounding (Gen28; combined 0.66).
  - Lower cluster: 0.306257 (Gen22) and 0.308909 (Gen29), both 0.65 combined.
  - Outlier regression: 0.299 balancedness for the more complex pipeline with A/B tail and fallback (Gen26; 0.65 combined).
- The consistent ≈0.0007–0.0008 edge from ~0.3111 to 0.311848 aligns with the current best’s combination of:
  - Searchsorted-based targeted swaps and refine_steps=2 at the GPU packing stage,
  - Hybrid apportionment with a small fixed Sainte-Laguë tail and a guarded one-step fix-up.
- Diversity-aware tie-breakers and heap-based GPU placements maintain competitive scores (~0.3111) but do not surpass the hybrid + targeted-swap recipe; single-pass stripe and batched rounding approaches systematically trail by ~0.001–0.0015.
- More elaborate replication tails (A/B method) and global fallbacks correlate with the worst balancedness among correct programs (0.299–0.306), indicating that added complexity without precise improvement guards can degrade balancing quality.

# META RECOMMENDATIONS

The following are actionable recommendations for the next program generations:

1. Add a bounded 2×2 exchange option in the GPU packing refinement. After computing the current best 1×1 swap between the heaviest and lightest packs, also evaluate swapping two items from the heaviest with two from the lightest (top-2 heavy candidates × bottom-2 light candidates → choose the best pair-pair). Apply at most one 2×2 exchange per iteration, only if it strictly reduces the max pack load more than the best 1×1 swap, keeping refine_steps=2 and all logic on CPU.

2. Expand the swap candidate light side to include the second-lightest pack. In _refine_single_layer, run the same searchsorted-based best-swap selection against both the lightest and second-lightest packs, then choose the single swap that minimizes the new global max (tie-break by smaller heavy–light delta). Keep the strict improvement guard and update loads incrementally; this keeps cost low while avoiding early exits when the true best partner isn’t the absolute lightest.

3. Add a conditional second replication fix-up when the first improvement is small. After the current one-step donor→receiver move (guarded by new_peak < cur_max), recompute avg and cur_max; if the improvement is shallow (e.g., new_peak > 0.9 * old_peak), evaluate one more move from the updated top-2 donors to the bottom-2 receivers (only donors with count > 1), pick the single best move by minimizing the new peak, and apply it only if it strictly improves. Cap at two moves per row total to preserve speed and determinism.

4. Micro-tune the hybrid replication tail around 10% with a tiny, deterministic A/B/C check. Compute tail0 = round(0.10 * num_redundant), then evaluate tail in {max(1, tail0-1), tail0, tail0+1} by simulating only the tail picks (bulk remains D’Hondt), and select the tail size yielding the lowest predicted peak avg (max(weight/logcnt’) after tail). This keeps the proven hybrid behavior while allowing a minimal, low-cost adjustment that can help on skewed layers.

5. Make GPU-stage refinement depth adaptive to measured imbalance. Before refinement, compute delta = max_load - min_load and mean_load; if delta/mean_load > 0.12, set refine_steps=3, else keep 2 (group→node stays at 1). This triggers an extra micro-step only on harder instances where it pays off, maintaining speed 1.00 on typical cases while squeezing additional balance on outliers.