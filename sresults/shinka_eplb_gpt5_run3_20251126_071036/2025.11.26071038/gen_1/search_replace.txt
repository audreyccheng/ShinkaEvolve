<NAME>
diversity_aware_gpu_packing
</NAME>

<DESCRIPTION>
Introduce a diversity-aware packing heuristic for Step 3 (assigning physical experts to GPUs within a node) to avoid placing multiple replicas of the same logical expert on the same GPU. This reduces hotspotting and improves load balance across GPUs without significantly increasing runtime. The new function, balanced_packing_diverse, extends the existing greedy balanced_packing by adding a tie-break preference for packs with fewer replicas of the same logical expert label. It falls back to the standard greedy approach for rows without duplicate labels to retain speed. Minimal code changes: an extra import, a new helper function, and a small modification to Step 3 to use the diversity-aware packing.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
import torch
=======
import torch
from collections import defaultdict
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
    return pack_index, rank_in_pack


def replicate_experts(
        weight: torch.Tensor,
        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
=======
    return pack_index, rank_in_pack


def balanced_packing_diverse(weight: torch.Tensor,
                             label: torch.Tensor,
                             num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Balanced packing with diversity: prefer spreading items with the same label
    across different packs to reduce hotspotting when replicas of the same
    logical expert are placed on the same GPU.

    Parameters:
        weight: [X, n], the weight of each item
        label:  [X, n], an integer label for each item (e.g., logical expert id)
        num_packs: number of packs

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    num_layers, num_items = weight.shape
    assert num_items % num_packs == 0
    items_per_pack = num_items // num_packs

    # Trivial case: one item per pack, fallback to standard balanced packing
    if items_per_pack == 1 or num_packs == 1:
        return balanced_packing(weight, num_packs)

    # Ensure CPU tensors for Python-side loops
    device = weight.device
    w = weight.float()
    labels = label.to(dtype=torch.int64, device=device)

    # Pre-sort indices by descending weights per row
    sorted_indices = w.sort(dim=-1, descending=True).indices

    pack_index = torch.full_like(weight,
                                 fill_value=-1,
                                 dtype=torch.int64,
                                 device=device)
    rank_in_pack = torch.full_like(pack_index, fill_value=-1)

    for i in range(num_layers):
        row_sorted = sorted_indices[i].tolist()
        row_w = w[i]
        row_labels = labels[i]

        # Quick duplicate check to avoid overhead when not needed
        seen = set()
        has_dup = False
        for idx in range(num_items):
            l = int(row_labels[idx].item())
            if l in seen:
                has_dup = True
                break
            seen.add(l)

        if not has_dup:
            # No duplicate labels; standard greedy suffices and is faster
            pack_loads = [0.0] * num_packs
            pack_counts = [0] * num_packs
            for group in row_sorted:
                # choose among packs with capacity the one with min load
                best_p = min(
                    (p for p in range(num_packs) if pack_counts[p] < items_per_pack),
                    key=lambda p: pack_loads[p],
                )
                pack_index[i, group] = best_p
                rank_in_pack[i, group] = pack_counts[best_p]
                pack_counts[best_p] += 1
                pack_loads[best_p] += float(row_w[group].item())
            continue

        # Diversity-aware greedy
        pack_loads = [0.0] * num_packs
        pack_counts = [0] * num_packs
        label_counts = [defaultdict(int) for _ in range(num_packs)]

        for group in row_sorted:
            lab = int(row_labels[group].item())
            # Select pack preferring fewer repetitions of label, then lower load, then fewer items
            best_p = None
            best_key = None
            for p in range(num_packs):
                if pack_counts[p] >= items_per_pack:
                    continue
                rep = label_counts[p].get(lab, 0)
                key = (rep, pack_loads[p], pack_counts[p])
                if best_key is None or key < best_key:
                    best_key = key
                    best_p = p
            # Assign
            pack_index[i, group] = best_p
            rank_in_pack[i, group] = pack_counts[best_p]
            pack_counts[best_p] += 1
            pack_loads[best_p] += float(row_w[group].item())
            label_counts[best_p][lab] += 1

    return pack_index, rank_in_pack


def replicate_experts(
        weight: torch.Tensor,
        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
    # Step 3: pack physical_experts to GPUs
    # [num_layers * num_nodes, num_physical_experts // num_nodes]
    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,
                                                num_gpus // num_nodes)
    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
=======
    # Step 3: pack physical_experts to GPUs
    # [num_layers * num_nodes, num_physical_experts // num_nodes]
    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
    # Use diversity-aware packing to spread replicas of the same logical expert
    # across GPUs within a node, reducing hotspotting and improving balance.
    pack_index, rank_in_pack = balanced_packing_diverse(
        tokens_per_phy, phy2mlog, num_gpus // num_nodes)
    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
>>>>>>> REPLACE

</DIFF>