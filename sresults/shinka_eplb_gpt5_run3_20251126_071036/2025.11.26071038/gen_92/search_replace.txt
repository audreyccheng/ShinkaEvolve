<NAME>
diverse_gpu_tie_and_two_move_fixup
</NAME>

<DESCRIPTION>
This edit targets two low-overhead improvements:
1) Diversity-aware tie-breaking during GPU packing to avoid co-locating replicas of the same logical expert when loads are near-tied. This reduces peak GPU load by spreading replicas across GPUs with a negligible runtime cost. Implemented via a new balanced_packing_diverse function using a tiny same-label penalty for near-ties, plus an adaptive one-extra refinement step if the post-greedy imbalance ratio is still large. It is applied only at the GPU packing stage to preserve overall speed and determinism elsewhere.

2) Allow up to two guarded replication fix-up moves per row (top-2 donors to bottom-2 receivers), recomputing after the first move and only committing strictly improving actions. This further reduces the global peak without materially impacting runtime, because each row evaluates a tiny candidate set and at most two moves.

These changes are consistent, contained, and keep the algorithm fast while often improving balancedness in practice.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def balanced_packing(weight: torch.Tensor,
                     num_packs: int,
                     refine_steps: int = 1) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs
        refine_steps: small bounded number of refinement swaps per layer

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    packer = CapacityPacker(refine_steps=refine_steps)
    return packer.pack(weight, num_packs)
=======
def balanced_packing(weight: torch.Tensor,
                     num_packs: int,
                     refine_steps: int = 1) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Pack n weighted objects to m packs, such that each bin contains exactly
    n/m objects and the weights of all packs are as balanced as possible.

    Parameters:
        weight: [X, n], the weight of each item
        num_packs: number of packs
        refine_steps: small bounded number of refinement swaps per layer

    Returns:
        pack_index: [X, n], the pack index of each item
        rank_in_pack: [X, n], the rank of the item in the pack
    """
    packer = CapacityPacker(refine_steps=refine_steps)
    return packer.pack(weight, num_packs)


def balanced_packing_diverse(
    weights: torch.Tensor,
    labels: torch.Tensor,
    num_packs: int,
    refine_steps: int = 2,
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Diversity-aware variant of balanced packing used at the GPU stage.
    - Greedy assignment to the lightest pack with capacity.
    - On near-ties (within eps), prefer the pack with fewer same-label items.
    - After greedy, perform small bounded refinement; allow one extra step if residual
      imbalance is still noticeable.

    weights: [L, N], labels: [L, N] int64
    """
    L, N = weights.shape
    assert N % num_packs == 0
    capacity = N // num_packs

    if capacity == 1:
        pack_index = torch.arange(N, dtype=torch.int64, device=weights.device).expand(weights.shape)
        rank_in_pack = torch.zeros_like(pack_index, dtype=torch.int64)
        return pack_index, rank_in_pack

    w_cpu = weights.float().cpu()
    lab_cpu = labels.long().cpu()
    pack_index = torch.full((L, N), -1, dtype=torch.int64)
    rank_in_pack = torch.full_like(pack_index, -1)

    for li in range(L):
        w = w_cpu[li]
        labs = lab_cpu[li]
        order = torch.argsort(w, descending=True)

        load = [0.0] * num_packs
        counts = [0] * num_packs
        label_cnt = [dict() for _ in range(num_packs)]
        pidx = torch.empty(N, dtype=torch.int64)

        mean_w = float(torch.mean(w).item()) if N > 0 else 0.0
        eps = 1e-6 * max(mean_w, 1e-12)
        lam = 1e-8 * max(mean_w, 1e-12)

        for g in order.tolist():
            cand = [p for p in range(num_packs) if counts[p] < capacity]
            min_load = min(load[p] for p in cand)
            lbl = int(labs[g].item())
            eff = []
            for p in cand:
                ld = load[p]
                if ld - min_load <= eps:
                    same = label_cnt[p].get(lbl, 0)
                    eff.append(ld + lam * same)
                else:
                    eff.append(ld)
            best_idx = int(torch.argmin(torch.tensor(eff)).item())
            best_pack = cand[best_idx]

            pidx[g] = best_pack
            load[best_pack] += float(w[g].item())
            counts[best_pack] += 1
            label_cnt[best_pack][lbl] = label_cnt[best_pack].get(lbl, 0) + 1

        # Adaptive bounded refinement: add one extra step if imbalance still notable
        pack_w = torch.zeros(num_packs, dtype=w.dtype)
        pack_w.scatter_add_(0, pidx, w)
        delta = float((pack_w.max() - pack_w.min()).item())
        mean_ld = float(pack_w.mean().item())
        ratio = delta / max(mean_ld, 1e-12)
        steps = int(refine_steps + (1 if ratio > 0.02 else 0))

        pidx = CapacityPacker(refine_steps=steps)._refine_single_layer(w, pidx, num_packs, capacity)

        rnk = CapacityPacker._rank_from_packidx(pidx, num_packs)
        pack_index[li] = pidx
        rank_in_pack[li] = rnk

    return pack_index.to(weights.device), rank_in_pack.to(weights.device)
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
    # Step 3: pack physical experts to GPUs (still within nodes)
    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
    pack_index, rank_in_pack = balanced_packing(tokens_per_phy, num_gpus // num_nodes, refine_steps=2)
    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
    pphy2phy = PermOps.inverse(phy2pphy)
=======
    # Step 3: pack physical experts to GPUs (still within nodes) with diversity-aware tie-breaking
    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
    labels_per_phy = phy2mlog  # avoid co-locating replicas of the same meta-logical expert under near ties
    pack_index, rank_in_pack = balanced_packing_diverse(tokens_per_phy, labels_per_phy, num_gpus // num_nodes, refine_steps=2)
    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
    pphy2phy = PermOps.inverse(phy2pphy)
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
    # Strengthened replication fix-up per row with second-order tie-break:
    # Evaluate donors=top-2 (by avg) and receivers=bottom-2; choose the single move
    # that strictly reduces the peak, breaking ties by the new second-highest among
    # {baseline_other, new_d, new_r}.
    if num_log > 1 and num_redundant > 0:
        r_f = logcnt.to(dtype_f)
        avg = weight / r_f
        cur_max = avg.max(dim=-1).values
        argmax_idx = avg.argmax(dim=-1)
        kdon = min(2, num_log)
        krec = min(2, num_log)
        top_vals, top_idx = torch.topk(avg, k=kdon, dim=-1, largest=True)
        bot_vals, bot_idx = torch.topk(avg, k=krec, dim=-1, largest=False)

        for ri in range(n):
            best = None  # (new_peak, new_second, d, r)
            donors = top_idx[ri].tolist()
            receivers = bot_idx[ri].tolist()
            for d in donors:
                cd = int(logcnt[ri, d].item())
                if cd <= 1:
                    continue
                for r in receivers:
                    if d == r:
                        continue
                    cr = int(logcnt[ri, r].item())
                    # Baseline "other" peak: if donor is current max, use second-best; else use current max
                    baseline_other = float(cur_max[ri].item())
                    if d == int(argmax_idx[ri].item()):
                        if kdon >= 2:
                            baseline_other = float(top_vals[ri, 1].item())
                    new_d = float(weight[ri, d].item()) / float(cd - 1)
                    new_r = float(weight[ri, r].item()) / float(cr + 1)
                    # Predicted new peak and second-highest among the three contributors
                    a, b, c = baseline_other, new_d, new_r
                    new_peak = max(a, b, c)
                    new_second = (a + b + c) - min(a, b, c) - new_peak
                    if new_peak + eps < float(cur_max[ri].item()):
                        if best is None or (new_peak < best[0] - 0.0) or (abs(new_peak - best[0]) <= eps and new_second < best[1] - 0.0):
                            best = (new_peak, new_second, d, r)
            if best is not None:
                _, _, d, r = best
                donor_cols = torch.nonzero(phy2log[ri] == d, as_tuple=False).squeeze(1)
                if donor_cols.numel() == 0:
                    continue
                maxr_idx = torch.argmax(rank[ri, donor_cols]).item()
                col_idx = donor_cols[maxr_idx]
                new_rank = int(logcnt[ri, r].item())
                phy2log[ri, col_idx] = r
                rank[ri, col_idx] = new_rank
                logcnt[ri, d] -= 1
                logcnt[ri, r] += 1
=======
    # Strengthened replication fix-up per row with second-order tie-break, up to two moves:
    # At each step, evaluate donors=top-2 (by avg) and receivers=bottom-2; choose the single move
    # that strictly reduces the peak, breaking ties by the new second-highest among
    # {baseline_other, new_d, new_r}. Recompute and optionally repeat once.
    if num_log > 1 and num_redundant > 0:
        for ri in range(n):
            moves = 0
            while moves < 2:
                r_f_row = logcnt[ri].to(dtype_f)
                avg_row = weight[ri] / r_f_row
                cur_max_row = float(avg_row.max().item())
                kdon = min(2, num_log)
                krec = min(2, num_log)
                top_vals_row, top_idx_row = torch.topk(avg_row, k=kdon, largest=True)
                bot_vals_row, bot_idx_row = torch.topk(avg_row, k=krec, largest=False)
                argmax_idx_row = int(top_idx_row[0].item())
                second_row = float(top_vals_row[1].item() if kdon > 1 else top_vals_row[0].item())

                best = None  # (new_peak, new_second, d, r)
                donors = [int(x) for x in top_idx_row.tolist()]
                receivers = [int(x) for x in bot_idx_row.tolist()]
                for d in donors:
                    cd = int(logcnt[ri, d].item())
                    if cd <= 1:
                        continue
                    for r in receivers:
                        if d == r:
                            continue
                        cr = int(logcnt[ri, r].item())
                        baseline_other = second_row if d == argmax_idx_row else cur_max_row
                        new_d = float(weight[ri, d].item()) / float(cd - 1)
                        new_r = float(weight[ri, r].item()) / float(cr + 1)
                        a, b, c = baseline_other, new_d, new_r
                        new_peak = max(a, b, c)
                        new_second = (a + b + c) - min(a, b, c) - new_peak
                        if new_peak + eps < cur_max_row:
                            if best is None or (new_peak < best[0] - 0.0) or (abs(new_peak - best[0]) <= eps and new_second < best[1] - 0.0):
                                best = (new_peak, new_second, d, r)

                if best is None:
                    break

                _, _, d, r = best
                donor_cols = torch.nonzero(phy2log[ri] == d, as_tuple=False).squeeze(1)
                if donor_cols.numel() == 0:
                    break
                maxr_idx = torch.argmax(rank[ri, donor_cols]).item()
                col_idx = donor_cols[maxr_idx]
                new_rank = int(logcnt[ri, r].item())
                phy2log[ri, col_idx] = r
                rank[ri, col_idx] = new_rank
                logcnt[ri, d] -= 1
                logcnt[ri, r] += 1
                moves += 1
>>>>>>> REPLACE

</DIFF>