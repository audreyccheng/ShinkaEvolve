--- a/original.py
+++ b/original.py
@@ -1,474 +1,651 @@
 # EVOLVE-BLOCK-START
 """
 Expert parallelism load balancer (EPLB) for vLLM.
 
-This module implements the core rearrangement algorithm.
-
-This version uses:
-- Water-filling replica allocation to minimize the maximum per-replica load.
-- Diversity-aware heap-based packing for GPU placement within nodes.
+Completely new approach:
+
+- Replica allocation:
+  Priority-queue apportionment (Huntington–Hill bulk) with a tiny D’Hondt
+  micro-tail A/B and a guarded donor→receiver fix-up capped at two moves.
+
+- Placement:
+  Interleaved-by-label zigzag packing to enforce exact capacities while
+  spreading replicas of the same logical expert, followed by a bounded local
+  refinement that checks both the lightest and second-lightest partner packs,
+  and optionally a 2x2 exchange. Refinement depth is adaptive to imbalance.
+
+This preserves the same API and outputs as prior versions.
 """
 
 import torch
+import heapq
 from collections import defaultdict
-
-
-def balanced_packing(weight: torch.Tensor,
-                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:
-    """
-    Pack n weighted objects to m packs, such that each bin contains exactly
-    n/m objects and the weights of all packs are as balanced as possible.
-
-    Parameters:
-        weight: [X, n], the weight of each item
-        num_packs: number of packs
+from typing import Tuple, List
+
+
+def _inverse_rows(perm: torch.Tensor) -> torch.Tensor:
+    inv = torch.empty_like(perm)
+    inv.scatter_(
+        1,
+        perm,
+        torch.arange(perm.size(1), dtype=torch.int64, device=perm.device).expand(perm.shape),
+    )
+    return inv
+
+
+def _zigzag_packing_from_sorted_indices(sorted_idx: torch.Tensor, num_packs: int) -> Tuple[torch.Tensor, torch.Tensor]:
+    """
+    Deterministic zigzag packing given sorted item indices per row.
+
+    For each row, items are assigned in blocks of size num_packs. Within block k:
+      - If k is even, assign to packs 0..num_packs-1
+      - If k is odd,  assign to packs num_packs-1..0
+    This ensures exact capacity per pack and spreads heavy items.
+
+    Args:
+      sorted_idx: [L, N], int64 per-row sorted indices (desc by weight)
+      num_packs: m
 
     Returns:
-        pack_index: [X, n], the pack index of each item
-        rank_in_pack: [X, n], the rank of the item in the pack
-    """
-    num_layers, num_groups = weight.shape
-    assert num_groups % num_packs == 0
-    groups_per_pack = num_groups // num_packs
-
-    if groups_per_pack == 1:
-        pack_index = torch.arange(num_groups,
-                                  dtype=torch.int64,
-                                  device=weight.device).expand(num_layers, num_groups)
+      pack_index: [L, N]
+      rank_in_pack: [L, N]
+    """
+    L, N = sorted_idx.shape
+    assert N % num_packs == 0
+    cap = N // num_packs
+
+    pack_index = torch.empty((L, N), dtype=torch.int64, device=sorted_idx.device)
+    rank_in_pack = torch.empty((L, N), dtype=torch.int64, device=sorted_idx.device)
+
+    for i in range(L):
+        for pos in range(N):
+            b = pos // num_packs  # block id
+            r = pos % num_packs   # position in block
+            p = r if (b % 2 == 0) else (num_packs - 1 - r)
+            g = int(sorted_idx[i, pos].item())
+            pack_index[i, g] = p
+            rank_in_pack[i, g] = b
+
+    return pack_index, rank_in_pack
+
+
+def _grouped_interleaved_sequence(row_w: torch.Tensor, row_labels: torch.Tensor) -> List[int]:
+    """
+    Build an interleaved sequence of item indices by label using a heap keyed by
+    remaining total label mass (sum of remaining weights in that label). This
+    naturally spaces replicas of the same label.
+
+    Returns:
+      seq: list of indices (length N)
+    """
+    N = row_w.numel()
+    if N == 0:
+        return []
+    # Group by label with items sorted by descending weight
+    label2items = defaultdict(list)
+    for idx in range(N):
+        label2items[int(row_labels[idx].item())].append(idx)
+    for lab in label2items:
+        label2items[lab].sort(key=lambda j: float(row_w[j].item()), reverse=True)
+
+    # Heap of (-remaining_mass, label). Initialize with total mass per label.
+    heap = []
+    for lab, items in label2items.items():
+        total = sum(float(row_w[j].item()) for j in items)
+        heapq.heappush(heap, (-total, lab))
+
+    seq = []
+    while heap:
+        neg_mass, lab = heapq.heappop(heap)
+        items = label2items[lab]
+        # Pop one from this label
+        j = items.pop(0)
+        seq.append(j)
+        new_mass = -neg_mass - float(row_w[j].item())
+        if items:
+            heapq.heappush(heap, (-new_mass, lab))
+    return seq
+
+
+def packed_zigzag_interleaved(
+    weights: torch.Tensor,
+    labels: torch.Tensor,
+    num_packs: int
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    """
+    Interleaved-by-label zigzag packing with exact capacity per pack.
+
+    Steps per row:
+      - Build an interleaved sequence by label weight mass.
+      - Assign items to packs using deterministic zigzag order (exact capacity).
+      - Return pack_index and rank_in_pack.
+
+    This avoids scanning all packs per item and tends to spread same-label items.
+    """
+    L, N = weights.shape
+    assert N % num_packs == 0
+    cap = N // num_packs
+    device = weights.device
+
+    if cap == 1:
+        idx = torch.arange(N, dtype=torch.int64, device=device)
+        pack_index = (idx % num_packs).expand(L, N).clone()
         rank_in_pack = torch.zeros_like(pack_index, dtype=torch.int64)
         return pack_index, rank_in_pack
 
-    # Longest-processing-time greedy with capacity constraints
-    indices = weight.float().sort(-1, descending=True).indices.cpu()
-    pack_index = torch.full((num_layers, num_groups),
-                             fill_value=-1,
-                             dtype=torch.int64,
-                             device="cpu")
-    rank_in_pack = torch.full_like(pack_index, fill_value=-1)
-    for i in range(num_layers):
-        pack_loads = [0.0] * num_packs
-        pack_counts = [0] * num_packs
-        for g in indices[i].tolist():
-            # choose pack with capacity and minimal load
-            best_p = None
-            best_load = None
+    pack_index = torch.full((L, N), -1, dtype=torch.int64, device=device)
+    rank_in_pack = torch.full_like(pack_index, -1)
+
+    # Precompute zigzag pack order of length N
+    order = []
+    for b in range(cap):
+        if b % 2 == 0:
             for p in range(num_packs):
-                if pack_counts[p] >= groups_per_pack:
-                    continue
-                if best_load is None or pack_loads[p] < best_load:
-                    best_load = pack_loads[p]
-                    best_p = p
-            pack_index[i, g] = best_p
-            rank_in_pack[i, g] = pack_counts[best_p]
-            pack_counts[best_p] += 1
-            pack_loads[best_p] += float(weight[i, g].item())
+                order.append(p)
+        else:
+            for p in reversed(range(num_packs)):
+                order.append(p)
+
+    for i in range(L):
+        seq = _grouped_interleaved_sequence(weights[i], labels[i])  # list of indices length N
+        # Assign according to zigzag order
+        fill_counts = [0] * num_packs
+        for pos, g in enumerate(seq):
+            p = order[pos]
+            r = fill_counts[p]
+            pack_index[i, g] = p
+            rank_in_pack[i, g] = r
+            fill_counts[p] += 1
+
     return pack_index, rank_in_pack
 
 
-def _waterfill_counts_row(w: torch.Tensor, target_total: int) -> torch.Tensor:
-    """
-    Compute integer replica counts c_i >= 1 that approximately minimize max_i w_i / c_i
-    subject to sum c_i == target_total using water-filling + greedy fill.
-
-    Parameters:
-        w: [num_log], float tensor (on CPU)
-        target_total: int, total replicas to allocate
-
-    Returns:
-        counts: [num_log], int64
-    """
-    num_log = w.numel()
-    assert target_total >= num_log  # at least one per expert
-
-    if target_total == num_log:
-        return torch.ones(num_log, dtype=torch.int64, device=w.device)
-
-    maxw = float(w.max().item()) if num_log > 0 else 0.0
-    # Binary search T such that sum max(1, ceil(w_i / T)) <= target_total
-    lo = 0.0
-    hi = max(maxw, 1.0)
-    # Handle all-zero quickly
-    if maxw == 0.0:
-        counts = torch.ones(num_log, dtype=torch.int64, device=w.device)
-        extras = target_total - num_log
-        if extras > 0:
-            base_add = extras // num_log
-            rem = extras % num_log
-            if base_add > 0:
-                counts += base_add
-            if rem > 0:
-                counts[:rem] += 1
+def _refine_row_heavy_light(
+    row_w: torch.Tensor,
+    pack_index_row: torch.Tensor,
+    rank_row: torch.Tensor,
+    num_packs: int
+) -> None:
+    """
+    Bounded local refinement in-place for a single row after initial packing.
+
+    - Consider heaviest vs both lightest and second-lightest packs.
+    - Evaluate best 1x1 swap using top-2 heavy candidates and bottom-2 light candidates.
+    - If there's a strictly better 2x2 swap against the best light pack, apply it instead.
+    - Optionally run one extra swap if imbalance is still high (adaptive depth).
+    """
+    N = row_w.numel()
+    cap = N // num_packs
+    # Build groups per pack and loads
+    pack_groups = [[] for _ in range(num_packs)]
+    loads = [0.0] * num_packs
+    for g in range(N):
+        p = int(pack_index_row[g].item())
+        pack_groups[p].append(g)
+        loads[p] += float(row_w[g].item())
+
+    def one_step() -> bool:
+        # Identify heavy, light1, light2
+        h = max(range(num_packs), key=lambda k: loads[k])
+        others = [p for p in range(num_packs) if p != h]
+        if not others:
+            return False
+        light_sorted = sorted(others, key=lambda k: loads[k])
+        light_candidates = light_sorted[:min(2, len(light_sorted))]
+
+        # Gather weights tensors for candidate packs
+        if not pack_groups[h]:
+            return False
+        h_idx_tensor = torch.tensor(pack_groups[h], dtype=torch.int64, device=row_w.device)
+        h_w = row_w[h_idx_tensor]
+        kh = min(2, h_w.numel())
+        if kh == 0:
+            return False
+        top_h = torch.topk(h_w, kh).indices.tolist()
+        cur_imb = max(loads) - min(loads)
+
+        best1 = None  # (new_peak, new_imb, h, l, ai, bi, a_item, b_item, wa, wb)
+        best1_light = None
+        for l in light_candidates:
+            if not pack_groups[l]:
+                continue
+            if loads[h] <= loads[l]:
+                continue
+            l_idx_tensor = torch.tensor(pack_groups[l], dtype=torch.int64, device=row_w.device)
+            l_w = row_w[l_idx_tensor]
+            kl = min(2, l_w.numel())
+            if kl == 0:
+                continue
+            bot_l = torch.topk(l_w, kl, largest=False).indices.tolist()
+            other_max = max([loads[p] for p in range(num_packs) if p != h and p != l], default=float("-inf"))
+            other_min = min([loads[p] for p in range(num_packs) if p != h and p != l], default=float("inf"))
+            for ai in top_h:
+                a_item = int(h_idx_tensor[ai].item())
+                wa = float(h_w[ai].item())
+                for bi in bot_l:
+                    b_item = int(l_idx_tensor[bi].item())
+                    wb = float(l_w[bi].item())
+                    new_h = loads[h] - wa + wb
+                    new_l = loads[l] - wb + wa
+                    new_peak = max(new_h, new_l, other_max)
+                    new_bottom = min(new_h, new_l, other_min)
+                    new_imb = new_peak - new_bottom
+                    cand = (new_peak, new_imb, h, l, ai, bi, a_item, b_item, wa, wb)
+                    if best1 is None or cand < best1:
+                        best1 = cand
+                        best1_light = l
+
+        applied = False
+        if best1 is not None and best1[1] + 1e-9 < cur_imb:
+            # Evaluate 2x2 against the best light only if strictly better than best1
+            _, _, h_sel, l_sel, ai, bi, a_item, b_item, wa, wb = best1
+            # Prepare possible 2x2 only if both packs have at least 2 items
+            h_idx_tensor = torch.tensor(pack_groups[h_sel], dtype=torch.int64, device=row_w.device)
+            l_idx_tensor = torch.tensor(pack_groups[l_sel], dtype=torch.int64, device=row_w.device)
+            h_w = row_w[h_idx_tensor]
+            l_w = row_w[l_idx_tensor]
+            if h_w.numel() >= 2 and l_w.numel() >= 2:
+                top2_h = torch.topk(h_w, 2).indices.tolist()
+                bot2_l = torch.topk(l_w, 2, largest=False).indices.tolist()
+                ai0, ai1 = top2_h[0], top2_h[1]
+                bi0, bi1 = bot2_l[0], bot2_l[1]
+                a0, a1 = int(h_idx_tensor[ai0].item()), int(h_idx_tensor[ai1].item())
+                b0, b1 = int(l_idx_tensor[bi0].item()), int(l_idx_tensor[bi1].item())
+                wa2 = float(h_w[ai0].item() + h_w[ai1].item())
+                wb2 = float(l_w[bi0].item() + l_w[bi1].item())
+                other_max = max([loads[p] for p in range(num_packs) if p != h_sel and p != l_sel], default=float("-inf"))
+                other_min = min([loads[p] for p in range(num_packs) if p != h_sel and p != l_sel], default=float("inf"))
+                new_h2 = loads[h_sel] - wa2 + wb2
+                new_l2 = loads[l_sel] - wb2 + wa2
+                new_peak2 = max(new_h2, new_l2, other_max)
+                new_bottom2 = min(new_h2, new_l2, other_min)
+                new_imb2 = new_peak2 - new_bottom2
+
+                # Compare with best1 by new_peak, then new_imb
+                if (new_peak2 + 1e-9 < best1[0]) or (abs(new_peak2 - best1[0]) <= 1e-9 and new_imb2 + 1e-9 < best1[1]):
+                    # Apply 2x2
+                    loads[h_sel] = new_h2
+                    loads[l_sel] = new_l2
+                    # Replace two entries in lists (by local indices)
+                    pack_groups[h_sel][ai0] = b0
+                    pack_groups[h_sel][ai1] = b1
+                    pack_groups[l_sel][bi0] = a0
+                    pack_groups[l_sel][bi1] = a1
+                    applied = True
+                else:
+                    # Apply best 1x1
+                    new_h = loads[h_sel] - wa + wb
+                    new_l = loads[l_sel] - wb + wa
+                    loads[h_sel] = new_h
+                    loads[l_sel] = new_l
+                    pack_groups[h_sel][ai] = b_item
+                    pack_groups[l_sel][bi] = a_item
+                    applied = True
+            else:
+                # Apply best 1x1
+                new_h = loads[h_sel] - wa + wb
+                new_l = loads[l_sel] - wb + wa
+                loads[h_sel] = new_h
+                loads[l_sel] = new_l
+                pack_groups[h_sel][ai] = b_item
+                pack_groups[l_sel][bi] = a_item
+                applied = True
+
+        if applied:
+            # Rewrite pack_index_row and rank_row from pack_groups
+            for p in range(num_packs):
+                for r, g in enumerate(pack_groups[p]):
+                    pack_index_row[g] = p
+                    rank_row[g] = r
+        return applied
+
+    # Adaptive depth: try one step, and if relative imbalance large, try one more
+    mean_load = (sum(loads) / max(1, num_packs))
+    delta = max(loads) - min(loads)
+    applied1 = one_step()
+    if applied1:
+        if mean_load > 0 and (delta / mean_load > 0.12):
+            _ = one_step()
+
+
+def interleaved_zigzag_packing_with_refine(
+    weights: torch.Tensor,
+    labels: torch.Tensor,
+    num_packs: int
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    """
+    Wrapper that performs interleaved-by-label zigzag packing plus bounded refinement.
+    """
+    pack_index, rank_in_pack = packed_zigzag_interleaved(weights, labels, num_packs)
+    # Refine per row
+    L, N = weights.shape
+    if num_packs >= 2 and N // num_packs >= 1:
+        for i in range(L):
+            _refine_row_heavy_light(weights[i], pack_index[i], rank_in_pack[i], num_packs)
+    return pack_index, rank_in_pack
+
+
+def _apportion_huntington_hill(w: torch.Tensor, total: int) -> torch.Tensor:
+    """
+    Allocate integer counts >=1 summing to 'total' using Huntington–Hill highest averages.
+
+    Uses a max-heap of marginal benefits: score = w / sqrt(c*(c+1)).
+    """
+    n = w.numel()
+    assert total >= n
+    counts = torch.ones(n, dtype=torch.int64, device=w.device)
+    seats = total - n
+    if seats == 0:
         return counts
 
-    for _ in range(40):
-        mid = 0.5 * (lo + hi)
-        # counts_i = max(1, ceil(w_i / mid))
-        c = torch.ceil(w / mid).to(torch.int64)
-        c = torch.maximum(c, torch.ones_like(c))
-        s = int(c.sum().item())
-        if s <= target_total:
-            hi = mid
-        else:
-            lo = mid
-
-    # Base counts from hi guarantee <= target_total
-    counts = torch.ceil(w / hi).to(torch.int64)
-    counts = torch.maximum(counts, torch.ones_like(counts))
-    s = int(counts.sum().item())
-
-    # Greedy water-filling for remaining extras
-    extras = target_total - s
-    while extras > 0:
-        k = min(extras, num_log)
-        # Select top-k by current w_i / c_i
-        scores = w / counts.to(w.dtype)
-        topk_idx = torch.argsort(scores, descending=True)[:k]
-        counts[topk_idx] += 1
-        extras -= k
+    if float(w.max().item()) == 0.0:
+        # Evenly distribute
+        base = seats // n
+        rem = seats % n
+        counts += base
+        if rem > 0:
+            counts[:rem] += 1
+        return counts
+
+    # Max-heap of (-score, idx)
+    heap = []
+    for i in range(n):
+        c = int(counts[i].item())
+        score = float(w[i].item()) / max((c * (c + 1)) ** 0.5, 1e-12)
+        heapq.heappush(heap, (-score, i))
+    for _ in range(seats):
+        if not heap:
+            break
+        neg, i = heapq.heappop(heap)
+        counts[i] += 1
+        c = int(counts[i].item())
+        score = float(w[i].item()) / max((c * (c + 1)) ** 0.5, 1e-12)
+        heapq.heappush(heap, (-score, i))
     return counts
 
 
-def replicate_experts_waterfill(
+def _apportion_dhondt_tail(base_counts: torch.Tensor, w: torch.Tensor, tail: int) -> torch.Tensor:
+    """
+    Starting from base_counts, assign 'tail' seats by D'Hondt (w / (c+1)).
+    """
+    counts = base_counts.clone()
+    if tail <= 0:
+        return counts
+    # Max-heap (-score, i)
+    heap = []
+    for i in range(w.numel()):
+        c = int(counts[i].item())
+        score = float(w[i].item()) / float(c + 1)
+        heapq.heappush(heap, (-score, i))
+    for _ in range(tail):
+        neg, i = heapq.heappop(heap)
+        counts[i] += 1
+        c = int(counts[i].item())
+        score = float(w[i].item()) / float(c + 1)
+        heapq.heappush(heap, (-score, i))
+    return counts
+
+
+def _allocate_replicas_row(w: torch.Tensor, total: int) -> torch.Tensor:
+    """
+    Row-wise replica count allocation with HH bulk + D'Hondt tail A/B micro-tune + fix-ups.
+    """
+    n = w.numel()
+    assert total >= n
+    if total == n:
+        return torch.ones(n, dtype=torch.int64, device=w.device)
+
+    # HH bulk
+    counts = _apportion_huntington_hill(w, total)
+
+    # Micro-tuned tail (~10% +/- 1 by simulation over tail only)
+    seats = total - n
+    if seats > 0 and float(w.max().item()) > 0.0:
+        tail0 = int(round(0.10 * seats))
+        candidates = sorted({max(0, tail0 - 1), tail0, tail0 + 1})
+        best_counts = counts
+        best_peak = float((w / counts.to(w.dtype)).max().item())
+        # Simulate: keep bulk HH result minus 'tail' seats, then re-add tail by D'Hondt
+        # To approximate, we back off tail seats evenly from counts>1 (cheap approximation)
+        for tail in candidates:
+            if tail == 0:
+                continue
+            backoff = min(tail, int((counts > 1).sum().item()))
+            if backoff <= 0:
+                continue
+            # Remove 'backoff' seats from the lowest-avg donors to simulate freeing capacity for tail reallocation
+            temp = counts.clone()
+            for _ in range(backoff):
+                donors = (temp > 1)
+                if not bool(donors.any()):
+                    break
+                avg = w / temp.to(w.dtype)
+                avg_mask = avg.clone()
+                avg_mask[~donors] = float("-inf")
+                d = int(torch.argmax(avg_mask).item())
+                temp[d] -= 1
+            # Re-add tail by D'Hondt
+            cand_counts = _apportion_dhondt_tail(temp, w, tail)
+            peak = float((w / cand_counts.to(w.dtype)).max().item())
+            if peak + 1e-12 < best_peak:
+                best_peak = peak
+                best_counts = cand_counts
+        counts = best_counts
+
+    # Conditional donor→receiver fix-up (up to two moves)
+    if n > 1:
+        max_moves = 2
+        for move in range(max_moves):
+            avg = w / counts.to(w.dtype)
+            cur_peak = float(avg.max().item())
+            donors_mask = counts > 1
+            if not bool(donors_mask.any()):
+                break
+            # donors: top-2 by avg among counts>1
+            k_d = min(2, int(donors_mask.sum().item()))
+            avg_mask = avg.clone()
+            avg_mask[~donors_mask] = float("-inf")
+            donors = torch.topk(avg_mask, k=k_d).indices.tolist()
+            # receivers: bottom-2 overall
+            k_r = min(2, n)
+            receivers = torch.topk(-avg, k=k_r).indices.tolist()
+            best_pair = None
+            best_peak = cur_peak
+            for d in donors:
+                for r in receivers:
+                    if d == r:
+                        continue
+                    c_try = counts.clone()
+                    c_try[d] -= 1
+                    c_try[r] += 1
+                    peak_try = float((w / c_try.to(w.dtype)).max().item())
+                    if peak_try + 1e-12 < best_peak:
+                        best_peak = peak_try
+                        best_pair = (d, r)
+            if best_pair is None:
+                break
+            # If first improvement is shallow, allow the second move; else stop early
+            counts[best_pair[0]] -= 1
+            counts[best_pair[1]] += 1
+            if move == 0:
+                if best_peak <= 0.9 * cur_peak:
+                    break  # good improvement; stop early
+
+    return counts
+
+
+def replicate_experts_apportion(
     weight: torch.Tensor,
     num_phy: int
-) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
-    """
-    Water-filling replication to minimize the maximum per-replica load,
-    with a one-step donor→receiver fix-up to reduce peak average load.
-
-    Parameters:
-        weight: [X, num_log] (CPU float)
-        num_phy: total number of replicas
-
-    Returns:
-        phy2log: [X, num_phy], logical expert id for each physical expert
-        rank:    [X, num_phy], replica rank per logical expert
-        logcnt:  [X, num_log], replica counts per logical expert
-    """
-    n, num_log = weight.shape
+) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
+    """
+    Apportionment-based replication (HH bulk + micro-tail + guarded fix-ups).
+    """
+    L, num_log = weight.shape
     assert num_phy >= num_log
     device = weight.device
 
-    phy2log_list = []
-    rank_list = []
-    logcnt_list = []
-
-    exp_ids = torch.arange(num_log, dtype=torch.int64, device=device)
-    for i in range(n):
-        w = weight[i]  # [num_log], float CPU
-        counts = _waterfill_counts_row(w, num_phy)  # int64
-
-        # One-step replication fix-up: move 1 replica from donor (max avg) to
-        # receiver (min avg) iff it strictly reduces the max average load.
-        if num_log > 1:
-            counts_safe = torch.clamp(counts, min=1)
-            avg = w / counts_safe.to(w.dtype)
-            donor = int(torch.argmax(avg).item())
-            receiver = int(torch.argmin(avg).item())
-            if donor != receiver and counts[donor] > 1:
-                counts_try = counts.clone()
-                counts_try[donor] -= 1
-                counts_try[receiver] += 1
-                avg_new = w / counts_try.to(w.dtype)
-                if float(avg_new.max().item()) + 1e-9 < float(avg.max().item()):
-                    counts = counts_try  # accept improvement
-
-        logcnt_list.append(counts)
-
-        # Build phy2log and rank (contiguous blocks per logical expert)
-        phy2log_i = torch.repeat_interleave(exp_ids, counts)
-        # ranks: 0..count-1 for each expert
+    phy2log_rows = []
+    rank_rows = []
+    cnt_rows = []
+    ids = torch.arange(num_log, dtype=torch.int64, device=device)
+
+    for i in range(L):
+        w = weight[i].float()
+        counts = _allocate_replicas_row(w, num_phy)
+        cnt_rows.append(counts)
+
+        # Build phy2log and rank with contiguous blocks
+        phy2log_i = torch.repeat_interleave(ids, counts)
         starts = torch.cumsum(counts, dim=0) - counts
         arange_phy = torch.arange(num_phy, dtype=torch.int64, device=device)
         rank_i = arange_phy - torch.repeat_interleave(starts, counts)
 
-        phy2log_list.append(phy2log_i)
-        rank_list.append(rank_i)
-
-    phy2log = torch.stack(phy2log_list, dim=0)
-    rank = torch.stack(rank_list, dim=0)
-    logcnt = torch.stack(logcnt_list, dim=0)
+        phy2log_rows.append(phy2log_i)
+        rank_rows.append(rank_i)
+
+    phy2log = torch.stack(phy2log_rows, dim=0)
+    rank = torch.stack(rank_rows, dim=0)
+    logcnt = torch.stack(cnt_rows, dim=0)
     return phy2log, rank, logcnt
 
 
-def pack_diverse_heap(
-    weights: torch.Tensor,
-    labels: torch.Tensor,
-    num_packs: int
-) -> tuple[torch.Tensor, torch.Tensor]:
-    """
-    Diversity-aware heap-like greedy packing with exact capacity per pack,
-    plus a tiny k=2 refinement swap between heaviest and lightest packs.
-
-    Parameters:
-        weights: [X, n], float CPU
-        labels:  [X, n], int64 CPU
-        num_packs: number of packs
-
-    Returns:
-        pack_index: [X, n], assigned pack id per item
-        rank_in_pack: [X, n], position within pack
-    """
-    num_layers, n_items = weights.shape
-    assert n_items % num_packs == 0
-    cap = n_items // num_packs
-
-    if cap == 1:
-        # Each pack gets exactly one item
-        idx = torch.arange(n_items, dtype=torch.int64, device=weights.device)
-        pack_index = (idx % num_packs).expand(num_layers, n_items).clone()
+def pack_groups_to_nodes_zigzag(weight: torch.Tensor, num_packs: int) -> Tuple[torch.Tensor, torch.Tensor]:
+    """
+    Pack expert groups to nodes using per-row sorted indices and zigzag packing.
+    This enforces exact capacity and is extremely fast.
+    """
+    L, num_groups = weight.shape
+    assert num_groups % num_packs == 0
+    if num_groups // num_packs == 1:
+        pack_index = torch.arange(num_groups, dtype=torch.int64, device=weight.device).expand(L, num_groups)
         rank_in_pack = torch.zeros_like(pack_index, dtype=torch.int64)
         return pack_index, rank_in_pack
 
-    pack_index = torch.full((num_layers, n_items), -1, dtype=torch.int64, device=weights.device)
-    rank_in_pack = torch.full_like(pack_index, -1)
-
-    sorted_idx_all = weights.sort(dim=-1, descending=True).indices  # [X, n]
-
-    for i in range(num_layers):
-        row_w = weights[i]
-        row_labels = labels[i]
-        sorted_idx = sorted_idx_all[i].tolist()
-
-        loads = [0.0] * num_packs
-        counts = [0] * num_packs
-        label_counts = [defaultdict(int) for _ in range(num_packs)]
-        pack_groups = [[] for _ in range(num_packs)]  # items per pack
-
-        # Greedy assignment with diversity-aware tie-breaking
-        for g in sorted_idx:
-            lab = int(row_labels[g].item())
-            wv = float(row_w[g].item())
-
-            best_p = None
-            best_key = None
-            for p in range(num_packs):
-                if counts[p] >= cap:
-                    continue
-                rep = label_counts[p].get(lab, 0)
-                key = (rep, loads[p], counts[p])
-                if best_key is None or key < best_key:
-                    best_key = key
-                    best_p = p
-
-            pack_index[i, g] = best_p
-            rank_in_pack[i, g] = counts[best_p]
-            counts[best_p] += 1
-            loads[best_p] += wv
-            label_counts[best_p][lab] += 1
-            pack_groups[best_p].append(g)
-
-        # One micro 2-opt refinement between heaviest and lightest packs
-        if num_packs >= 2:
-            h = max(range(num_packs), key=lambda k: loads[k])
-            l = min(range(num_packs), key=lambda k: loads[k])
-            if pack_groups[h] and pack_groups[l]:
-                diff = loads[h] - loads[l]
-                if diff > 0:
-                    # Select top-2 from heavy and bottom-2 from light
-                    h_idx_tensor = torch.tensor(pack_groups[h], dtype=torch.int64, device=row_w.device)
-                    l_idx_tensor = torch.tensor(pack_groups[l], dtype=torch.int64, device=row_w.device)
-                    h_w = row_w[h_idx_tensor]
-                    l_w = row_w[l_idx_tensor]
-                    k = min(2, h_w.numel(), l_w.numel())
-                    if k > 0:
-                        top_h = torch.topk(h_w, k).indices
-                        bot_l = torch.topk(-l_w, k).indices  # bottom-k
-                        other_max = max([loads[p] for p in range(num_packs) if p != h and p != l], default=float("-inf"))
-                        other_min = min([loads[p] for p in range(num_packs) if p != h and p != l], default=float("inf"))
-                        cur_imb = max(loads) - min(loads)
-
-                        best = None
-                        best_vals = None
-                        for ai in top_h.tolist():
-                            wa = float(h_w[ai].item())
-                            a_item = int(h_idx_tensor[ai].item())
-                            for bi in bot_l.tolist():
-                                wb = float(l_w[bi].item())
-                                b_item = int(l_idx_tensor[bi].item())
-                                new_h = loads[h] - wa + wb
-                                new_l = loads[l] - wb + wa
-                                new_max = max(new_h, new_l, other_max)
-                                new_min = min(new_h, new_l, other_min)
-                                new_imb = new_max - new_min
-                                if new_imb + 1e-9 < cur_imb:
-                                    if best is None or new_imb < best_vals[0]:
-                                        best = (ai, bi, a_item, b_item, wa, wb)
-                                        best_vals = (new_imb, new_h, new_l)
-
-                        # Apply best swap if any improvement found
-                        if best is not None:
-                            ai, bi, a_item, b_item, wa, wb = best
-                            # Update loads
-                            loads[h] = loads[h] - wa + wb
-                            loads[l] = loads[l] - wb + wa
-                            # Swap membership
-                            pack_groups[h][ai] = b_item
-                            pack_groups[l][bi] = a_item
-                            # Update indices
-                            pack_index[i, a_item] = l
-                            pack_index[i, b_item] = h
-                            # Update ranks only for affected packs
-                            for r, g in enumerate(pack_groups[h]):
-                                rank_in_pack[i, g] = r
-                            for r, g in enumerate(pack_groups[l]):
-                                rank_in_pack[i, g] = r
-
-    return pack_index, rank_in_pack
+    sorted_idx = weight.float().sort(-1, descending=True).indices
+    return _zigzag_packing_from_sorted_indices(sorted_idx, num_packs)
 
 
 def rebalance_experts_hierarchical(
     weight: torch.Tensor,
     num_physical_experts: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ):
     """
-    Parameters:
-        weight: [num_moe_layers, num_logical_experts]
-        num_physical_experts: number of physical experts after replication
-        num_groups: number of expert groups
-        num_nodes: number of server nodes, where the intra-node network
-        (e.g, NVLink) is faster
-        num_gpus: number of GPUs, must be a multiple of `num_nodes`
-
-    Returns:
-        physical_to_logical_map: [num_moe_layers, num_physical_experts]
-        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]
-        logical_count: [num_moe_layers, num_logical_experts]
+    Hierarchical policy:
+      1) Pack logical groups to nodes (zigzag)
+      2) Allocate replicas within nodes via apportionment
+      3) Place physical replicas to GPUs within nodes (interleaved zigzag + refinement)
     """
     num_layers, num_logical_experts = weight.shape
     assert num_logical_experts % num_groups == 0
     group_size = num_logical_experts // num_groups
     assert num_groups % num_nodes == 0
     groups_per_node = num_groups // num_nodes
     assert num_gpus % num_nodes == 0
     assert num_physical_experts % num_gpus == 0
     phy_experts_per_gpu = num_physical_experts // num_gpus
 
     def inverse(perm: torch.Tensor) -> torch.Tensor:
-        inv = torch.empty_like(perm)
-        inv.scatter_(
-            1,
-            perm,
-            torch.arange(perm.size(1), dtype=torch.int64,
-                         device=perm.device).expand(perm.shape),
-        )
-        return inv
-
-    # Step 1: pack groups to nodes (capacity-aware LPT)
+        return _inverse_rows(perm)
+
+    # Step 1: groups -> nodes by zigzag packing on group token sums
     tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)
-    group_pack_index, group_rank_in_pack = balanced_packing(
-        tokens_per_group, num_nodes)
-    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *
-                 group_size).unsqueeze(-1) +
-                torch.arange(group_size,
-                             dtype=torch.int64,
-                             device=group_pack_index.device)).flatten(-2)
+    group_pack_index, group_rank_in_pack = pack_groups_to_nodes_zigzag(tokens_per_group, num_nodes)
+
+    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +
+                torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)
     mlog2log = inverse(log2mlog)
 
-    # Step 2: replicate within nodes using water-filling
-    # Reorder to meta-logical within nodes: [num_layers * num_nodes, num_logical_experts // num_nodes]
-    tokens_per_mlog = weight.gather(-1, mlog2log).view(
-        -1, num_logical_experts // num_nodes)
-
-    phy2mlog, phyrank, mlogcnt = replicate_experts_waterfill(
+    # Step 2: replicate within nodes using apportionment
+    tokens_per_mlog = weight.gather(-1, mlog2log).view(-1, num_logical_experts // num_nodes)
+    phy2mlog, phyrank, mlogcnt = replicate_experts_apportion(
         tokens_per_mlog, num_physical_experts // num_nodes
     )
 
-    # Step 3: pack physical experts to GPUs in each node with diversity-aware heap
+    # Step 3: place physical experts to GPUs within nodes using interleaved zigzag + refinement
     tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)
-    # each row corresponds to a (layer, node)
     gpus_per_node = num_gpus // num_nodes
-    pack_index, rank_in_pack = pack_diverse_heap(
+    pack_index, rank_in_pack = interleaved_zigzag_packing_with_refine(
         tokens_per_phy, phy2mlog, gpus_per_node
     )
     phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack
     pphy2phy = inverse(phy2pphy)
 
     pphy2mlog = phy2mlog.gather(-1, pphy2phy)  # [num_layers * num_nodes, num_phy_per_node]
     # convert back to global logical indexing
-    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(
-        0,
-        num_logical_experts,
-        num_logical_experts // num_nodes,
-        device=group_pack_index.device,
-    ).view(1, -1, 1)).flatten(-2)
+    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) +
+                 torch.arange(0, num_logical_experts, num_logical_experts // num_nodes,
+                              device=group_pack_index.device).view(1, -1, 1)).flatten(-2)
     pphy2log = mlog2log.gather(-1, pphy2mlog)
     pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)
     logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)
     return pphy2log, pphyrank, logcnt
 
 
 def rebalance_experts(
     weight: torch.Tensor,
     num_replicas: int,
     num_groups: int,
     num_nodes: int,
     num_gpus: int,
 ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """
     Entry point for expert-parallelism load balancer.
 
     Parameters:
         weight: [layers, num_logical_experts], the load statistics for all
             logical experts
         num_replicas: number of physical experts, must be a multiple of
             `num_gpus`
         num_groups: number of expert groups
         num_nodes: number of server nodes, where the intra-node network
             (e.g, NVLink) is faster
         num_gpus: number of GPUs, must be a multiple of `num_nodes`
 
     Returns:
-        physical_to_logical_map: [layers, num_replicas], the expert index of
-            each replica
-        logical_to_physical_map: [layers, num_logical_experts, X], the replica
-            indices for each expert
-        expert_count: [layers, num_logical_experts], number of physical
-            replicas for each logical expert
+        physical_to_logical_map: [layers, num_replicas]
+        logical_to_physical_map: [layers, num_logical_experts, X]
+        expert_count: [layers, num_logical_experts]
     """
     num_layers, num_logical_experts = weight.shape
     weight = weight.float().cpu()
     if num_groups % num_nodes == 0:
-        # hierarchical policy
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
-            weight, num_replicas, num_groups, num_nodes, num_gpus)
+            weight, num_replicas, num_groups, num_nodes, num_gpus
+        )
     else:
-        # global policy (treat as single node)
         phy2log, phyrank, logcnt = rebalance_experts_hierarchical(
-            weight, num_replicas, 1, 1, num_gpus)
-
+            weight, num_replicas, 1, 1, num_gpus
+        )
+
+    # Build logical -> physical map
     num_redundant_experts = num_replicas - num_logical_experts
     maxlogcnt = num_redundant_experts + 1
     log2phy: torch.Tensor = torch.full(
-        (num_layers, num_logical_experts, maxlogcnt),
-        -1,
-        dtype=torch.int64,
-        device=logcnt.device,
+        (num_layers, num_logical_experts, maxlogcnt), -1, dtype=torch.int64, device=logcnt.device
     )
-    # scatter physical indices into logical->physical map
     log2phy.view(num_layers, -1).scatter_(
         -1,
         phy2log * maxlogcnt + phyrank,
-        torch.arange(num_replicas, dtype=torch.int64,
-                     device=log2phy.device).expand(num_layers, -1),
+        torch.arange(num_replicas, dtype=torch.int64, device=log2phy.device).expand(num_layers, -1),
     )
     return phy2log, log2phy, logcnt
 
 
 # EVOLVE-BLOCK-END
 
 
 # This part remains fixed (not evolved)
 def run_eplb(weight: torch.Tensor, num_replicas: int, num_groups: int,
              num_nodes: int, num_gpus: int):
     """Run the expert parallelism load balancer"""
     phy2log, log2phy, logcnt = rebalance_experts(
         weight, num_replicas, num_groups, num_nodes, num_gpus
     )
     return phy2log, log2phy, logcnt
 
 
 __all__ = ["rebalance_experts", "run_eplb"]