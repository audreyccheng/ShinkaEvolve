{
  "unprocessed_programs": [],
  "meta_summary": "**Program Name: Hierarchical Expert Load Balancer for vLLM**\n- **Implementation**: Implements a greedy, hierarchical rearrangement using PyTorch: groups are packed to nodes, experts replicated within nodes, and physical experts packed to GPUs via a balanced_packing heuristic, with permutations managed by scatter/gather and an inverse mapping. Computation is performed primarily on CPU (weight.float().cpu()) with per-layer greedy choices and fixed replica counts used to build phy\u2192log, log\u2192phy, and expert count maps.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.0 over 5 workloads).\n- **Feedback**: The approach is very fast due to simple greedy packing and CPU-based tensor ops, but yields only moderate load balance. Hierarchical constraints and per-layer greedy decisions (e.g., weight/logcnt replication and group-first packing) likely limit balancedness compared to more global or optimal assignments.\n**Program Identifier:** Generation 0 - Patch Name initial_program - Correct Program: True\n\n**Program Name: Hierarchical EPLB with diversity-aware packing**\n- **Implementation**: Implements a three-stage hierarchical strategy: groups\u2192nodes via balanced packing, per-node \u201cwater-filling\u201d replication, and diversity-aware greedy assignment of replicas to GPUs that minimizes label collocation while equalizing per-pack load; permutation inverses and scatter build physical\u2194logical maps. Trivial/fast paths (e.g., one item per pack, no-duplicate labels) and CPU-friendly loops via list conversions keep overhead low.\n- **Performance**: Combined score 0.66 (balancedness 0.3111, speed 1.00 across 5 workloads); all validations pass.\n- **Feedback**: The diversity-aware step reduces hotspotting of identical labels, but the equal-items-per-pack constraint and greedy choices yield only moderate load balance. Efficient early exits and lightweight loops drive the top speed score.\n**Program Identifier:** Generation 1 - Patch Name diversity_aware_gpu_packing - Correct Program: True\n\n**Program Name: Hierarchical Expert Parallelism Load Balancer**\n- **Implementation**: Uses greedy sort-based bin packing per layer with a small local 2-opt-style refinement between heaviest/lightest packs. Replication is selected via argmax(weight/logcnt) within nodes, then replicas are packed to GPUs; final mappings are built via gather/scatter and inverse permutations, operating on CPU tensors for simplicity.\n- **Performance**: Combined score 0.65 (balancedness 0.306805, speed 1.00 across 5 workloads); all validations passed.\n- **Feedback**: Excellent speed shows the greedy + limited refinement is lightweight, but moderate balancedness indicates room for stronger global balancing. More refinement iterations or smarter swap selection could improve balance; hierarchical policy is used when groups align with nodes, otherwise a global fallback is applied.\n**Program Identifier:** Generation 2 - Patch Name local_swap_refine_in_packing - Correct Program: True\n\n**Program Name: Hierarchical Serpentine Expert Load Balancer**\n- **Implementation**: Uses vectorized serpentine (snake) packing per layer to evenly assign sorted experts to packs, and a d'Hondt-like greedy replicator that allocates extra replicas by weight/count. A hierarchical pipeline packs groups to nodes, replicates within nodes, then packs physical experts to GPUs using amortized loads, implemented with PyTorch scatter/gather and permutation inverses (with a fast path when groups_per_pack==1).\n- **Performance**: Combined score 0.63 (balancedness 0.254285, speed 1.00; 5 workloads).\n- **Feedback**: Fully vectorized design achieves maximal speed but modest balancedness, suggesting the greedy replication plus snake packing underperform under skewed loads. Correctness is solid (passes all tests); balancedness may improve with local swap/refinement or more global optimization.\n**Program Identifier:** Generation 3 - Patch Name eplb_snake_pipeline - Correct Program: True\n\n**Program Name: Hierarchical EPLB with Diversity-Aware Packing**\n- **Implementation**: Uses a hierarchical three-stage pipeline: (1) greedy group-to-node packing with up to 4 heaviest/lightest swaps, (2) per-node expert replication via an incremental ratio (weight/count) greedy update, and (3) GPU placement with diversity-aware packing that prioritizes label spread then projected load. Mapping inverses and placements are built via scatter/gather for efficiency, with selective CPU sorting and minimal refinement to keep runtime low.\n- **Performance**: Combined score 0.65 (balancedness 0.3068, speed 1.00 over 5 workloads).\n- **Feedback**: The implementation is correct and passes all validation tests. It prioritizes speed\u2014greedy choices and limited local refinement yield maximal runtime performance but only moderate balance; diversity-aware packing helps reduce hotspotting, though the capped refinements likely limit balancedness gains.\n**Program Identifier:** Generation 4 - Patch Name diverse_pack_projload_and_incremental_ratio - Correct Program: True\n\n**Program Name: Hierarchical EPLB with diversity-aware packing**\n- **Implementation**: Uses a hierarchical strategy: groups are packed to nodes via balanced_packing, logical experts are replicated with Hamilton\u2019s method (largest remainder) in replicate_experts, and physical experts are assigned to GPUs using a diversity-aware greedy packer that spreads identical labels to reduce hotspotting. Mappings are built with vectorized torch ops (scatter/inverse), and the top-level casts weights to CPU for consistent, fast execution.\n- **Performance**: Combined score 0.64 (balancedness 0.2897, speed 1.0 across 5 workloads); all validation tests pass.\n- **Feedback**: The approach is very fast due to simple greedy policies and vectorized construction of mappings, but balancedness is modest, indicating the greedy packing and proportional replication may under-serve heavy experts or miss global optimality. Consider stronger global balancing heuristics or improved tie-breaking beyond label diversity to raise balancedness without sacrificing speed.\n**Program Identifier:** Generation 5 - Patch Name proportional_quota_replication - Correct Program: True\n\n**Program Name: Hierarchical EPLB with greedy packing and swap refinement**\n- **Implementation**: Implements a hierarchical load balancer: packs logical expert groups to nodes, greedily replicates experts within nodes, then balances physical experts across GPUs. The core balanced_packing uses per-layer greedy assignment with fixed per-pack item counts and a lightweight single-swap refinement; replicate_experts greedily allocates by current load (weight/logcnt), and mappings are built via scatter/gather operations.\n- **Performance**: Combined score 0.66 (balancedness 0.3111, speed 1.0 over 5 workloads).\n- **Feedback**: The approach is very fast due to simple greedy passes and minimal refinement, but achieves only moderate balance. Single-swap local improvement and equal-cardinality packing likely cap balancedness; nevertheless, the solution is correct and passes all validation tests.\n**Program Identifier:** Generation 6 - Patch Name single_swap_refinement - Correct Program: True\n\n**Program Name: Hierarchical EPLB with Greedy Packing and D\u2019Hondt Replication**\n- **Implementation**: Uses a capacity-constrained greedy packer (sort-by-weight, place into lightest pack) with a single heaviest\u2194lightest swap refinement and deterministic in-pack ranks; replica allocation is a vectorized D\u2019Hondt-style water-filling. Hierarchical mapping packs groups to nodes and GPUs via permutation ops (including a scatter-based inverse), with replication done per node and final mappings produced by gather/scatter.\n- **Performance**: Combined score 0.66 (balancedness 0.3111, speed 1.00 across 5 workloads).\n- **Feedback**: The lightweight greedy packing favors speed but leaves noticeable imbalance; increasing refinement steps or applying smarter swap/refinement strategies could raise balancedness with modest cost. Vectorized allocation and permutation-based layout keep the approach simple and fast, and all validation tests pass.\n**Program Identifier:** Generation 7 - Patch Name moe_eplb_modular_refine - Correct Program: True\n\n**Program Name: Hierarchical EPLB with Greedy Packing and Swap Refinement**\n- **Implementation**: Uses per-layer greedy balanced_packing (descending sort with capacity-constrained assignment) plus up to two bounded heavy-light swaps to reduce imbalance while reassigning ranks. Applies a hierarchical strategy: pack groups to nodes, greedily replicate experts via weight/logcnt, then pack physical experts to GPUs; mappings are constructed via gather/scatter and inverse permutations.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.00 across 5 workloads); passes all validation tests.\n- **Feedback**: Excellent speed from lightweight greedy steps and limited refinement, but modest balancedness suggests under-optimization of load distribution. More aggressive or global refinement could improve balance at the expense of speed.\n**Program Identifier:** Generation 8 - Patch Name multi_swap_refinement_balanced_packing - Correct Program: True\n\n**Program Name: RR-WF Packing with Hare Replication**\n- **Implementation**: Implements Round-Robin Waterfilling packing to assign items to packs in capacity rounds, with a lightweight post-swap refinement; uses Hare (Hamilton) largest-remainder to allocate replicas proportionally, then a hierarchical pipeline (group\u2192node packing, intra-node replication, GPU packing) built with vectorized torch gathers/scatters and an inverse-permutation utility. The entry rebalance forces weight to float().cpu() and selects hierarchical when groups % nodes == 0, otherwise a global fallback.\n- **Performance**: Combined score 0.64 (balancedness 0.289706, speed 1.000000 over 5 workloads); program is correct and passes all validation tests.\n- **Feedback**: Excellent speed stems from simple per-round vectorized ops, stable sorting, and minimal refinement; per-row loops (remainders/topk, final swap) are lightweight. Balance is improved over naive strategies but remains moderate, suggesting headroom beyond the single-pass RR-WF plus optional one-swap refinement.\n**Program Identifier:** Generation 9 - Patch Name rr_waterfill_hare - Correct Program: True\n\n**Program Name: Hierarchical Water-Fill + Diverse Packing EPLB**\n- **Implementation**: Implements a three-stage hierarchical balancer: capacity-aware LPT group-to-node packing, intra-node water-filling replica allocation via binary search with greedy extras, and GPU placement using a diversity-aware heap that minimizes (label repeats, load, fill). It builds contiguous phy2log/rank with repeat_interleave, inverts mappings where needed, and scatters logical\u2192physical indices; falls back to a single-node policy when groups aren\u2019t node-aligned.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000) across 5 workloads; all validations passed.\n- **Feedback**: The approach is very fast due to lightweight CPU sorts, vectorized gathers/scatters, and simple greedy heuristics, but achieves only moderate balance. Balancedness likely suffers from the greedy capacity-constrained packers and per-node isolation; tuning the packing objective or more global optimization could improve load balance.\n**Program Identifier:** Generation 10 - Patch Name waterfill_diverse_heap - Correct Program: True\n\n**Program Name: Hierarchical Diverse Expert Load Balancer**\n- **Implementation**: Implements hierarchical packing of logical experts to nodes and GPUs: greedy capacity-constrained packing with K=2 heavy\u2013light swaps, diversity-aware packing to spread identical labels, and a hybrid D\u2019Hondt\u2192Sainte-Lagu\u00eb replication strategy. Uses vectorized gathers/scatters and invert-permutation helpers to build physical\u2194logical maps and replica ranks.\n- **Performance**: Combined score 0.65 (balancedness 0.304918, speed 1.00 over 5 workloads).\n- **Feedback**: All tests passed; packing and replication are fast, yielding top speed but only moderate balance. The limited K=2 refinement and local (per-node) swaps likely cap balancedness despite diversity-aware placement.\n**Program Identifier:** Generation 11 - Patch Name k2_swap_tail_hybrid - Correct Program: True\n\n**Program Name: Hierarchical Water-Filling EPLB**\n- **Implementation**: Implements a hierarchical policy: packs expert groups to nodes with capacity-aware LPT, performs per-node water-filling replication with a one-step donor\u2192receiver fix, then assigns to GPUs via a diversity-aware heap plus a k=2 swap refinement. Builds physical\u2192logical and logical\u2192physical maps via scatter/gather with contiguous replica ranks.\n- **Performance**: Combined score 0.66 (balancedness 0.311, speed 1.00 over 5 workloads); passes all validation tests.\n- **Feedback**: Runtime is excellent, indicating low overhead from the greedy/heap steps and CPU tensor ops. Balancedness is modest, suggesting the single-step fix and limited 2-opt swap cap equalization, leaving room for deeper refinement if needed.\n**Program Identifier:** Generation 12 - Patch Name k2_swap_tail_fixup - Correct Program: True\n\n**Program Name: Hierarchical EPLB with greedy packing**\n- **Implementation**: Implements a hierarchical expert-parallel balancer: groups are greedily packed to nodes with a bounded multi-swap refinement (max_swaps=2), replicas are allocated per layer via a D\u2019Hondt-like scheme with a one-step donor\u2192receiver fix-up, and physical experts are then packed to GPUs; mappings use gather/scatter with inverse permutations for determinism. Packing operates on CPU tensors with stable rank reassignment to maintain per-pack ordering.\n- **Performance**: Combined score 0.66 (balancedness 0.311, speed 1.000 over 5 workloads).\n- **Feedback**: Excellent speed stems from simple greedy heuristics and tightly bounded refinement, but balancedness is moderate, likely limited by the two-swap bound and single-step replica fix-up. Hierarchical placement benefits topology-aware locality when groups are node-divisible; otherwise it falls back to a global policy.\n**Program Identifier:** Generation 13 - Patch Name replication_fixup_one_step - Correct Program: True\n\n**Program Name: Hierarchical EPLB with Greedy Packing and Replication**\n- **Implementation**: Uses per-layer sorted greedy balanced_packing with exact items-per-pack and a bounded (max 2) multi-swap refinement to reduce imbalance. Replication applies a D\u2019Hondt-like greedy allocation plus a one-step donor\u2192receiver fix-up; a hierarchical pipeline packs groups to nodes, replicates within nodes, then packs physical experts to GPUs with inverse maps and stable ranks.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000 over 5 workloads); passes all validation tests.\n- **Feedback**: Excellent speed but moderate balancedness, likely limited by the small swap budget and single-step replication correction. The hierarchical policy preserves locality and determinism; increasing refinement steps could improve balance at some runtime cost.\n**Program Identifier:** Generation 14 - Patch Name one_step_replication_fixup - Correct Program: True\n\n**Program Name: EPLB Hybrid Hierarchical Balancer**\n- **Implementation**: Implements hierarchical expert balancing with a hybrid D\u2019Hondt\u2192Sainte-Lagu\u00eb replica apportionment (plus a one-step fix-up), greedy capacity-constrained packing with k=2 best-improvement swaps, and a diversity-aware GPU assignment tie-breaker. Uses permutation inversion utilities and staged refinement (1 step for groups\u2192nodes, 2 steps for physical\u2192GPU packing).\n- **Performance**: Combined score 0.0; fails all validation tests.\n- **Feedback**: The replica fix-up moves a seat from the highest-average \u201cdonor\u201d to the lowest-average \u201creceiver,\u201d which worsens imbalance (direction should be receiver\u2192donor), likely breaking correctness. Additionally, layout reshaping/gather assumes block-contiguous permutations and the forced .cpu() cast may violate expected device semantics, contributing to failures.\n**Program Identifier:** Generation 15 - Patch Name hydra_k2_diversity_apportion - Correct Program: False\n\n**Program Name: Hierarchical EPLB with Greedy Packing**\n- **Implementation**: Uses capacity-constrained greedy packing over sorted weights with bounded multi-swap refinement; hierarchical grouping packs groups\u2192nodes and replicas\u2192GPUs via inverse-permutation mappings. Replication applies a D\u2019Hondt allocation with a Sainte-Lagu\u00eb tail and a single donor\u2192receiver fix-up, with deterministic rank reassignment.\n- **Performance**: Combined score 0.65; balancedness_score 0.304918, speed_score 1.000000 across 5 workloads.\n- **Feedback**: The limited refinement (refine_steps=1\u20132) and one-step fix-up prioritize speed but leave noticeable imbalance. Increasing swap iterations or applying richer local search could improve balancedness at some runtime cost; all validation tests pass.\n**Program Identifier:** Generation 16 - Patch Name hybrid_sainte_lague_tail_and_stage_refine - Correct Program: True\n\n**Program Name: Hierarchical Expert Parallel Load Balancer**\n- **Implementation**: Implements a hierarchical EPLB: packs expert groups to nodes via greedy balanced_packing with bounded multi-swap refinement, replicates experts using a hybrid D\u2019Hondt (bulk) + Sainte-Lagu\u00eb (tail) allocator plus a one-step donor\u2192receiver fix-up, then packs physical replicas to GPUs. Uses CPU-based sorting/scatter and stable rank reassignment, with inverse mappings for logical/physical indices.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.000000 over 5 workloads); all validation tests passed.\n- **Feedback**: Excellent speed indicates the greedy, bounded-swap design and CPU-side ops keep overhead low. Balancedness is moderate, likely constrained by the single-step replica fix-up and small refine_steps; allowing more refinement or multi-move adjustments could improve load balance.\n**Program Identifier:** Generation 17 - Patch Name hybrid_sainte_lague_tail_and_stage_specific_refine - Correct Program: True\n\n**Program Name: Hierarchical Diversity-Aware EPLB**\n- **Implementation**: Performs hierarchical load balancing: packs expert groups to nodes with greedy balanced packing, replicates experts per node by max normalized load, then assigns replicas to GPUs using diversity-aware greedy packing with a single k=2 best-improvement swap; mappings use vectorized gather/scatter for efficiency. Includes a fast path when labels are unique and tie-breakers to reduce label duplication per pack.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.00 over 5 workloads).\n- **Feedback**: Passes all validation tests; the diversity-aware greedy plus minimal refinement achieves excellent speed but only moderate balance. More aggressive refinement (multi-swap or iterative improvement) could raise balancedness, though current design already reduces hotspotting from replica co-location.\n**Program Identifier:** Generation 18 - Patch Name k2_micro_swap_diverse - Correct Program: True\n\n**Program Name: Hierarchical EPLB with greedy packing and hybrid apportionment**\n- **Implementation**: Implements a hierarchical load balancer: packs groups to nodes, replicates experts within nodes using a hybrid D\u2019Hondt (bulk) + Sainte-Lagu\u00eb (tail) apportionment with a one-step fix-up, then assigns physical experts to GPUs via balanced packing. The packing is a sorted greedy placement with capacity constraints plus bounded local multi-swap refinement; mappings are constructed with gather/scatter and inverse permutations.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.0 over 5 workloads).\n- **Feedback**: Correct and passes all validation tests. Excellent speed due to bounded refinement and lightweight apportionment, while moderate balancedness indicates potential gains from higher refine_steps or multi-iteration replication adjustments.\n**Program Identifier:** Generation 19 - Patch Name refine_packing_and_hybrid_replication - Correct Program: True\n\n**Program Name: Hierarchical Expert Load Balancer with Hybrid Replication**\n- Implementation: Greedy capacity-constrained packing with per-layer targeted heavy-light swaps (searchsorted-based) determines group/node/GPU placement; a hierarchical layout uses permutation inverses to keep groups contiguous and then packs per-GPU experts after replication. Replication employs hybrid apportionment (D\u2019Hondt bulk + Sainte-Lagu\u00eb tail) with a one-step donor\u2192receiver fix-up; deterministic intra-pack ranks and vectorized torch ops are used throughout.\n- Performance: Combined score 0.66 (balancedness 0.311848, speed 1.000, 5 workloads).\n- Feedback: Extremely fast but only moderately balanced; limited refinement steps and the single-step replication fix-up likely cap balancedness. Increasing refine_steps or enabling multi-swap/multi-move adjustments could improve balance at some cost in speed.\n**Program Identifier:** Generation 20 - Patch Name moe_eplb_crossover_capacityperm - Correct Program: True\n\n**Program Name: Hierarchical Water-Filling Expert Load Balancer**\n- **Implementation**: Uses hierarchical staging: capacity-aware LPT to pack groups to nodes, water-filling replication per node with a top-2 donor/bottom-2 receiver fix-up, and diversity-aware heap GPU packing with a k=2 swap refinement. Physical\u2194logical maps and ranks are built via repeat_interleave and scatter/gather with exact per-pack capacities.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000; 5 workloads).\n- **Feedback**: Correct and fast, but balancedness is only moderate, suggesting the greedy packing and limited local refinements reduce peaks but don\u2019t fully equalize load. Stronger global adjustments could further improve balance without sacrificing speed.\n**Program Identifier:** Generation 21 - Patch Name top2_replica_fix_and_tieaware_pack - Correct Program: True\n\n**Program Name: Hierarchical EPLB with greedy packing and quota replication**\n- **Implementation**: Uses a capacity-constrained greedy packer with bounded k=2 best-improvement swaps and an adaptive second swap; replication allocates via a D\u2019Hondt bulk phase followed by per-row Sainte-Lagu\u00eb vs Huntington\u2013Hill selection and a single-move donor\u2192receiver fix-up. Hierarchical policy packs groups to nodes and physical experts to GPUs, with fast permutation inverses and scatter-based map construction and deterministic in-pack ranking.\n- **Performance**: Combined score 0.65 (balancedness 0.306257, speed 1.000000 across 5 workloads).\n- **Feedback**: Minimal refinement and simple scans preserve peak speed but cap balance quality; adaptive tail sizing (via coefficient of variation) and hierarchical staging improve robustness. The implementation is correct and passes all validation tests.\n**Program Identifier:** Generation 22 - Patch Name eplb_k2swap_hybrid_apportion - Correct Program: True\n\n**Program Name: Hierarchical EPLB with Hybrid Replication**\n- **Implementation**: Greedy capacity-constrained packing with deterministic in-pack ranking and a bounded k=2 heaviest-lightest swap refinement, combined with hierarchical node/group/GPU placement. Replication uses a hybrid D\u2019Hondt (bulk) + Sainte-Lagu\u00eb (tail) apportionment with a single best-improving donor\u2192receiver fix-up to reduce peak average load.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.0 across 5 workloads); all validations pass.\n- **Feedback**: The algorithm is very fast due to greedy packing and limited local refinement, but balancedness is modest, suggesting the k=2 swap and single fix-up move under-correct skew. Increasing refine_steps, considering larger swap candidate sets, or multiple fix-up iterations could improve balance at some cost to speed.\n**Program Identifier:** Generation 23 - Patch Name adaptive_refine_and_replication_fixup_k2 - Correct Program: True\n\n**Program Name: Hierarchical EPLB with k-swap refinement**\n- **Implementation**: Implements a hierarchical three-stage balancer: groups-to-nodes via greedy packing plus bounded k=2 best-improvement swaps, intra-node expert replication using apportionment (D\u2019Hondt bulk + Sainte-Lagu\u00eb/Huntington\u2013Hill tail) with a one-move fix-up, then GPU packing with a diversity-aware tie-break; deterministic ranking and inverse-permutation utilities included. Core loops run on CPU for control flow, with small-k refinement and optional adaptive second swap for improved local balance.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.000000 over 5 workloads); all validations pass.\n- **Feedback**: Achieves maximal speed due to lightweight CPU-driven greedy packing and limited refinement, but balancedness is modest. Increasing refine_steps/k, strengthening swap search, or more aggressive diversity penalties could improve balance at some runtime cost.\n**Program Identifier:** Generation 24 - Patch Name dhh_diverse_k2_refine - Correct Program: True\n\n**Program Name: EPLB: Hierarchical Hybrid Load Balancer**\n- **Implementation**: Uses a CPU-friendly greedy balanced_packing with per-layer sort, capacity-aware assignment, and bounded heaviest-lightest refinement swaps; replicate_experts applies a hybrid apportionment (D\u2019Hondt bulk + Sainte-Lagu\u00eb tail) plus a one-step donor\u2192receiver fix-up. Hierarchical rebalancing packs groups to nodes, replicates within nodes, then packs replicas to GPUs, with permutation inverses and rank-aware log2phy scatter for final maps.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.0 over 5 workloads); all validation tests pass.\n- **Feedback**: Excellent speed stems from greedy CPU paths and limited refinement, while balancedness is modest likely due to small refine_steps and single-step replication fix-up. The hierarchical policy favors locality and stability but may under-balance globally in some distributions.\n**Program Identifier:** Generation 25 - Patch Name moe_eplb_hybrid_refine - Correct Program: True\n\n**Program Name: Hierarchical Expert Parallelism Load Balancer (EPLB)**\n- **Implementation**: Staged pipeline: GroupPacker does LPT-style greedy packing with a bounded k=2 best-improvement swap; ReplicaAllocator apportions replicas via D\u2019Hondt bulk then A/B between Sainte-Lagu\u00eb and Huntington\u2013Hill with a single donor\u2192receiver correction; GpuPlacer uses greedy projected-load assignment with a diversity-aware tie-breaker and adaptive k=2 refinement. Hierarchical mapping packs groups to nodes, allocates within nodes, then places to GPUs, with a global fallback when grouping doesn\u2019t align with nodes.\n- **Performance**: Combined score 0.65 (balancedness 0.299, speed 1.000) over 5 workloads; all validations passed.\n- **Feedback**: Perfect speed shows the heuristics are lightweight and implementation efficient, while modest balancedness suggests the greedy choices and single bounded swaps leave residual imbalance. The apportionment tail improves peak per-replica averages and the diversity tie-breaker helps spread replicas, but limited refinement likely caps balancing quality.\n**Program Identifier:** Generation 26 - Patch Name eplb_pipeline_apportion_refine - Correct Program: True\n\n**Program Name: Hierarchical Water-Filling Expert Load Balancer**\n- **Implementation**: Uses a hierarchical 3-step pipeline: capacity-aware LPT packing of groups to nodes, per-(layer,node) water-filling replication with a small donor\u2192receiver fix-up, and diversity-aware heap packing to GPUs with a unique-label fast path and a single 2-opt swap; mappings are built via vectorized torch gather/scatter with contiguous ranks per expert. Assumes exact per-pack capacity and enforces at least one replica per logical expert.\n- **Performance**: Combined score 0.66 (balancedness 0.3111, speed 1.00 over 5 workloads); all validation tests passed.\n- **Feedback**: The approach is fast due to simple sorts, limited local refinements, and lightweight CPU tensor ops, but balancedness is modest, likely bounded by the greedy packing and shallow (k\u22642) fix-ups. Deeper refinement or broader swap searches could improve balance at a small runtime cost.\n**Program Identifier:** Generation 27 - Patch Name moe_eplb_waterfill_diverse_refine - Correct Program: True\n\n**Program Name: Hierarchical EPLB with zig-zag packing and water-filling**\n- **Implementation**: Implements a hierarchical balancer: groups are stripe-packed (zig-zag) to nodes, experts replicated in-node via water-filling (binary search + greedy rounding), and replicas stripe-packed to GPUs using per-replica average load; mappings are built with scatter/gather and permutation inverses for deterministic phy2log/log2phy. Runs on CPU tensors, enforces exact per-pack capacity, and ignores refine_steps in packing for simplicity.\n- **Performance**: Combined score 0.66 (balancedness 0.310534, speed 1.000000) over 5 workloads; all validations pass.\n- **Feedback**: Extremely fast but only moderately balanced; the single-pass stripe packing and batched rounding in water-filling (full-round increments before top-k) likely cap balance quality. Iterative refinement or stricter marginal-benefit allocation for remaining replicas could improve balancedness without large runtime penalties.\n**Program Identifier:** Generation 28 - Patch Name waterfill_stripe_moe - Correct Program: True\n\n**Program Name: Hierarchical Expert Load Balancer with K-swap Refinement**\n- **Implementation**: Greedy capacity-constrained packing per row with bounded k=2 best-improvement swaps, plus a diversity-aware tie-break for near-ties. Hierarchical pipeline: pack groups to nodes, replicate via vectorized minimax with a one-move fix-up, then pack replicas to GPUs; outputs deterministic ranks and mappings.\n- **Performance**: Combined score 0.65 (balancedness 0.308909, speed 1.000000 over 5 workloads).\n- **Feedback**: Excellent speed comes from lightweight CPU control flow and vectorized allocation steps, but the small k and swap budget bias toward speed over balance, resulting in modest balancedness. Increasing refine_steps/k or strengthening the post-allocation fix-up could improve balance; the very small diversity penalty likely has limited impact.\n**Program Identifier:** Generation 29 - Patch Name moe_eplb_minimax_replicator - Correct Program: True\n\n**Program Name: Hierarchical EPLB with hybrid replication and k-candidate refinement**\n- **Implementation**: Uses greedy balanced packing with fixed pack sizes and a bounded k=2 swap refinement to reduce per-pack imbalance, while maintaining stable in-pack ranks. Replication is allocated via a hybrid apportionment (D'Hondt bulk, Sainte-Lagu\u00eb tail) plus a single best donor\u2192receiver fix-up; a hierarchical pipeline maps groups\u2192nodes, replicates within nodes, then packs physical experts to GPUs, using gather/scatter inversion for mappings.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.000000 over 5 workloads); all tests pass.\n- **Feedback**: The bounded refinement (k=2, max 2 swaps) and single-move fix-up keep it very fast but limit final balance quality. Increasing swap candidates/iterations or allowing multi-move adjustments could improve balancedness at some cost to speed.\n**Program Identifier:** Generation 30 - Patch Name k2_refine_and_stronger_replication_fix - Correct Program: True\n\n**Program Name: Hierarchical Waterfill and Diverse Packing EPLB**\n- **Implementation**: Uses water-filling replica allocation (binary search + greedy extras) with a small donor\u2192receiver fix-up, hierarchical group-to-node LPT-like packing, and diversity-aware heap GPU placement with label-aware tie-breaking plus bounded 1x1/2x2 swaps; mappings are composed via gather/scatter with inverse permutations. Optimized fast paths exist for trivial capacities and unique labels, and ranks are kept contiguous per expert.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000; 5 workloads).\n- **Feedback**: The approach is very fast due to lightweight CPU tensor ops, constrained candidate sets, and early-exit paths, but achieves only moderate balance. Limited refinement depth (k=2 swaps, two fix-up iterations) and greedy packing likely leave headroom for improved peak smoothing, though the solution is correct and passes all validation.\n**Program Identifier:** Generation 31 - Patch Name light2_and_pair_swap_refine - Correct Program: True\n\n**Program Name: Hierarchical Water-Filling EPLB with Diverse GPU Packing**\n- **Implementation**: Uses hierarchical placement: group-to-node balanced packing (capacity-aware LPT), per-node water-filling replica allocation with a donor\u2192receiver fix-up, and diversity-aware heap GPU packing with a bounded 1x1 swap refinement. Mapping is built via vectorized gather/scatter with contiguous replica ranks and robust edge-case handling (e.g., capacity=1, all-zero weights).\n- **Performance**: Combined score 0.66; balancedness 0.3111, speed 1.0000 across 5 workloads.\n- **Feedback**: Passes all validation tests; excellent speed from greedy heuristics and minimal refinement. Balancedness is modest, likely limited by the single-step donor/receiver tweak and bounded 1x1 swap, indicating potential gains from deeper or iterative refinement.\n**Program Identifier:** Generation 32 - Patch Name replicate_fixup_and_light2_refinement - Correct Program: True\n\n**Program Name: Hierarchical EPLB with hybrid replication**\n- **Implementation**: Implements a hierarchical 3-stage balancer: groups\u2192nodes via greedy packing with 1-step k\u00d7k refinement, intra-node replication using a hybrid D\u2019Hondt/Sainte-Lagu\u00eb (tail selected by simulation) plus a capped 2-move fix-up, and GPU placement via diversity-aware packing with adaptive depth and optional 2\u00d72 swaps. Deterministic ranks and inverse permutations ensure stable layouts; control-flow heavy parts run on CPU.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.00) over 5 workloads; passes all validation tests.\n- **Feedback**: Strong speed stems from bounded k=2 candidate swaps and limited refinement, but balancedness is modest, likely due to 1-step refinement in group packing and disabled 2\u00d72/second-light in the base pass. Increasing refine_steps or enabling broader light-pack candidates/2\u00d72 in balanced_packing should improve balance with minimal impact on speed.\n**Program Identifier:** Generation 33 - Patch Name moe_eplb_dualrefine_tailabc - Correct Program: True\n\n**Program Name: Diversity-aware hierarchical expert load balancer**\n- **Implementation**: Uses hierarchical packing (groups\u2192nodes, replicate within nodes, then pack to GPUs) with water-filling-based replication plus a donor\u2192receiver fix-up, and a diversity-aware balanced packing that spreads identical labels and applies a single 1x1 swap refinement between heavy/light packs. Provides invertible mappings and ranks, with a CPU float normalization at the entry point for consistency.\n- **Performance**: Combined score 0.66 (balancedness_score 0.3111, speed_score 1.0; 5 workloads).\n- **Feedback**: Excellent speed stems from greedy assignment and bounded refinement. Balancedness is moderate; the one-swap refinement and local greedy choices improve hotspots but leave headroom for further imbalance reduction. The program is correct and passed all validation tests.\n**Program Identifier:** Generation 34 - Patch Name waterfill_and_light_pair_refine - Correct Program: True\n\n**Program Name: Hierarchical Expert Load Balancer with Hybrid Replication**\n- **Implementation**: Uses greedy capacity-constrained packing with deterministic in-pack ranking and a targeted heaviest\u2194lightest swap (searchsorted-based) refinement; fast-path for capacity==1. Employs a hierarchical policy (groups\u2192nodes), hybrid D\u2019Hondt then Sainte-Lagu\u00eb replication with a local fix-up move, and diversity-aware GPU packing with label-based tie-breaking and adaptive refinement; permutation inverses and scatters map between logical/meta/global IDs.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.000 across 5 workloads); all validations passed.\n- **Feedback**: Lightweight refinement and greedy heuristics keep runtime excellent but leave balance suboptimal; increasing refinement steps or broader swap neighborhoods could improve balancedness. CPU staging in the diversity-aware step and Python loops did not impact this benchmark\u2019s speed but may be a scalability consideration for larger inputs.\n**Program Identifier:** Generation 35 - Patch Name eplb_hybrid_diverse_fixup - Correct Program: True\n\n**Program Name: Waterfill-Based Hierarchical vLLM Expert Balancer**\n- **Implementation**: Implements hierarchical placement: capacity-aware LPT group-to-node packing, water-filling replica allocation with guarded donor\u2192receiver fix-ups (up to two moves), and diversity-aware heap-based GPU packing with bounded 1x1/2x2 swaps and adaptive refinement; builds physical\u2194logical maps via tensor gather/scatter with exact per-pack capacities and contiguous replica ranks. \n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000 over 5 workloads); all validation tests pass.\n- **Feedback**: The bounded refinement and capped fix-ups prioritize speed but limit balancedness improvements, leaving residual peak load after water-filling. Diversity-aware packing improves label spread, yet the conservative swap policy may under-optimize the global max load across GPUs.\n**Program Identifier:** Generation 36 - Patch Name wf_diverse_refine2x2_adaptive - Correct Program: True\n\n**Program Name: HH+D\u2019Hondt Zigzag Expert Load Balancer**\n- **Implementation**: Hierarchical policy: groups\u2192nodes via deterministic zigzag packing, per-node replication via Huntington\u2013Hill bulk apportionment with a small D\u2019Hondt tail micro-tune and capped donor\u2192receiver fix-ups, and GPU placement via label-interleaved zigzag with bounded heavy/light refinement (including optional 2x2 swaps). Exact pack capacities are enforced, with permutation inverses used to map between logical/physical indices.\n- **Performance**: Achieved a combined score of 0.0 and failed all validation tests.\n- **Feedback**: Results likely violate the expected output contract (e.g., forcing tensors to CPU in rebalance_experts, potential permutation/device mismatches), causing correctness failures. Preserve input device for outputs, verify permutation assumptions for inverse mappings, and ensure the hierarchical remapping and refinement steps produce bitwise-identical outputs to prior versions.\n**Program Identifier:** Generation 37 - Patch Name interleaved_zigzag_apportion - Correct Program: False\n\n**Program Name: EPLB hierarchical expert load balancer**\n- **Implementation**: Uses greedy balanced packing per layer with bounded 1x1 and 2x2 swap refinement, and a hybrid replication scheme (D\u2019Hondt bulk + Sainte-Lagu\u00eb tail) with a small tail auto-tuning and per-row fix-up. Mapping is built hierarchically (groups\u2192nodes\u2192GPUs) with rank-consistent scatter/gather permutations.\n- **Performance**: Combined score 0.66: balancedness 0.311848 and speed 1.00 over 5 workloads; all validation tests pass.\n- **Feedback**: Excellent speed is driven by capped refinement (max_swaps=2) and lightweight candidate sets, but balancedness is modest, likely due to conservative swap limits and limited post-replication adjustments. Increasing swap depth/candidates or strengthening the fix-up stage could improve load balance.\n**Program Identifier:** Generation 38 - Patch Name adaptive_tail_fix_and_refine_2x2_secondlight - Correct Program: True\n\n**Program Name: Hierarchical EPLB with Adaptive Refinement**\n- **Implementation**: Implements a hierarchical expert-parallel load balancer: groups\u2192nodes\u2192GPUs using greedy balanced_packing with adaptive bounded refinement (2\u20133 swaps, 1x1 and optional 2x2 exchanges) and hybrid expert replication (bulk D\u2019Hondt + tail Sainte-Lagu\u00eb) with a conditional second fix-up move. Uses gather/scatter-based inverses and rank maintenance to keep per-pack cardinality constraints while staying mostly on CPU for fast small-k operations.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.0 across 5 workloads).\n- **Feedback**: The program is correct and passes all validation tests. Speed benefits from bounded local search and lightweight replication tweaks, but limited swap depth and narrow candidate sets likely constrain balancedness; allowing deeper/more global refinements or broader candidate selection could improve balance at some runtime cost.\n**Program Identifier:** Generation 39 - Patch Name adaptive_refine_and_two_step_fixup - Correct Program: True\n\n**Program Name: Hierarchical EPLB with adaptive k-swap refinement**\n- **Implementation**: Implements hierarchical expert rebalancing: greedy capacity-constrained packing per row with adaptive k-candidate 1x1 and optional 2x2 swaps, integer water-filling (with D\u2019Hondt continuation) for replication, and diversity-aware GPU packing; deterministic mappings are built via scatter/gather permutations and per-pack ranks. Control-flow-heavy steps run on CPU, with adaptive refinement depth based on imbalance.\n- **Performance**: Combined score 0.65 (balancedness 0.309358, speed 1.000000 over 5 workloads); all validation tests pass.\n- **Feedback**: Maximal speed is driven by bounded refinement and lightweight heuristics, but balancedness is moderate, suggesting under-correction by the limited k and swap budget. Increasing refine_steps/k or enabling more aggressive 2x2 exchanges could improve load balance at a minor speed trade-off.\n**Program Identifier:** Generation 40 - Patch Name waterfill_apportion_moe - Correct Program: True\n\n**Program Name: Hierarchical vLLM Expert Load Balancer**\n- **Implementation**: Three-stage hierarchical EPLB: capacity-aware LPT packs groups to nodes, water-filling allocates replicas with a guarded donor\u2192receiver fix-up (\u22642 moves/row), and diversity-aware GPU packing uses bounded 1x1/2x2 refinements. Includes unique-label fast path, exact capacity constraints, all-zero handling, and contiguous phy\u2192log/rank with inverse log\u2192phy construction.\n- **Performance**: Combined score 0.66 (balancedness 0.311, speed 1.00) over 5 workloads; all validations passed.\n- **Feedback**: Heuristics strongly favor speed, but bounded refinements and shallow-improvement guards limit balance quality on hard cases. Consider deeper/global refinements or broader donor/receiver candidate sets to improve balancedness with minimal speed impact.\n**Program Identifier:** Generation 41 - Patch Name moe_eplb_waterfill_diverse_packing_v7 - Correct Program: True\n\n**Program Name: Hierarchical EPLB with Hybrid Replication**\n- **Implementation**: Uses greedy sorted packing with bounded heavy\u2013light multi-swap refinement to balance fixed-size packs and maintains stable ranks; replication combines D\u2019Hondt (bulk) and Sainte-Lagu\u00eb (tail) with an adaptive tail search and a single best donor\u2192receiver fix-up. The hierarchical pipeline packs groups to nodes, replicates within nodes, then packs physical experts to GPUs, composing mappings via inverse/scatter in PyTorch (CPU-based tensors for speed).\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.00 across 5 workloads).\n- **Feedback**: The solution is correct and very fast, likely due to limited refinement steps and efficient vectorized operations. Balancedness is moderate, suggesting that broader swap/refinement or deeper search could further reduce peak load without severely impacting speed.\n**Program Identifier:** Generation 42 - Patch Name top2_tail_adaptive_and_second_light_refine - Correct Program: True\n\n**Program Name: EPLB hierarchical MoE load balancer**\n- **Implementation**: Uses a greedy capacity packer that assigns heaviest items to the lightest packs with a searchsorted-driven, strictly-improving heaviest\u2194lightest swap refinement; adds a diversity-aware variant for GPU packing with near-tie label penalties and adaptive refinement. Replication blends D\u2019Hondt (bulk) and Sainte-Lagu\u00eb (tail) apportionment with a single best donor\u2192receiver fix-up; hierarchical grouping across nodes/GPUs with deterministic ranks and inverse-permutation utilities.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.000000; 5 workloads); all validation tests passed.\n- **Feedback**: Runtime is excellent; balancedness is moderate, reflecting the bounded refine_steps and single-move replication fix-up prioritizing speed over full optimality. The hierarchical and diversity-aware choices improve placement spread, but allowing more refinement or multi-swap strategies could further reduce peak imbalance.\n**Program Identifier:** Generation 43 - Patch Name best_swap_second_light_and_peak_guard - Correct Program: True\n\n**Program Name: Hierarchical Water-Filling EPLB**\n- **Implementation**: Hierarchical three-stage balancer: LPT-style group-to-node packing, water-filling replica allocation with a small donor\u2192receiver fix-up, and diversity-aware heap placement to GPUs with bounded 1x1/2x2 swaps plus a unique-label fast path. Final mappings are assembled via gather/scatter with inverse permutations.\n- **Performance**: Combined score 0.66 (balancedness 0.311, speed 1.000 across 5 workloads); all validations passed.\n- **Feedback**: Runtime is excellent due to greedy packing and lightweight refinements, but balancedness is modest, likely limited by local heuristics and shallow swap depth (k\u22642, \u22642 passes). Broader refinement or tuned diversity penalties may improve peak smoothing without large speed regressions.\n**Program Identifier:** Generation 44 - Patch Name adaptive_gpu_refine_and_second_heaviest_swap - Correct Program: True\n\n**Program Name: Hierarchical water-fill expert load balancer**\n- **Implementation**: Uses a three-stage hierarchical policy: capacity-aware LPT to pack groups to nodes, integer water-filling with a single donor\u2192receiver fix-up for replication, and diversity-aware heap packing to GPUs with one top-2 vs bottom-2 swap refinement; mapping inverses/scatters build the final indices, all on CPU tensors.\n- **Performance**: Combined score 0.66 (balancedness_score 0.311077, speed_score 1.000000 over 5 workloads); passes all validation tests.\n- **Feedback**: Excellent speed stems from simple greedy passes and bounded refinements, but balancedness is moderate\u2014single-step fix-ups and limited swaps likely leave load skew under high variance. The hierarchical placement preserves capacity and diversity constraints reliably, suggesting deeper or iterative refinements could further improve balance.\n**Program Identifier:** Generation 45 - Patch Name top2_fixup_and_gpu_refine - Correct Program: True\n\n**Program Name: Hierarchical EPLB with bounded refinement**\n- **Implementation**: Uses greedy capacity-constrained packing per row with a k-candidate (k=2) heavy\u2013light swap refiner, plus a diversity-aware variant that can consider second-light/heavy and optional 2x2 exchanges. Replication uses a hybrid D\u2019Hondt (bulk) + Sainte-Lagu\u00eb (tuned tail) apportionment with a small local fix-up, embedded in a hierarchical group\u2192node\u2192GPU layout with permutation inverses for mapping.\n- **Performance**: Combined score 0.66; balancedness_score 0.311848; speed_score 1.000000 over 5 workloads.\n- **Feedback**: Excellent speed stems from CPU-side control flow with small-k local search and limited refinement steps. Balancedness is moderate; enabling deeper refinement or broader 2x2 exchanges in the initial packing stage could improve load balance while retaining most of the speed benefits.\n**Program Identifier:** Generation 46 - Patch Name k2_refine_second_heavy_gpu_stage - Correct Program: True\n\n**Program Name: Hierarchical Water-Fill Expert Load Balancer**\n- **Implementation**: Uses a hierarchical pipeline: capacity-aware LPT packing of groups to nodes with a micro 1x1 swap, water-filling replica allocation with a small donor\u2192receiver fix-up, and diversity-aware heap packing to GPUs with label-aware 1x1/2x2 refinements and a unique-label fast path. Final mappings are built via inverse indexing and scatter in PyTorch.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000 across 5 workloads); all validation tests pass.\n- **Feedback**: Excellent speed stems from greedy heuristics and bounded refinements, but balancedness is moderate, indicating the local swap depth (limited iterations and k=2 scope) trades peak smoothing for runtime. Expanding refinement breadth/depth or more global moves could improve balance at potential cost to speed.\n**Program Identifier:** Generation 47 - Patch Name micro_refine_groups_to_nodes - Correct Program: True\n\n**Program Name: Hierarchical water-fill expert load balancer**\n- **Implementation**: Hierarchical three-stage policy: capacity-constrained LPT packs groups to nodes, intra-node water-filling replica allocation with small donor\u2192receiver fix-up, and diversity-aware heap packing to GPUs with a unique-label fast path, label-spreading seed, and bounded 1x1/2x2 refinements. Mappings use Torch scatter/gather with contiguous replica ranks per expert.\n- **Performance**: Combined score 0.66 (balancedness 0.3111, speed 1.00 over 5 workloads); all validation tests passed.\n- **Feedback**: Heuristics deliver perfect speed but only moderate balance, likely constrained by integer water-fill rounding and shallow fix-up/refinement depth. Slightly deeper refinement or broader swap neighborhoods may improve balancedness with minimal runtime impact.\n**Program Identifier:** Generation 48 - Patch Name moe_eplb_waterfill_seeded_diverse - Correct Program: True\n\n**Program Name: Minimax heap-LPT EPLB**\n- **Implementation**: Uses minimax-threshold replica allocation via binary search and a deterministic heap-based fixed-capacity LPT packer for group\u2192node and replica\u2192GPU; mappings are built with scatter/gather permutations (PermOps.inverse) in a hierarchical pipeline.\n- **Performance**: Combined score 0.0; fails all validation tests.\n- **Feedback**: Forcing all tensors to CPU alters output device/dtype versus the expected API, likely causing failures. The hierarchical permutation/gather logic relies on strict divisibility assumptions and may misindex in edge cases, suggesting the need to preserve device, validate invariants, and robustly handle non-divisible configurations.\n**Program Identifier:** Generation 49 - Patch Name minimax_threshold_packer - Correct Program: False\n\n**Program Name: Hierarchical Water-Filling EPLB with Diverse Packing**\n- **Implementation**: Uses a hierarchical pipeline: capacity-aware LPT group-to-node packing, per-node water-filling replica counts with bounded donor\u2192receiver fix-ups, and diversity-aware heap GPU packing with a limited 1x1 swap refinement. Efficient mappings are built via repeat_interleave/scatter with inverse permutation helpers to maintain contiguous ranks and exact per-pack capacities.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000) over 5 workloads.\n- **Feedback**: The algorithm is correct and fast, passing all validation with perfect speed due to simple greedy passes and bounded refinements. Balancedness is moderate, indicating the conservative fix-up depth and limited swap search likely trade optimal load smoothing for speed.\n**Program Identifier:** Generation 50 - Patch Name two_stage_fixup_and_broadened_refine - Correct Program: True\n\n**Program Name: Hierarchical EPLB with Diversity Packing**\n- **Implementation**: Implements a hierarchical expert load balancer: groups\u2192nodes via balanced_packing, replication via water-filling with a donor\u2192receiver fix-up, and GPU placement via diversity-aware greedy packing with a bounded 1x1 swap refinement; mappings are built with gather/scatter inverses in PyTorch. Falls back to a global policy when groups don\u2019t align with nodes.\n- **Performance**: Combined score 0.66 (balancedness 0.3111, speed 1.00 over 5 workloads); passes all validation tests.\n- **Feedback**: The diversity-aware packing and micro-swap keep speed perfect while reducing hotspotting, but balancedness remains modest. Stronger or multi-swap refinement could further improve balance without sacrificing performance.\n**Program Identifier:** Generation 51 - Patch Name two_stage_fixups_and_broadened_refinement - Correct Program: True\n\n**Program Name: Hierarchical EPLB with water-filling and diversity packing**\n- **Implementation**: Implements a three-stage hierarchical balancer: (1) greedy balanced packing of expert groups to nodes, (2) per-node water-filling replica allocation with small donor\u2192receiver fix-ups, and (3) GPU assignment via diversity-aware greedy packing plus bounded heavy\u2194light swaps. Uses contiguous replica ranks, permutation inverses, and CPU-friendly Torch ops (repeat_interleave, gather/scatter) to build phy\u2194log maps efficiently.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000 across 5 workloads).\n- **Feedback**: Program is correct and passes all validation tests; the approach maximizes speed while achieving moderate balance. The limited swap refinement (top-2 donors/receivers and few candidates) preserves speed but may cap balancedness compared to more exhaustive optimization.\n**Program Identifier:** Generation 52 - Patch Name moe_eplb_waterfill_diverse_seed - Correct Program: True\n\n**Program Name: Hierarchical Expert Load Balancer (EPLB)**\n- **Implementation**: Implements a hierarchical rebalance: groups are packed to nodes via a greedy balanced_packing with limited 1x1/2x2 swap refinement, experts are replicated per node using apportionment heuristics (D\u2019Hondt bulk, Sainte-Lagu\u00eb/Huntington\u2013Hill tail) plus a one-move fix-up, then replicas are packed to GPUs; deterministic tie-breaking is used throughout. All computations run on CPU tensors with permutation inversions built via scatter/gather to produce phy\u2192log, log\u2192phy, and counts.\n- **Performance**: Combined score 0.0; fails all validation tests.\n- **Feedback**: The custom replication/packing heuristics and tie-breaking diverge from the expected EPLB behavior, producing incorrect mappings despite determinism and shape/device consistency. The apportionment-based replication and bounded swap refinements likely misalign with reference outputs, leading to systematic failures across test cases.\n**Program Identifier:** Generation 53 - Patch Name moe_eplb_tail_ab_hill_swap3_tiebreak - Correct Program: False\n\n**Program Name: EPLB: Hybrid Replica Allocation and GPU Packing**\n- **Implementation**: Uses a hierarchical planner: capacity-aware LPT to pack groups to nodes, per-(layer,node) replica allocation via D\u2019Hondt bulk with a 10% tail micro-AB between Sainte-Lagu\u00eb and Huntington\u2013Hill plus a guarded donor\u2192receiver fix-up, and intra-node GPU packing via a diversity-aware greedy with broadened 1\u00d71 swaps and a bounded two-swap fallback. Builds contiguous replica blocks and deterministic tie-breaking, with a unique-label fast path and minimal 1\u00d71 refinement.\n- **Performance**: Combined score 0.65 (balancedness 0.299, speed 1.000 across 5 workloads); all validation tests pass.\n- **Feedback**: Runtime efficiency is excellent due to simple priority queues, small local refinements, and limited tensor ops, but balancedness is modest, indicating the 10% tail and local swap scope can under-correct peak loads. Expanding refinement depth or tail size may improve peak reduction without sacrificing the current strong speed profile.\n**Program Identifier:** Generation 54 - Patch Name eplb_strategic_pipeline - Correct Program: True\n\n**Program Name: Hierarchical EPLB with Greedy Packing and Hybrid Replication**\n- **Implementation**: Implements a hierarchical EPLB: groups are greedily packed to nodes, intra-node replication uses a hybrid D'Hondt bulk plus Sainte-Lagu\u00eb/Huntington\u2013Hill tail with deterministic tie-breakers and a donor\u2192receiver fix-up, and physical experts are packed to GPUs with a capacity-constrained greedy packer plus k=2 swap refinement. Permutation/rank bookkeeping uses inverse-permutation ops and scatter to build physical\u2194logical maps.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.00 across 5 workloads); passes all validation tests.\n- **Feedback**: Lightweight greedy packing with bounded local refinement achieves maximal speed but only moderate balance. The hierarchical, deterministic pipeline preserves locality and correctness; increasing refinement depth or stronger global balancing could improve balancedness.\n**Program Identifier:** Generation 55 - Patch Name tie_aware_allocation_and_refine_k2_second_light - Correct Program: True\n\n**Program Name: Hierarchical EPLB with hybrid replication and packing**\n- **Implementation**: Greedy capacity-constrained packing with small k=2 swap refinement assigns items per layer; replication uses a hybrid apportionment (bulk D\u2019Hondt, tail Sainte-Lagu\u00eb vs Huntington\u2013Hill with predictive tie-breaking) plus a single best donor\u2192receiver fix-up, all within a hierarchical pipeline (group\u2192node packing, intra-node replication, GPU packing) with deterministic permutations and scatter-based maps.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.00 over 5 workloads); all validations passed.\n- **Feedback**: Speed is excellent due to lightweight greedy packing and bounded refinements; balancedness is moderate, indicating potential gains from deeper or multi-swap refinements or broader post-replication moves at some cost. Deterministic tie-breaking and hierarchical staging likely improved stability and mapping coherence.\n**Program Identifier:** Generation 56 - Patch Name replication_tie_break_tail_ab - Correct Program: True\n\n**Program Name: Cyclic-HH-Po2C Hierarchical Load Balancer**\n- **Implementation**: Three-stage design: cyclic group-to-node assignment after per-layer load sort; Huntington\u2013Hill replica apportionment per node with a tiny guarded donor\u2192receiver fix-up; deterministic power-of-two choices GPU packing with exact capacities, 64-bit hash-based candidate selection, duplicate-label penalty, and a final 1x1 micro-swap. Mappings are built via gather/scatter with inverse permutations, ensuring exact capacity fulfillment and determinism.\n- **Performance**: Combined score 0.65 (balancedness 0.298, speed 1.000 across 5 workloads).\n- **Feedback**: The implementation is correct and very fast, but balancedness lags due to limited post-allocation adjustments. The minimal fix-up and single-pass swap likely under-correct peak loads; more aggressive rebalancing or additional swap iterations could improve balancedness.\n**Program Identifier:** Generation 57 - Patch Name hill_divisor_plus_pow2_packing - Correct Program: True\n\n**Program Name: Hierarchical EPLB with hybrid replication**\n- **Implementation**: Implements greedy balanced packing with per-layer bounded k=2 swap refinement, and a hybrid replication policy: bulk D\u2019Hondt allocation followed by a per-step choice between Sainte-Lagu\u00eb and Huntington\u2013Hill using deterministic tie-breaking and a single best donor\u2192receiver fix-up. It applies a hierarchical mapping (groups\u2192nodes\u2192GPUs) with inverse scatter/gather to produce stable physical\u2194logical maps and ranks.\n- **Performance**: Combined score 0.66 with perfect speed (1.0) over 5 workloads; balancedness is moderate (0.311848).\n- **Feedback**: The approach is correct and very fast, owed to greedy packing with limited refinement and lightweight vectorized operations. Balancedness trails, suggesting gains from increasing swap iterations/candidate breadth or strengthening the post-optimization move while preserving determinism and runtime.\n**Program Identifier:** Generation 58 - Patch Name replicate_tail_ab_tiebreak - Correct Program: True\n\n**Program Name: Hierarchical Expert Load Balancer with Hybrid Replication**\n- **Implementation**: Uses greedy capacity-constrained packing with deterministic tie-breaking and bounded heaviest\u2013lightest swap refinement, plus a diversity-aware variant for GPU placement. Replicates experts via hybrid apportionment (D\u2019Hondt bulk, Sainte\u2013Lagu\u00eb/Huntington\u2013Hill tail with predicted-peak minimization) and a donor\u2192receiver fix-up within a hierarchical group\u2192node\u2192GPU layout using efficient permutation/scatter operations.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.0 across 5 workloads).\n- **Feedback**: Correctness verified (all validation tests pass); the limited refinement yields excellent speed but only moderate balancedness, suggesting room for stronger refinement or broader swap search if balance needs outweigh throughput.\n**Program Identifier:** Generation 59 - Patch Name dhondt_tail_ab_tiebreak_and_refine_light2 - Correct Program: True\n\n**Program Name: Hierarchical EPLB with hybrid replication**\n- **Implementation**: Greedy capacity-constrained packing with a searchsorted-based heaviest/lightest swap refinement, plus a diversity-aware variant that prefers label spread under near-ties. Replication uses a hybrid D\u2019Hondt (bulk) and Sainte-Lagu\u00eb/Huntington\u2013Hill (tail) apportionment with a targeted fix-up move, all integrated into a hierarchical groups\u2192nodes\u2192GPUs mapping with deterministic tie-breaks and permutation inverses.\n- **Performance**: Combined score 0.65 (balancedness 0.306257, speed 1.00 across 5 workloads).\n- **Feedback**: The limited refinement steps favor speed but yield only moderate balance; increasing refine_steps or extending the fix-up search could improve balancedness with modest overhead. The CPU round-trip in the diversity-aware packer remained fast here, and deterministic tie-breaking enhances reproducibility.\n**Program Identifier:** Generation 60 - Patch Name replicate_tail_ab_tiebreak_and_fixup_second_order - Correct Program: True\n\n**Program Name: Hierarchical Huntington\u2013Hill EPLB**\n- **Implementation**: Three-stage hierarchical balancer: groups-to-nodes via capacity-aware LPT; replicas via batched Huntington\u2013Hill (starting at 1); GPU placement via deterministic power-of-two choices using a stable 64-bit hash with exact per-GPU capacity and tie-breaks to reduce label colocation. Builds mappings with scatter/inverse operations and maintains O(n) amortized passes for low CPU overhead.\n- **Performance**: Combined score 0.64 (balancedness 0.289431, speed 1.000000; 5 workloads).\n- **Feedback**: Correct and deterministic with excellent speed, but balancedness is modest\u2014hash-based two-choice plus capacity-constrained greedy fallback can leave residual load skew. Multi-pass refinement or stronger tie-breaks could improve pack balance without sacrificing performance.\n**Program Identifier:** Generation 61 - Patch Name hh_drr_power2 - Correct Program: True\n\n**Program Name: Hybrid Apportionment + Latin-Spread EPLB**\n- **Implementation**: Groups are mapped to nodes via balanced round-robin with strict capacity; within nodes, replicas are allocated using D\u2019Hondt bulk with an adaptive Huntington\u2013Hill vs Sainte\u2013Lagu\u00eb tail, then assigned to GPUs via Latin dispersion with heaviest-move capacity fixing and a guarded 1x1 swap. Tensor mappings are preserved using scatter/gather with inverse permutations to maintain the existing API and shapes.\n- **Performance**: Combined 0.65 with balancedness 0.2928 and speed 1.00 across 5 workloads; all validations passed.\n- **Feedback**: The method is very fast due to simple argmax apportionment and localized GPU adjustments, but balancedness is modest, indicating residual peak load skew. The BRR placement and single-swap fix likely under-correct heterogeneity; multi-swap or more global balancing could improve peak smoothing.\n**Program Identifier:** Generation 62 - Patch Name ahh_latin_spread - Correct Program: True\n\n**Program Name: Hierarchical EPLB with diversity-aware packing**\n- **Implementation**: Uses a three-stage hierarchical strategy: (1) greedy balanced packing of groups to nodes, (2) water-filling-based expert replication with a single donor\u2192receiver fix-up, and (3) GPU assignment via a diversity-aware greedy packer with a bounded 1x1 swap refinement. Efficient index mappings (inverse/gather/scatter) maintain replica ranks and logical-physical maps.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000 over 5 workloads).\n- **Feedback**: The bounded local refinement and greedy heuristics deliver maximal speed but only moderate balance. Diversity-aware spreading reduces replica hotspotting, yet the single-swap refinement likely caps balancedness improvements; nevertheless, the implementation is correct and passes all validation tests.\n**Program Identifier:** Generation 63 - Patch Name two_stage_fixup_and_broadened_gpu_swap - Correct Program: True\n\n**Program Name: Hierarchical Expert Load Balancer with Hybrid Replication**\n- **Implementation**: Uses greedy balanced packing per layer with bounded k=2 swap refinement, and a hybrid expert replication scheme (bulk D\u2019Hondt, tail Sainte-Lagu\u00eb/Huntington\u2013Hill chosen by predicted peak with CV-scaled tail), followed by a single best move fix-up. Applies hierarchical grouping (groups\u2192nodes\u2192GPUs) and constructs mappings via scatter/gather with deterministic tie-breaking.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.000000) over 5 workloads.\n- **Feedback**: Achieves high speed due to simple greedy packing, small-k refinement, and vectorized gathers/scatters. Lower balancedness suggests limited swap depth and heuristic tail selection under-correct some imbalances, though the algorithm is correct and passes all validation tests.\n**Program Identifier:** Generation 64 - Patch Name adaptive_tail_and_broadened_refine - Correct Program: True\n\n**Program Name: Hierarchical Expert Load Balancer with Hybrid Replication**\n- **Implementation**: Implements hierarchical EPLB: group-to-node greedy packing with bounded k=2 best-swap refinement, expert replication via hybrid apportionment (D\u2019Hondt bulk + Sainte-Lagu\u00eb/Huntington\u2013Hill tail with deterministic tie-breaks), and diversity-aware GPU assignment with adaptive refinement. Deterministic ranks/inverses are computed row-wise, with a vectorized fix-up that can reassign a replica from top to bottom averages to reduce peak load.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.00 across 5 workloads); all validations passed.\n- **Feedback**: Extremely fast due to lightweight CPU greedy passes and limited bounded refinements, but balancedness is moderate. Increasing swap depth/k, tuning diversity tie-break strength, or adjusting the replication tail/fix-up criteria could improve balance at a small speed cost.\n**Program Identifier:** Generation 65 - Patch Name moe_eplb_crossover_diverse_k2 - Correct Program: True\n\n**Program Name: Hierarchical EPLB with Greedy Packing**\n- **Implementation**: Hierarchical expert rebalancing: groups-to-nodes via a capacity-constrained greedy packer with deterministic 1x1 swap refinement (and optional two-swap chain), intra-node replication using bulk D\u2019Hondt with an adaptively sized Sainte-Lagu\u00eb/heuristic tail and a one-commit fix-up; GPU packing uses label-aware tie-breaks to spread replicas, with permutation inverses for mapping. Control-flow-heavy stages run on CPU for speed, with deterministic ranks within packs.\n- **Performance**: Combined score 0.66 (balancedness 0.311, speed 1.00 over 5 workloads); all validations pass.\n- **Feedback**: Excellent speed stems from CPU-side greedy assignment and bounded refinements, but balancedness is modest, suggesting the limited swap horizon and heuristic replication leave load peaks. Increasing refine_steps, enabling the two-swap chain more broadly, or widening donor/receiver candidates could improve peak balance.\n**Program Identifier:** Generation 66 - Patch Name eplb_pipeline_modular - Correct Program: True\n\n**Program Name: Hierarchical EPLB with k-candidate refinement**\n- **Implementation**: Implements a hierarchical expert-parallel balancer: greedy per-row capacity packing with bounded k=2 heavy\u2013light swap refinement; GPU packing uses diversity-aware tie-breaking. Replication uses D\u2019Hondt for bulk, per-step Sainte-Lagu\u00eb/Huntington\u2013Hill selection for the tail, and a one-move donor\u2192receiver fix-up, with CPU-side torch loops, permutation utilities, and deterministic ranks.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.000000 across 5 workloads).\n- **Feedback**: The program is correct and passes all validation, achieving excellent speed but only moderate balance. The bounded refinement and heuristic replication favor throughput over optimal equalization; deeper refinement or broader candidate consideration could improve balancedness.\n**Program Identifier:** Generation 67 - Patch Name donor_baseline_tie_break_and_light2_search - Correct Program: True\n\n**Program Name: Greedy hierarchical expert load balancer**\n- **Implementation**: Uses greedy lightest-pack assignment with capacity, followed by broadened targeted swap refinement (top-2 donors vs bottom-3 receivers via searchsorted, applying only strictly improving swaps). Replication employs a hybrid apportionment (D\u2019Hondt bulk plus Sainte-Lagu\u00eb/Huntington\u2013Hill tail) with deterministic tie-breakers and a single best fix-up move, embedded in a hierarchical pipeline (group\u2192node\u2192GPU) with a diversity-aware packer and fast permutation inverses via scatter.\n- **Performance**: Combined score 0.66; balancedness_score 0.311848; speed_score 1.000000 across 5 workloads.\n- **Feedback**: Excellent speed comes from greedy placement, bounded refinement, and vectorized scatter-based ops; however, balancedness is modest, suggesting raising refine_steps or expanding the swap search could improve balance. Hierarchical and diversity-aware choices yield deterministic layouts; removing CPU round-trips in the diverse packer may further optimize throughput.\n**Program Identifier:** Generation 68 - Patch Name broaden_refine_and_adaptive_tail - Correct Program: True\n\n**Program Name: Hierarchical Hybrid Expert Load Balancer (EPLB)**\n- **Implementation**: Greedy sort-based balanced_packing enforces equal-count bins and performs bounded heaviest\u2013lightest swap refinement with stable ranks. replicate_experts uses a CV-adaptive hybrid of D\u2019Hondt (bulk) and Sainte-Lagu\u00eb (tail) plus a local donor\u2192receiver fix-up, embedded in a hierarchical group\u2192node\u2192GPU mapping, with CPU-friendly operations to minimize overhead.\n- **Performance**: Combined score 0.66 (balancedness 0.312, speed 1.00) across 5 workloads.\n- **Feedback**: The approach attains maximal speed but only moderate balance; limited refinement steps and a single fix-up move constrain balancing quality. Modestly increasing refine_steps or strengthening the replication adjustment could improve balancedness with a small runtime trade-off; all validations passed.\n**Program Identifier:** Generation 69 - Patch Name adaptive_tail_and_2x2_fixup - Correct Program: True\n\n**Program Name: Hierarchical Expert Load Balancer with Greedy Refinement**\n- **Implementation**: Implements a hierarchical EPLB: groups are greedily packed to nodes with a small 2-item swap refinement; experts are replicated in-node using a hybrid allocator (bulk D\u2019Hondt plus per-step Sainte-Lagu\u00eb/Huntington\u2013Hill A/B with a post-fix move), then physical experts are packed to GPUs, using torch-based deterministic ops (inverse permutations, scatter) and a capacity==1 fast path. Deterministic tie-breakers ensure stable mappings across steps.\n- **Performance**: Achieved combined score 0.66 (balancedness 0.311848, speed 1.000000) over 5 workloads.\n- **Feedback**: Heuristics (greedy pack + limited 2-way swaps and hybrid replication) maximize speed but leave measurable imbalance; broader refinement or larger candidate sets could improve balancedness. The hierarchical policy and deterministic design pass all checks and favor reproducibility and fast execution.\n**Program Identifier:** Generation 70 - Patch Name adaptive_tail_cv - Correct Program: True\n\n**Program Name: Hierarchical Water-Fill EPLB with Diverse Packing**\n- **Implementation**: Implements a hierarchical policy: groups are LPT/ capacity-constrained packed to nodes, per-node replicas are assigned via water-filling with a peak-aware tail and up to two donor\u2192receiver fixes, and GPU placement uses a diversity-aware heap with bounded 1x1/2x2 swaps and a unique-label fast path. Produces physical\u2194logical mappings and ranks via tensor-based scatter/gather with exact per-pack capacities.\n- **Performance**: Combined score 0.66 (balancedness 0.311, speed 1.00 over 5 workloads); all validation tests passed.\n- **Feedback**: Excellent speed comes from greedy tensor ops and tightly bounded refinements, but balancedness is moderate, likely limited by the small number of fix-up iterations and local swap scope. Consider deeper or adaptive refinement and broader swap candidates when residual imbalance remains high.\n**Program Identifier:** Generation 71 - Patch Name adaptive_tail_waterfill - Correct Program: True\n\n**Program Name: Hierarchical Expert Load Balancer with Hybrid Replication**\n- **Implementation**: Implements hierarchical packing (groups\u2192nodes\u2192GPUs) via greedy CPU assignment with deterministic tie-breaking and bounded k=2 best-swap refinement, plus a diversity-aware GPU packer. Replication uses a hybrid apportionment (bulk D\u2019Hondt, per-row tail choice vs Sainte-Lagu\u00eb) with a vectorized peak-minimizing fix-up; mappings are built via scatter/gather-based inverses.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.000 across 5 workloads).\n- **Feedback**: Correct and passes all validations; achieves excellent speed from lightweight greedy control flow and limited refinement. Balance quality is moderate, likely constrained by k=2 and small swap budgets; increasing k or refinement steps could improve balancedness at some runtime cost.\n**Program Identifier:** Generation 72 - Patch Name adaptive_tail_and_second_fixup - Correct Program: True\n\n**Program Name: Hierarchical EPLB with hybrid water-filling**\n- **Implementation**: Uses binary-searched water-filling with a hybrid D\u2019Hondt/Sainte\u2013Lagu\u00eb tail allocator and a small donor\u2192receiver fix-up, then hierarchical packing (groups\u2192nodes\u2192GPUs) with diversity-aware heap placement and bounded 1x1/2x2 refinements; mappings are built via gather/scatter and inverse permutations. Fast paths (e.g., unique labels, cap=1) and CPU-friendly tensor ops keep it efficient.\n- **Performance**: Combined score 0.66 (balancedness 0.311, speed 1.000 across 5 workloads); all validations passed.\n- **Feedback**: Runtime is excellent, but load balance is mediocre\u2014likely due to greedy LPT/heap packing and the limited two-iteration fix-up not fully smoothing peaks. The unique-label fast path and bounded refinements help speed, yet more aggressive balancing or deeper refinements may improve the peak load.\n**Program Identifier:** Generation 73 - Patch Name hybrid_tail_waterfill_and_peak_tiebreaks - Correct Program: True\n\n**Program Name: EPLB: hierarchical water-fill and diverse packing**\n- **Implementation**: Implements a hierarchical load balancer: groups are packed to nodes (greedy balanced_packing), logical experts are replicated via water-filling with a hybrid D\u2019Hondt/Sainte\u2013Lagu\u00eb tail and a single donor\u2192receiver fix-up, and intra-node GPU placement uses diversity-aware greedy packing with a bounded 1x1 swap refinement. Final mappings are built with vectorized scatter/gather for efficiency.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000 over 5 workloads).\n- **Feedback**: Excellent speed stems from bounded candidate sets, single-swap refinements, and vectorized operations; however, balancedness is only moderate, likely due to the conservative one-pass refinement steps. Allowing iterative or broader swap/refinement passes could improve balance at some cost to runtime.\n**Program Identifier:** Generation 74 - Patch Name hybrid_tail_waterfill - Correct Program: True\n\n**Program Name: Hierarchical waterfill and diverse-heap MoE balancer**\n- **Implementation**: Implements a hierarchical policy: capacity-aware LPT to assign groups to nodes, per-node water-filling replica counts with a D\u2019Hondt/Sainte\u2013Lagu\u00eb hybrid tail and a small donor\u2192receiver fix-up, then diversity-aware greedy GPU packing with a label-spreading seed and bounded 1x1/2x2 swap refinement. Uses Torch tensor ops (sort/topk/repeat_interleave/scatter) and fast paths for unique labels and unit capacities to keep it lean and deterministic.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000 over 5 workloads); all tests passed.\n- **Feedback**: Excellent speed indicates the heuristics and fast paths are efficient, but the relatively low balancedness suggests the limited refinement (max two donor moves and shallow swap passes) leaves peak loads higher than optimal. Increasing refinement depth/iterations, stronger global swap searches, or more peak-aware tie-breaking during greedy placement could improve balance without severely impacting speed.\n**Program Identifier:** Generation 75 - Patch Name hybrid_tail_waterfill - Correct Program: True\n\n**Program Name: Hybrid apportionment EPLB with slack-spread packing**\n- **Implementation**: Uses hierarchical packing (groups\u2192nodes via LPT-style balanced_packing), then per-node hybrid apportionment (bulk D\u2019Hondt plus peak-aware Sainte\u2013Lagu\u00eb/D\u2019Hondt tail) to assign replica counts, followed by slack-first label-spread GPU placement with capacity and anti-duplication plus a single bounded 1x1 peak-capping swap; builds contiguous physical-to-logical maps and ranks via vectorized PyTorch ops. This design ensures \u22651 replica per expert, maintains per-GPU capacity constraints, and minimizes peak load with low overhead.\n- **Performance**: Combined score 0.65 (balancedness 0.306931, speed 1.000000 across 5 workloads); program is correct and passes all validation tests.\n- **Feedback**: Excellent speed stems from vectorized bulk apportionment and limiting to a single smoothing swap; balancedness is reasonable but leaves room for further peak reduction via additional or smarter local refinements. Robust handling of zero-weight and edge cases was observed, with stable hierarchical behavior across nodes/GPUs.\n**Program Identifier:** Generation 76 - Patch Name apportion_slack_spread_moe - Correct Program: True\n\n**Program Name: Hierarchical expert-parallel load balancer with hybrid replication**\n- **Implementation**: Implements a hierarchical three-stage EPLB: (1) greedy balanced_packing with bounded multi-swap refinement to place groups on nodes, (2) hybrid replication using D'Hondt bulk plus peak-aware Sainte-Lagu\u00eb tail with a one-move fix-up, and (3) packing physical experts to GPUs. Uses Torch scatter/inverse permutation mappings and runs sorting/packing on CPU floats.\n- **Performance**: Combined score 0.66: balancedness_score 0.311848 and speed_score 1.000000 across 5 workloads; all validation tests pass.\n- **Feedback**: Greedy packing with limited refinement and a single local replication fix-up maximizes speed but leaves noticeable imbalance. Raising refine_steps or allowing multiple improving moves per row could improve balancedness at some cost to speed; hierarchical mode assumes groups divide nodes, otherwise it falls back to a global policy.\n**Program Identifier:** Generation 77 - Patch Name adaptive_tail_and_fixup - Correct Program: True\n\n**Program Name: Hierarchical EPLB with Hybrid Replication**\n- **Implementation**: Uses a greedy, CPU-friendly balanced_packing with bounded heavy\u2013light refinement; replicate_experts combines bulk D\u2019Hondt allocation with a peak-aware Sainte\u2013Lagu\u00eb vs D\u2019Hondt tail and a donor/receiver fix-up. A hierarchical pipeline packs groups to nodes, replicates within nodes, and packs to GPUs with stable rank reassignment; weights are processed on CPU for speed.\n- **Performance**: Combined 0.66 (balancedness 0.311077, speed 1.00 across 5 workloads).\n- **Feedback**: Correct and passes all tests; excellent speed from greedy packing and limited refinement, but moderate balancedness indicates room for stronger refinement or fix-up to reduce peak load. The hierarchical policy likely improves locality while inheriting the core heuristic\u2019s balancing limits.\n**Program Identifier:** Generation 78 - Patch Name moe_eplb_peakaware_tail_fixup - Correct Program: True\n\n**Program Name: Hierarchical water-fill expert load balancer**\n- **Implementation**: Uses hierarchical mapping: balanced packing of groups to nodes, water-filling replica allocation with a small donor\u2192receiver fix-up, then diversity-aware greedy GPU placement with label-spreading and bounded 1x1/2x2 refinement. Final maps are built via scatter/inverse permutations, keeping per-expert contiguous ranks and CPU-based tensor ops.\n- **Performance**: Combined score 0.0 (fails all validation tests).\n- **Feedback**: A critical bug in pack_diverse_heap references undefined variable num_items (should be n_items), causing a runtime failure. Beyond this, the approach depends on permutation assumptions in inverse/scatter and tight capacity arithmetic; add validations for per-pack capacity and index ranges to prevent subtle mapping errors.\n**Program Identifier:** Generation 79 - Patch Name peakaware_fixup_and_diversity_guard - Correct Program: False\n\n**Program Name: Hierarchical Water-Fill EPLB with Diversity-Aware Packing**\n- **Implementation**: Uses hierarchical balancing: capacity-aware LPT to pack groups to nodes, per-node water-filling replica allocation with a hybrid D\u2019Hondt/Sainte\u2013Lagu\u00eb tail and a bounded donor\u2192receiver fix-up, then diversity-aware heap packing to GPUs with label-spreading seed, label repeats penalty, and small 1x1/2x2 refinements. Efficient indexing via scatter/gather, repeat_interleave, and topk keeps operations CPU-friendly and avoids full sorts.\n- **Performance**: Combined score 0.63 (balancedness 0.2534, speed 1.00 over 5 workloads); passes all validation tests.\n- **Feedback**: Runtime is excellent due to batched topk fills, fast unique-label path, and bounded refinements, but load balance is modest\u2014likely limited by small candidate sets (top-3 x bottom-3), few refinement passes, and greedy capacity-constrained placements. The hybrid water-fill tail reduces peaks but doesn\u2019t fully compensate for the constrained local search in packing.\n**Program Identifier:** Generation 80 - Patch Name moe_eplb_hybrid3 - Correct Program: True\n\n**Program Name: Hierarchical EPLB with swap refinement**\n- **Implementation**: Implements a hierarchical expert-parallel balancer: greedy capacity-constrained packing per row with bounded 1x1 swap refinement and a 2x2 fallback, diversity-aware GPU packing, and replication via D\u2019Hondt bulk plus Sainte\u2013Lagu\u00eb tail with peak/second-peak tie-breaking and a small peak-focused fix-up. Uses permutation inverses and gathers/scatters to maintain contiguous per-node layouts and maps between logical/meta/physical experts; initial greedy stages run on CPU for fast sorting.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000 over 5 workloads).\n- **Feedback**: The design favors speed via lightweight heuristics and limited refinement, passing all validations but yielding moderate balancedness. Increasing refinement steps or expanding swap search beyond top-2/bottom-2 could improve balance with minimal runtime impact.\n**Program Identifier:** Generation 81 - Patch Name peak_aware_tail_balancer - Correct Program: True\n\n**Program Name: Hierarchical Water-Fill & Diverse-Heap EPLB**\n- **Implementation**: Uses water-filling replication with an adaptive hybrid tail (D\u2019Hondt/Sainte\u2013Lagu\u00eb) and limited donor\u2192receiver fix-ups; capacity-constrained LPT packing with 1x1/2x2 micro-swaps; and a diversity-aware heap GPU placement with label-aware tie-breaks, a unique-label fast path, and adaptive refinement depth. Follows a hierarchical pipeline (groups\u2192nodes, replicate per node, pack to GPUs) and builds mappings via repeat_interleave/cumsum and scatter-based inverses.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000 across 5 workloads); passes all validation tests.\n- **Feedback**: Very fast due to lightweight heuristics and bounded refinements, but balancing is moderate\u2014local 1x1/2x2 swaps and 1\u20133 refinement steps may under-optimize peak smoothing. Consider deeper or broader refinement (more candidates/steps or multi-pack swaps) and stronger label-duplication penalties to improve balancedness.\n**Program Identifier:** Generation 82 - Patch Name moe_eplb_hybrid_tail_peak_refine - Correct Program: True\n\n**Program Name: Water-Filling + Diverse Packing EPLB**\n- **Implementation**: Hierarchical policy: LPT-based balanced_packing assigns groups to nodes; water-filling replica allocation uses binary search with bulk D\u2019Hondt plus an adaptive Sainte\u2013Lagu\u00eb tail and a small donor\u2192receiver fix-up; GPU placement uses a diversity-aware heap with a unique-label fast path, label-spreading seed, greedy label-aware fill, and bounded 1x1/2x2 refinements. Efficient tensor gathers/scatters build contiguous phy\u2194log and rank maps with exact capacity enforcement.\n- **Performance**: Combined score 0.66 (balancedness 0.3111, speed 1.00 across 5 workloads); passes all validation tests.\n- **Feedback**: Heuristics prioritize speed\u2014limited fix-up iterations and shallow refinement keep runtime high (speed=1.0) but yield only moderate load balance; deeper donor/receiver search or more refinement passes could lift balancedness at some cost. Robust handling of edge cases (all-zero weights, unique labels) and strict capacity/count constraints is a strength.\n**Program Identifier:** Generation 83 - Patch Name peak_aware_tail_and_fixup_tiebreaks - Correct Program: True\n\n**Program Name: Hierarchical EPLB with hybrid replication and refinement**\n- **Implementation**: Implements a hierarchical expert load balancer: groups are greedily packed with bounded k-candidate 1x1 swaps (optional 2x2), replication uses a hybrid apportionment (D\u2019Hondt bulk then peak-aware choice among D\u2019Hondt/Sainte\u2013Lagu\u00eb/Huntington\u2013Hill), and final GPU packing applies a diversity-aware tie-breaker with adaptive refinement. Control-flow heavy steps run on CPU with deterministic rank assignments and permutation utilities for stable mappings.\n- **Performance**: Combined score 0.66 (balancedness_score 0.311077, speed_score 1.00) over 5 workloads; all validations passed.\n- **Feedback**: Excellent speed indicates the bounded refinement and CPU-centric loops are efficient, but moderate balancedness suggests conservative refinement settings (k=2, few steps, limited 2x2 in the default path) leave room for better load equalization. Increasing refine_steps or enabling adaptive/second-candidate options in the base packing could improve balancedness at a small cost to speed.\n**Program Identifier:** Generation 84 - Patch Name peak_aware_tail_and_adaptive_refine - Correct Program: True\n\n**Program Name: Hierarchical EPLB with hybrid replication**\n- **Implementation**: Implements a hierarchical three-step balancer: groups-to-nodes packing (balanced_packing), hybrid expert replication (bulk D\u2019Hondt, tail D\u2019Hondt vs Sainte-Lagu\u00eb with fix-ups), and GPU assignment via diversity-aware greedy packing with bounded best-swap refinement. Uses PyTorch gather/scatter and inverse permutations to maintain mappings on CPU.\n- **Performance**: Combined score 0.0 (fails all validation tests).\n- **Feedback**: balanced_packing is incomplete for groups_per_pack > 1 (no return), and the intended greedy+refinement logic is accidentally left as unreachable dead code after a return in _balanced_packing_diverse. These wiring errors break the group-to-node packing step; fixing balanced_packing and removing unreachable code should allow the otherwise reasonable algorithmic choices to be exercised.\n**Program Identifier:** Generation 85 - Patch Name diverse_gpu_packing_and_two_move_fixup - Correct Program: False\n\n**Program Name: Hierarchical EPLB with diversity-aware packing and hybrid apportionment**\n- **Implementation**: Greedy balanced packing with label-aware tie-breaks and adaptive 1x1/2x2 swap refinement; replication uses hybrid D\u2019Hondt bulk plus peak-aware Sainte-Lagu\u00eb/Huntington\u2013Hill tail with a conditional second fix-up, composed in a hierarchical groups\u2192nodes\u2192GPUs placement.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.000000 over 5 workloads); all validations passed.\n- **Feedback**: The diversity-aware ties and bounded refinements keep runtime maximal but leave noticeable residual imbalance; increasing refinement depth/k or tuning thresholds (eps, swap passes) may improve balancedness. The hierarchical policy and vectorized apportionment provide determinism and scalability, but conservative CPU-side adjustments likely cap balance quality.\n**Program Identifier:** Generation 86 - Patch Name adaptive_tail_and_diverse_gpu_tie_break - Correct Program: True\n\n**Program Name: Hierarchical Waterfill with Diverse GPU Packing EPLB**\n- **Implementation**: Implements a hierarchical pipeline: capacity-aware LPT group-to-node packing, per-node water-filling replica allocation via threshold binary search with bulk D\u2019Hondt and adaptive Sainte\u2013Lagu\u00eb tail plus up to two donor\u2192receiver fix-ups, and a diversity-aware heap GPU placement with bounded 1x1/2x2 refinements. Uses CPU torch ops with sort/topk, gather/scatter-based inverses, and builds contiguous block mappings for replicas and ranks.\n- **Performance**: Combined score 0.66 (balancedness 0.3111, speed 1.0 across 5 workloads); all validations pass.\n- **Feedback**: Excellent speed stems from vectorized topk and tightly bounded refinement loops, but balancedness is modest due to greedy packing and limited post-optimization. Deeper or broader refinements could improve balance at the cost of runtime.\n**Program Identifier:** Generation 87 - Patch Name hybrid_waterfill_peakaware_tail_and_diverse_refine - Correct Program: True\n\n**Program Name: Hierarchical EPLB with Greedy Packing and Adaptive Replication**\n- **Implementation**: Implements hierarchical expert load balancing: greedy lightest-bin packing with deterministic ranks and bounded refinement (best 1x1 swap via searchsorted plus a 2x2 fallback), a diversity-aware variant for GPU placement, and a hybrid replication allocator (bulk D\u2019Hondt; tail chooses among Sainte-Lagu\u00eb/Huntington\u2013Hill/D\u2019Hondt to minimize predicted peak) with a small fix-up. Uses stable tie-breaking, row-wise inverse permutations, and maps groups\u2192nodes\u2192GPUs under capacity constraints with vectorized PyTorch operations.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.000000) over 5 workloads.\n- **Feedback**: Correct and passes all validations; runtime is excellent due to greedy placement and limited local refinements. Balancedness is modest, likely limited by small refine_steps and local swaps; deeper refinement or larger multi-item exchanges may improve balance.\n**Program Identifier:** Generation 88 - Patch Name peak_aware_tail_and_2x2_refine - Correct Program: True\n\n**Program Name: Hierarchical EPLB with Equal-Proportions and Peak-Aware Packing**\n- **Implementation**: Uses hierarchical steps: LPT group-to-node packing with a single bounded 1x1 swap; Equal-Proportions (Huntington\u2013Hill) replication with up to two donor\u2192receiver fixes; and GPU packing via peak-aware, diversity-tied greedy assignment limited to the top-2 candidate packs, plus bounded 1x1 swap (and 2x2 fallback) with adaptive refinement depth. Employs exact capacities, inverse permutations, and scatter/gather to construct physical\u2194logical maps efficiently.\n- **Performance**: Combined score 0.64 (balancedness 0.289431, speed 1.000000 over 5 workloads).\n- **Feedback**: The restricted candidate set and bounded refinements deliver excellent speed, but balancedness is modest, indicating residual peak skew likely from limited fix-ups and early stopping. The implementation is correct and passes all validation tests.\n**Program Identifier:** Generation 89 - Patch Name eplb_equalprop_rr_peak - Correct Program: True\n\n**Program Name: Hybrid Hierarchical EPLB with Greedy Packing**\n- **Implementation**: Uses a hierarchical pipeline: groups\u2192nodes packing via a greedy capacity packer with searchsorted-based heaviest/lightest swaps, intra-node replication with a hybrid D\u2019Hondt/Sainte\u2013Lagu\u00eb allocator plus a guarded fix-up, and GPU packing with a diversity-aware tie-breaker; deterministic ranks and row-wise inverse permutations are provided for stable layouts. A diversity variant adaptively adjusts refinement steps based on imbalance and performs light CPU staging for weights/labels.\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.000000 over 5 workloads).\n- **Feedback**: Excellent speed stems from simple greedy placement and bounded refinements, but balance quality is moderate due to limited swap depth and hierarchical constraints. The approach is correct and stable across tests; further balance improvements likely require deeper refinement or stronger post-replication adjustments.\n**Program Identifier:** Generation 90 - Patch Name peak_aware_tail_and_two_step_fixup - Correct Program: True\n\n**Program Name: Hierarchical water-fill MoE load balancer**\n- **Implementation**: Hierarchical pipeline: capacity-constrained LPT group-to-node packing, per-node water-filling replica allocation with an adaptive D\u2019Hondt/Sainte-Lagu\u00eb tail and up to 2 donor\u2192receiver fixes, and diversity-aware GPU packing with a label-spreading seed plus bounded 1x1/2x2 refinements. Includes fast paths for cap=1 and unique labels, and constructs inverse mappings for logical\u2194physical indices.\n- **Performance**: Combined score 0.66 (balancedness 0.3111, speed 1.0000 over 5 workloads); passes all validation.\n- **Feedback**: Excellent speed suggests the heuristics and fast paths are efficient, but balancedness is moderate\u2014likely limited by the small fix-up budget and bounded refinement scope. Increasing donor\u2192receiver iterations, allowing deeper or broader swap search, or tuning tie-breakers could further reduce peak load.\n**Program Identifier:** Generation 91 - Patch Name adaptive_tail_and_lpt_tiebreak - Correct Program: True\n\n**Program Name: Hierarchical expert-parallel load balancer**\n- **Implementation**: Greedy capacity-constrained packing with a small targeted swap refinement; hierarchical pipeline that packs groups to nodes, replicates experts via hybrid D\u2019Hondt + Sainte-Lagu\u00eb/Huntington\u2013Hill, then diversity-aware packing of replicas to GPUs, with deterministic tie-breaking and permutation inverses for layout. Includes a post-replication fix-up (up to two moves) to reduce peak load and diversity-aware near-tie handling to avoid co-locating identical replicas.\n- **Performance**: Combined score 0.66 (balancedness_score 0.311848, speed_score 1.000000 over 5 workloads); passes all validation tests.\n- **Feedback**: Excellent speed stems from greedy heuristics and limited local refinement, but balancedness is moderate, suggesting a trade-off; increasing refinement depth or stronger local search could improve balance at potential runtime cost. CPU-side operations in the diversity-aware stage and deterministic tie-breaks improved stability without hurting measured speed.\n**Program Identifier:** Generation 92 - Patch Name diverse_gpu_tie_and_two_move_fixup - Correct Program: True\n\n**Program Name: Hierarchical water-fill diverse packing EPLB**\n- **Implementation**: Hierarchical 3-stage pipeline: capacity-aware LPT (balanced_packing) assigns groups to nodes, water-filling replica allocation with small donor\u2192receiver fix-up sets per-expert replicas, and a diversity-aware heap packs replicas to GPUs using a label-spreading seed, projected-load/label-repeat tie-breaks, and bounded 1x1/2x2 refinements. PyTorch sort/topk and scatter/gather build contiguous phy\u2194log mappings and dense logical\u2192physical tables efficiently.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.00 across 5 workloads).\n- **Feedback**: The program is correct and passes all validations; it is very fast due to greedy/top-k procedures and limited refinement passes. Balancedness is moderate, indicating the bounded refinement depth and local swaps trade some load smoothing for speed.\n**Program Identifier:** Generation 93 - Patch Name group_packing_refine_1x1 - Correct Program: True\n\n**Program Name: Hierarchical EPLB with Greedy Packing and Hybrid Replication**\n- **Implementation**: Uses a hierarchical 3-stage pipeline (group-to-node packing, intra-node replication, GPU packing). balanced_packing performs greedy sorted assignment with bounded 1x1/2x2 refinements; replicate_experts combines D\u2019Hondt bulk with an adaptive Sainte-Lagu\u00eb tail and O(1) peak-minimizing fix-ups, with deterministic tie-breaks and tensor gather/scatter inverses.\n- **Performance**: Combined score 0.66: balancedness_score 0.311077 and speed_score 1.000000 across 5 workloads.\n- **Feedback**: Conservative refinement (refine_steps=1\u20132), a small adaptive tail (alpha=0.10), and hierarchical constraints likely limited balance quality despite excellent throughput. All validations pass, indicating a fast, stable baseline that could trade modest extra compute for improved balancedness.\n**Program Identifier:** Generation 94 - Patch Name hybrid_tail_ab_and_2x2_refine - Correct Program: True\n\n**Program Name: Peak-aware hierarchical expert load balancer**\n- **Implementation**: Hierarchical 3-stage strategy: capacity-aware LPT for group\u2192node packing, peak-aware water-filling replication with adaptive D\u2019Hondt vs Sainte\u2013Lagu\u00eb tail and small donor\u2192receiver fix-up, and diversity-aware GPU packing using label-spreading seeds, greedy fill, and bounded 1x1/2x2 refinements. Robust indexing via inverse/scatter maps and integer-safe counts ensure contiguous replica ranks and stable capacity constraints.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000 over 5 workloads); all validations pass.\n- **Feedback**: The bounded refinement and batched water-filling deliver excellent speed, but balancedness remains moderate, indicating residual peaks after greedy placement. Increasing refinement aggressiveness or widening donor/receiver and swap search could further reduce max per-replica load without large runtime impact.\n**Program Identifier:** Generation 95 - Patch Name hybrid_peakaware_waterfill_diversepack_v3 - Correct Program: True\n\n**Program Name: Hierarchical EPLB with greedy packing and hybrid replication**\n- **Implementation**: Uses capacity-constrained greedy packing per layer with deterministic ranks and a bounded k=2 local refinement (1x1 swaps with 2x2 fallback, adaptive extra step) plus a diversity-aware tie-breaker on near-load ties. Replication allocates redundant experts via a hybrid apportionment (bulk D\u2019Hondt, tail Sainte-Lagu\u00eb vs D\u2019Hondt with lexicographic peak minimization and imbalance gating) and a per-row fix-up; mapping is organized hierarchically (groups\u2192nodes\u2192GPUs) with permutation inverses.\n- **Performance**: Combined score 0.66 (balancedness_score 0.311077, speed_score 1.000000 across 5 workloads).\n- **Feedback**: Excellent speed stems from greedy placement and tightly bounded refinement, while balancedness is modest due to limited swap depth and local heuristics. Increasing refine_steps or adding broader swap candidates/global adjustments would likely improve balance at some runtime cost; the program is correct and passes all validations.\n**Program Identifier:** Generation 96 - Patch Name adaptive_tail_and_packer_2x2 - Correct Program: True\n\n**Program Name: Water-Filling Expert Replication with Diverse GPU Packing**\n- **Implementation**: Hierarchical pipeline: capacity-aware LPT packs groups to nodes, per-node replica counts via water-filling (binary-searched threshold) with a hybrid D\u2019Hondt/Sainte\u2013Lagu\u00eb greedy tail and up to two donor\u2192receiver fix-ups, then diversity-aware GPU packing using a unique-label fast path, label-spreading seed, projected-load/label-aware greedy, and bounded 1x1/2x2 refinement. Efficient tensor gather/scatter builds physical\u2194logical maps and supports hierarchical or global modes based on group/node divisibility.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.00 across 5 workloads); all validations pass.\n- **Feedback**: Speed is excellent due to greedy heuristics and shallow refinement, but balancedness is modest, indicating remaining peak-load skew. Allowing more fix-up iterations or deeper/localized swap searches could improve balance with a small runtime trade-off.\n**Program Identifier:** Generation 97 - Patch Name adaptive_tail_and_group_tiebreak - Correct Program: True\n\n**Program Name: Hierarchical Expert Load Balancer with Hybrid Replication**\n- **Implementation**: Uses a hierarchical planner: groups\u2192nodes via a unified LPT-based CapacityPacker (optional label-diversity for GPU placement with bounded 1x1/2x2 refinements), and within-node replication via a water-filling ReplicaAllocator (binary search + bulk D\u2019Hondt with Sainte\u2013Lagu\u00eb tail and guarded fix-ups). Indexing is handled with row-wise inverse permutations and blocked mappings; API and I/O remain unchanged.\n- **Performance**: Combined score 0.66 (balancedness 0.311077, speed 1.000000 over 5 workloads).\n- **Feedback**: Program is correct and passes all validations; excellent speed suggests low-overhead heuristics and efficient tensor gathers/scatters. Balancedness is moderate, indicating room for deeper refinement or broader swap searches if higher balance is required.\n**Program Identifier:** Generation 98 - Patch Name tiered_waterfill_capacity_packer - Correct Program: True\n\n**Program Name: Hybrid hierarchical EPLB with peak-aware replication**\n- **Implementation**: Uses greedy sorted packing per layer with bounded multi-swap refinement and stable rank maintenance, plus hierarchical mapping (groups\u2192nodes\u2192GPUs) via inverse permutations. Replication is hybrid: bulk D\u2019Hondt, tail chooses between D\u2019Hondt and Sainte-Lagu\u00eb via peak prediction, followed by a targeted donor/receiver fix-up (optionally a second move if improvement is shallow).\n- **Performance**: Combined score 0.66 (balancedness 0.311848, speed 1.0 across 5 workloads).\n- **Feedback**: Extremely fast due to bounded refinement and efficient scatter/gather logic, passing all validations. Balancedness is moderate; increasing refine_steps or strengthening the local repair/multi-swap search could improve balance at some cost to speed.\n**Program Identifier:** Generation 99 - Patch Name adaptive_tail_and_refine_fixup - Correct Program: True",
  "meta_scratch_pad": "## Successful Algorithmic Patterns\n- Hybrid replication with D\u2019Hondt bulk plus a Sainte\u2013Lagu\u00eb tail, followed by a single, strictly peak-reducing donor\u2192receiver fix-up achieves the top balancedness at perfect speed.\n  - Current best program (code shown; balancedness 0.311848, speed 1.00) implements exactly this: D\u2019Hondt for bulk, Sainte\u2013Lagu\u00eb for the last ~10% of replicas, and a one-step fix-up using top-2 average-load analysis to ensure a strict global-peak reduction.\n  - Matching top performers using this pattern include:\n    - Hybrid Hierarchical EPLB with Greedy Packing (Gen90, 0.311848, 1.00)\n    - Hierarchical expert-parallel load balancer (Gen92, 0.311848, 1.00; hybrid replication, small targeted swap, up to two post-replication moves)\n    - Hybrid hierarchical EPLB with peak-aware replication (Gen99, 0.311848, 1.00)\n- Bounded, targeted swap refinement after greedy, capacity-constrained packing consistently improves load smoothing at negligible cost.\n  - The best program\u2019s CapacityPacker assigns to the lightest pack then runs a searchsorted-based heaviest\u2194lightest 1x1 swap; refine_steps=1 at groups\u2192nodes and refine_steps=2 at the GPU layer. Gen90/92/99 share this bounded refinement theme and also score 0.311848 at speed 1.00.\n- Deterministic, hierarchical pipeline with CPU-friendly tensor ops preserves throughput and stability.\n  - Groups\u2192nodes via balanced_packing, intra-node replication, then GPU packing; stable mappings via PermOps.inverse and gather/scatter. All top programs with this structure maintain speed 1.00 and pass validations.\n\n## Ineffective Approaches\n- Water-filling-centric replication underperforms the hybrid tail, yielding a consistent but lower balancedness cluster around ~0.3111 at the same speed.\n  - Examples: Hierarchical water-fill MoE load balancer (Gen91, 0.3111, 1.00), Hierarchical water-fill diverse packing EPLB (Gen93, 0.311077, 1.00), Peak-aware hierarchical expert load balancer (Gen95, 0.311077, 1.00), Water-Filling Expert Replication with Diverse GPU Packing (Gen97, 0.311077, 1.00).\n  - Feedback points to small fix-up budgets and bounded refinement scopes as limiting peak reduction; water-filling\u2019s allocation is less effective at curbing residual skew than the hybrid tail.\n- \u201cHybrid replication\u201d variants without the strict peak-aware fix-up or with conservative tails/refinements do not reach the top balancedness.\n  - Gen94, Gen96, and Gen98 describe hybrid apportionment and small fix-ups yet land at 0.311077 with speed 1.00; feedback notes conservative refinement (e.g., refine_steps=1\u20132) and small/adaptive tails likely constrain balance gains.\n- Alternative apportionment like Equal-Proportions (Huntington\u2013Hill) lags the hybrid-tail leaders (from previous insights).\n  - Gen89 achieved 0.289431 balancedness (speed 1.00), clearly below the 0.311848 cluster, indicating inferior peak control.\n- Overly restrictive candidate sets and shallow search materially harm balancedness (previous insights).\n  - A prior constrained water-fill variant (Gen80) degraded to 0.2534 balancedness (speed 1.00), attributed to small candidate pools and too few refinements.\n\n## Implementation Insights\n- Why the current best program is effective:\n  - Replication allocator: D\u2019Hondt bulk plus a Sainte\u2013Lagu\u00eb tail assigns most replicas proportionally and uses a peak-aware one-step donor\u2192receiver move only when it strictly lowers the global max (computed via top-2 per-expert average loads). This concentrates effort where it matters without extra passes.\n  - Packing and refinement: Greedy placement to the lightest pack (respecting capacity) followed by a searchsorted-based best 1x1 swap between heaviest and lightest packs reduces the max-load gap. Limited iterations (1 at groups\u2192nodes, 2 at GPU packing) keep runtime minimal while smoothing peaks.\n  - Deterministic, CPU-friendly ops: fast path when capacity==1, simple per-row loops for control logic, and vectorized torch ops (argsort, topk, scatter_add_) for efficiency. Stable ranks and PermOps.inverse ensure consistent mappings.\n- Diversity-aware GPU packing is not essential for top scores under these workloads.\n  - Some top performers include diversity-aware tie handling (e.g., Gen92) and still hit 0.311848; the current best achieves the same score without explicit label-aware tie-breakers, suggesting replication + swap refinement dominate balance here.\n- \u201cHybrid but conservative\u201d implementations can miss the top balancedness.\n  - Programs that nominally use D\u2019Hondt+Sainte\u2013Lagu\u00eb but with conservative tails (e.g., small alpha) or minimal refinement (Gen94/96/98 at 0.311077) indicate the importance of a properly tuned tail and a decisive, peak-aware fix-up.\n\n## Performance Analysis\n- Best-in-class cluster: 0.311848 balancedness at speed 1.00 (combined 0.66).\n  - Achieved by the current best program and Generations 90, 92, and 99, all combining hybrid-tail replication with a bounded, targeted swap refinement.\n- Water-filling cluster: ~0.311077\u20130.3111 balancedness at speed 1.00 (combined 0.66).\n  - Generations 91, 93, 95, 97 fall into this range; they are uniformly fast but consistently trail the hybrid-tail leaders by ~0.0007\u20130.0008 in balancedness.\n- \u201cHybrid but moderate\u201d group: 0.311077 balancedness at speed 1.00 despite hybrid language.\n  - Gen94/96/98 indicate that simply adopting a hybrid allocator is insufficient; the specific tail proportion, fix-up trigger, and refinement aggressiveness correlate with whether a program reaches 0.311848 or stays in the ~0.311077 band.\n- Speed parity across strategies (all at 1.00) highlights balancedness as the differentiator.\n  - Given identical speed, the decisive factor is the replication allocator\u2019s tail/fix-up design plus a minimal but effective local swap refinement, as exemplified by the current best and Gen90/92/99.",
  "meta_recommendations": "1. Make the hybrid tail adaptive with a peak-aware chooser for the last T replica picks per row. Keep the D\u2019Hondt bulk, but for the final T = max(1, round(0.1 \u00b7 num_redundant_row \u00b7 clamp(cur_max/mean(avg) \u2212 1, 0, 1))) picks, compute both D\u2019Hondt and Sainte\u2013Lagu\u00eb winners and select the option that minimizes (new_peak, new_second_peak, receiver_count) after simulating the pick. Implement this inside replicate_experts with a vectorized two-candidate evaluation to preserve the current speed and determinism.\n\n2. Expand the replication fix-up to at most two peak-focused moves with a tiny candidate set. For each row, form donors from the top-2 experts by current average load and receivers from the bottom-2; evaluate all donor\u2192receiver moves and apply the one that strictly reduces (new_peak, new_second_peak, post-move donor_avg) lexicographically, then optionally recompute once and repeat. Continue choosing the donor\u2019s highest-rank physical column to move for O(row) cost and deterministic behavior.\n\n3. Add a bounded 2x2 fallback swap in CapacityPacker when the 1x1 swap stalls. If the searchsorted 1x1 swap yields no strict max-load reduction, consider up to four pairings between the top-2 heaviest-pack items and top-2 lightest-pack items, committing only if it reduces the max pack load; update pack_w incrementally and early-exit. Enable this at both layers (groups\u2192nodes with refine_steps=1 and GPUs with refine_steps=2), gated by capacity >= 2 to avoid overhead when unnecessary.\n\n4. Introduce diversity-aware tie-breaking and a single co-location fix at the GPU layer. Extend balanced_packing to accept optional item labels (logical expert ids) and, on equal-load ties during greedy placement, choose the pack where that label is least seen (prefer unseen), falling back to the lightest pack otherwise. After packing, allow one cheap label-aware swap between two GPUs that removes a duplicate replica co-location without increasing the max load.\n\n5. Make refinement steps adaptive to residual imbalance with early-exit guards. After greedy packing, compute delta_ratio = (max(pack_w) \u2212 min(pack_w)) / mean(pack_w); if delta_ratio exceeds a small threshold (e.g., >0.03 for groups\u2192nodes, >0.02 for GPUs), permit exactly one extra refinement iteration for that row, stopping immediately when no strict reduction occurs. Retain the fast path and current fixed limits otherwise to keep runtime at parity with the best program.",
  "meta_recommendations_history": [
    "1. Upgrade the heaviest\u2194lightest swap to a small k-candidate best-improvement step. For each refinement iteration, take top-2 heaviest items from the heaviest pack and bottom-2 lightest from the lightest pack (via torch.topk on masked weights), evaluate all 4 pair swaps, and perform the swap that minimizes |delta - 2*(wa - wb)|. Keep refine_steps at 1\u20132 and cap k=2 to retain 1.00 speed while extracting extra balance beyond the single-pair heuristic.\n\n2. Add a one-step replication \u201cfix-up\u201d after D\u2019Hondt allocation. Compute per-expert avg = weight / logcnt, pick donor = argmax(avg) and receiver = argmin(avg), and test moving one replica from donor to receiver; apply the move only if it strictly reduces the max(avg) (ties skip). This preserves the proven D\u2019Hondt backbone but corrects occasional over-replication for a heavy expert with negligible overhead.\n\n3. Make GPU placement diversity-aware as a tie-breaker, not a hard constraint. During the greedy selection for GPU packs, when multiple packs have near-equal load (within epsilon), prefer the pack with fewer replicas of the same logical expert; alternatively, add a tiny penalty term load\u2019 = load + lambda * same_expert_count for the candidate pack. Keep the single swap refinement, allowing label spreading to avoid hotspots without hurting the balance metric.\n\n4. Use a hybrid D\u2019Hondt\u2192Sainte-Lagu\u00eb schedule for the last few replicas. Allocate extras with D\u2019Hondt (benefit = weight / (r+1)) until the final 10% (at least 1), then switch to Sainte-Lagu\u00eb (benefit = weight / (2r+1)) for those tail placements. This tempered tail can reduce peak per-replica averages on outlier experts while keeping the proven water-filling behavior for the bulk.\n\n5. Apply stage-specific refinement: keep refine_steps=1 for group\u2192node packing but use refine_steps=2 only in the final GPU packing stage. This targets the most balance-sensitive stage (physical\u2192GPU) with one extra micro-swap while preserving the speed profile elsewhere. Implement by parameterizing balanced_packing with refine_steps and passing 2 only for Step 3.",
    "1. Upgrade the refinement swap to a bounded k-candidate best-improvement step. In balanced_packing, take top-2 heaviest items from the heaviest pack and bottom-2 lightest from the lightest pack (via torch.topk on masked indices) and evaluate all 4 pair swaps; apply the single swap that yields the smallest new_delta = |delta - 2*(wi - wj)| if it strictly improves. Keep refine_steps at 1 for group\u2192node and 2 for physical\u2192GPU to retain 1.00 speed while extracting the extra ~0.0005\u20130.001 balancedness seen from stronger final-stage refinement.\n\n2. Make refinement adaptive: allow a second swap only when the first improvement is small. After the first swap in an iteration, recompute delta; if new_delta > 0.8 * old_delta (i.e., <20% gain), attempt one more swap within the same layer; otherwise break early. Apply this only in the physical\u2192GPU packing stage (refine_steps=2 thresholded), preserving speed but capturing occasional extra gains beyond the current best.\n\n3. Strengthen the replication fix-up with a single best move chosen from top-2 donors and bottom-2 receivers. After the D\u2019Hondt\u2192Sainte-Lagu\u00eb allocation, compute avg = weight/logcnt, take donors = topk(avg, k=2) and receivers = bottom-2; evaluate moving one replica for all valid donor\u2192receiver pairs (donor count > 1), pick the move that minimizes the new peak, and apply it only if new_peak < cur_max. Still perform at most one move per row and reassign the donor\u2019s highest-rank column, preserving correctness and the proven speed profile.\n\n4. Use an adaptive tail apportionment and test Huntington\u2013Hill for the final replicas. Set tail = max(1, round(num_redundant * min(0.15, max(0.05, cv/2)))) where cv = std(weight)/mean(weight) per row; use D\u2019Hondt for the bulk and for the tail either Sainte-Lagu\u00eb (current) or Huntington\u2013Hill (benefit = weight / sqrt(r*(r+1))) and pick the better of the two in a lightweight A/B per row (deterministic). This keeps the bulk behavior that works while giving skewed layers a stronger dampening on peak averages.\n\n5. Add a diversity-aware tie-breaker only under near-ties in GPU packing. When selecting the next pack in the greedy step, if the top-2 candidate packs\u2019 loads differ by < epsilon (e.g., 1e-6 * mean item weight), prefer the pack with fewer replicas of the same logical expert; implement by a tiny penalty load\u2019 = load + lambda * same_expert_count with lambda scaled to be negligible outside ties. Keep the existing refine_steps=2 so the final micro-swaps still drive the main balance gains.",
    "1. Add a bounded 2\u00d72 exchange option in the GPU packing refinement. After computing the current best 1\u00d71 swap between the heaviest and lightest packs, also evaluate swapping two items from the heaviest with two from the lightest (top-2 heavy candidates \u00d7 bottom-2 light candidates \u2192 choose the best pair-pair). Apply at most one 2\u00d72 exchange per iteration, only if it strictly reduces the max pack load more than the best 1\u00d71 swap, keeping refine_steps=2 and all logic on CPU.\n\n2. Expand the swap candidate light side to include the second-lightest pack. In _refine_single_layer, run the same searchsorted-based best-swap selection against both the lightest and second-lightest packs, then choose the single swap that minimizes the new global max (tie-break by smaller heavy\u2013light delta). Keep the strict improvement guard and update loads incrementally; this keeps cost low while avoiding early exits when the true best partner isn\u2019t the absolute lightest.\n\n3. Add a conditional second replication fix-up when the first improvement is small. After the current one-step donor\u2192receiver move (guarded by new_peak < cur_max), recompute avg and cur_max; if the improvement is shallow (e.g., new_peak > 0.9 * old_peak), evaluate one more move from the updated top-2 donors to the bottom-2 receivers (only donors with count > 1), pick the single best move by minimizing the new peak, and apply it only if it strictly improves. Cap at two moves per row total to preserve speed and determinism.\n\n4. Micro-tune the hybrid replication tail around 10% with a tiny, deterministic A/B/C check. Compute tail0 = round(0.10 * num_redundant), then evaluate tail in {max(1, tail0-1), tail0, tail0+1} by simulating only the tail picks (bulk remains D\u2019Hondt), and select the tail size yielding the lowest predicted peak avg (max(weight/logcnt\u2019) after tail). This keeps the proven hybrid behavior while allowing a minimal, low-cost adjustment that can help on skewed layers.\n\n5. Make GPU-stage refinement depth adaptive to measured imbalance. Before refinement, compute delta = max_load - min_load and mean_load; if delta/mean_load > 0.12, set refine_steps=3, else keep 2 (group\u2192node stays at 1). This triggers an extra micro-step only on harder instances where it pays off, maintaining speed 1.00 on typical cases while squeezing additional balance on outliers.",
    "1. Expand the replication fix-up to consider top-2 donors and bottom-2 receivers, then apply a single best move per row. In replicate_experts, compute donor candidates as the top-2 experts by avg load with count > 1, and receiver candidates as the lightest and second-lightest experts; evaluate all 2\u00d72 donor\u2192receiver moves and pick the one minimizing the new peak, applying it only if new_peak < cur_max. Move the donor\u2019s highest-rank replica as now to preserve determinism and keep this to at most one move per row.\n\n2. Broaden the GPU-stage 1\u00d71 swap partner search to include both the lightest and second-lightest packs and optionally the second-heaviest pack as donor. In CapacityPacker._refine_single_layer, compute the searchsorted-based best swap for (heaviest\u2194lightest) and (heaviest\u2194second-lightest); also evaluate the best swap for (second-heaviest\u2194lightest). Choose the single swap that yields the lowest new global max (strictly better), keep refine_steps=2, and maintain CPU-only, deterministic updates.\n\n3. Add a bounded 2\u00d72 exchange as a fallback when no 1\u00d71 swap strictly improves the max. After attempting the 1\u00d71 swap in _refine_single_layer, if delta remains large (e.g., delta/mean_load > 0.10), evaluate swapping the top-2 heavy items with the two best-matching light items (from the lightest or second-lightest pack via the same searchsorted nearest pairing). Apply at most one 2\u00d72 exchange per iteration and only if it beats the best 1\u00d71\u2019s improvement.\n\n4. Make the hybrid replication tail fraction adapt based on measured skew, with a tiny, deterministic A/B/C probe limited to the tail only. Compute a skew signal per row (e.g., top_weight/mean or coefficient of variation); if skew is high, evaluate tail in {tail0, tail0+1}, else in {tail0-1, tail0}, where tail0 = round(0.10 * num_redundant) and tail \u2265 1. Simulate only the tail picks (bulk stays D\u2019Hondt), select the tail size that minimizes the predicted max avg, and then run the existing one-step fix-up.\n\n5. Make refinement depth adaptive at both stages while preserving speed and determinism. Before GPU packing refinement, compute delta/mean; if > 0.12 set refine_steps=3, else keep 2 (groups\u2192nodes remains 1 by default). For group\u2192node packing, allow refine_steps=2 only when delta/mean > 0.20; otherwise keep 1, using the same searchsorted swap logic and strict improvement guard to avoid unnecessary work.",
    "1. Expand the guarded replication fix-up to evaluate a small 2\u00d72 donor/receiver candidate set with a second-order tie-breaker. In replicate_experts, form donors as the top-2 experts by avg load with count > 1 and receivers as the bottom-2 by avg; evaluate all four donor\u2192receiver single moves and choose the one that yields the lowest new peak, breaking ties by the lowest new second-highest avg. Apply at most one move per row, move the donor\u2019s highest-rank replica for determinism, and keep the strict new_peak < cur_max guard.\n\n2. Tail allocator micro-AB across divisor methods restricted to the tail only. Keep D\u2019Hondt for the bulk, but for the last T = max(1, ceil(0.1 \u00b7 num_redundant)) picks, simulate two sequences: Sainte-Lagu\u00eb (weight/(2r-1)) and Huntington\u2013Hill (weight/sqrt(r(r+1))). Select the tail that minimizes the predicted post-tail max avg (ties by second-highest), then perform the existing single fix-up; this preserves determinism and cost while probing a potentially better tail.\n\n3. Broaden GPU-stage 1\u00d71 swap partner search to k=3 packs with a single best global swap. In CapacityPacker._refine_single_layer, consider donors from {heaviest, second-heaviest} and receivers from {lightest, second-lightest, third-lightest}; for each donor\u2013receiver pair, compute the best swap via the existing searchsorted nearest match and pick the swap that minimizes the post-swap global max (strict improvement only). Keep refine_steps=2 and incremental load updates to maintain speed and determinism.\n\n4. Add a bounded chained two-swap fallback when 1\u00d71 cannot strictly reduce the max and imbalance is pronounced. If (max_load - min_load)/mean_load > 0.10 and no 1\u00d71 swap helps, attempt two sequential heavy\u2194light swaps: first choose the best heavy\u2194light swap by searchsorted, update loads, then repeat once with updated heavy/light. Apply the chain only if it strictly reduces the final max vs. the best single-swap candidate; at most one such chain per iteration.\n\n5. Deterministic benefit tie-breaking to smooth replication picks and reduce future peaks. When computing D\u2019Hondt/Sainte-Lagu\u00eb/Hill benefits, if multiple experts tie for the maximum, break ties by smaller current avg load (weight/logcnt), then by lower expert index; this tends to spread early tail picks away from emerging peaks. This is a zero-cost change that preserves determinism and can shave edge cases where equal benefits lead to suboptimal peak growth.",
    "1. Upgrade the one-move replication fix-up to a 2\u00d72 candidate set with a strict peak guard and second-order tie-break. In replicate_experts, form donors as the top-2 avg-loaded experts with count > 1 and receivers as the bottom-2 by avg; evaluate all four donor\u2192receiver single moves and choose the one that minimizes the new peak (ties by the new second-highest avg). Move only one highest-rank donor replica per row and keep the strict new_peak < cur_max guard to preserve speed and determinism.\n\n2. Make the Sainte-Lagu\u00eb tail length adaptive per row based on load dispersion. Replace the fixed tail = max(1, ceil(0.1 \u00b7 num_redundant)) with tail = clamp(1, num_redundant, round(alpha \u00b7 num_redundant \u00b7 s)), where alpha=0.10 and s is a bounded dispersion score (e.g., coefficient of variation of weight across experts clamped to [0.7, 1.3]). This keeps the hybrid structure but gives heavier-tailed rows a slightly longer SL tail, which tends to reduce peaks without extra cost.\n\n3. Tail micro-AB between Sainte-Lagu\u00eb and Huntington\u2013Hill restricted to the last T picks. Keep D\u2019Hondt for the bulk; for the last T tail picks, simulate two sequences of T allocations (SL vs HH) on per-row weight/logcnt and compute the predicted post-tail max avg (ties by second-highest). Choose the better tail deterministically, then run the existing guarded single fix-up; this adds a tiny CPU-only branch while preserving speed.\n\n4. Broaden GPU-stage 1\u00d71 swap partner search and pick the single best global swap per iteration. In CapacityPacker._refine_single_layer for the GPU stage (refine_steps=2), consider donors from {heaviest, second-heaviest} and receivers from {lightest, second-lightest, third-lightest}; for each pair, use the existing searchsorted nearest-match to evaluate the swap that minimizes the post-swap global max. Apply the best strictly improving swap, update pack loads incrementally, and keep one swap per step to retain speed.\n\n5. Add a bounded two-swap chain fallback only under pronounced imbalance. If after attempting the best single 1\u00d71 swap no strict improvement is possible and (max_load - min_load)/mean_load > 0.10, perform at most one two-swap chain: execute the best heavy\u2194light swap, recompute h/l, then attempt one more swap; commit the chain only if it strictly lowers the final max vs. the single-swap baseline. Apply this chain only in the GPU stage and keep refine_steps unchanged to preserve",
    "1. Make the hybrid tail switch point peak-driven and adaptive per row. Keep D\u2019Hondt for the bulk, but for the last T = max(1, round(0.1 \u00b7 num_redundant)) allocations, choose at each step between the D\u2019Hondt pick and the Sainte\u2013Lagu\u00eb pick by predicting the new peak average after assigning to each candidate: new_avg[c] = weight[c]/(cnt[c]+1), new_peak = max(new_avg). Commit the pick that yields the lower new_peak (tie by lower second-highest avg), then proceed; this preserves the hybrid structure while targeting peak reduction where it matters.\n\n2. Strengthen the one-move replication fix-up with a small, peak-aware candidate set and lexicographic tie-breaks. For each row, form donors as the top-3 experts by current avg with count",
    "1. Make the hybrid tail truly adaptive per step with a peak-aware chooser. Keep the current D\u2019Hondt bulk, but for the last T = max(1, round(0.1 \u00b7 num_redundant)) allocations in each row, at every step evaluate both the D\u2019Hondt and Sainte\u2013Lagu\u00eb best picks by simulating the new averages and selecting the assignment that minimizes (new_peak, new_second_peak, receiver_count). This preserves the winning hybrid structure while directly optimizing the objective at the tail where it matters most.\n\n2. Expand the guarded replication fix-up from 1 to up to 2 moves with a tiny, peak-focused candidate set. For each row, form donors from the top-2 experts by current average and receivers from the bottom-2; evaluate all candidate donor\u2192receiver moves in parallel, pick the one minimizing (new_peak, new_second_peak, post-move donor_avg) and apply only if it strictly reduces the peak, then optionally repeat once. Continue to select the physical column with the highest replica rank for the donor, keeping the operation cheap and deterministic.\n\n3. Add a bounded 2x2 fallback swap in the packer when the 1x1 heaviest\u2194lightest swap stalls. In CapacityPacker._refine_single_layer, if the 1x1 searchsorted swap yields no strict delta reduction, try at most one 2x2 swap between the heaviest and lightest packs by evaluating the best pair among top-2 heavy items and top-2 light items (minimizing the resulting max pack load). Commit only if it reduces the peak; this retains speed while unlocking small extra gains seen in diverse-aware variants.\n\n4. Introduce diversity-aware tie-breaking for GPU packing to avoid co-locating replicas of the same logical expert. Extend balanced_packing to accept optional item labels and per-pack label counts; during greedy placement, when multiple GPUs have equal current load, choose the GPU where the item\u2019s label (logical expert id) is least frequent in that pack (prefer unseen). Use this only as a tie-breaker to keep the fast lightest-pack heuristic intact and maintain determinism.\n\n5. Make GPU-layer refinement steps adaptive to residual imbalance. After the initial greedy placement at the GPU layer, compute delta = max(pack_w) \u2212 min(pack_w); if delta/mean(pack_w) exceeds a small threshold (e.g., 2%), allow one extra refinement step (refine_steps = 3 instead of 2) for that row only. This targets the few outlier rows that still drive the global peak, preserving the 1.00 speed profile on average.",
    "1. Make the hybrid tail adaptive per row and per step with a peak-aware chooser. Keep the current D\u2019Hondt bulk, but for the last T allocations in each row compute both the D\u2019Hondt and Sainte\u2013Lagu\u00eb winners, simulate the post-pick averages, and choose the one minimizing (new_peak, new_second_peak, receiver_count). Set T row-wise as T = max(1, round(0.1 \u00b7 num_redundant_row \u00b7 clamp((cur_max/mean(avg) \u2212 1), 0, 1))), and implement this inside replicate_experts by vectorizing the two-candidate evaluation to preserve speed.\n\n2. Expand the guarded replication fix-up to up to 2 moves with a tiny, peak-focused candidate set. For each row, form donors from the top-2 experts by current average and receivers from the bottom-2, evaluate all donor\u2192receiver moves, and apply the one that strictly reduces (new_peak, new_second_peak, post-move donor_avg) lexicographically; then recompute and optionally repeat once. Continue to pick the donor\u2019s highest-rank physical column for the move to keep O(row) time and determinism.\n\n3. Add a bounded 2x2 fallback swap in CapacityPacker._refine_single_layer when the 1x1 swap stalls. If the searchsorted 1x1 swap provides no strict delta reduction, evaluate at most four pairings between the top-2 heavy-pack items and top-2 light-pack items, committing only if it reduces the max pack load; update pack_w incrementally. Enable this at both layers (groups\u2192nodes with refine_steps=1 and GPUs with refine_steps=2) without increasing refine_steps counts.\n\n4. Introduce diversity-aware tie-breaking in GPU packing to avoid replica co-location. Extend balanced_packing to accept optional item labels (logical expert ids) and maintain a tiny per-pack label counter; on equal-load ties during greedy placement, select the pack where the label count is minimal (prefer unseen), otherwise choose the lightest pack as usual. Apply this only at the GPU layer to preserve the proven lightest-pack heuristic and determinism.\n\n5. Make refinement steps adaptive to residual imbalance with early-exit guards. After greedy packing, compute delta_ratio = (max(pack_w) \u2212 min(pack_w)) / mean(pack_w); if delta_ratio > 0.02 at the GPU layer, allow one extra refinement iteration (effective refine_steps=3) for that row only; similarly, if the groups\u2192nodes delta_ratio > 0.03, permit a single extra refinement iteration there. Keep the existing 1x1-first policy, stop as soon as no strict delta reduction is achieved, and retain the fast path when imbalance is already small.",
    "1. Make the hybrid tail adaptive with a peak-aware chooser for the last T replica picks per row. Keep the D\u2019Hondt bulk, but for the final T = max(1, round(0.1 \u00b7 num_redundant_row \u00b7 clamp(cur_max/mean(avg) \u2212 1, 0, 1))) picks, compute both D\u2019Hondt and Sainte\u2013Lagu\u00eb winners and select the option that minimizes (new_peak, new_second_peak, receiver_count) after simulating the pick. Implement this inside replicate_experts with a vectorized two-candidate evaluation to preserve the current speed and determinism.\n\n2. Expand the replication fix-up to at most two peak-focused moves with a tiny candidate set. For each row, form donors from the top-2 experts by current average load and receivers from the bottom-2; evaluate all donor\u2192receiver moves and apply the one that strictly reduces (new_peak, new_second_peak, post-move donor_avg) lexicographically, then optionally recompute once and repeat. Continue choosing the donor\u2019s highest-rank physical column to move for O(row) cost and deterministic behavior.\n\n3. Add a bounded 2x2 fallback swap in CapacityPacker when the 1x1 swap stalls. If the searchsorted 1x1 swap yields no strict max-load reduction, consider up to four pairings between the top-2 heaviest-pack items and top-2 lightest-pack items, committing only if it reduces the max pack load; update pack_w incrementally and early-exit. Enable this at both layers (groups\u2192nodes with refine_steps=1 and GPUs with refine_steps=2), gated by capacity >= 2 to avoid overhead when unnecessary.\n\n4. Introduce diversity-aware tie-breaking and a single co-location fix at the GPU layer. Extend balanced_packing to accept optional item labels (logical expert ids) and, on equal-load ties during greedy placement, choose the pack where that label is least seen (prefer unseen), falling back to the lightest pack otherwise. After packing, allow one cheap label-aware swap between two GPUs that removes a duplicate replica co-location without increasing the max load.\n\n5. Make refinement steps adaptive to residual imbalance with early-exit guards. After greedy packing, compute delta_ratio = (max(pack_w) \u2212 min(pack_w)) / mean(pack_w); if delta_ratio exceeds a small threshold (e.g., >0.03 for groups\u2192nodes, >0.02 for GPUs), permit exactly one extra refinement iteration for that row, stopping immediately when no strict reduction occurs. Retain the fast path and current fixed limits otherwise to keep runtime at parity with the best program."
  ],
  "total_programs_meta_processed": 100
}