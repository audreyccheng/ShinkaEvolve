--- a/original.py
+++ b/original.py
@@ -1,591 +1,644 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 GPU_MEM_SIZE = 80  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         A placement of models to GPUs
     """
     # Helper to compute KVPR for a GPU safely
     def kvpr(R, rem_mem):
         if rem_mem <= 0:
             return float('inf')
         return R / rem_mem
 
     # Early return for trivial cases
     empty = {gpu_id: [] for gpu_id in range(gpu_num)}
     if not models or gpu_num <= 0:
         return empty
 
     # Greedy min-max assignment using lookahead of resultant max KVPR for a given ordering
     def greedy_assign(sorted_models):
         placement = {gpu_id: [] for gpu_id in range(gpu_num)}
         rem_mem = [GPU_MEM_SIZE for _ in range(gpu_num)]   # remaining memory per GPU
         sum_r_over_s = [0.0 for _ in range(gpu_num)]       # sum of r_j / s_j per GPU
 
         for model in sorted_models:
             dR = model.req_rate / model.slo
             current_kvprs = [kvpr(sum_r_over_s[i], rem_mem[i]) for i in range(gpu_num)]
 
             best_gpu = None
             best_resulting_max = float('inf')
             best_new_gpu_kvpr = float('inf')
             best_new_rem = -1
 
             for gid in range(gpu_num):
                 if model.model_size <= rem_mem[gid]:
                     new_R = sum_r_over_s[gid] + dR
                     new_mem = rem_mem[gid] - model.model_size
                     new_gpu_kvpr = kvpr(new_R, new_mem)
 
                     # Compute resulting global max KVPR after placing on gid
                     resulting_max = new_gpu_kvpr
                     for j in range(gpu_num):
                         if j == gid:
                             continue
                         if current_kvprs[j] > resulting_max:
                             resulting_max = current_kvprs[j]
 
                     # Tie-breaking: minimize resulting max; then minimize this GPU's KVPR; then prefer more remaining memory.
                     if (resulting_max < best_resulting_max or
                         (resulting_max == best_resulting_max and new_gpu_kvpr < best_new_gpu_kvpr) or
                         (resulting_max == best_resulting_max and new_gpu_kvpr == best_new_gpu_kvpr and new_mem > best_new_rem)):
                         best_resulting_max = resulting_max
                         best_new_gpu_kvpr = new_gpu_kvpr
                         best_new_rem = new_mem
                         best_gpu = gid
 
             if best_gpu is None:
                 # No GPU can fit this model without exceeding memory
                 raise ValueError(
                     f"Unable to place model of size {model.model_size} GB on any GPU. "
                     f"Remaining per-GPU memory: {rem_mem}"
                 )
 
             # Commit placement
             placement[best_gpu].append(model)
             sum_r_over_s[best_gpu] += dR
             rem_mem[best_gpu] -= model.model_size
 
         return placement, rem_mem, sum_r_over_s
 
     # Local improvement: try moving or swapping models to reduce max KVPR
     def improve(placement, rem_mem, sum_r_over_s):
         def compute_all_kvprs():
             return [kvpr(sum_r_over_s[i], rem_mem[i]) for i in range(gpu_num)]
 
         eps = 1e-12
         # Limit the number of improvement iterations to keep it simple and fast
         max_iters = max(1, min(len(models), 6 * gpu_num))
         for _ in range(max_iters):
             kvprs = compute_all_kvprs()
             if not kvprs:
                 break
             current_max = max(kvprs)
             max_gid = max(range(gpu_num), key=lambda i: kvprs[i])
 
             improved = False
             best_move = None  # (src, dst, model, resulting_max, dst_new_kvpr, dst_new_mem)
 
             # Try moving a single model from the max-pressure GPU to another GPU
             for mdl in list(placement[max_gid]):
                 dR = mdl.req_rate / mdl.slo
                 size = mdl.model_size
 
                 # State after removing from source
                 src_new_R = sum_r_over_s[max_gid] - dR
                 src_new_mem = rem_mem[max_gid] + size
                 src_new_kvpr = kvpr(src_new_R, src_new_mem)
 
                 for dst in range(gpu_num):
                     if dst == max_gid:
                         continue
                     if size <= rem_mem[dst]:
                         dst_new_R = sum_r_over_s[dst] + dR
                         dst_new_mem = rem_mem[dst] - size
                         dst_new_kvpr = kvpr(dst_new_R, dst_new_mem)
 
                         # Compute resulting global max after move
                         resulting_max = dst_new_kvpr
                         if src_new_kvpr > resulting_max:
                             resulting_max = src_new_kvpr
                         for j in range(gpu_num):
                             if j == max_gid or j == dst:
                                 continue
                             if kvprs[j] > resulting_max:
                                 resulting_max = kvprs[j]
 
                         if resulting_max + eps < current_max:
                             # Tie-breakers: minimize resulting_max, then minimize dst kvpr, then maximize dst remaining mem
                             move_better = False
                             if best_move is None:
                                 move_better = True
                             else:
                                 _, _, _, best_res_max, best_dst_kvpr, best_dst_rem = best_move
                                 if (resulting_max < best_res_max or
                                     (resulting_max == best_res_max and dst_new_kvpr < best_dst_kvpr) or
                                     (resulting_max == best_res_max and dst_new_kvpr == best_dst_kvpr and dst_new_mem > best_dst_rem)):
                                     move_better = True
                             if move_better:
                                 best_move = (max_gid, dst, mdl, resulting_max, dst_new_kvpr, dst_new_mem)
 
             if best_move is not None:
                 # Apply the best move
                 src, dst, mdl, _, _, _ = best_move
                 placement[src].remove(mdl)
                 placement[dst].append(mdl)
                 dR = mdl.req_rate / mdl.slo
                 size = mdl.model_size
                 sum_r_over_s[src] -= dR
                 rem_mem[src] += size
                 sum_r_over_s[dst] += dR
                 rem_mem[dst] -= size
                 improved = True
             else:
                 # Try one pairwise swap between the most pressured GPU and others
                 best_swap = None  # (src, dst, a, b, resulting_max)
                 src = max_gid
                 # Cap the search to keep it fast
                 cap_a = max(3, min(10, len(placement[src])))
                 for ai, a in enumerate(list(placement[src])[:cap_a]):
                     aR = a.req_rate / a.slo
                     aS = a.model_size
                     for dst in range(gpu_num):
                         if dst == src or not placement[dst]:
                             continue
                         cap_b = max(3, min(10, len(placement[dst])))
                         for bi, b in enumerate(list(placement[dst])[:cap_b]):
                             bR = b.req_rate / b.slo
                             bS = b.model_size
 
                             # New states after swap
                             src_new_R = sum_r_over_s[src] - aR + bR
                             dst_new_R = sum_r_over_s[dst] - bR + aR
                             src_new_mem = rem_mem[src] + aS - bS
                             dst_new_mem = rem_mem[dst] + bS - aS
 
                             if src_new_mem < 0 or dst_new_mem < 0:
                                 continue
 
                             src_new_kvpr = kvpr(src_new_R, src_new_mem)
                             dst_new_kvpr = kvpr(dst_new_R, dst_new_mem)
 
                             # Compute resulting max across all GPUs
                             resulting_max = src_new_kvpr if src_new_kvpr > dst_new_kvpr else dst_new_kvpr
                             for j in range(gpu_num):
                                 if j == src or j == dst:
                                     continue
                                 if kvprs[j] > resulting_max:
                                     resulting_max = kvprs[j]
 
                             if resulting_max + eps < current_max:
                                 if best_swap is None or resulting_max < best_swap[4]:
                                     best_swap = (src, dst, a, b, resulting_max)
 
                 if best_swap is not None:
                     src, dst, a, b, _ = best_swap
                     # Apply swap
                     placement[src].remove(a)
                     placement[dst].append(a)
                     placement[dst].remove(b)
                     placement[src].append(b)
 
                     aR = a.req_rate / a.slo
                     bR = b.req_rate / b.slo
                     aS = a.model_size
                     bS = b.model_size
 
                     sum_r_over_s[src] = sum_r_over_s[src] - aR + bR
                     sum_r_over_s[dst] = sum_r_over_s[dst] - bR + aR
                     rem_mem[src] = rem_mem[src] + aS - bS
                     rem_mem[dst] = rem_mem[dst] + bS - aS
                     improved = True
                 else:
                     # Length-2 eject chain: src(max)->dst by evicting b from dst to k
                     best_chain = None  # (src, dst, k, a, b, resulting_max)
                     cap_a2 = min(6, len(placement[src]))
                     for a in list(placement[src])[:cap_a2]:
                         aR = a.req_rate / a.slo
                         aS = a.model_size
                         # Consider destination GPUs that currently cannot fit a without eviction
                         for dst in range(gpu_num):
                             if dst == src or not placement[dst]:
                                 continue
                             if rem_mem[dst] + 1e-9 >= aS:
                                 continue  # regular move would handle this
                             cap_b2 = min(6, len(placement[dst]))
                             for b in list(placement[dst])[:cap_b2]:
                                 bR = b.req_rate / b.slo
                                 bS = b.model_size
                                 # After evicting b from dst, can a fit?
                                 if rem_mem[dst] + bS + 1e-9 < aS:
                                     continue
                                 # Find a third GPU k to host b
                                 for k in range(gpu_num):
                                     if k == dst or k == src:
                                         continue
                                     if rem_mem[k] + 1e-9 < bS:
                                         continue
                                     # Compute new remaining memories
                                     rem_src_new = rem_mem[src] + aS
                                     rem_dst_new = rem_mem[dst] + bS - aS
                                     rem_k_new = rem_mem[k] - bS
                                     if rem_src_new <= 0 or rem_dst_new <= 0 or rem_k_new <= 0:
                                         # If any bucket becomes full with positive load, skip
                                         if (rem_src_new <= 0 and (sum_r_over_s[src] - aR) > 1e-12) or \
                                            (rem_dst_new <= 0 and (sum_r_over_s[dst] - bR + aR) > 1e-12) or \
                                            (rem_k_new <= 0 and (sum_r_over_s[k] + bR) > 1e-12):
                                             continue
                                     # New loads
                                     src_R_new = sum_r_over_s[src] - aR
                                     dst_R_new = sum_r_over_s[dst] - bR + aR
                                     k_R_new = sum_r_over_s[k] + bR
 
                                     # Compute KVPRs post chain
                                     src_kv = kvpr(src_R_new, rem_src_new)
                                     dst_kv = kvpr(dst_R_new, rem_dst_new)
                                     k_kv = kvpr(k_R_new, rem_k_new)
                                     resulting = src_kv if src_kv > dst_kv else dst_kv
                                     if k_kv > resulting:
                                         resulting = k_kv
                                     for g in range(gpu_num):
                                         if g == src or g == dst or g == k:
                                             continue
                                         if kvprs[g] > resulting:
                                             resulting = kvprs[g]
                                     if resulting + eps < current_max:
                                         if best_chain is None or resulting < best_chain[5]:
                                             best_chain = (src, dst, k, a, b, resulting)
                     if best_chain is not None:
                         src, dst, k, a, b, _ = best_chain
                         # Apply chain: b from dst->k, a from src->dst
                         placement[src].remove(a)
                         placement[dst].append(a)
                         placement[dst].remove(b)
                         placement[k].append(b)
 
                         aR = a.req_rate / a.slo
                         bR = b.req_rate / b.slo
                         aS = a.model_size
                         bS = b.model_size
 
                         # Update sums and remaining memory
                         sum_r_over_s[src] -= aR
                         rem_mem[src] += aS
 
                         sum_r_over_s[dst] = sum_r_over_s[dst] - bR + aR
                         rem_mem[dst] = rem_mem[dst] + bS - aS
 
                         sum_r_over_s[k] += bR
                         rem_mem[k] -= bS
                         improved = True
 
             if not improved:
                 break  # no improving move/swap found
 
         return placement, rem_mem, sum_r_over_s
 
     # Build multiple candidate orderings and choose the best after improvements
     def pressure_weight(m):
         denom = GPU_MEM_SIZE - m.model_size
         if denom <= 0:
             return float('inf')
         return (m.req_rate / m.slo) / denom
 
     def r_over_s(m):
         return (m.req_rate / m.slo)
 
     def density(m):
         # Prefer higher r/s per memory footprint
         sz = m.model_size if m.model_size > 0 else 1e-9
         return (m.req_rate / m.slo) / sz
 
     # Candidate orderings (descending where appropriate)
     orderings = [
         lambda ms: sorted(ms, key=pressure_weight, reverse=True),
         lambda ms: sorted(ms, key=r_over_s, reverse=True),
         lambda ms: sorted(ms, key=lambda m: m.model_size, reverse=True),  # size-desc
         lambda ms: sorted(ms, key=lambda m: m.model_size),                # size-asc
         lambda ms: sorted(ms, key=density, reverse=True),
         lambda ms: sorted(ms, key=lambda m: (r_over_s(m), -m.model_size), reverse=True),
     ]
 
     best_placement = None
     best_score = (float('inf'), float('inf'), float('inf'))
 
     for make_order in orderings:
         try:
             ordered = make_order(models)
             placement, rem_mem, sum_r_over_s = greedy_assign(ordered)
             # Refine with local improvements
             placement, rem_mem, sum_r_over_s = improve(placement, rem_mem, sum_r_over_s)
             # Evaluate
             kvprs = [kvpr(sum_r_over_s[i], rem_mem[i]) for i in range(gpu_num)]
             if kvprs:
                 sorted_k = sorted(kvprs, reverse=True)
                 cand_max = sorted_k[0]
                 cand_second = sorted_k[1] if len(sorted_k) > 1 else 0.0
                 cand_avg = sum(kvprs) / len(kvprs)
             else:
                 cand_max = 0.0
                 cand_second = 0.0
                 cand_avg = 0.0
             cand_score = (cand_max, cand_second, cand_avg)
             if cand_score < best_score:
                 best_score = cand_score
                 best_placement = placement
         except ValueError:
             # This ordering couldn't place all models due to per-GPU memory constraints
             continue
 
     if best_placement is None:
         # Fall back to an error consistent with greedy behavior
         # Try a final simple size-desc heuristic to produce a clearer error state
         ordered = sorted(models, key=lambda m: m.model_size, reverse=True)
         # This will raise ValueError if infeasible
         best_placement, _, _ = greedy_assign(ordered)
 
     # Parametric refinement: binary search on max KVPR target T using transformed weights
     S = GPU_MEM_SIZE
 
     def max_kvpr_of(placement_dict):
         max_v = 0.0
         for gid in range(gpu_num):
             bucket = placement_dict.get(gid, [])
             R = 0.0
             used = 0.0
             for m in bucket:
                 R += (m.req_rate / m.slo)
                 used += m.model_size
             v = kvpr(R, S - used)
             if v > max_v:
                 max_v = v
         return max_v
 
     def bfd_assign_for_T(T):
-        # Best-Fit-Decreasing in transformed space; feasibility implies KVPR <= T and memory <= S per GPU
+        # Balanced-slack aware BFD in transformed space; feasibility implies KVPR <= T and memory <= S per GPU
         capacity = T * S
         if T < 0:
             return None
         # Build items with weights
         items = []
         for m in models:
             dR = (m.req_rate / m.slo)
             w = dR + T * m.model_size
             if w < 0:
                 w = 0.0
             items.append((w, dR, m.model_size, m))
         # Sort by weight descending
         items.sort(key=lambda x: x[0], reverse=True)
 
         used_w = [0.0] * gpu_num
         bins_R = [0.0] * gpu_num
         bins_used_mem = [0.0] * gpu_num
         assign = {i: [] for i in range(gpu_num)}
 
         for w, dR, sz, m in items:
+            # Current per-GPU KVPRs
+            kv_now = []
+            for gid in range(gpu_num):
+                rem_g = S - bins_used_mem[gid]
+                kv_now.append(kvpr(bins_R[gid], rem_g))
+            # Precompute global max and second max of current KVPRs
+            if gpu_num > 0:
+                max_val = -1.0
+                second_val = -1.0
+                max_idx = -1
+                for idx, val in enumerate(kv_now):
+                    if val > max_val:
+                        second_val = max_val
+                        max_val = val
+                        max_idx = idx
+                    elif val > second_val:
+                        second_val = val
             best_bin = None
-            best_key = (float('inf'), float('inf'))
+            best_key = (float('inf'), float('inf'), float('-inf'))  # (resulting_max, K_after, -rem_after)
+
             for gid in range(gpu_num):
+                # Enforce both transformed capacity and memory feasibility
+                if bins_used_mem[gid] + sz > S + 1e-9:
+                    continue
                 nw = used_w[gid] + w
-                if nw <= capacity + 1e-9:
-                    rem_after = S - (bins_used_mem[gid] + sz)
-                    R_after = bins_R[gid] + dR
-                    if rem_after <= 0:
-                        kv_after = float('inf') if R_after > 1e-12 else 0.0
-                    else:
-                        kv_after = R_after / rem_after
-                    key = (nw, kv_after)
-                    if key < best_key:
-                        best_key = key
-                        best_bin = gid
+                if nw > capacity + 1e-9:
+                    continue
+
+                rem_after = S - (bins_used_mem[gid] + sz)
+                R_after = bins_R[gid] + dR
+                if rem_after <= 0:
+                    kv_after = float('inf') if R_after > 1e-12 else 0.0
+                else:
+                    kv_after = R_after / rem_after
+
+                # Resulting global max KVPR if we place on gid
+                other_max = second_val if gid == max_idx else max_val
+                resulting_max = kv_after if kv_after > other_max else other_max
+
+                # Slack left (prefer tighter but feasible packing as tie-break)
+                K_after = capacity - nw
+                key = (resulting_max, K_after, -rem_after)
+                if key < best_key:
+                    best_key = key
+                    best_bin = gid
+
             if best_bin is None:
                 return None
+
             used_w[best_bin] += w
             bins_R[best_bin] += dR
             bins_used_mem[best_bin] += sz
             assign[best_bin].append(m)
 
         # Validate memory and KVPR constraints for this T
         for gid in range(gpu_num):
             if bins_used_mem[gid] - S > 1e-6:
                 return None
             rem = S - bins_used_mem[gid]
             if rem <= 0:
                 # if no remaining memory, require zero R to avoid inf KVPR
                 if bins_R[gid] > 1e-12:
                     return None
             else:
                 if (bins_R[gid] / rem) - T > 1e-6:
                     return None
         return assign
 
     # Compute bounds for the binary search
     total_R = sum((m.req_rate / m.slo) for m in models)
     total_size = sum(m.model_size for m in models)
 
     # Lower bound 1: per-model bound T >= dR / (S - size) for any model
     lb1 = 0.0
     infeasible_single = False
     for m in models:
         dR = (m.req_rate / m.slo)
         denom = S - m.model_size
         if denom <= 0:
             if dR > 0:
                 infeasible_single = True
             continue
         if dR > 0:
             cand = dR / denom
             if cand > lb1:
                 lb1 = cand
 
     # Lower bound 2: global bound from totals
     denom2 = gpu_num * S - total_size
     if denom2 <= 0 and total_R > 0:
         # Not enough aggregate free memory to host any KV capacity
         return best_placement
     lb2 = 0.0 if denom2 <= 0 else (total_R / denom2 if total_R > 0 else 0.0)
 
     # Lower bound 3: pair bound for items that cannot co-reside on one GPU
     lb_pair = 0.0
     P = min(len(models), 200)
     by_size = sorted(models, key=lambda m: m.model_size, reverse=True)[:P]
     for i in range(len(by_size)):
         mi = by_size[i]; si = mi.model_size; ri = (mi.req_rate / mi.slo)
         for j in range(i + 1, len(by_size)):
             mj = by_size[j]; sj = mj.model_size; rj = (mj.req_rate / mj.slo)
             if si + sj > S:
                 denom = 2 * S - (si + sj)
                 if denom > 0:
                     cand = (ri + rj) / denom
                     if cand > lb_pair:
                         lb_pair = cand
 
-    # Lower bound 4: k-prefix bound (k up to min(gpu_num, 6))
+    # Lower bound 4: lightweight triplet bound for triples with si+sj+sk > 2S
+    lb_triplet = 0.0
+    if len(by_size) >= 3:
+        KTOP = min(8, len(by_size))
+        topK = by_size[:KTOP]
+        for i in range(len(by_size)):
+            si = by_size[i].model_size; ri = (by_size[i].req_rate / by_size[i].slo)
+            for j in range(i + 1, len(by_size)):
+                sj = by_size[j].model_size; rj = (by_size[j].req_rate / by_size[j].slo)
+                for mk in topK:
+                    if mk is by_size[i] or mk is by_size[j]:
+                        continue
+                    sk = mk.model_size; rk = (mk.req_rate / mk.slo)
+                    ssum = si + sj + sk
+                    if ssum > 2 * S:
+                        denom = 3 * S - ssum
+                        if denom > 0:
+                            cand = (ri + rj + rk) / denom
+                            if cand > lb_triplet:
+                                lb_triplet = cand
+
+    # Lower bound 5: k-prefix bound (k up to min(gpu_num, 6))
     lb_k = 0.0
     sized = sorted(((m.model_size, (m.req_rate / m.slo)) for m in models), key=lambda t: t[0], reverse=True)
     prefix_s, prefix_r = [], []
     cs = 0.0; cr = 0.0
     for s_i, r_i in sized:
         cs += s_i; cr += r_i
         prefix_s.append(cs); prefix_r.append(cr)
     for k in range(1, min(gpu_num, 6) + 1):
         threshold = (k - 1) * S
         idx = -1
         for t in range(len(prefix_s)):
             if prefix_s[t] > threshold:
                 idx = t
                 break
         if idx >= 0:
             numer = prefix_r[idx]
             denom = k * S - prefix_s[idx]
             if denom > 0 and numer > 0:
                 cand = numer / denom
                 if cand > lb_k:
                     lb_k = cand
 
     if infeasible_single:
         return best_placement
 
-    lower = max(0.0, lb1, lb2, lb_pair, lb_k)
+    lower = max(0.0, lb1, lb2, lb_pair, lb_triplet, lb_k)
     upper = max_kvpr_of(best_placement)
     if not (upper < float('inf')):
         return best_placement
     if lower > upper:
         lower = upper
 
     # Binary search to tighten T
     best_bsearch = None
     lo, hi = lower, upper
     for _ in range(24):  # light and fast
         mid = (lo + hi) / 2.0
         cand = bfd_assign_for_T(mid)
         if cand is not None:
             best_bsearch = cand
             hi = mid
         else:
             lo = mid
 
     # Choose the best between greedy-based and parametric candidate (optionally refine the latter)
     def summarize(placement):
         rem = [S] * gpu_num
         sr = [0.0] * gpu_num
         for gid in range(gpu_num):
             for m in placement.get(gid, []):
                 rem[gid] -= m.model_size
                 sr[gid] += (m.req_rate / m.slo)
         return rem, sr
 
     best_final = best_placement
     best_val = max_kvpr_of(best_final)
 
     if best_bsearch is not None:
         # Try a short local improvement on the parametric candidate
         rem_mem_b, sum_r_b = summarize(best_bsearch)
         improved_bsearch, rem_mem_b, sum_r_b = improve({i: list(best_bsearch.get(i, [])) for i in range(gpu_num)},
                                                        rem_mem_b, sum_r_b)
         val_b = max_kvpr_of(improved_bsearch)
         if val_b + 1e-12 < best_val:
             best_final = improved_bsearch
             best_val = val_b
 
     return best_final
 
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")