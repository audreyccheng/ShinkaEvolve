<NAME>
preplace_memonly_and_seeded_bsearch
</NAME>

<DESCRIPTION>
Introduce memory-only (slo==0 or req_rate==0) handling and pre-placement. The changes:
- Safely treat slo==0 demand as zero across the algorithm to avoid infinities.
- Build active_models excluding memory-only models; run greedy placement only on active models.
- After choosing the best greedy placement, place memory-only models largest-first onto GPUs with the most remaining memory.
- Keep the parametric T-search intact but make its demand computations safe.
These targeted edits reduce KVPR by isolating memory-only impact and prevent division-by-zero errors, while keeping code simple and correct.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        for model in sorted_models:
            dR = model.req_rate / model.slo
=======
        for model in sorted_models:
            dR = (model.req_rate / model.slo) if getattr(model, "slo", 0) != 0 else 0.0
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
            for mdl in list(placement[max_gid]):
                dR = mdl.req_rate / mdl.slo
                size = mdl.model_size
=======
            for mdl in list(placement[max_gid]):
                dR = (mdl.req_rate / mdl.slo) if getattr(mdl, "slo", 0) != 0 else 0.0
                size = mdl.model_size
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
                src, dst, mdl, _, _, _ = best_move
                placement[src].remove(mdl)
                placement[dst].append(mdl)
                dR = mdl.req_rate / mdl.slo
                size = mdl.model_size
=======
                src, dst, mdl, _, _, _ = best_move
                placement[src].remove(mdl)
                placement[dst].append(mdl)
                dR = (mdl.req_rate / mdl.slo) if getattr(mdl, "slo", 0) != 0 else 0.0
                size = mdl.model_size
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
                for ai, a in enumerate(list(placement[src])[:cap_a]):
                    aR = a.req_rate / a.slo
                    aS = a.model_size
=======
                for ai, a in enumerate(list(placement[src])[:cap_a]):
                    aR = (a.req_rate / a.slo) if getattr(a, "slo", 0) != 0 else 0.0
                    aS = a.model_size
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
                        for bi, b in enumerate(list(placement[dst])[:cap_b]):
                            bR = b.req_rate / b.slo
                            bS = b.model_size

                            # New states after swap
                            src_new_R = sum_r_over_s[src] - aR + bR
=======
                        for bi, b in enumerate(list(placement[dst])[:cap_b]):
                            bR = (b.req_rate / b.slo) if getattr(b, "slo", 0) != 0 else 0.0
                            bS = b.model_size

                            # New states after swap
                            src_new_R = sum_r_over_s[src] - aR + bR
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
                    cap_a2 = min(6, len(placement[src]))
                    for a in list(placement[src])[:cap_a2]:
                        aR = a.req_rate / a.slo
                        aS = a.model_size
=======
                    cap_a2 = min(6, len(placement[src]))
                    for a in list(placement[src])[:cap_a2]:
                        aR = (a.req_rate / a.slo) if getattr(a, "slo", 0) != 0 else 0.0
                        aS = a.model_size
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
                            cap_b2 = min(6, len(placement[dst]))
                            for b in list(placement[dst])[:cap_b2]:
                                bR = b.req_rate / b.slo
                                bS = b.model_size
=======
                            cap_b2 = min(6, len(placement[dst]))
                            for b in list(placement[dst])[:cap_b2]:
                                bR = (b.req_rate / b.slo) if getattr(b, "slo", 0) != 0 else 0.0
                                bS = b.model_size
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
                    aR = a.req_rate / a.slo
                    bR = b.req_rate / b.slo
                    aS = a.model_size
                    bS = b.model_size
=======
                    aR = (a.req_rate / a.slo) if getattr(a, "slo", 0) != 0 else 0.0
                    bR = (b.req_rate / b.slo) if getattr(b, "slo", 0) != 0 else 0.0
                    aS = a.model_size
                    bS = b.model_size
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
                        aR = a.req_rate / a.slo
                        bR = b.req_rate / b.slo
                        aS = a.model_size
                        bS = b.model_size
=======
                        aR = (a.req_rate / a.slo) if getattr(a, "slo", 0) != 0 else 0.0
                        bR = (b.req_rate / b.slo) if getattr(b, "slo", 0) != 0 else 0.0
                        aS = a.model_size
                        bS = b.model_size
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    def pressure_weight(m):
        denom = GPU_MEM_SIZE - m.model_size
        if denom <= 0:
            return float('inf')
        return (m.req_rate / m.slo) / denom

    def r_over_s(m):
        return (m.req_rate / m.slo)

    def density(m):
        # Prefer higher r/s per memory footprint
        sz = m.model_size if m.model_size > 0 else 1e-9
        return (m.req_rate / m.slo) / sz
=======
    def pressure_weight(m):
        denom = GPU_MEM_SIZE - m.model_size
        if denom <= 0:
            return float('inf')
        base = (m.req_rate / m.slo) if getattr(m, "slo", 0) != 0 else 0.0
        return base / denom

    def r_over_s(m):
        return (m.req_rate / m.slo) if getattr(m, "slo", 0) != 0 else 0.0

    def density(m):
        # Prefer higher r/s per memory footprint
        sz = m.model_size if m.model_size > 0 else 1e-9
        base = (m.req_rate / m.slo) if getattr(m, "slo", 0) != 0 else 0.0
        return base / sz
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    best_placement = None
=======
    # Split memory-only vs active models
    mem_only = [m for m in models if getattr(m, "slo", 0) == 0 or getattr(m, "req_rate", 0) == 0]
    active_models = [m for m in models if m not in mem_only]

    best_placement = None
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
            ordered = make_order(models)
            placement, rem_mem, sum_r_over_s = greedy_assign(ordered)
=======
            ordered = make_order(active_models)
            placement, rem_mem, sum_r_over_s = greedy_assign(ordered)
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    if best_placement is None:
        # Fall back to an error consistent with greedy behavior
        # Try a final simple size-desc heuristic to produce a clearer error state
        ordered = sorted(models, key=lambda m: m.model_size, reverse=True)
        # This will raise ValueError if infeasible
        best_placement, _, _ = greedy_assign(ordered)
=======
    if best_placement is None:
        # Fall back to an error consistent with greedy behavior
        # Try a final simple size-desc heuristic to produce a clearer error state
        ordered = sorted(active_models, key=lambda m: m.model_size, reverse=True)
        # This will raise ValueError if infeasible
        best_placement, _, _ = greedy_assign(ordered)
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    # Parametric refinement: binary search on max KVPR target T using transformed weights
=======
    # After greedy, place memory-only models by size onto GPUs with most remaining memory
    if mem_only:
        for gid in range(gpu_num):
            best_placement.setdefault(gid, [])
        rem_seed = [GPU_MEM_SIZE - sum(mm.model_size for mm in best_placement.get(gid, [])) for gid in range(gpu_num)]
        for m in sorted(mem_only, key=lambda x: x.model_size, reverse=True):
            # Skip if already placed
            already = False
            for gid in range(gpu_num):
                if m in best_placement.get(gid, []):
                    already = True
                    break
            if already:
                continue
            best_gid = None
            best_rem = -1.0
            for gid in range(gpu_num):
                if m.model_size <= rem_seed[gid] and rem_seed[gid] > best_rem:
                    best_rem = rem_seed[gid]
                    best_gid = gid
            if best_gid is None:
                raise ValueError(
                    f"Unable to place memory-only model of size {m.model_size} GB on any GPU. "
                    f"Remaining per-GPU memory after greedy: {rem_seed}"
                )
            best_placement[best_gid].append(m)
            rem_seed[best_gid] -= m.model_size

    # Parametric refinement: binary search on max KVPR target T using transformed weights
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    def max_kvpr_of(placement_dict):
        max_v = 0.0
        for gid in range(gpu_num):
            bucket = placement_dict.get(gid, [])
            R = 0.0
            used = 0.0
            for m in bucket:
                R += (m.req_rate / m.slo)
                used += m.model_size
            v = kvpr(R, S - used)
            if v > max_v:
                max_v = v
        return max_v
=======
    def max_kvpr_of(placement_dict):
        max_v = 0.0
        for gid in range(gpu_num):
            bucket = placement_dict.get(gid, [])
            R = 0.0
            used = 0.0
            for m in bucket:
                R += (m.req_rate / m.slo) if getattr(m, "slo", 0) != 0 else 0.0
                used += m.model_size
            v = kvpr(R, S - used)
            if v > max_v:
                max_v = v
        return max_v
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    def bfd_assign_for_T(T):
        # Balanced-slack aware BFD in transformed space; feasibility implies KVPR <= T and memory <= S per GPU
        capacity = T * S
        if T < 0:
            return None
        # Build items with weights
        items = []
        for m in models:
            dR = (m.req_rate / m.slo)
            w = dR + T * m.model_size
            if w < 0:
                w = 0.0
            items.append((w, dR, m.model_size, m))
=======
    def bfd_assign_for_T(T):
        # Balanced-slack aware BFD in transformed space; feasibility implies KVPR <= T and memory <= S per GPU
        capacity = T * S
        if T < 0:
            return None
        # Build items with weights
        items = []
        for m in models:
            dR = (m.req_rate / m.slo) if getattr(m, "slo", 0) != 0 else 0.0
            w = dR + T * m.model_size
            if w < 0:
                w = 0.0
            items.append((w, dR, m.model_size, m))
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    total_R = sum((m.req_rate / m.slo) for m in models)
=======
    total_R = sum(((m.req_rate / m.slo) if getattr(m, "slo", 0) != 0 else 0.0) for m in models)
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    for m in models:
        dR = (m.req_rate / m.slo)
=======
    for m in models:
        dR = (m.req_rate / m.slo) if getattr(m, "slo", 0) != 0 else 0.0
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    for i in range(len(by_size)):
        mi = by_size[i]; si = mi.model_size; ri = (mi.req_rate / mi.slo)
        for j in range(i + 1, len(by_size)):
            mj = by_size[j]; sj = mj.model_size; rj = (mj.req_rate / mj.slo)
=======
    for i in range(len(by_size)):
        mi = by_size[i]; si = mi.model_size; ri = ((mi.req_rate / mi.slo) if getattr(mi, "slo", 0) != 0 else 0.0)
        for j in range(i + 1, len(by_size)):
            mj = by_size[j]; sj = mj.model_size; rj = ((mj.req_rate / mj.slo) if getattr(mj, "slo", 0) != 0 else 0.0)
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
        for i in range(len(by_size)):
            si = by_size[i].model_size; ri = (by_size[i].req_rate / by_size[i].slo)
            for j in range(i + 1, len(by_size)):
                sj = by_size[j].model_size; rj = (by_size[j].req_rate / by_size[j].slo)
                for mk in topK:
                    if mk is by_size[i] or mk is by_size[j]:
                        continue
                    sk = mk.model_size; rk = (mk.req_rate / mk.slo)
=======
        for i in range(len(by_size)):
            si = by_size[i].model_size; ri = ((by_size[i].req_rate / by_size[i].slo) if getattr(by_size[i], "slo", 0) != 0 else 0.0)
            for j in range(i + 1, len(by_size)):
                sj = by_size[j].model_size; rj = ((by_size[j].req_rate / by_size[j].slo) if getattr(by_size[j], "slo", 0) != 0 else 0.0)
                for mk in topK:
                    if mk is by_size[i] or mk is by_size[j]:
                        continue
                    sk = mk.model_size; rk = ((mk.req_rate / mk.slo) if getattr(mk, "slo", 0) != 0 else 0.0)
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    sized = sorted(((m.model_size, (m.req_rate / m.slo)) for m in models), key=lambda t: t[0], reverse=True)
=======
    sized = sorted(((m.model_size, ((m.req_rate / m.slo) if getattr(m, "slo", 0) != 0 else 0.0)) for m in models), key=lambda t: t[0], reverse=True)
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
        for gid in range(gpu_num):
            for m in placement.get(gid, []):
                rem[gid] -= m.model_size
                sr[gid] += (m.req_rate / m.slo)
        return rem, sr
=======
        for gid in range(gpu_num):
            for m in placement.get(gid, []):
                rem[gid] -= m.model_size
                sr[gid] += (m.req_rate / m.slo) if getattr(m, "slo", 0) != 0 else 0.0
        return rem, sr
>>>>>>> REPLACE
</DIFF>