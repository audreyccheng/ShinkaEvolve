--- a/original.py
+++ b/original.py
@@ -1,512 +1,581 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 GPU_MEM_SIZE = 80  # GB
+
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         A placement of models to GPUs
     """
     if gpu_num <= 0:
         raise ValueError("gpu_num must be positive")
 
-    # Helpers
+    # ---------------- Core utilities ----------------
     def safe_div(num, den):
         if den <= 0:
             return float('inf') if num > 0 else 0.0
         return num / den
 
-    def kvpr(numer, rem_mem):
-        return safe_div(numer, rem_mem)
-
-    # Extract and validate model stats
-    items = []
-    total_mem = 0.0
-    total_n = 0.0
-    for m in models:
-        ms = float(getattr(m, "model_size"))
-        slo = float(getattr(m, "slo"))
-        rr = float(getattr(m, "req_rate"))
-        if ms < 0:
-            raise ValueError("Model size must be non-negative")
-        if ms > GPU_MEM_SIZE + 1e-9:
-            raise ValueError(f"Model of size {ms} GB cannot fit into a single GPU of size {GPU_MEM_SIZE} GB")
-        if slo <= 0:
-            raise ValueError("Model SLO must be positive")
-        n = rr / slo  # r/s
-        items.append((m, ms, n))
-        total_mem += ms
-        total_n += n
-
-    if not items:
-        return {g: [] for g in range(gpu_num)}
-
-    total_capacity = gpu_num * GPU_MEM_SIZE
-    if total_mem - total_capacity > 1e-9:
-        raise ValueError("Total model memory exceeds total GPU memory")
-
-    # Measured max KVPR of a placement
-    def measured_max_kvpr(plc):
-        msum = [sum(getattr(m, "model_size") for m in plc.get(g, [])) for g in range(gpu_num)]
-        nsum = [sum((getattr(m, "req_rate") / getattr(m, "slo")) for m in plc.get(g, [])) for g in range(gpu_num)]
-        vals = [kvpr(nsum[g], GPU_MEM_SIZE - msum[g]) for g in range(gpu_num)]
-        return max(vals) if vals else 0.0
-
-    # Memory-oriented packing strategies (fall-back to ensure feasibility)
-    def memory_pack(order="size_desc", strategy="dual"):
-        # order: "size_desc" or "ratio_desc"
-        # strategy:
-        #   - "dual": first 30% largest use max-free, rest best-fit
-        #   - "bestfit": classic best-fit decreasing
-        #   - "maxfree": place on GPU with most remaining memory
-        #   - "firstfit": first GPU that fits
-        if order == "size_desc":
-            ordered = sorted(items, key=lambda it: (it[1], it[2]), reverse=True)
-        else:
-            # ratio_desc by n per GB
-            ordered = sorted(items, key=lambda it: (safe_div(it[2], max(it[1], 1e-9)), it[2]), reverse=True)
-
-        placement = {g: [] for g in range(gpu_num)}
-        rem = [GPU_MEM_SIZE] * gpu_num
-        numer = [0.0] * gpu_num
-
-        split_idx = max(0, int(0.3 * len(ordered))) if strategy == "dual" else 0
-        for idx, (mdl, ms, dn) in enumerate(ordered):
+    def kvpr_value(numer, used_mem):
+        return safe_div(numer, GPU_MEM_SIZE - used_mem)
+
+    # ---------------- Planner encapsulation ----------------
+    class Planner:
+        def __init__(self, G, models_list):
+            self.G = int(G)
+            self.models = list(models_list)
+            self.items = []  # (idx, obj, size, demand=n/s)
+            self.total_mem = 0.0
+            self.total_n = 0.0
+            self._extract()
+
+        def _extract(self):
+            for idx, m in enumerate(self.models):
+                ms = float(getattr(m, "model_size"))
+                slo = float(getattr(m, "slo"))
+                rr = float(getattr(m, "req_rate"))
+                if ms < 0:
+                    raise ValueError("Model size must be non-negative")
+                if ms > GPU_MEM_SIZE + 1e-9:
+                    raise ValueError(f"Model of size {ms} GB cannot fit into a single GPU of size {GPU_MEM_SIZE} GB")
+                if slo <= 0:
+                    raise ValueError("Model SLO must be positive")
+                n = rr / slo
+                self.items.append((idx, m, ms, n))
+                self.total_mem += ms
+                self.total_n += n
+
+            if self.total_mem - self.G * GPU_MEM_SIZE > 1e-9:
+                raise ValueError("Total model memory exceeds total GPU memory")
+
+        # Lower bounds for T
+        def bounds_T(self):
+            if not self.items:
+                return 0.0
+            total_capacity = self.G * GPU_MEM_SIZE
+            # Per-item and global
+            per_item_lb = max(safe_div(n, max(GPU_MEM_SIZE - ms, 1e-9)) for _, _, ms, n in self.items)
+            global_lb = safe_div(self.total_n, max(total_capacity - self.total_mem, 1e-9))
+
+            # Heavy-pair bound
+            pair_lb = 0.0
+            if self.G >= 2 and len(self.items) >= 2:
+                L = min(len(self.items), 120)
+                heavy = sorted(self.items, key=lambda it: it[2], reverse=True)[:L]
+                for i in range(L):
+                    mi, ni = heavy[i][2], heavy[i][3]
+                    for j in range(i + 1, L):
+                        mj, nj = heavy[j][2], heavy[j][3]
+                        if mi + mj > GPU_MEM_SIZE + 1e-12:
+                            denom = 2.0 * GPU_MEM_SIZE - (mi + mj)
+                            pair_lb = max(pair_lb, safe_div(ni + nj, max(denom, 1e-9)))
+
+            # Triplet bound (light): top P by size; for each i,j pick k among top 8 by size
+            triplet_lb = 0.0
+            if self.G >= 3 and len(self.items) >= 3:
+                P = min(len(self.items), 60)
+                top_mem = sorted(self.items, key=lambda it: it[2], reverse=True)[:P]
+                cap_k = 8
+                for i in range(P):
+                    mi, ni = top_mem[i][2], top_mem[i][3]
+                    for j in range(i + 1, min(P, i + 1 + 12)):
+                        mj, nj = top_mem[j][2], top_mem[j][3]
+                        # candidates for k: next up to cap_k
+                        k_hi = min(P, j + 1 + cap_k)
+                        for k in range(j + 1, k_hi):
+                            mk, nk = top_mem[k][2], top_mem[k][3]
+                            tot_m = mi + mj + mk
+                            if tot_m > 2.0 * GPU_MEM_SIZE + 1e-12:
+                                denom = 3.0 * GPU_MEM_SIZE - tot_m
+                                triplet_lb = max(triplet_lb, safe_div(ni + nj + nk, max(denom, 1e-9)))
+
+            # k-prefix bound for small k
+            kprefix_lb = 0.0
+            by_m = sorted(self.items, key=lambda it: it[2], reverse=True)
+            for k in range(1, min(self.G, 6) + 1):
+                sum_m = 0.0
+                sum_n = 0.0
+                for _, _, ms, n in by_m:
+                    sum_m += ms
+                    sum_n += n
+                    if sum_m > (k - 1) * GPU_MEM_SIZE + 1e-12:
+                        break
+                denom = k * GPU_MEM_SIZE - sum_m
+                kprefix_lb = max(kprefix_lb, safe_div(sum_n, max(denom, 1e-9)))
+
+            return max(0.0, per_item_lb, global_lb, pair_lb, triplet_lb, kprefix_lb)
+
+        # Parametric pack with in-placement T update (two-phase)
+        def pack_at_T(self, T, choose_rule="tight", return_plc=False):
+            # choose_rule: "tight" (transformed best-fit), "min_kvpr" (minimize new global max KVPR),
+            #              "hybrid" (blend max KVPR, local KVPR, and mem imbalance)
+            items = self.items
+            if not items:
+                plc = {g: [] for g in range(self.G)}
+                return True, plc if return_plc else True
+
+            # State
+            m_sum = [0.0] * self.G
+            n_sum = [0.0] * self.G
+            used_cap = [0.0] * self.G  # equals n_sum + T * m_sum
+            placed = [[] for _ in range(self.G)]
+
+            # Build initial order
+            def weight(it, Tloc):
+                return it[3] + Tloc * it[2]
+
+            remaining = list(items)
+            remaining.sort(key=lambda it: (weight(it, T), it[3], it[2]), reverse=True)
+            N = len(remaining)
+            phase_trigger = max(1, int(0.4 * N))
+            cap = GPU_MEM_SIZE * T
+            eps = 1e-12
+
+            def current_kvprs():
+                return [kvpr_value(n_sum[g], m_sum[g]) for g in range(self.G)]
+
+            idx = 0
+            bumped_once = False
+            while idx < len(remaining):
+                it = remaining[idx]
+                _, mdl, ms, n = it
+                w = n + T * ms
+
+                # Select GPU according to rule
+                best_g = None
+                best_key = None
+
+                # Precompute values for hybrid rule
+                cur_kv = current_kvprs() if choose_rule != "tight" else None
+                if choose_rule == "hybrid":
+                    total_m_after = sum(m_sum) + ms
+                    avg_mem_frac = total_m_after / (self.G * GPU_MEM_SIZE) if self.G > 0 else 0.0
+                    Tnorm = max(T, 1e-12)
+                    alpha = 0.15
+                    # adapt alpha when var(cur_kv) small
+                    if cur_kv:
+                        avg_k = sum(cur_kv) / len(cur_kv)
+                        var_k = sum((x - avg_k) ** 2 for x in cur_kv) / max(1, len(cur_kv) - 1)
+                        if var_k < (0.02 * max(T, 1e-9)) ** 2:
+                            alpha = 0.25
+                    beta = 0.05
+
+                for g in range(self.G):
+                    if m_sum[g] + ms > GPU_MEM_SIZE + eps:
+                        continue
+                    if used_cap[g] + w > cap + eps:
+                        continue
+
+                    if choose_rule == "tight":
+                        residual = cap - (used_cap[g] + w)
+                        key = (residual, -(GPU_MEM_SIZE - (m_sum[g] + ms)), g)
+                    elif choose_rule == "min_kvpr":
+                        # minimize new global max KVPR
+                        new_n = n_sum[g] + n
+                        new_m = m_sum[g] + ms
+                        new_local = kvpr_value(new_n, new_m)
+                        new_max = new_local
+                        for k in range(self.G):
+                            if k != g and cur_kv[k] > new_max:
+                                new_max = cur_kv[k]
+                        residual = cap - (used_cap[g] + w)
+                        key = (new_max, new_local, residual, -(GPU_MEM_SIZE - new_m), g)
+                    else:  # hybrid
+                        new_n = n_sum[g] + n
+                        new_m = m_sum[g] + ms
+                        new_local = kvpr_value(new_n, new_m)
+                        new_max = new_local
+                        for k in range(self.G):
+                            if k != g and cur_kv[k] > new_max:
+                                new_max = cur_kv[k]
+                        mem_frac_after = new_m / GPU_MEM_SIZE
+                        mem_imb = abs(mem_frac_after - avg_mem_frac)
+                        J = max(0.0, new_max) / max(T, 1e-12) + alpha * (max(0.0, new_local) / max(T, 1e-12)) + beta * mem_imb
+                        residual = cap - (used_cap[g] + w)
+                        key = (J, new_max, new_local, residual, -(GPU_MEM_SIZE - new_m), g)
+
+                    if best_key is None or key < best_key:
+                        best_key = key
+                        best_g = g
+
+                if best_g is None:
+                    return (False, None) if return_plc else False
+
+                # Place and update
+                placed[best_g].append(mdl)
+                m_sum[best_g] += ms
+                n_sum[best_g] += n
+                used_cap[best_g] += w
+                idx += 1
+
+                # Two-phase T update after ~40% placed (only once)
+                if (not bumped_once) and idx >= phase_trigger and idx < N:
+                    # recompute a lower bound using remaining items and residual memory
+                    rem_items = remaining[idx:]
+                    rem_sum_m = sum(it2[2] for it2 in rem_items)
+                    rem_sum_n = sum(it2[3] for it2 in rem_items)
+                    total_residual_mem = sum(GPU_MEM_SIZE - m_sum[g] for g in range(self.G))
+                    denom = max(total_residual_mem - rem_sum_m, 1e-9)
+                    lb_rem = safe_div(rem_sum_n, denom)
+                    # Also respect per-item lower bound for remaining
+                    lb_rem = max(lb_rem, max(safe_div(it2[3], max(GPU_MEM_SIZE - it2[2], 1e-9)) for it2 in rem_items))
+                    if lb_rem > T + 1e-12:
+                        # Bump T, update capacity and used_cap
+                        T = lb_rem
+                        cap = GPU_MEM_SIZE * T
+                        for g in range(self.G):
+                            used_cap[g] = n_sum[g] + T * m_sum[g]
+                        # Rebuild remaining order by new T
+                        rem_sorted = rem_items[:]
+                        rem_sorted.sort(key=lambda it2: (weight(it2, T), it2[3], it2[2]), reverse=True)
+                        remaining[idx:] = rem_sorted
+                        bumped_once = True
+
+            if return_plc:
+                return True, {g: placed[g] for g in range(self.G)}
+            return True
+
+        # Search for feasible T via exponential + binary search using "tight" rule
+        def search_T(self, low_bound):
+            if not self.items:
+                return 0.0, 0.0
+            T = max(low_bound, 1e-9)
+            # Exponential growth until feasible
+            feasible = False
+            for _ in range(50):
+                ok = self.pack_at_T(T, choose_rule="tight", return_plc=False)
+                if ok:
+                    feasible = True
+                    break
+                T *= 2.0
+            if not feasible:
+                raise ValueError("Unable to find a feasible packing for any KVPR threshold")
+            high = T
+            low = low_bound
+            # Binary search
+            for _ in range(40):
+                mid = (low + high) / 2.0
+                ok = self.pack_at_T(mid, choose_rule="tight", return_plc=False)
+                if ok:
+                    high = mid
+                else:
+                    low = mid
+            return low, high  # low = infeasible side, high = feasible side (near-optimal)
+
+        # Build candidates across a T-neighborhood and multiple choose rules
+        def build_candidates(self, T_low, T_high):
+            if not self.items:
+                return [{g: [] for g in range(self.G)}]
+            Ts = [T_high * 0.985, T_high * 0.99, T_high, T_high * 1.005, T_high * 1.01, T_high * 1.015, (T_low + T_high) / 2.0]
+            rules = ["tight", "hybrid", "min_kvpr"]
             candidates = []
-            for g in range(gpu_num):
-                if ms <= rem[g]:
-                    # For tie-breaking, consider post local kvpr
-                    new_k = kvpr(numer[g] + dn, rem[g] - ms)
-                    candidates.append((g, rem[g] - ms, new_k))
-
-            if not candidates:
-                return None  # failed to place
-
-            chosen = None
-            if strategy == "bestfit" or (strategy == "dual" and idx >= split_idx):
-                # Best-fit: minimize residual; tie by smaller local kvpr; then gpu id
-                candidates.sort(key=lambda x: (x[1], x[2], x[0]))
-                chosen = candidates[0][0]
-            elif strategy == "maxfree" or (strategy == "dual" and idx < split_idx):
-                # Max-free: maximize residual; tie by smaller local kvpr; then gpu id
-                candidates.sort(key=lambda x: (-x[1], x[2], x[0]))
-                chosen = candidates[0][0]
-            else:
-                # First-fit
-                chosen = min(candidates, key=lambda x: x[0])[0]
-
-            placement[chosen].append(mdl)
-            rem[chosen] -= ms
-            numer[chosen] += dn
-
-        return placement
-
-    # Regret-based insertion tailored for min-max KVPR
-    def regret_insertion():
-        placement = {g: [] for g in range(gpu_num)}
-        rem = [GPU_MEM_SIZE] * gpu_num
-        numer = [0.0] * gpu_num
-
-        unassigned = list(items)
-
-        # Precompute top1/top2 of current kvprs for O(1) max-except calculations
-        def top12(vals):
-            top = (-1, -float('inf'))
-            second = (-1, -float('inf'))
-            for i, v in enumerate(vals):
-                if v > top[1]:
-                    second = top
-                    top = (i, v)
-                elif v > second[1]:
-                    second = (i, v)
-            return top, second
-
-        # Iteratively insert models
-        while unassigned:
-            current_kvprs = [kvpr(numer[g], rem[g]) for g in range(gpu_num)]
-            (top_idx, top_val), (sec_idx, sec_val) = top12(current_kvprs)
-
-            best_model = None
-            best_gpu = None
-            best_new_max = float('inf')
-            best_regret = -float('inf')
-
-            # Evaluate regret for each model
-            for (mdl, ms, dn) in unassigned:
-                feasible = []
-                for g in range(gpu_num):
-                    if ms <= rem[g]:
-                        new_local = kvpr(numer[g] + dn, rem[g] - ms)
-                        base_other = top_val if g != top_idx else sec_val
-                        new_max = new_local if new_local > base_other else base_other
-                        feasible.append((g, new_max, new_local))
-                if not feasible:
-                    continue
-
-                feasible.sort(key=lambda x: (x[1], x[2]))  # sort by new_max then local
-                best = feasible[0]
-                second = feasible[1] if len(feasible) > 1 else (None, float('inf'), float('inf'))
-                regret = second[1] - best[1]  # larger regret => more critical
-
-                # Choose the model with largest regret, then smaller best new_max
-                if (regret > best_regret or
-                    (regret == best_regret and best[1] < best_new_max)):
-                    best_regret = regret
-                    best_new_max = best[1]
-                    best_model = (mdl, ms, dn)
-                    best_gpu = best[0]
-
-            if best_model is None:
-                # No feasible candidate in this step; fail to allow fallback
-                return None
-
-            # Commit placement
-            mdl, ms, dn = best_model
-            placement[best_gpu].append(mdl)
-            rem[best_gpu] -= ms
-            numer[best_gpu] += dn
-            unassigned.remove(best_model)
-
-        return placement
-
-    # Local search: move and swap to reduce global max KVPR
-    def improve_local(plc, max_iters=4000, eps=1e-12):
-        per_g = {g: list(plc.get(g, [])) for g in range(gpu_num)}
-        mem = [sum(getattr(m, "model_size") for m in per_g[g]) for g in range(gpu_num)]
-        num = [sum((getattr(m, "req_rate") / getattr(m, "slo")) for m in per_g[g]) for g in range(gpu_num)]
-
-        def kvpr_g(g, msum=None, nsum=None):
-            msum = mem[g] if msum is None else msum
-            nsum = num[g] if nsum is None else nsum
-            return kvpr(nsum, GPU_MEM_SIZE - msum)
-
-        def global_max_vals():
-            vals = [kvpr_g(g) for g in range(gpu_num)]
-            return max(vals), vals
-
-        it = 0
-        while it < max_iters:
-            it += 1
-            cur_max, cur_vals = global_max_vals()
-            worst = max(range(gpu_num), key=lambda g: cur_vals[g])
-            improved = False
-            best_move = None
-            best_new_max = cur_max
-
-            worst_models = list(per_g[worst])
-
-            # Try single-item moves out of worst GPU
-            for mdl in worst_models:
-                ms = float(getattr(mdl, "model_size"))
-                dn = float(getattr(mdl, "req_rate")) / float(getattr(mdl, "slo"))
-                for tgt in range(gpu_num):
-                    if tgt == worst:
-                        continue
-                    if mem[tgt] + ms > GPU_MEM_SIZE + 1e-12:
-                        continue
-
-                    src_mem = mem[worst] - ms
-                    src_num = num[worst] - dn
-                    tgt_mem = mem[tgt] + ms
-                    tgt_num = num[tgt] + dn
-
-                    src_k = kvpr(src_num, GPU_MEM_SIZE - src_mem)
-                    tgt_k = kvpr(tgt_num, GPU_MEM_SIZE - tgt_mem)
-
-                    # New max among unchanged GPUs
-                    new_max = max(src_k, tgt_k)
-                    for g in range(gpu_num):
-                        if g != worst and g != tgt:
-                            if cur_vals[g] > new_max:
-                                new_max = cur_vals[g]
-
-                    if new_max + eps < best_new_max:
-                        best_new_max = new_max
-                        best_move = ("move", mdl, worst, tgt, ms, dn)
-                        improved = True
-
-            if improved:
-                # Apply best single move
+            for T in Ts:
+                for rule in rules:
+                    ok, plc = self.pack_at_T(T, choose_rule=rule, return_plc=True)
+                    if ok and plc is not None:
+                        candidates.append(plc)
+            return candidates
+
+        # Scoring helpers
+        def score_tuple(self, plc):
+            kvprs = []
+            for g in range(self.G):
+                used = sum(getattr(m, 'model_size') for m in plc.get(g, []))
+                numer = sum((getattr(m, 'req_rate') / getattr(m, 'slo')) for m in plc.get(g, []))
+                kvprs.append(kvpr_value(numer, used))
+            if not kvprs:
+                return (0.0, 0.0, 0.0)
+            ks = sorted(kvprs, reverse=True)
+            mx = ks[0]
+            second = ks[1] if len(ks) > 1 else ks[0]
+            avg = sum(ks) / len(ks)
+            return (mx, second, avg)
+
+        # Local refinement: moves, 2-opt swaps, short eject chains
+        def refine_local(self, plc, move_budget=120, swap_budget=12, eject_budget=8, eps=1e-12):
+            per_g = {g: list(plc.get(g, [])) for g in range(self.G)}
+            mem = [sum(getattr(m, "model_size") for m in per_g[g]) for g in range(self.G)]
+            num = [sum((getattr(m, "req_rate") / getattr(m, "slo")) for m in per_g[g]) for g in range(self.G)]
+
+            def kv_g(g, msum=None, nsum=None):
+                msum = mem[g] if msum is None else msum
+                nsum = num[g] if nsum is None else nsum
+                return kvpr_value(nsum, msum)
+
+            def globals_vals():
+                vals = [kv_g(g) for g in range(self.G)]
+                return max(vals), vals
+
+            # Moves from worst
+            moves = 0
+            while moves < move_budget:
+                cur_max, vals = globals_vals()
+                worst = max(range(self.G), key=lambda g: vals[g])
+                improved = False
+                best_move = None
+                best_new_max = cur_max
+                worst_models = list(per_g[worst])
+
+                for mdl in worst_models:
+                    ms = float(getattr(mdl, "model_size"))
+                    dn = float(getattr(mdl, "req_rate")) / float(getattr(mdl, "slo"))
+                    for tgt in range(self.G):
+                        if tgt == worst:
+                            continue
+                        if mem[tgt] + ms > GPU_MEM_SIZE + 1e-12:
+                            continue
+                        src_mem = mem[worst] - ms
+                        src_num = num[worst] - dn
+                        tgt_mem = mem[tgt] + ms
+                        tgt_num = num[tgt] + dn
+                        src_k = kvpr_value(src_num, src_mem)
+                        tgt_k = kvpr_value(tgt_num, tgt_mem)
+                        new_max = max(src_k, tgt_k)
+                        for g in range(self.G):
+                            if g != worst and g != tgt:
+                                if vals[g] > new_max:
+                                    new_max = vals[g]
+                        if new_max + eps < best_new_max:
+                            best_new_max = new_max
+                            best_move = ("move", mdl, worst, tgt, ms, dn)
+                            improved = True
+
+                if not improved:
+                    break
+
                 _, mdl, src, tgt, ms, dn = best_move
                 per_g[src].remove(mdl)
                 per_g[tgt].append(mdl)
                 mem[src] -= ms
                 num[src] -= dn
                 mem[tgt] += ms
                 num[tgt] += dn
-                continue  # iterate again
-
-            # Try swaps between worst GPU and others (first improving swap)
-            found_swap = False
-            for mdl_a in worst_models:
-                ms_a = float(getattr(mdl_a, "model_size"))
-                dn_a = float(getattr(mdl_a, "req_rate")) / float(getattr(mdl_a, "slo"))
-                for tgt in range(gpu_num):
-                    if tgt == worst:
-                        continue
-                    for mdl_b in list(per_g[tgt]):
+                moves += 1
+
+            # 2-opt swaps between worst two GPUs
+            attempts = 0
+            while attempts < swap_budget:
+                cur_max, vals = globals_vals()
+                worst = max(range(self.G), key=lambda g: vals[g])
+                if self.G <= 1:
+                    break
+                second = max([g for g in range(self.G) if g != worst], key=lambda g: vals[g])
+                a_list = sorted(per_g[worst], key=lambda m: (getattr(m, "req_rate") / getattr(m, "slo")), reverse=True)[:6]
+                b_list = sorted(per_g[second], key=lambda m: (getattr(m, "req_rate") / getattr(m, "slo")), reverse=True)[:6]
+                improved = False
+                best_pair = None
+                best_new_max = cur_max
+
+                for mdl_a in a_list:
+                    ms_a = float(getattr(mdl_a, "model_size"))
+                    dn_a = float(getattr(mdl_a, "req_rate")) / float(getattr(mdl_a, "slo"))
+                    for mdl_b in b_list:
                         ms_b = float(getattr(mdl_b, "model_size"))
                         dn_b = float(getattr(mdl_b, "req_rate")) / float(getattr(mdl_b, "slo"))
-
-                        # Memory feasibility after swap
                         if mem[worst] - ms_a + ms_b > GPU_MEM_SIZE + 1e-12:
                             continue
-                        if mem[tgt] - ms_b + ms_a > GPU_MEM_SIZE + 1e-12:
+                        if mem[second] - ms_b + ms_a > GPU_MEM_SIZE + 1e-12:
                             continue
-
                         src_mem = mem[worst] - ms_a + ms_b
                         src_num = num[worst] - dn_a + dn_b
-                        tgt_mem = mem[tgt] - ms_b + ms_a
-                        tgt_num = num[tgt] - dn_b + dn_a
-
-                        src_k = kvpr(src_num, GPU_MEM_SIZE - src_mem)
-                        tgt_k = kvpr(tgt_num, GPU_MEM_SIZE - tgt_mem)
-
+                        tgt_mem = mem[second] - ms_b + ms_a
+                        tgt_num = num[second] - dn_b + dn_a
+                        src_k = kvpr_value(src_num, src_mem)
+                        tgt_k = kvpr_value(tgt_num, tgt_mem)
                         new_max = max(src_k, tgt_k)
-                        for g in range(gpu_num):
-                            if g != worst and g != tgt:
-                                if cur_vals[g] > new_max:
-                                    new_max = cur_vals[g]
-
-                        if new_max + eps < cur_max:
-                            # Apply first improving swap
-                            per_g[worst].remove(mdl_a)
-                            per_g[tgt].remove(mdl_b)
-                            per_g[worst].append(mdl_b)
-                            per_g[tgt].append(mdl_a)
-                            mem[worst] = src_mem
-                            num[worst] = src_num
-                            mem[tgt] = tgt_mem
-                            num[tgt] = tgt_num
-                            found_swap = True
+                        for g in range(self.G):
+                            if g != worst and g != second and vals[g] > new_max:
+                                new_max = vals[g]
+                        if new_max + eps < best_new_max:
+                            best_new_max = new_max
+                            best_pair = (mdl_a, mdl_b, src_mem, src_num, tgt_mem, tgt_num)
+
+                if best_pair is None:
+                    break
+                mdl_a, mdl_b, src_mem, src_num, tgt_mem, tgt_num = best_pair
+                per_g[worst].remove(mdl_a)
+                per_g[second].remove(mdl_b)
+                per_g[worst].append(mdl_b)
+                per_g[second].append(mdl_a)
+                mem[worst] = src_mem
+                num[worst] = src_num
+                mem[second] = tgt_mem
+                num[second] = tgt_num
+                attempts += 1
+
+            # Short eject chains (length-2)
+            chains = 0
+            while chains < eject_budget:
+                cur_max, vals = globals_vals()
+                worst = max(range(self.G), key=lambda g: vals[g])
+                improved = False
+                best_chain = None
+                best_new_max = cur_max
+
+                # Try worst -> g2 then g2 -> g3
+                for mdl in list(per_g[worst]):
+                    ms = float(getattr(mdl, "model_size"))
+                    dn = float(getattr(mdl, "req_rate")) / float(getattr(mdl, "slo"))
+                    for g2 in range(self.G):
+                        if g2 == worst or mem[g2] + ms > GPU_MEM_SIZE + 1e-12:
+                            continue
+                        # Simulate move worst->g2
+                        w_mem = mem[worst] - ms
+                        w_num = num[worst] - dn
+                        g2_mem = mem[g2] + ms
+                        g2_num = num[g2] + dn
+                        w_k = kvpr_value(w_num, w_mem)
+                        g2_k = kvpr_value(g2_num, g2_mem)
+                        base_vals = list(vals)
+                        base_vals[worst] = w_k
+                        base_vals[g2] = g2_k
+                        # From g2, try eject one model to g3
+                        for mdl2 in list(per_g[g2]):
+                            if mdl2 is mdl:
+                                continue
+                            ms2 = float(getattr(mdl2, "model_size"))
+                            dn2 = float(getattr(mdl2, "req_rate")) / float(getattr(mdl2, "slo"))
+                            for g3 in range(self.G):
+                                if g3 in (g2, worst):
+                                    continue
+                                if mem[g3] + ms2 > GPU_MEM_SIZE + 1e-12:
+                                    continue
+                                g2_mem2 = g2_mem - ms2
+                                g2_num2 = g2_num - dn2
+                                g3_mem2 = mem[g3] + ms2
+                                g3_num2 = num[g3] + dn2
+                                g2_k2 = kvpr_value(g2_num2, g2_mem2)
+                                g3_k2 = kvpr_value(g3_num2, g3_mem2)
+                                # compute new max
+                                new_vals = list(base_vals)
+                                new_vals[g2] = g2_k2
+                                new_vals[g3] = g3_k2
+                                new_max = max(new_vals)
+                                if new_max + eps < best_new_max:
+                                    best_new_max = new_max
+                                    best_chain = ("chain", mdl, worst, g2, ms, dn, mdl2, g2, g3, ms2, dn2)
+                                    improved = True
+                        if improved:
                             break
-                    if found_swap:
+                    if improved:
                         break
-                if found_swap:
+
+                if not improved:
                     break
 
-            if found_swap:
-                continue
-
-            # No improving move or swap found; stop
-            break
-
-        return {g: per_g.get(g, []) for g in range(gpu_num)}
-
-    # Build multiple initial candidates
-
-    # Parametric T-based transformed packing to directly minimize max KVPR
-    def parametric_pack_candidates():
-        # Lower bounds on T
-        indiv_lb = max(safe_div(n, max(GPU_MEM_SIZE - ms, 1e-9)) for _, ms, n in items)
-        global_lb = safe_div(total_n, max(total_capacity - total_mem, 1e-9))
-
-        # Pair bound for heavy pairs that cannot co-reside
-        pair_lb = 0.0
-        if gpu_num >= 2 and len(items) >= 2:
-            L = min(len(items), 80)
-            heavy = sorted(items, key=lambda it: it[1], reverse=True)[:L]
-            for i in range(len(heavy)):
-                _, mi, ni = heavy[i]
-                for j in range(i + 1, len(heavy)):
-                    _, mj, nj = heavy[j]
-                    if mi + mj > GPU_MEM_SIZE + 1e-12:
-                        denom = 2.0 * GPU_MEM_SIZE - (mi + mj)
-                        pair_lb = max(pair_lb, safe_div(ni + nj, max(denom, 1e-9)))
-
-        # k-prefix bound (small k)
-        kprefix_lb = 0.0
-        if items:
-            by_m = sorted(items, key=lambda it: it[1], reverse=True)
-            max_k = min(gpu_num, 4)
-            for k in range(1, max_k + 1):
-                s_m = 0.0
-                s_n = 0.0
-                for (_, ms, n) in by_m:
-                    s_m += ms
-                    s_n += n
-                    if s_m > (k - 1) * GPU_MEM_SIZE + 1e-12:
-                        break
-                denom = k * GPU_MEM_SIZE - s_m
-                kprefix_lb = max(kprefix_lb, safe_div(s_n, max(denom, 1e-9)))
-
-        low_T = max(0.0, indiv_lb, global_lb, pair_lb, kprefix_lb)
-
-        def try_pack(T, ordering=0, return_placement=False):
-            cap = GPU_MEM_SIZE * T
-            eps = 1e-12
-            if ordering == 0:
-                ordered = sorted(items, key=lambda it: (it[2] + T * it[1], it[2], it[1]), reverse=True)
-            else:
-                ordered = sorted(items, key=lambda it: (safe_div(it[2], max(GPU_MEM_SIZE - it[1], 1e-9)), it[1]), reverse=True)
-
-            m_sum = [0.0] * gpu_num
-            n_sum = [0.0] * gpu_num
-            used_cap = [0.0] * gpu_num
-            plc = [[] for _ in range(gpu_num)]
-
-            for mdl, ms, n in ordered:
-                w = n + T * ms
-                best_g = None
-                best_resid = float('inf')
-                for g in range(gpu_num):
-                    if m_sum[g] + ms > GPU_MEM_SIZE + eps:
-                        continue
-                    resid = cap - (used_cap[g] + w)
-                    if resid >= -eps and resid < best_resid - 1e-15:
-                        best_resid = resid
-                        best_g = g
-                if best_g is None:
-                    return (False, None) if return_placement else False
-                plc[best_g].append(mdl)
-                m_sum[best_g] += ms
-                n_sum[best_g] += n
-                used_cap[best_g] += w
-
-            if return_placement:
-                return True, {g: plc[g] for g in range(gpu_num)}
-            return True
-
-        def try_any(T, need_plc=False):
-            if need_plc:
-                feas = []
-                ok0, p0 = try_pack(T, 0, True)
-                ok1, p1 = try_pack(T, 1, True)
-                if ok0: feas.append(p0)
-                if ok1: feas.append(p1)
-                return (len(feas) > 0), feas
-            else:
-                return try_pack(T, 0, False) or try_pack(T, 1, False)
-
-        # Exponential search for an initial feasible T
-        T = max(low_T, 1e-9)
-        found = False
-        for _ in range(50):
-            if try_any(T, need_plc=False):
-                found = True
-                break
-            T *= 2.0
-        if not found:
-            return []
-
-        # Binary search to tighten T
-        low, high = low_T, T
-        for _ in range(40):
-            mid = (low + high) / 2.0
-            if try_any(mid, need_plc=False):
-                high = mid
-            else:
-                low = mid
-
-        # Build placements at near-optimal T
-        ok, feas_plcs = try_any(high, need_plc=True)
+                # Apply best chain
+                _, mdl, src, mid, ms, dn, mdl2, src2, tgt2, ms2, dn2 = best_chain
+                # move mdl: src -> mid
+                per_g[src].remove(mdl)
+                per_g[mid].append(mdl)
+                mem[src] -= ms
+                num[src] -= dn
+                mem[mid] += ms
+                num[mid] += dn
+                # move mdl2: src2(=mid) -> tgt2
+                per_g[src2].remove(mdl2)
+                per_g[tgt2].append(mdl2)
+                mem[src2] -= ms2
+                num[src2] -= dn2
+                mem[tgt2] += ms2
+                num[tgt2] += dn2
+                chains += 1
+
+            return {g: per_g.get(g, []) for g in range(self.G)}
+
+    # ---------------- Orchestration ----------------
+    planner = Planner(gpu_num, models)
+
+    # Empty input quick return
+    if not planner.items:
+        return {g: [] for g in range(gpu_num)}
+
+    low_T_bound = planner.bounds_T()
+    low, high = planner.search_T(low_T_bound)
+
+    # Build diversified candidates around T_high
+    candidates = planner.build_candidates(low, high)
+
+    # If somehow none, try a direct pack at high
+    if not candidates:
+        ok, plc = planner.pack_at_T(high, choose_rule="tight", return_plc=True)
         if not ok:
-            return []
-        return feas_plcs
-
-    candidates = []
-
-    # Add parametric T-based candidates first
-    try:
-        candidates.extend(parametric_pack_candidates())
-    except Exception:
-        # If any numerical oddity occurs, fall back to other strategies
-        pass
-
-    # Candidate A: regret-based insertion (KVPR-aware)
-    plc_regret = regret_insertion()
-    if plc_regret is not None:
-        candidates.append(plc_regret)
-
-    # Candidate B: memory-balanced dual strategy
-    plc_dual = memory_pack(order="size_desc", strategy="dual")
-    if plc_dual is not None:
-        candidates.append(plc_dual)
-
-    # Fallbacks to ensure a feasible start
-    if not candidates:
-        for strat in ("bestfit", "maxfree", "firstfit"):
-            plc_try = memory_pack(order="size_desc", strategy=strat if strat != "firstfit" else "firstfit")
-            if plc_try is not None:
-                candidates.append(plc_try)
-                break
-
-    if not candidates:
-        raise ValueError("Unable to construct any feasible placement")
-
-    # Improve each candidate locally and pick the best by measured max KVPR
-    improved_candidates = []
+            raise ValueError("Unable to construct any feasible placement")
+        candidates = [plc]
+
+    # Select best by lexicographic KVPR tuple
+    best_plc = None
+    best_score = None
     for plc in candidates:
-        improved_candidates.append(improve_local(plc))
-
-    best_plc = None
-    best_score = float('inf')
-    for plc in improved_candidates:
-        score = measured_max_kvpr(plc)
-        if score < best_score:
-            best_score = score
+        st = planner.score_tuple(plc)
+        if best_score is None or st < best_score:
+            best_score = st
             best_plc = plc
 
-    # Ensure all GPUs are present
+    # Local refinement with bounded budgets
+    best_plc = planner.refine_local(best_plc, move_budget=100, swap_budget=12, eject_budget=6)
+
+    # Ensure all GPUs present
     for g in range(gpu_num):
         best_plc.setdefault(g, [])
 
     return best_plc
 
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")
