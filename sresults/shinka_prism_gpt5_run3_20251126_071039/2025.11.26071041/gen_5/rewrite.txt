# EVOLVE-BLOCK-START
"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""

GPU_MEM_SIZE = 80  # GB

def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Args:
        gpu_num: Number of GPUs
        models: List of models to place

    Returns:
        A placement of models to GPUs
    """
    if gpu_num <= 0:
        raise ValueError("gpu_num must be positive")

    # Extract per-model stats
    items = []
    total_mem = 0.0
    total_n = 0.0
    for idx, m in enumerate(models):
        ms = float(m.model_size)
        if ms < 0:
            raise ValueError("Model size must be non-negative")
        if ms > GPU_MEM_SIZE + 1e-9:
            raise ValueError(f"Model of size {ms} GB cannot fit into a single GPU of size {GPU_MEM_SIZE} GB")
        slo = float(m.slo)
        if slo <= 0:
            # Treat as impossible, consistent with minimization problem (infinite pressure)
            raise ValueError("Model SLO must be positive")
        n = float(m.req_rate) / slo  # r_j / s_j
        items.append((idx, m, ms, n))
        total_mem += ms
        total_n += n

    if not items:
        return {gpu_id: [] for gpu_id in range(gpu_num)}

    total_capacity_mem = gpu_num * GPU_MEM_SIZE
    if total_mem - total_capacity_mem > 1e-9:
        # Impossible: total memory exceeds available
        raise ValueError("Total model memory exceeds total GPU memory")

    # Lower bound on T from individual models and global aggregation
    def safe_div(num, den):
        if den <= 0:
            return float('inf')
        return num / den

    indiv_lb = max(safe_div(n, GPU_MEM_SIZE - ms) for _, _, ms, n in items)
    global_lb = safe_div(total_n, max(total_capacity_mem - total_mem, 1e-9))
    low_T = max(0.0, indiv_lb, global_lb)

    # Feasibility check using best-fit decreasing on transformed weights: w_i(T) = n_i + T * m_i
    def try_pack(T, order_variant=0, return_placement=False):
        cap = GPU_MEM_SIZE * T  # capacity per GPU in transformed space
        # Build sorted order
        if order_variant == 0:
            # Primary: sort by decreasing transformed weight; break by higher n, then larger m
            ordered = sorted(items, key=lambda it: (it[3] + T * it[2], it[3], it[2]), reverse=True)
        else:
            # Alternative: sort by "intrinsic KVPR" pressure n/(80-m) and then by memory
            ordered = sorted(items, key=lambda it: (safe_div(it[3], max(GPU_MEM_SIZE - it[2], 1e-9)), it[2]), reverse=True)

        # Per-GPU state
        n_sum = [0.0] * gpu_num
        m_sum = [0.0] * gpu_num
        used_cap = [0.0] * gpu_num  # equals n_sum + T * m_sum
        place_lists = [[] for _ in range(gpu_num)]

        for it in ordered:
            idx, mdl, ms, n = it
            w = n + T * ms
            best_gpu = None
            best_residual = float('inf')

            for g in range(gpu_num):
                # Memory feasibility
                if m_sum[g] + ms > GPU_MEM_SIZE + 1e-12:
                    continue
                # Transformed capacity feasibility
                residual = cap - (used_cap[g] + w)
                if residual >= -1e-12:  # allow tiny numerical slack
                    # Best-fit: minimize residual to tighten packing
                    if residual < best_residual - 1e-15:
                        best_residual = residual
                        best_gpu = g

            if best_gpu is None:
                return (False, None) if return_placement else False

            # Place item
            place_lists[best_gpu].append(mdl)
            n_sum[best_gpu] += n
            m_sum[best_gpu] += ms
            used_cap[best_gpu] += w

        if return_placement:
            return True, {g: place_lists[g] for g in range(gpu_num)}
        return True

    # Find an initial feasible high_T by exponential search (monotonic feasibility in T)
    # Start from a reasonable T guess
    T = max(low_T, 1e-9)
    feasible = False
    for _ in range(60):
        if try_pack(T, 0) or try_pack(T, 1):
            feasible = True
            break
        T *= 2.0

    if not feasible:
        # Even with very large T, packing failed -> truly infeasible
        raise ValueError("Unable to find a feasible packing for any KVPR threshold")

    high_T = T
    low = low_T
    high = high_T

    # Binary search to minimize T
    for _ in range(40):
        mid = (low + high) / 2.0
        if mid <= 0:
            high = mid
            continue
        if try_pack(mid, 0) or try_pack(mid, 1):
            high = mid
        else:
            low = mid

    # Build final placement at near-optimal T using both orderings and pick the better (smaller measured max KVPR)
    def build_and_score(T):
        ok0, plc0 = try_pack(T, 0, return_placement=True)
        ok1, plc1 = try_pack(T, 1, return_placement=True)
        candidates = []
        if ok0: candidates.append(plc0)
        if ok1: candidates.append(plc1)

        # Fallback (shouldn't happen): use the last feasible order we had
        if not candidates:
            ok0, plc0 = try_pack(high, 0, return_placement=True)
            if ok0:
                candidates.append(plc0)

        def kvpr_of(plc):
            kvprs = []
            for g in range(gpu_num):
                used = sum(getattr(m, 'model_size') for m in plc.get(g, []))
                numer = sum((getattr(m, 'req_rate') / getattr(m, 'slo')) for m in plc.get(g, []))
                denom = GPU_MEM_SIZE - used
                if denom <= 0:
                    kv = float('inf') if numer > 0 else 0.0
                else:
                    kv = numer / denom
                kvprs.append(kv)
            return max(kvprs)

        best_plc = None
        best_score = float('inf')
        for plc in candidates:
            score = kvpr_of(plc)
            if score < best_score:
                best_score = score
                best_plc = plc
        return best_plc

    placement = build_and_score(high)
    if placement is None:
        # As a very last resort, place greedily by minimizing current KVPR (should be rare)
        placement = {g: [] for g in range(gpu_num)}
        rem_mem = [GPU_MEM_SIZE] * gpu_num
        numer = [0.0] * gpu_num
        # Sort by decreasing n and then by size
        sorted_models = sorted(models, key=lambda m: ((m.req_rate / m.slo), m.model_size), reverse=True)
        for m in sorted_models:
            best_g = None
            best_ratio = float('inf')
            for g in range(gpu_num):
                if m.model_size <= rem_mem[g] and rem_mem[g] > 0:
                    cur = numer[g] / rem_mem[g]
                    if cur < best_ratio:
                        best_ratio = cur
                        best_g = g
            if best_g is None:
                raise ValueError("Unable to place models with greedy fallback")
            placement[best_g].append(m)
            numer[best_g] += m.req_rate / m.slo
            rem_mem[best_g] -= m.model_size

    # Ensure all GPUs are represented in dict keys
    for g in range(gpu_num):
        placement.setdefault(g, [])

    return placement

# EVOLVE-BLOCK-END


def run_placement(gpu_num, models):
    """
    Main entry point that will be called by the evaluator.
    
    Args:
        gpu_num: Number of GPUs
        models: List of models to place
    
    Returns:
        Dictionary containing GPU placements
    """
    return compute_model_placement(gpu_num, models)


if __name__ == "__main__":
    # Test the algorithm
    import os
    import sys
    
    # Add the openevolve_examples directory to the path to import evaluator
    def find_repo_root(start_path):
        """Find the repository root by looking for openevolve_examples directory."""
        current = os.path.abspath(start_path)
        while current != os.path.dirname(current):  # Stop at filesystem root
            if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                return current
            current = os.path.dirname(current)
        raise RuntimeError("Could not find openevolve_examples directory")
    
    repo_root = find_repo_root(os.path.dirname(__file__))
    sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
    
    from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
    import numpy as np

    test_cases = generate_test_gpu_models()
    all_kvpr = []
    for i, (gpu_num, gpu_models) in enumerate(test_cases):
        results = compute_model_placement(gpu_num, gpu_models)
        max_kvpr = calculate_kvcache_pressure(results)
        all_kvpr.append(safe_float(max_kvpr))

    avg_kvpr = np.mean(all_kvpr)
    if avg_kvpr != 0:
        avg_kvpr = 1.0 / avg_kvpr

    print(f"Max KVPR: {avg_kvpr:.3f}")