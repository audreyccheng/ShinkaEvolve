--- a/original.py
+++ b/original.py
@@ -1,249 +1,396 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 GPU_MEM_SIZE = 80  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         A placement of models to GPUs
     """
     if gpu_num <= 0:
         raise ValueError("gpu_num must be positive")
 
-    # Extract per-model stats
+    # Helpers
+    def safe_div(num, den):
+        if den <= 0:
+            return float('inf') if num > 0 else 0.0
+        return num / den
+
+    def kvpr(numer, rem_mem):
+        return safe_div(numer, rem_mem)
+
+    # Extract and validate model stats
     items = []
     total_mem = 0.0
     total_n = 0.0
-    for idx, m in enumerate(models):
-        ms = float(m.model_size)
+    for m in models:
+        ms = float(getattr(m, "model_size"))
+        slo = float(getattr(m, "slo"))
+        rr = float(getattr(m, "req_rate"))
         if ms < 0:
             raise ValueError("Model size must be non-negative")
         if ms > GPU_MEM_SIZE + 1e-9:
             raise ValueError(f"Model of size {ms} GB cannot fit into a single GPU of size {GPU_MEM_SIZE} GB")
-        slo = float(m.slo)
         if slo <= 0:
-            # Treat as impossible, consistent with minimization problem (infinite pressure)
             raise ValueError("Model SLO must be positive")
-        n = float(m.req_rate) / slo  # r_j / s_j
-        items.append((idx, m, ms, n))
+        n = rr / slo  # r/s
+        items.append((m, ms, n))
         total_mem += ms
         total_n += n
 
     if not items:
-        return {gpu_id: [] for gpu_id in range(gpu_num)}
-
-    total_capacity_mem = gpu_num * GPU_MEM_SIZE
-    if total_mem - total_capacity_mem > 1e-9:
-        # Impossible: total memory exceeds available
+        return {g: [] for g in range(gpu_num)}
+
+    total_capacity = gpu_num * GPU_MEM_SIZE
+    if total_mem - total_capacity > 1e-9:
         raise ValueError("Total model memory exceeds total GPU memory")
 
-    # Lower bound on T from individual models and global aggregation
-    def safe_div(num, den):
-        if den <= 0:
-            return float('inf')
-        return num / den
-
-    indiv_lb = max(safe_div(n, GPU_MEM_SIZE - ms) for _, _, ms, n in items)
-    global_lb = safe_div(total_n, max(total_capacity_mem - total_mem, 1e-9))
-    low_T = max(0.0, indiv_lb, global_lb)
-
-    # Feasibility check using best-fit decreasing on transformed weights: w_i(T) = n_i + T * m_i
-    def try_pack(T, order_variant=0, return_placement=False):
-        cap = GPU_MEM_SIZE * T  # capacity per GPU in transformed space
-        # Build sorted order
-        if order_variant == 0:
-            # Primary: sort by decreasing transformed weight; break by higher n, then larger m
-            ordered = sorted(items, key=lambda it: (it[3] + T * it[2], it[3], it[2]), reverse=True)
+    # Measured max KVPR of a placement
+    def measured_max_kvpr(plc):
+        msum = [sum(getattr(m, "model_size") for m in plc.get(g, [])) for g in range(gpu_num)]
+        nsum = [sum((getattr(m, "req_rate") / getattr(m, "slo")) for m in plc.get(g, [])) for g in range(gpu_num)]
+        vals = [kvpr(nsum[g], GPU_MEM_SIZE - msum[g]) for g in range(gpu_num)]
+        return max(vals) if vals else 0.0
+
+    # Memory-oriented packing strategies (fall-back to ensure feasibility)
+    def memory_pack(order="size_desc", strategy="dual"):
+        # order: "size_desc" or "ratio_desc"
+        # strategy:
+        #   - "dual": first 30% largest use max-free, rest best-fit
+        #   - "bestfit": classic best-fit decreasing
+        #   - "maxfree": place on GPU with most remaining memory
+        #   - "firstfit": first GPU that fits
+        if order == "size_desc":
+            ordered = sorted(items, key=lambda it: (it[1], it[2]), reverse=True)
         else:
-            # Alternative: sort by "intrinsic KVPR" pressure n/(80-m) and then by memory
-            ordered = sorted(items, key=lambda it: (safe_div(it[3], max(GPU_MEM_SIZE - it[2], 1e-9)), it[2]), reverse=True)
-
-        # Per-GPU state
-        n_sum = [0.0] * gpu_num
-        m_sum = [0.0] * gpu_num
-        used_cap = [0.0] * gpu_num  # equals n_sum + T * m_sum
-        place_lists = [[] for _ in range(gpu_num)]
-
-        for it in ordered:
-            idx, mdl, ms, n = it
-            w = n + T * ms
+            # ratio_desc by n per GB
+            ordered = sorted(items, key=lambda it: (safe_div(it[2], max(it[1], 1e-9)), it[2]), reverse=True)
+
+        placement = {g: [] for g in range(gpu_num)}
+        rem = [GPU_MEM_SIZE] * gpu_num
+        numer = [0.0] * gpu_num
+
+        split_idx = max(0, int(0.3 * len(ordered))) if strategy == "dual" else 0
+        for idx, (mdl, ms, dn) in enumerate(ordered):
+            candidates = []
+            for g in range(gpu_num):
+                if ms <= rem[g]:
+                    # For tie-breaking, consider post local kvpr
+                    new_k = kvpr(numer[g] + dn, rem[g] - ms)
+                    candidates.append((g, rem[g] - ms, new_k))
+
+            if not candidates:
+                return None  # failed to place
+
+            chosen = None
+            if strategy == "bestfit" or (strategy == "dual" and idx >= split_idx):
+                # Best-fit: minimize residual; tie by smaller local kvpr; then gpu id
+                candidates.sort(key=lambda x: (x[1], x[2], x[0]))
+                chosen = candidates[0][0]
+            elif strategy == "maxfree" or (strategy == "dual" and idx < split_idx):
+                # Max-free: maximize residual; tie by smaller local kvpr; then gpu id
+                candidates.sort(key=lambda x: (-x[1], x[2], x[0]))
+                chosen = candidates[0][0]
+            else:
+                # First-fit
+                chosen = min(candidates, key=lambda x: x[0])[0]
+
+            placement[chosen].append(mdl)
+            rem[chosen] -= ms
+            numer[chosen] += dn
+
+        return placement
+
+    # Regret-based insertion tailored for min-max KVPR
+    def regret_insertion():
+        placement = {g: [] for g in range(gpu_num)}
+        rem = [GPU_MEM_SIZE] * gpu_num
+        numer = [0.0] * gpu_num
+
+        unassigned = list(items)
+
+        # Precompute top1/top2 of current kvprs for O(1) max-except calculations
+        def top12(vals):
+            top = (-1, -float('inf'))
+            second = (-1, -float('inf'))
+            for i, v in enumerate(vals):
+                if v > top[1]:
+                    second = top
+                    top = (i, v)
+                elif v > second[1]:
+                    second = (i, v)
+            return top, second
+
+        # Iteratively insert models
+        while unassigned:
+            current_kvprs = [kvpr(numer[g], rem[g]) for g in range(gpu_num)]
+            (top_idx, top_val), (sec_idx, sec_val) = top12(current_kvprs)
+
+            best_model = None
             best_gpu = None
-            best_residual = float('inf')
-
-            for g in range(gpu_num):
-                # Memory feasibility
-                if m_sum[g] + ms > GPU_MEM_SIZE + 1e-12:
+            best_new_max = float('inf')
+            best_regret = -float('inf')
+
+            # Evaluate regret for each model
+            for (mdl, ms, dn) in unassigned:
+                feasible = []
+                for g in range(gpu_num):
+                    if ms <= rem[g]:
+                        new_local = kvpr(numer[g] + dn, rem[g] - ms)
+                        base_other = top_val if g != top_idx else sec_val
+                        new_max = new_local if new_local > base_other else base_other
+                        feasible.append((g, new_max, new_local))
+                if not feasible:
                     continue
-                # Transformed capacity feasibility
-                residual = cap - (used_cap[g] + w)
-                if residual >= -1e-12:  # allow tiny numerical slack
-                    # Best-fit: minimize residual to tighten packing
-                    if residual < best_residual - 1e-15:
-                        best_residual = residual
-                        best_gpu = g
-
-            if best_gpu is None:
-                return (False, None) if return_placement else False
-
-            # Place item
-            place_lists[best_gpu].append(mdl)
-            n_sum[best_gpu] += n
-            m_sum[best_gpu] += ms
-            used_cap[best_gpu] += w
-
-        if return_placement:
-            return True, {g: place_lists[g] for g in range(gpu_num)}
-        return True
-
-    # Find an initial feasible high_T by exponential search (monotonic feasibility in T)
-    # Start from a reasonable T guess
-    T = max(low_T, 1e-9)
-    feasible = False
-    for _ in range(60):
-        if try_pack(T, 0) or try_pack(T, 1):
-            feasible = True
+
+                feasible.sort(key=lambda x: (x[1], x[2]))  # sort by new_max then local
+                best = feasible[0]
+                second = feasible[1] if len(feasible) > 1 else (None, float('inf'), float('inf'))
+                regret = second[1] - best[1]  # larger regret => more critical
+
+                # Choose the model with largest regret, then smaller best new_max
+                if (regret > best_regret or
+                    (regret == best_regret and best[1] < best_new_max)):
+                    best_regret = regret
+                    best_new_max = best[1]
+                    best_model = (mdl, ms, dn)
+                    best_gpu = best[0]
+
+            if best_model is None:
+                # No feasible candidate in this step; fail to allow fallback
+                return None
+
+            # Commit placement
+            mdl, ms, dn = best_model
+            placement[best_gpu].append(mdl)
+            rem[best_gpu] -= ms
+            numer[best_gpu] += dn
+            unassigned.remove(best_model)
+
+        return placement
+
+    # Local search: move and swap to reduce global max KVPR
+    def improve_local(plc, max_iters=4000, eps=1e-12):
+        per_g = {g: list(plc.get(g, [])) for g in range(gpu_num)}
+        mem = [sum(getattr(m, "model_size") for m in per_g[g]) for g in range(gpu_num)]
+        num = [sum((getattr(m, "req_rate") / getattr(m, "slo")) for m in per_g[g]) for g in range(gpu_num)]
+
+        def kvpr_g(g, msum=None, nsum=None):
+            msum = mem[g] if msum is None else msum
+            nsum = num[g] if nsum is None else nsum
+            return kvpr(nsum, GPU_MEM_SIZE - msum)
+
+        def global_max_vals():
+            vals = [kvpr_g(g) for g in range(gpu_num)]
+            return max(vals), vals
+
+        it = 0
+        while it < max_iters:
+            it += 1
+            cur_max, cur_vals = global_max_vals()
+            worst = max(range(gpu_num), key=lambda g: cur_vals[g])
+            improved = False
+            best_move = None
+            best_new_max = cur_max
+
+            worst_models = list(per_g[worst])
+
+            # Try single-item moves out of worst GPU
+            for mdl in worst_models:
+                ms = float(getattr(mdl, "model_size"))
+                dn = float(getattr(mdl, "req_rate")) / float(getattr(mdl, "slo"))
+                for tgt in range(gpu_num):
+                    if tgt == worst:
+                        continue
+                    if mem[tgt] + ms > GPU_MEM_SIZE + 1e-12:
+                        continue
+
+                    src_mem = mem[worst] - ms
+                    src_num = num[worst] - dn
+                    tgt_mem = mem[tgt] + ms
+                    tgt_num = num[tgt] + dn
+
+                    src_k = kvpr(src_num, GPU_MEM_SIZE - src_mem)
+                    tgt_k = kvpr(tgt_num, GPU_MEM_SIZE - tgt_mem)
+
+                    # New max among unchanged GPUs
+                    new_max = max(src_k, tgt_k)
+                    for g in range(gpu_num):
+                        if g != worst and g != tgt:
+                            if cur_vals[g] > new_max:
+                                new_max = cur_vals[g]
+
+                    if new_max + eps < best_new_max:
+                        best_new_max = new_max
+                        best_move = ("move", mdl, worst, tgt, ms, dn)
+                        improved = True
+
+            if improved:
+                # Apply best single move
+                _, mdl, src, tgt, ms, dn = best_move
+                per_g[src].remove(mdl)
+                per_g[tgt].append(mdl)
+                mem[src] -= ms
+                num[src] -= dn
+                mem[tgt] += ms
+                num[tgt] += dn
+                continue  # iterate again
+
+            # Try swaps between worst GPU and others (first improving swap)
+            found_swap = False
+            for mdl_a in worst_models:
+                ms_a = float(getattr(mdl_a, "model_size"))
+                dn_a = float(getattr(mdl_a, "req_rate")) / float(getattr(mdl_a, "slo"))
+                for tgt in range(gpu_num):
+                    if tgt == worst:
+                        continue
+                    for mdl_b in list(per_g[tgt]):
+                        ms_b = float(getattr(mdl_b, "model_size"))
+                        dn_b = float(getattr(mdl_b, "req_rate")) / float(getattr(mdl_b, "slo"))
+
+                        # Memory feasibility after swap
+                        if mem[worst] - ms_a + ms_b > GPU_MEM_SIZE + 1e-12:
+                            continue
+                        if mem[tgt] - ms_b + ms_a > GPU_MEM_SIZE + 1e-12:
+                            continue
+
+                        src_mem = mem[worst] - ms_a + ms_b
+                        src_num = num[worst] - dn_a + dn_b
+                        tgt_mem = mem[tgt] - ms_b + ms_a
+                        tgt_num = num[tgt] - dn_b + dn_a
+
+                        src_k = kvpr(src_num, GPU_MEM_SIZE - src_mem)
+                        tgt_k = kvpr(tgt_num, GPU_MEM_SIZE - tgt_mem)
+
+                        new_max = max(src_k, tgt_k)
+                        for g in range(gpu_num):
+                            if g != worst and g != tgt:
+                                if cur_vals[g] > new_max:
+                                    new_max = cur_vals[g]
+
+                        if new_max + eps < cur_max:
+                            # Apply first improving swap
+                            per_g[worst].remove(mdl_a)
+                            per_g[tgt].remove(mdl_b)
+                            per_g[worst].append(mdl_b)
+                            per_g[tgt].append(mdl_a)
+                            mem[worst] = src_mem
+                            num[worst] = src_num
+                            mem[tgt] = tgt_mem
+                            num[tgt] = tgt_num
+                            found_swap = True
+                            break
+                    if found_swap:
+                        break
+                if found_swap:
+                    break
+
+            if found_swap:
+                continue
+
+            # No improving move or swap found; stop
             break
-        T *= 2.0
-
-    if not feasible:
-        # Even with very large T, packing failed -> truly infeasible
-        raise ValueError("Unable to find a feasible packing for any KVPR threshold")
-
-    high_T = T
-    low = low_T
-    high = high_T
-
-    # Binary search to minimize T
-    for _ in range(40):
-        mid = (low + high) / 2.0
-        if mid <= 0:
-            high = mid
-            continue
-        if try_pack(mid, 0) or try_pack(mid, 1):
-            high = mid
-        else:
-            low = mid
-
-    # Build final placement at near-optimal T using both orderings and pick the better (smaller measured max KVPR)
-    def build_and_score(T):
-        ok0, plc0 = try_pack(T, 0, return_placement=True)
-        ok1, plc1 = try_pack(T, 1, return_placement=True)
-        candidates = []
-        if ok0: candidates.append(plc0)
-        if ok1: candidates.append(plc1)
-
-        # Fallback (shouldn't happen): use the last feasible order we had
-        if not candidates:
-            ok0, plc0 = try_pack(high, 0, return_placement=True)
-            if ok0:
-                candidates.append(plc0)
-
-        def kvpr_of(plc):
-            kvprs = []
-            for g in range(gpu_num):
-                used = sum(getattr(m, 'model_size') for m in plc.get(g, []))
-                numer = sum((getattr(m, 'req_rate') / getattr(m, 'slo')) for m in plc.get(g, []))
-                denom = GPU_MEM_SIZE - used
-                if denom <= 0:
-                    kv = float('inf') if numer > 0 else 0.0
-                else:
-                    kv = numer / denom
-                kvprs.append(kv)
-            return max(kvprs)
-
-        best_plc = None
-        best_score = float('inf')
-        for plc in candidates:
-            score = kvpr_of(plc)
-            if score < best_score:
-                best_score = score
-                best_plc = plc
-        return best_plc
-
-    placement = build_and_score(high)
-    if placement is None:
-        # As a very last resort, place greedily by minimizing current KVPR (should be rare)
-        placement = {g: [] for g in range(gpu_num)}
-        rem_mem = [GPU_MEM_SIZE] * gpu_num
-        numer = [0.0] * gpu_num
-        # Sort by decreasing n and then by size
-        sorted_models = sorted(models, key=lambda m: ((m.req_rate / m.slo), m.model_size), reverse=True)
-        for m in sorted_models:
-            best_g = None
-            best_ratio = float('inf')
-            for g in range(gpu_num):
-                if m.model_size <= rem_mem[g] and rem_mem[g] > 0:
-                    cur = numer[g] / rem_mem[g]
-                    if cur < best_ratio:
-                        best_ratio = cur
-                        best_g = g
-            if best_g is None:
-                raise ValueError("Unable to place models with greedy fallback")
-            placement[best_g].append(m)
-            numer[best_g] += m.req_rate / m.slo
-            rem_mem[best_g] -= m.model_size
-
-    # Ensure all GPUs are represented in dict keys
+
+        return {g: per_g.get(g, []) for g in range(gpu_num)}
+
+    # Build multiple initial candidates
+    candidates = []
+
+    # Candidate A: regret-based insertion (KVPR-aware)
+    plc_regret = regret_insertion()
+    if plc_regret is not None:
+        candidates.append(plc_regret)
+
+    # Candidate B: memory-balanced dual strategy
+    plc_dual = memory_pack(order="size_desc", strategy="dual")
+    if plc_dual is not None:
+        candidates.append(plc_dual)
+
+    # Fallbacks to ensure a feasible start
+    if not candidates:
+        for strat in ("bestfit", "maxfree", "firstfit"):
+            plc_try = memory_pack(order="size_desc", strategy=strat if strat != "firstfit" else "firstfit")
+            if plc_try is not None:
+                candidates.append(plc_try)
+                break
+
+    if not candidates:
+        raise ValueError("Unable to construct any feasible placement")
+
+    # Improve each candidate locally and pick the best by measured max KVPR
+    improved_candidates = []
+    for plc in candidates:
+        improved_candidates.append(improve_local(plc))
+
+    best_plc = None
+    best_score = float('inf')
+    for plc in improved_candidates:
+        score = measured_max_kvpr(plc)
+        if score < best_score:
+            best_score = score
+            best_plc = plc
+
+    # Ensure all GPUs are present
     for g in range(gpu_num):
-        placement.setdefault(g, [])
-
-    return placement
+        best_plc.setdefault(g, [])
+
+    return best_plc
 
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
     
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
     
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
     
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
     
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
     
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")
 
