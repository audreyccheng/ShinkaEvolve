<NAME>
preplace_mem_only_and_residual_T
</NAME>

<DESCRIPTION>
I introduce a simple but impactful improvement: pre-place all memory-only models (slo==0) by balancing memory across GPUs before running the T-based balanced-slack placement on the remaining models. This avoids inflating the T lower bound and failing early due to infinite demand ratios, and it preserves per-GPU residual capacities. Concretely:

- Pre-place memory-only models using a largest-first, worst-fit by remaining memory strategy; track base placement and used memory per GPU.
- Build the items list only from models with slo>0 (finite dR).
- Tighten the global lower bound to account for pre-placed memory: total residual memory is gpu_num*S − used0_total − total_size.
- Initialize the balanced-slack packer with the pre-placed state (assign already has base models, used_mem starts from base_used, K_g is initialized as T*S − (sumR_g + T*used_g·T)).
- Update the internal greedy candidate generator to start from the pre-placed base and operate only on the remaining models, with guards against division-by-zero.

This keeps the code simple yet systematically improves feasibility and KVPR balancing by not letting memory-only models distort the rate-based optimization phase. It should increase the combined score by reducing the maximum KVPR while retaining robustness.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Extract per-model attributes once
    items = []
    total_R = 0.0
    total_size = 0.0
    for m in models:
        slo = float(m.slo)
        dR = float(m.req_rate) / slo if slo != 0 else float('inf')
        s = float(m.model_size)
        items.append({'obj': m, 'dR': dR, 'size': s})
        total_R += 0.0 if dR == float('inf') else dR
        total_size += s
=======
    # Pre-place memory-only (slo==0) models by memory balancing, then build items for slo>0 models
    def _preplace_memory_only(all_models, S, gpu_num):
        base = {i: [] for i in range(gpu_num)}
        used = [0.0] * gpu_num
        mem_only = [m for m in all_models if getattr(m, "slo", 0) == 0]
        if mem_only:
            mem_only = sorted(mem_only, key=lambda m: float(m.model_size), reverse=True)
            for m in mem_only:
                size = float(m.model_size)
                # Choose GPU with most remaining memory that can fit
                best = None
                best_rem = -1.0
                for gid in range(gpu_num):
                    rem = S - used[gid]
                    if size <= rem and rem > best_rem:
                        best_rem = rem
                        best = gid
                if best is None:
                    raise ValueError(
                        f"Unable to place memory-only model of size {m.model_size} GB on any GPU. "
                        f"Remaining per-GPU memory: {[S - u for u in used]}"
                    )
                base[best].append(m)
                used[best] += size
        return base, used

    base_pre, base_used = _preplace_memory_only(models, S, gpu_num)
    used0_total = sum(base_used)

    # Extract per-model attributes once (only slo>0)
    rem_models = [m for m in models if getattr(m, "slo", 0) != 0]
    items = []
    total_R = 0.0
    total_size = 0.0
    for m in rem_models:
        slo = float(m.slo)
        dR = float(m.req_rate) / slo if slo != 0 else 0.0
        s = float(m.model_size)
        items.append({'obj': m, 'dR': dR, 'size': s})
        total_R += dR
        total_size += s
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
        # Global bound: T >= total_R / (gpu_num*S - total_size)
        denom2 = gpu_num * S - total_size
        if denom2 <= 0 and total_R > 0:
            return float('inf'), True, infeasible_single
        lb2 = 0.0 if total_R <= 0 or denom2 <= 0 else (total_R / denom2)
=======
        # Global bound accounting for pre-placed memory-only models: T >= total_R / (gpu_num*S - used0_total - total_size)
        denom2 = gpu_num * S - used0_total - total_size
        if denom2 <= 0 and total_R > 0:
            return float('inf'), True, infeasible_single
        lb2 = 0.0 if total_R <= 0 or denom2 <= 0 else (total_R / denom2)
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
            # per-GPU states
            assign = {i: [] for i in range(gpu_num)}
            used_mem = [0.0] * gpu_num
            sum_R = [0.0] * gpu_num
            T_current = T
            K = [T_current * S] * gpu_num  # KV slack
=======
            # per-GPU states (start from pre-placed memory-only models)
            assign = {i: list(base_pre.get(i, [])) for i in range(gpu_num)}
            used_mem = list(base_used)
            sum_R = [0.0] * gpu_num
            for gid in range(gpu_num):
                for m0 in assign[gid]:
                    if getattr(m0, "slo", 0) != 0:
                        sum_R[gid] += float(m0.req_rate) / float(m0.slo)
            T_current = T
            K = [T_current * S - (sum_R[g] + T_current * used_mem[g]) for g in range(gpu_num)]  # KV slack
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
    def greedy_assign(sorted_models):
        placement = {gpu_id: [] for gpu_id in range(gpu_num)}
        rem_mem = [S for _ in range(gpu_num)]
        sum_r = [0.0 for _ in range(gpu_num)]
        for model in sorted_models:
            dR = float(model.req_rate) / float(model.slo)
            current_kvprs = [kvpr(sum_r[i], rem_mem[i]) for i in range(gpu_num)]
            best_gpu = None
            best_resulting_max = float('inf')
            best_new_gpu_kvpr = float('inf')
            best_new_rem = -1.0
            for gid in range(gpu_num):
                if model.model_size <= rem_mem[gid]:
                    new_R = sum_r[gid] + dR
                    new_mem = rem_mem[gid] - model.model_size
                    new_gpu_kvpr = kvpr(new_R, new_mem)
                    resulting_max = new_gpu_kvpr
                    for j in range(gpu_num):
                        if j == gid:
                            continue
                        if current_kvprs[j] > resulting_max:
                            resulting_max = current_kvprs[j]
                    if (resulting_max < best_resulting_max or
                        (resulting_max == best_resulting_max and new_gpu_kvpr < best_new_gpu_kvpr) or
                        (resulting_max == best_resulting_max and new_gpu_kvpr == best_new_gpu_kvpr and new_mem > best_new_rem)):
                        best_resulting_max = resulting_max
                        best_new_gpu_kvpr = new_gpu_kvpr
                        best_new_rem = new_mem
                        best_gpu = gid
            if best_gpu is None:
                raise ValueError("Greedy ordering cannot place model due to memory.")
            placement[best_gpu].append(model)
            rem_mem[best_gpu] -= model.model_size
            sum_r[best_gpu] += dR
        return placement, rem_mem, sum_r
=======
    def greedy_assign(sorted_models):
        placement = {gpu_id: list(base_pre.get(gpu_id, [])) for gpu_id in range(gpu_num)}
        rem_mem = [S - base_used[i] for i in range(gpu_num)]
        sum_r = [0.0 for _ in range(gpu_num)]
        for gid in range(gpu_num):
            for m0 in placement[gid]:
                if getattr(m0, "slo", 0) != 0:
                    sum_r[gid] += float(m0.req_rate) / float(m0.slo)
        for model in sorted_models:
            slo = float(getattr(model, "slo", 0))
            dR = (float(model.req_rate) / slo) if slo != 0 else 0.0
            current_kvprs = [kvpr(sum_r[i], rem_mem[i]) for i in range(gpu_num)]
            best_gpu = None
            best_resulting_max = float('inf')
            best_new_gpu_kvpr = float('inf')
            best_new_rem = -1.0
            for gid in range(gpu_num):
                if model.model_size <= rem_mem[gid]:
                    new_R = sum_r[gid] + dR
                    new_mem = rem_mem[gid] - model.model_size
                    new_gpu_kvpr = kvpr(new_R, new_mem)
                    resulting_max = new_gpu_kvpr
                    for j in range(gpu_num):
                        if j == gid:
                            continue
                        if current_kvprs[j] > resulting_max:
                            resulting_max = current_kvprs[j]
                    if (resulting_max < best_resulting_max or
                        (resulting_max == best_resulting_max and new_gpu_kvpr < best_new_gpu_kvpr) or
                        (resulting_max == best_resulting_max and new_gpu_kvpr == best_new_gpu_kvpr and new_mem > best_new_rem)):
                        best_resulting_max = resulting_max
                        best_new_gpu_kvpr = new_gpu_kvpr
                        best_new_rem = new_mem
                        best_gpu = gid
            if best_gpu is None:
                raise ValueError("Greedy ordering cannot place model due to memory.")
            placement[best_gpu].append(model)
            rem_mem[best_gpu] -= model.model_size
            sum_r[best_gpu] += dR
        return placement, rem_mem, sum_r
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
        for make_order in orderings:
            try:
                ordered = make_order(models)
                placement, rem_mem, sum_r = greedy_assign(ordered)
                placement, rem_mem, sum_r = greedy_improve(placement, rem_mem, sum_r)
                val = eval_max_kvpr(placement)
                if val < bestv:
                    bestv = val
                    best = placement
            except Exception:
                continue
=======
        for make_order in orderings:
            try:
                ordered = make_order(rem_models)
                placement, rem_mem, sum_r = greedy_assign(ordered)
                placement, rem_mem, sum_r = greedy_improve(placement, rem_mem, sum_r)
                val = eval_max_kvpr(placement)
                if val < bestv:
                    bestv = val
                    best = placement
            except Exception:
                continue
>>>>>>> REPLACE

</DIFF>