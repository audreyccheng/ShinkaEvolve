--- a/original.py
+++ b/original.py
@@ -1,589 +1,521 @@
 # EVOLVE-BLOCK-START
-"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
+"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs
+   Dual-driven water-filling rebalancing without T-parameter search.
+"""
 
 GPU_MEM_SIZE = 80  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         A placement of models to GPUs
     """
-    # Basic checks and trivial return
+    # Trivial returns
     placement_empty = {i: [] for i in range(gpu_num)}
     if gpu_num <= 0 or not models:
         return placement_empty
 
     S = GPU_MEM_SIZE
-
-    # Helper: KVPR safely
+    eps = 1e-12
+
+    # KVPR helper
     def kvpr(R, rem_mem):
         if rem_mem <= 0:
             return float('inf')
         return R / rem_mem
 
-    # Greedy fallback minimizing local max-KVPR increase (robust to slo==0)
-    def _greedy_fallback_kvpr(gpu_num, models, S):
-        # Sort by demand ratio descending; tie by size desc
-        sorted_models = sorted(
-            models,
-            key=lambda m: ((m.req_rate / m.slo) if getattr(m, "slo", 0) != 0 else 0.0, m.model_size),
-            reverse=True
-        )
-        placement = {i: [] for i in range(gpu_num)}
-        rem_mem = [S] * gpu_num
-        sum_R = [0.0] * gpu_num
-        for m in sorted_models:
-            dR = (m.req_rate / m.slo) if getattr(m, "slo", 0) != 0 else 0.0
-            size = float(m.model_size)
-            best_gid = None
-            best_val = float('inf')
-            for gid in range(gpu_num):
-                if size <= rem_mem[gid]:
-                    rem = rem_mem[gid] - size
-                    if rem <= 0:
-                        continue
-                    val = (sum_R[gid] + dR) / rem
-                    if val < best_val:
-                        best_val = val
-                        best_gid = gid
-            if best_gid is None:
-                raise ValueError(
-                    f"Unable to place model of size {m.model_size} GB on any GPU. "
-                    f"Remaining per-GPU memory: {rem_mem}"
-                )
-            placement[best_gid].append(m)
-            sum_R[best_gid] += dR
-            rem_mem[best_gid] -= size
+    # Split models into memory-only (slo==0) and rate-contributing (slo>0)
+    mem_only = []
+    active = []
+    for m in models:
+        slo = float(getattr(m, "slo", 0.0))
+        sz = float(getattr(m, "model_size", 0.0))
+        if slo == 0.0:
+            mem_only.append(m)
+        else:
+            dr = float(getattr(m, "req_rate", 0.0)) / slo
+            active.append((m, dr, sz))
+
+    # Place memory-only models first: largest size first onto GPU with most free memory
+    placement = {i: [] for i in range(gpu_num)}
+    used_mem = [0.0] * gpu_num
+    sum_R = [0.0] * gpu_num
+
+    mem_only_sorted = sorted(mem_only, key=lambda x: float(getattr(x, "model_size", 0.0)), reverse=True)
+    for m in mem_only_sorted:
+        sz = float(getattr(m, "model_size", 0.0))
+        if sz > S + eps:
+            # Cannot place a single model anywhere
+            raise ValueError(f"Model size {sz} exceeds GPU memory {S}")
+        # choose GPU with most remaining memory (tie: least items)
+        best = None
+        best_key = None
+        for g in range(gpu_num):
+            rem = S - used_mem[g]
+            if sz <= rem + eps:
+                key = (rem, -len(placement[g]))
+                if best_key is None or key > best_key:
+                    best_key = key
+                    best = g
+        if best is None:
+            # No feasible placement due to memory
+            raise ValueError("Unable to place memory-only model due to memory constraints")
+        placement[best].append(m)
+        used_mem[best] += sz
+        # sum_R unchanged (no KV contribution)
+
+    # If no active models, return memory-only placement
+    if not active:
         return placement
 
-    # Extract per-model attributes once
-    items = []
-    total_R = 0.0
-    total_size = 0.0
-    for m in models:
-        slo = float(m.slo)
-        # Treat slo==0 as memory-only: no KVPR contribution but consumes memory
-        dR = float(m.req_rate) / slo if slo != 0 else 0.0
-        s = float(m.model_size)
-        items.append({'obj': m, 'dR': dR, 'size': s})
-        total_R += dR
-        total_size += s
-
-    # Lower bounds on optimal T
-    def compute_lower_bound():
-        # Per-item bound: T >= dR / (S - s)
-        lb1 = 0.0
-        infeasible_single = False
-        for it in items:
-            dR = it['dR']; s = it['size']
-            denom = S - s
-            if denom <= 0:
-                if dR > 0 and dR != float('inf'):
-                    infeasible_single = True
-                elif dR == float('inf'):
-                    infeasible_single = True
+    # Build convenience lists for active models
+    # Element: (model, dR, size)
+    active_sorted_seed = sorted(
+        active,
+        key=lambda t: (t[1] / max(S - t[2], 1e-9), t[1], t[2]),  # priority by intrinsic pressure
+        reverse=True
+    )
+
+    # Initial greedy minimax placement for active models
+    def greedy_minimax_place(seed_items):
+        for (m, dr, sz) in seed_items:
+            # choose GPU minimizing resulting max KVPR
+            best_gpu = None
+            best_result = float('inf')
+            best_local = float('inf')
+            for g in range(gpu_num):
+                if used_mem[g] + sz > S + eps:
+                    continue
+                new_used = used_mem[g] + sz
+                rem_g = S - new_used
+                if rem_g <= 0:
+                    continue
+                new_R = sum_R[g] + dr
+                new_k_g = kvpr(new_R, rem_g)
+                # compute resulting max KVPR across GPUs
+                resulting = new_k_g
+                for h in range(gpu_num):
+                    if h == g:
+                        continue
+                    other = kvpr(sum_R[h], S - used_mem[h])
+                    if other > resulting:
+                        resulting = other
+                if (resulting < best_result or
+                    (abs(resulting - best_result) <= 1e-12 and (new_k_g < best_local)) or
+                    (abs(resulting - best_result) <= 1e-12 and abs(new_k_g - best_local) <= 1e-12 and (S - new_used) > (S - used_mem[best_gpu] - sz) if best_gpu is not None else True)):
+                    best_result = resulting
+                    best_local = new_k_g
+                    best_gpu = g
+            if best_gpu is None:
+                # Fall back: place where it fits with least local KVPR
+                best_gpu = None
+                best_local = float('inf')
+                for g in range(gpu_num):
+                    if used_mem[g] + sz <= S + eps:
+                        rem = S - (used_mem[g] + sz)
+                        if rem > 0:
+                            k = kvpr(sum_R[g] + dr, rem)
+                            if k < best_local:
+                                best_local = k
+                                best_gpu = g
+                if best_gpu is None:
+                    raise ValueError("Unable to place active model due to memory.")
+            placement[best_gpu].append(m)
+            used_mem[best_gpu] += sz
+            sum_R[best_gpu] += dr
+
+    greedy_minimax_place(active_sorted_seed)
+
+    # Evaluate current per-GPU KVPRs
+    def per_gpu_kvprs(used_mem_list=None, sum_R_list=None):
+        um = used_mem if used_mem_list is None else used_mem_list
+        rr = sum_R if sum_R_list is None else sum_R_list
+        arr = []
+        for g in range(gpu_num):
+            arr.append(kvpr(rr[g], S - um[g]))
+        return arr
+
+    # Dual sensitivities: α_g for memory, β_g for rate
+    def dual_sensitivities():
+        alpha = [0.0] * gpu_num
+        beta = [0.0] * gpu_num
+        for g in range(gpu_num):
+            rem = max(eps, S - used_mem[g])
+            beta[g] = 1.0 / rem
+            alpha[g] = sum_R[g] / (rem * rem)
+        return alpha, beta
+
+    # Try moving a single model and return the improvement (delta in max KVPR)
+    def try_move(model, dr, sz, src, dst):
+        if src == dst:
+            return None
+        if used_mem[dst] + sz > S + eps:
+            return None
+        # simulate
+        rem_src_new = S - (used_mem[src] - sz)
+        rem_dst_new = S - (used_mem[dst] + sz)
+        if rem_src_new <= 0 or rem_dst_new <= 0:
+            return None
+        kvprs_before = per_gpu_kvprs()
+        cur_max = max(kvprs_before)
+        # new kvprs only for src and dst
+        new_src_k = kvpr(sum_R[src] - dr, rem_src_new)
+        new_dst_k = kvpr(sum_R[dst] + dr, rem_dst_new)
+        # resulting max
+        res = new_src_k if new_src_k > new_dst_k else new_dst_k
+        for g in range(gpu_num):
+            if g == src or g == dst:
                 continue
-            if dR > 0 and dR != float('inf'):
-                cand = dR / denom
-                if cand > lb1:
-                    lb1 = cand
-
-        # Global bound: T >= total_R / (gpu_num*S - total_size)
-        denom2 = gpu_num * S - total_size
-        if denom2 <= 0 and total_R > 0:
-            return float('inf'), True, infeasible_single
-        lb2 = 0.0 if total_R <= 0 or denom2 <= 0 else (total_R / denom2)
-
-        # Pair bound: for pairs that cannot co-reside (s_i + s_j > S)
-        lb_pair = 0.0
-        P = min(len(items), 200)
-        by_size = sorted(items, key=lambda x: x['size'], reverse=True)[:P]
-        for i in range(len(by_size)):
-            si = by_size[i]['size']; ri = by_size[i]['dR']
-            for j in range(i + 1, len(by_size)):
-                sj = by_size[j]['size']; rj = by_size[j]['dR']
-                if si + sj > S:
-                    denom = 2 * S - (si + sj)
-                    if denom > 0:
-                        ri_f = 0.0 if ri == float('inf') else ri
-                        rj_f = 0.0 if rj == float('inf') else rj
-                        cand = (ri_f + rj_f) / denom
-                        if cand > lb_pair:
-                            lb_pair = cand
-
-        # k-bin prefix bound for k in {1..min(gpu_num,4)}
-        lb_k = 0.0
-        sorted_by_size = sorted(items, key=lambda x: x['size'], reverse=True)
-        prefix_sizes = []
-        prefix_rates = []
-        cs = 0.0; cr = 0.0
-        for it in sorted_by_size:
-            cs += it['size']; cr += (0.0 if it['dR'] == float('inf') else it['dR'])
-            prefix_sizes.append(cs); prefix_rates.append(cr)
-        for k in range(1, min(gpu_num, 4) + 1):
-            threshold = (k - 1) * S
-            idx = -1
-            for t in range(len(prefix_sizes)):
-                if prefix_sizes[t] > threshold:
-                    idx = t
+            if kvprs_before[g] > res:
+                res = kvprs_before[g]
+        return cur_max - res  # positive means improvement
+
+    # Execute move
+    def apply_move(model, dr, sz, src, dst):
+        placement[src].remove(model)
+        placement[dst].append(model)
+        used_mem[src] -= sz
+        used_mem[dst] += sz
+        sum_R[src] -= dr
+        sum_R[dst] += dr
+
+    # Gather active item lookup: map model -> (dr, sz, gpu)
+    def build_active_index():
+        idx = {}
+        for g in range(gpu_num):
+            for m in placement[g]:
+                slo = float(getattr(m, "slo", 0.0))
+                if slo > 0:
+                    dr = float(getattr(m, "req_rate", 0.0)) / slo
+                    sz = float(getattr(m, "model_size", 0.0))
+                    idx[m] = (dr, sz, g)
+        return idx
+
+    # Water-level surplus/deficit balancing
+    def balance_surplus_deficit(max_iters=2, move_cap=20):
+        it = 0
+        moves = 0
+        while it < max_iters and moves < move_cap:
+            it += 1
+            rems = [max(eps, S - used_mem[g]) for g in range(gpu_num)]
+            total_R = sum(sum_R)
+            total_rem = sum(rems)
+            if total_rem <= eps:
+                break
+            R_hat = total_R / total_rem
+            # deficit (positive means need more R)
+            deficit = [R_hat * rems[g] - sum_R[g] for g in range(gpu_num)]
+            donors = sorted([g for g in range(gpu_num) if deficit[g] < -1e-9],
+                            key=lambda g: deficit[g])  # most negative first
+            receivers = sorted([g for g in range(gpu_num) if deficit[g] > 1e-9],
+                               key=lambda g: deficit[g], reverse=True)
+            if not donors or not receivers:
+                break
+
+            improved = False
+            kv_before = max(per_gpu_kvprs())
+            # attempt to move one model per iteration
+            for src in donors:
+                # models on src sorted by "helpfulness" = dr*beta[src] + sz*alpha[src]
+                alpha, beta = dual_sensitivities()
+                cand_models = []
+                for m in list(placement[src]):
+                    slo = float(getattr(m, "slo", 0.0))
+                    if slo <= 0:
+                        continue
+                    dr = float(getattr(m, "req_rate", 0.0)) / slo
+                    sz = float(getattr(m, "model_size", 0.0))
+                    score = dr * beta[src] + sz * alpha[src]
+                    cand_models.append((score, m, dr, sz))
+                cand_models.sort(key=lambda x: x[0], reverse=True)
+                moved = False
+                for _, m, dr, sz in cand_models:
+                    # try receivers by largest deficit
+                    for dst in receivers:
+                        if used_mem[dst] + sz > S + eps:
+                            continue
+                        gain = try_move(m, dr, sz, src, dst)
+                        if gain is not None and gain > 1e-12:
+                            apply_move(m, dr, sz, src, dst)
+                            moves += 1
+                            improved = True
+                            moved = True
+                            break
+                    if moved:
+                        break
+                if improved:
                     break
-            if idx >= 0:
-                numer = prefix_rates[idx]
-                denom = k * S - prefix_sizes[idx]
-                if denom > 0 and numer > 0:
-                    cand = numer / denom
-                    if cand > lb_k:
-                        lb_k = cand
-
-        lower = max(0.0, lb1, lb2, lb_pair, lb_k)
-        return lower, False, infeasible_single
-
-    lower, infeasible_global, infeasible_single = compute_lower_bound()
-    if infeasible_single or infeasible_global or lower == float('inf'):
-        # Fallback: simple greedy minimizing local KVPR increase
-        return _greedy_fallback_kvpr(gpu_num, models, S)
-
-    # Deterministic small-random helper
-    import random
-    rng = random.Random(len(items) * 1009 + gpu_num * 9173)
-
-    # Slack-equalization packer for a given T
-    # K_g = T*S - (sumR_g + T*used_mem_g); placing (dR, s) consumes w = dR + T*s
-    def assign_balanced_slack(T, order='w_desc', seed_H=0, choose_rule='tight', seeds=1):
-        if T < 0:
-            return None
-        # Build enriched list
-        enriched = []
-        for it in items:
-            dR = it['dR']; sz = it['size']
-            w = dR + T * sz
-            if w < 0:
-                w = 0.0
-            enriched.append([w, dR, sz, it['obj']])
-
-        # Orderings
-        if order == 'w_desc':
-            enriched.sort(key=lambda x: x[0], reverse=True)
-        elif order == 'intrinsic_desc':
-            # dR/(S - sz)
-            enriched.sort(key=lambda x: (x[1] / max(S - x[2], 1e-9)), reverse=True)
-        elif order == 'size_desc':
-            enriched.sort(key=lambda x: x[2], reverse=True)
-        elif order == 'density_desc':
-            enriched.sort(key=lambda x: (x[1] / (x[2] if x[2] > 0 else 1e-9)), reverse=True)
-        else:
-            enriched.sort(key=lambda x: x[0], reverse=True)
-
-        best_assign = None
-        best_val = float('inf')
-
-        for _ in range(max(1, seeds)):
-            # per-GPU states
-            assign = {i: [] for i in range(gpu_num)}
-            used_mem = [0.0] * gpu_num
-            sum_R = [0.0] * gpu_num
-            K = [T * S] * gpu_num  # KV slack
-
-            # Seeding: spread top H intrinsic-pressure models using worst-fit on K
-            H = int(seed_H) if seed_H else 0
-            if H > 0 and len(enriched) > 0:
-                intrinsic = sorted(enriched, key=lambda x: (x[1] / max(S - x[2], 1e-9)), reverse=True)
-                seeds_list = intrinsic[:min(H, len(enriched))]
-                # Remove seeds from enriched while preserving original sequence among the rest
-                seed_ids = set(id(x) for x in seeds_list)
-                remaining = [x for x in enriched if id(x) not in seed_ids]
-                for w, dR, sz, m in seeds_list:
-                    # Choose GPU with largest K that can fit both memory and slack
-                    candidates = []
-                    for gid in range(gpu_num):
-                        if used_mem[gid] + sz <= S + 1e-9 and K[gid] >= w - 1e-9:
-                            candidates.append(gid)
-                    if not candidates:
-                        assign = None
+            if not improved:
+                # no improving move found
+                break
+
+    # Dual-based targeted rebalancing from the most pressured GPU
+    def dual_rebalance(passes=3, move_budget=24):
+        for _ in range(passes):
+            kvprs = per_gpu_kvprs()
+            cur_max = max(kvprs)
+            max_gid = max(range(gpu_num), key=lambda g: kvprs[g])
+            alpha, beta = dual_sensitivities()
+            # Rank models on the most loaded GPU by estimated incremental harm
+            candidates = []
+            for m in list(placement[max_gid]):
+                slo = float(getattr(m, "slo", 0.0))
+                if slo <= 0:
+                    continue
+                dr = float(getattr(m, "req_rate", 0.0)) / slo
+                sz = float(getattr(m, "model_size", 0.0))
+                harm = dr * beta[max_gid] + sz * alpha[max_gid]
+                candidates.append((harm, m, dr, sz))
+            if not candidates:
+                break
+            candidates.sort(key=lambda x: x[0], reverse=True)
+
+            moved_any = False
+            moves_left = move_budget
+            for _, m, dr, sz in candidates:
+                if moves_left <= 0:
+                    break
+                # choose destination by minimal incremental cost
+                best_dst = None
+                best_cost = float('inf')
+                for g in range(gpu_num):
+                    if g == max_gid:
+                        continue
+                    if used_mem[g] + sz > S + eps:
+                        continue
+                    cost = dr * beta[g] + sz * alpha[g]
+                    if cost < best_cost:
+                        best_cost = cost
+                        best_dst = g
+                if best_dst is None:
+                    continue
+                gain = try_move(m, dr, sz, max_gid, best_dst)
+                if gain is not None and gain > 1e-12:
+                    apply_move(m, dr, sz, max_gid, best_dst)
+                    moved_any = True
+                    moves_left -= 1
+            if not moved_any:
+                break
+
+    # Limited swap improvement between hottest GPU and others
+    def limited_swaps(swap_budget=8):
+        swaps = 0
+        while swaps < swap_budget:
+            kvprs = per_gpu_kvprs()
+            cur_max = max(kvprs)
+            src = max(range(gpu_num), key=lambda g: kvprs[g])
+            improved = False
+            # limit consideration
+            src_models = [m for m in placement[src] if float(getattr(m, "slo", 0.0)) > 0.0]
+            src_models = sorted(
+                src_models,
+                key=lambda m: (float(getattr(m, "req_rate", 0.0)) / float(getattr(m, "slo", 1.0))),
+                reverse=True
+            )[:min(10, len(src_models))]
+
+            for a in src_models:
+                a_slo = float(getattr(a, "slo", 0.0))
+                a_dr = float(getattr(a, "req_rate", 0.0)) / a_slo
+                a_sz = float(getattr(a, "model_size", 0.0))
+                for dst in range(gpu_num):
+                    if dst == src or not placement[dst]:
+                        continue
+                    # try a few dst models
+                    dst_models = [b for b in placement[dst] if float(getattr(b, "slo", 0.0)) > 0.0]
+                    dst_models = sorted(
+                        dst_models,
+                        key=lambda b: (float(getattr(b, "req_rate", 0.0)) / float(getattr(b, "slo", 1.0)))
+                    )[:min(8, len(dst_models))]
+
+                    for b in dst_models:
+                        b_slo = float(getattr(b, "slo", 0.0))
+                        b_dr = float(getattr(b, "req_rate", 0.0)) / b_slo
+                        b_sz = float(getattr(b, "model_size", 0.0))
+                        # check memory feasibility after swap
+                        mem_src_new = used_mem[src] - a_sz + b_sz
+                        mem_dst_new = used_mem[dst] - b_sz + a_sz
+                        if mem_src_new > S + eps or mem_dst_new > S + eps:
+                            continue
+                        rem_src = S - mem_src_new
+                        rem_dst = S - mem_dst_new
+                        if rem_src <= 0 or rem_dst <= 0:
+                            continue
+                        # compute resulting max kvpr
+                        kvprs_before = per_gpu_kvprs()
+                        new_src_k = kvpr(sum_R[src] - a_dr + b_dr, rem_src)
+                        new_dst_k = kvpr(sum_R[dst] - b_dr + a_dr, rem_dst)
+                        res = max(new_src_k, new_dst_k)
+                        for g in range(gpu_num):
+                            if g == src or g == dst:
+                                continue
+                            if kvprs_before[g] > res:
+                                res = kvprs_before[g]
+                        if res + 1e-12 < cur_max:
+                            # apply swap
+                            placement[src].remove(a); placement[src].append(b)
+                            placement[dst].remove(b); placement[dst].append(a)
+                            used_mem[src] = mem_src_new
+                            used_mem[dst] = mem_dst_new
+                            sum_R[src] = sum_R[src] - a_dr + b_dr
+                            sum_R[dst] = sum_R[dst] - b_dr + a_dr
+                            swaps += 1
+                            improved = True
+                            break
+                    if improved:
                         break
-                    # Worst-fit on K; tie-break by larger memory slack, then lower resulting local kvpr
-                    def key_fn(g):
-                        rem = S - (used_mem[g] + sz)
-                        kvnew = kvpr(sum_R[g] + dR, rem) if rem > 0 else float('inf')
-                        return (-K[g], -(rem), kvnew)
-                    candidates.sort(key=lambda g: key_fn(g))
-                    chosen = candidates[0]
-                    if rng.random() < 0.2 and len(candidates) > 1:
-                        chosen = candidates[rng.randrange(min(2, len(candidates)))]
-                    assign[chosen].append(m)
-                    used_mem[chosen] += sz
-                    sum_R[chosen] += dR
-                    K[chosen] -= w
-                if assign is None:
+                if improved:
+                    break
+            if not improved:
+                break
+
+    # Run improvements
+    dual_rebalance(passes=3, move_budget=24)
+    balance_surplus_deficit(max_iters=2, move_cap=20)
+    dual_rebalance(passes=2, move_budget=16)
+    limited_swaps(swap_budget=6)
+
+    # Final pass: single-item descent until no improvement
+    def single_pass_descent(max_moves=20):
+        moves = 0
+        while moves < max_moves:
+            kvprs = per_gpu_kvprs()
+            cur_max = max(kvprs)
+            src = max(range(gpu_num), key=lambda g: kvprs[g])
+            best = None  # (gain, src, dst, model, dr, sz)
+            for m in list(placement[src]):
+                slo = float(getattr(m, "slo", 0.0))
+                if slo <= 0:
                     continue
-                enriched = remaining
-
-            # Main packing: equalize K by choosing GPU that leaves minimal nonnegative K'
-            for w, dR, sz, m in enriched:
-                options = []
-                for gid in range(gpu_num):
-                    if used_mem[gid] + sz <= S + 1e-9 and K[gid] >= w - 1e-9:
-                        K_after = K[gid] - w
-                        if choose_rule == 'min_kvpr':
-                            rem = S - (used_mem[gid] + sz)
-                            if rem <= 0:
-                                continue
-                            kv_new = kvpr(sum_R[gid] + dR, rem)
-                            options.append((gid, K_after, kv_new))
-                        else:
-                            options.append((gid, K_after, None))
-                if not options:
-                    assign = None
-                    break
-                # Selection: prefer minimal K_after (tightest feasible), tie-break by min kvpr or larger mem slack
-                if choose_rule == 'min_kvpr':
-                    options.sort(key=lambda t: (t[1], t[2]))
-                else:
-                    options.sort(key=lambda t: t[1])
-                chosen = options[0][0]
-                # Tiny randomization among top-2 close in K_after
-                if len(options) > 1:
-                    bestK = options[0][1]
-                    near = [op for op in options if abs(op[1] - bestK) <= max(1e-9, 0.001 * (T * S + 1e-9))]
-                    if len(near) >= 2 and rng.random() < 0.25:
-                        chosen = near[rng.randrange(min(2, len(near)))][0]
-
-                assign[chosen].append(m)
-                used_mem[chosen] += sz
-                sum_R[chosen] += dR
-                K[chosen] -= w
-
-            if assign is None:
-                continue
-
-            # Validate constraints strictly
-            ok = True
-            for gid in range(gpu_num):
-                if used_mem[gid] - S > 1e-6:
-                    ok = False; break
-                rem = S - used_mem[gid]
-                if rem <= 0 and sum_R[gid] > 0:
-                    ok = False; break
-                if T > 0 and (sum_R[gid] / max(rem, 1e-12)) - T > 1e-6:
-                    ok = False; break
-            if not ok:
-                continue
-
-            # Keep best by measured max KVPR
-            val = 0.0
-            for gid in range(gpu_num):
-                rem = S - used_mem[gid]
-                val = max(val, kvpr(sum_R[gid], rem))
-            if val < best_val:
-                best_val = val
-                best_assign = assign
-
-        return best_assign
-
-    # Find the first feasible T by multiplicative sweep from the lower bound
-    def find_first_feasible_T(start_T):
-        T = max(0.0, start_T)
-        growth = 1.08
-        max_steps = 40
-        for _ in range(max_steps):
-            for (order, choose_rule, seedH, seeds) in [
-                ('w_desc', 'tight', min(4, max(1, gpu_num)), 1),
-                ('intrinsic_desc', 'tight', min(3, max(1, gpu_num - 1)), 1),
-                ('w_desc', 'min_kvpr', 0, 1),
-            ]:
-                cand = assign_balanced_slack(T, order=order, seed_H=seedH, choose_rule=choose_rule, seeds=1)
-                if cand is not None:
-                    return T, cand
-            T = T * growth if T > 0 else 1e-6
-        return None, None
-
-    T_feas, placement_at_T = find_first_feasible_T(lower)
-    if placement_at_T is None:
-        # As a safety net
-        return _greedy_fallback_kvpr(gpu_num, models, S)
-
-    # Helper: evaluate max KVPR of a placement
-    def eval_max_kvpr(placement_dict):
-        max_v = 0.0
-        for gid in range(gpu_num):
-            bucket = placement_dict.get(gid, [])
-            used = 0.0
-            R = 0.0
-            for m in bucket:
-                used += float(m.model_size)
-                R += float(m.req_rate / m.slo) if getattr(m, "slo", 0) != 0 else 0.0
-            val = kvpr(R, S - used)
-            if val > max_v:
-                max_v = val
-        return max_v
-
-    # Build a compact set of candidate T values around the first feasible T
-    candidates_T = []
-    for mul in [0.975, 0.985, 0.99, 1.0, 1.005, 1.01, 1.02, 1.03]:
-        val = max(lower, T_feas * mul)
-        candidates_T.append(val)
-    candidates_T.append(0.5 * (lower + T_feas))
-    candidates_T = sorted(set(round(t, 12) for t in candidates_T))
-
-    # Try a few packing variants per T and pick best by measured KVPR
-    variants = [
-        ('w_desc', 'tight', min(4, max(1, gpu_num)), 3),
-        ('intrinsic_desc', 'tight', min(3, max(1, gpu_num - 1))),
-        ('intrinsic_desc', 'min_kvpr', 0),
-        ('density_desc', 'tight', 0),
-        ('w_desc', 'min_kvpr', 0),
-        ('size_desc', 'tight', 0),
-    ]
-
-    best_placement = placement_at_T
-    best_val = eval_max_kvpr(best_placement)
-
-    for T in candidates_T:
-        for var in variants:
-            # Unpack with defaults for backward compatibility tuple length
-            if len(var) == 4:
-                order, choose_rule, seedH, seeds = var
-            else:
-                order, choose_rule, seedH = var
-                seeds = 1
-            cand = assign_balanced_slack(T, order=order, seed_H=seedH, choose_rule=choose_rule, seeds=seeds)
-            if cand is None:
-                continue
-            val = eval_max_kvpr(cand)
-            if val < best_val:
-                best_val = val
-                best_placement = cand
-
-    # Light binary search on T to tighten the feasible threshold further
-    def _feasible_assign_for_T(T):
-        best = None
-        bestv = float('inf')
-        for (order, choose_rule, seedH, seeds) in [
-            ('w_desc', 'tight', min(4, max(1, gpu_num)), 1),
-            ('intrinsic_desc', 'tight', min(3, max(1, gpu_num - 1)), 1),
-            ('w_desc', 'min_kvpr', 0, 1),
-            ('intrinsic_desc', 'min_kvpr', 0, 1),
-        ]:
-            cand = assign_balanced_slack(T, order=order, seed_H=seedH, choose_rule=choose_rule, seeds=seeds)
-            if cand is not None:
-                v = eval_max_kvpr(cand)
-                if v < bestv:
-                    bestv = v
-                    best = cand
-        return best
-
-    lo, hi = max(0.0, lower), max(lower, best_val)
-    if hi < float('inf'):
-        for _ in range(10):
-            mid = (lo + hi) / 2.0
-            cand = _feasible_assign_for_T(mid)
-            if cand is not None:
-                hi = mid
-                v = eval_max_kvpr(cand)
-                if v < best_val:
-                    best_val = v
-                    best_placement = cand
-            else:
-                lo = mid
-
-    # Short bounded local search focusing on the most loaded GPU: moves then swaps
-    def local_refine(placement, move_budget=20, swap_budget=10):
-        buckets = {gid: list(placement.get(gid, [])) for gid in range(gpu_num)}
-        used_mem = [0.0] * gpu_num
-        sum_R = [0.0] * gpu_num
-        for gid in range(gpu_num):
-            for m in buckets[gid]:
-                used_mem[gid] += float(m.model_size)
-                sum_R[gid] += float(m.req_rate / m.slo) if getattr(m, "slo", 0) != 0 else 0.0
-
-        def current_kvprs():
-            return [kvpr(sum_R[g], S - used_mem[g]) for g in range(gpu_num)]
-
-        def apply_move(src, dst, mdl):
-            dR = float(mdl.req_rate / mdl.slo) if getattr(mdl, "slo", 0) != 0 else 0.0
-            s = float(mdl.model_size)
-            buckets[src].remove(mdl); buckets[dst].append(mdl)
-            sum_R[src] -= dR; sum_R[dst] += dR
-            used_mem[src] -= s; used_mem[dst] += s
-
-        moves_left = move_budget
-        swaps_left = swap_budget
-        while moves_left > 0 or swaps_left > 0:
-            kvprs = current_kvprs()
-            cur_max = max(kvprs) if kvprs else 0.0
-            max_gid = max(range(gpu_num), key=lambda g: kvprs[g]) if kvprs else 0
-
-            improved = False
-            best_move = None  # (src,dst,mdl,resulting_max)
-            # Single-item moves
-            for mdl in list(buckets[max_gid]):
-                dR = float(mdl.req_rate / mdl.slo) if getattr(mdl, "slo", 0) != 0 else 0.0
-                s = float(mdl.model_size)
-                R_src_new = sum_R[max_gid] - dR
-                mem_src_new = used_mem[max_gid] - s
-                rem_src_new = S - mem_src_new
-                if rem_src_new <= 0:
+                dr = float(getattr(m, "req_rate", 0.0)) / slo
+                sz = float(getattr(m, "model_size", 0.0))
+                for dst in range(gpu_num):
+                    if dst == src:
+                        continue
+                    g = try_move(m, dr, sz, src, dst)
+                    if g is not None and g > 1e-12:
+                        if best is None or g > best[0]:
+                            best = (g, src, dst, m, dr, sz)
+            if best is None:
+                break
+            _, s, d, m, dr, sz = best
+            apply_move(m, dr, sz, s, d)
+            moves += 1
+
+    single_pass_descent(max_moves=20)
+
+    # Safety: ensure memory is not exceeded
+    for g in range(gpu_num):
+        mem = sum(float(getattr(m, "model_size", 0.0)) for m in placement[g])
+        if mem - S > 1e-6:
+            # Should not happen due to checks; in case, roll back to a simpler feasible greedy
+            return _greedy_fallback_kvpr(gpu_num, models, S)
+
+    return placement
+
+
+def _greedy_fallback_kvpr(gpu_num, models, S):
+    """Simple greedy baseline minimizing the resulting max KVPR at each insertion."""
+    def kvpr(R, rem_mem):
+        if rem_mem <= 0:
+            return float('inf')
+        return R / rem_mem
+
+    placement = {i: [] for i in range(gpu_num)}
+    used_mem = [0.0] * gpu_num
+    sum_R = [0.0] * gpu_num
+
+    # sort by intrinsic pressure, tie by dR then size
+    def model_key(m):
+        slo = float(getattr(m, "slo", 0.0))
+        sz = float(getattr(m, "model_size", 0.0))
+        dr = (float(getattr(m, "req_rate", 0.0)) / slo) if slo > 0 else 0.0
+        denom = max(1e-9, S - sz)
+        return (dr / denom, dr, sz)
+
+    ordered = sorted(models, key=model_key, reverse=True)
+    for m in ordered:
+        sz = float(getattr(m, "model_size", 0.0))
+        slo = float(getattr(m, "slo", 0.0))
+        dr = (float(getattr(m, "req_rate", 0.0)) / slo) if slo > 0 else 0.0
+        best_gpu = None
+        best_res = float('inf')
+        for g in range(gpu_num):
+            if used_mem[g] + sz <= S + 1e-12:
+                new_used = used_mem[g] + sz
+                rem = S - new_used
+                if rem <= 0:
                     continue
-                kv_src_new = kvpr(R_src_new, rem_src_new)
-                for dst in range(gpu_num):
-                    if dst == max_gid:
-                        continue
-                    if used_mem[dst] + s > S:
-                        continue
-                    rem_dst_new = S - (used_mem[dst] + s)
-                    if rem_dst_new <= 0:
-                        continue
-                    R_dst_new = sum_R[dst] + dR
-                    kv_dst_new = kvpr(R_dst_new, rem_dst_new)
-                    resulting = kv_dst_new if kv_dst_new > kv_src_new else kv_src_new
-                    for g in range(gpu_num):
-                        if g == max_gid or g == dst:
-                            continue
-                        if kvprs[g] > resulting:
-                            resulting = kvprs[g]
-                    if resulting + 1e-12 < cur_max:
-                        if best_move is None or resulting < best_move[3]:
-                            best_move = (max_gid, dst, mdl, resulting)
-            if best_move is not None and moves_left > 0:
-                src, dst, mdl, _ = best_move
-                apply_move(src, dst, mdl)
-                moves_left -= 1
-                improved = True
-            else:
-                # Try limited swaps
-                best_swap = None  # (src,dst,a,b,resulting_max)
-                if swaps_left > 0:
-                    cap_a = min(10, len(buckets[max_gid]))
-                    for a in list(buckets[max_gid])[:cap_a]:
-                        aR = float(a.req_rate / a.slo) if getattr(a, "slo", 0) != 0 else 0.0
-                        aS = float(a.model_size)
-                        for dst in range(gpu_num):
-                            if dst == max_gid or not buckets[dst]:
-                                continue
-                            cap_b = min(10, len(buckets[dst]))
-                            for b in list(buckets[dst])[:cap_b]:
-                                bR = float(b.req_rate / b.slo) if getattr(b, "slo", 0) != 0 else 0.0
-                                bS = float(b.model_size)
-                                mem_src_new = used_mem[max_gid] - aS + bS
-                                mem_dst_new = used_mem[dst] - bS + aS
-                                if mem_src_new > S or mem_dst_new > S:
-                                    continue
-                                rem_src = S - mem_src_new
-                                rem_dst = S - mem_dst_new
-                                if rem_src <= 0 or rem_dst <= 0:
-                                    continue
-                                R_src_new = sum_R[max_gid] - aR + bR
-                                R_dst_new = sum_R[dst] - bR + aR
-                                kv_src_new = kvpr(R_src_new, rem_src)
-                                kv_dst_new = kvpr(R_dst_new, rem_dst)
-                                resulting = kv_src_new if kv_src_new > kv_dst_new else kv_dst_new
-                                for g in range(gpu_num):
-                                    if g == max_gid or g == dst:
-                                        continue
-                                    val = kvpr(sum_R[g], S - used_mem[g])
-                                    if val > resulting:
-                                        resulting = val
-                                if resulting + 1e-12 < cur_max:
-                                    if best_swap is None or resulting < best_swap[4]:
-                                        best_swap = (max_gid, dst, a, b, resulting)
-                if best_swap is not None:
-                    src, dst, a, b, _ = best_swap
-                    buckets[src].remove(a); buckets[src].append(b)
-                    buckets[dst].remove(b); buckets[dst].append(a)
-                    aR = float(a.req_rate / a.slo) if getattr(a, "slo", 0) != 0 else 0.0
-                    bR = float(b.req_rate / b.slo) if getattr(b, "slo", 0) != 0 else 0.0
-                    aS = float(a.model_size); bS = float(b.model_size)
-                    sum_R[src] = sum_R[src] - aR + bR
-                    sum_R[dst] = sum_R[dst] - bR + aR
-                    used_mem[src] = used_mem[src] - aS + bS
-                    used_mem[dst] = used_mem[dst] - bS + aS
-                    swaps_left -= 1
-                    improved = True
-            if not improved:
-                break
-        return buckets
-
-    # First refinement on T-based best
-    refined = local_refine(best_placement, move_budget=18, swap_budget=8)
-    # Greedy baseline and its refinement
-    greedy_baseline = _greedy_fallback_kvpr(gpu_num, models, S)
-    greedy_refined = local_refine(greedy_baseline, move_budget=12, swap_budget=6)
-
-    # Select the best by measured max KVPR
-    candidates = [
-        best_placement,
-        refined,
-        greedy_refined,
-    ]
-    best_final = candidates[0]
-    best_score = eval_max_kvpr(best_final)
-    for cand in candidates[1:]:
-        val = eval_max_kvpr(cand)
-        if val < best_score:
-            best_score = val
-            best_final = cand
-
-    # Final check: memory safety
-    for gid in range(gpu_num):
-        mem = sum(float(m.model_size) for m in best_final.get(gid, []))
-        if mem - S > 1e-6:
-            return refined
-
-    return best_final
-
+                new_R = sum_R[g] + dr
+                new_k = kvpr(new_R, rem)
+                res = new_k
+                for h in range(gpu_num):
+                    if h == g:
+                        continue
+                    other = kvpr(sum_R[h], S - used_mem[h])
+                    if other > res:
+                        res = other
+                if res < best_res:
+                    best_res = res
+                    best_gpu = g
+        if best_gpu is None:
+            raise ValueError("Unable to place model due to memory constraints.")
+        placement[best_gpu].append(m)
+        used_mem[best_gpu] += sz
+        sum_R[best_gpu] += dr
+    return placement
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")