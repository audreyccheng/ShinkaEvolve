--- a/original.py
+++ b/original.py
@@ -1,611 +1,635 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 GPU_MEM_SIZE = 80  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         A placement of models to GPUs
     """
     # Helper to compute KVPR for a GPU safely
     def kvpr(R, rem_mem):
         if rem_mem <= 0:
             return float('inf')
         return R / rem_mem
 
     # Lexicographic scoring: (max KVPR, second-worst KVPR, average KVPR)
     def lex_score(kvprs):
         if not kvprs:
             return (0.0, 0.0, 0.0)
         kv_sorted = sorted(kvprs, reverse=True)
         maxv = kv_sorted[0]
         second = kv_sorted[1] if len(kv_sorted) > 1 else kv_sorted[0]
         avg = sum(kv_sorted) / float(len(kv_sorted))
         return (maxv, second, avg)
 
     def is_better_score(a, b, eps=1e-12):
         # Return True if score a is strictly better (smaller lexicographically) than b
         if a[0] + eps < b[0]:
             return True
         if abs(a[0] - b[0]) <= eps:
             if a[1] + eps < b[1]:
                 return True
             if abs(a[1] - b[1]) <= eps:
                 return a[2] < b[2]
         return False
 
     # Early return for trivial cases
     empty = {gpu_id: [] for gpu_id in range(gpu_num)}
     if not models or gpu_num <= 0:
         return empty
 
     # Greedy min-max assignment using lookahead of resultant max KVPR for a given ordering
     def greedy_assign(sorted_models):
         placement = {gpu_id: [] for gpu_id in range(gpu_num)}
         rem_mem = [GPU_MEM_SIZE for _ in range(gpu_num)]   # remaining memory per GPU
         sum_r_over_s = [0.0 for _ in range(gpu_num)]       # sum of r_j / s_j per GPU
 
         for model in sorted_models:
             dR = model.req_rate / model.slo
             current_kvprs = [kvpr(sum_r_over_s[i], rem_mem[i]) for i in range(gpu_num)]
 
             best_gpu = None
             best_resulting_max = float('inf')
             best_new_gpu_kvpr = float('inf')
             best_new_rem = -1
 
             for gid in range(gpu_num):
                 if model.model_size <= rem_mem[gid]:
                     new_R = sum_r_over_s[gid] + dR
                     new_mem = rem_mem[gid] - model.model_size
                     new_gpu_kvpr = kvpr(new_R, new_mem)
 
                     # Compute resulting global max KVPR after placing on gid
                     resulting_max = new_gpu_kvpr
                     for j in range(gpu_num):
                         if j == gid:
                             continue
                         if current_kvprs[j] > resulting_max:
                             resulting_max = current_kvprs[j]
 
                     # Tie-breaking: minimize resulting max; then minimize this GPU's KVPR; then prefer more remaining memory.
                     if (resulting_max < best_resulting_max or
                         (resulting_max == best_resulting_max and new_gpu_kvpr < best_new_gpu_kvpr) or
                         (resulting_max == best_resulting_max and new_gpu_kvpr == best_new_gpu_kvpr and new_mem > best_new_rem)):
                         best_resulting_max = resulting_max
                         best_new_gpu_kvpr = new_gpu_kvpr
                         best_new_rem = new_mem
                         best_gpu = gid
 
             if best_gpu is None:
                 # No GPU can fit this model without exceeding memory
                 raise ValueError(
                     f"Unable to place model of size {model.model_size} GB on any GPU. "
                     f"Remaining per-GPU memory: {rem_mem}"
                 )
 
             # Commit placement
             placement[best_gpu].append(model)
             sum_r_over_s[best_gpu] += dR
             rem_mem[best_gpu] -= model.model_size
 
         return placement, rem_mem, sum_r_over_s
 
     # Local improvement: try moving or swapping models to reduce max KVPR
     def improve(placement, rem_mem, sum_r_over_s):
         def compute_all_kvprs():
             return [kvpr(sum_r_over_s[i], rem_mem[i]) for i in range(gpu_num)]
 
         eps = 1e-12
         # Limit the number of improvement iterations to keep it simple and fast
         max_iters = max(1, min(len(models), 6 * gpu_num))
         for _ in range(max_iters):
             kvprs = compute_all_kvprs()
             if not kvprs:
                 break
             current_max = max(kvprs)
             max_gid = max(range(gpu_num), key=lambda i: kvprs[i])
 
             improved = False
             best_move = None  # (src, dst, model, resulting_max, dst_new_kvpr, dst_new_mem)
 
             # Try moving a single model from the max-pressure GPU to another GPU
             for mdl in list(placement[max_gid]):
                 dR = mdl.req_rate / mdl.slo
                 size = mdl.model_size
 
                 # State after removing from source
                 src_new_R = sum_r_over_s[max_gid] - dR
                 src_new_mem = rem_mem[max_gid] + size
                 src_new_kvpr = kvpr(src_new_R, src_new_mem)
 
                 for dst in range(gpu_num):
                     if dst == max_gid:
                         continue
                     if size <= rem_mem[dst]:
                         dst_new_R = sum_r_over_s[dst] + dR
                         dst_new_mem = rem_mem[dst] - size
                         dst_new_kvpr = kvpr(dst_new_R, dst_new_mem)
 
                         # Compute resulting global max after move
                         resulting_max = dst_new_kvpr
                         if src_new_kvpr > resulting_max:
                             resulting_max = src_new_kvpr
                         for j in range(gpu_num):
                             if j == max_gid or j == dst:
                                 continue
                             if kvprs[j] > resulting_max:
                                 resulting_max = kvprs[j]
 
                         if resulting_max + eps < current_max:
                             # Tie-breakers: minimize resulting_max, then minimize dst kvpr, then maximize dst remaining mem
                             move_better = False
                             if best_move is None:
                                 move_better = True
                             else:
                                 _, _, _, best_res_max, best_dst_kvpr, best_dst_rem = best_move
                                 if (resulting_max < best_res_max or
                                     (resulting_max == best_res_max and dst_new_kvpr < best_dst_kvpr) or
                                     (resulting_max == best_res_max and dst_new_kvpr == best_dst_kvpr and dst_new_mem > best_dst_rem)):
                                     move_better = True
                             if move_better:
                                 best_move = (max_gid, dst, mdl, resulting_max, dst_new_kvpr, dst_new_mem)
 
             if best_move is not None:
                 # Apply the best move
                 src, dst, mdl, _, _, _ = best_move
                 placement[src].remove(mdl)
                 placement[dst].append(mdl)
                 dR = mdl.req_rate / mdl.slo
                 size = mdl.model_size
                 sum_r_over_s[src] -= dR
                 rem_mem[src] += size
                 sum_r_over_s[dst] += dR
                 rem_mem[dst] -= size
                 improved = True
             else:
                 # Try one pairwise swap between the most pressured GPU and others
                 best_swap = None  # (src, dst, a, b, resulting_max)
                 src = max_gid
                 # Cap the search to keep it fast
                 cap_a = max(3, min(10, len(placement[src])))
                 for ai, a in enumerate(list(placement[src])[:cap_a]):
                     aR = a.req_rate / a.slo
                     aS = a.model_size
                     for dst in range(gpu_num):
                         if dst == src or not placement[dst]:
                             continue
                         cap_b = max(3, min(10, len(placement[dst])))
                         for bi, b in enumerate(list(placement[dst])[:cap_b]):
                             bR = b.req_rate / b.slo
                             bS = b.model_size
 
                             # New states after swap
                             src_new_R = sum_r_over_s[src] - aR + bR
                             dst_new_R = sum_r_over_s[dst] - bR + aR
                             src_new_mem = rem_mem[src] + aS - bS
                             dst_new_mem = rem_mem[dst] + bS - aS
 
                             if src_new_mem < 0 or dst_new_mem < 0:
                                 continue
 
                             src_new_kvpr = kvpr(src_new_R, src_new_mem)
                             dst_new_kvpr = kvpr(dst_new_R, dst_new_mem)
 
                             # Compute resulting max across all GPUs
                             resulting_max = src_new_kvpr if src_new_kvpr > dst_new_kvpr else dst_new_kvpr
                             for j in range(gpu_num):
                                 if j == src or j == dst:
                                     continue
                                 if kvprs[j] > resulting_max:
                                     resulting_max = kvprs[j]
 
                             if resulting_max + eps < current_max:
                                 if best_swap is None or resulting_max < best_swap[4]:
                                     best_swap = (src, dst, a, b, resulting_max)
 
                 if best_swap is not None:
                     src, dst, a, b, _ = best_swap
                     # Apply swap
                     placement[src].remove(a)
                     placement[dst].append(a)
                     placement[dst].remove(b)
                     placement[src].append(b)
 
                     aR = a.req_rate / a.slo
                     bR = b.req_rate / b.slo
                     aS = a.model_size
                     bS = b.model_size
 
                     sum_r_over_s[src] = sum_r_over_s[src] - aR + bR
                     sum_r_over_s[dst] = sum_r_over_s[dst] - bR + aR
                     rem_mem[src] = rem_mem[src] + aS - bS
                     rem_mem[dst] = rem_mem[dst] + bS - aS
                     improved = True
 
             if not improved:
                 break  # no improving move/swap found
 
         return placement, rem_mem, sum_r_over_s
 
     # Build multiple candidate orderings and choose the best after improvements
     def pressure_weight(m):
         denom = GPU_MEM_SIZE - m.model_size
         if denom <= 0:
             return float('inf')
         return (m.req_rate / m.slo) / denom
 
     def r_over_s(m):
         return (m.req_rate / m.slo)
 
     def density(m):
         # Prefer higher r/s per memory footprint
         sz = m.model_size if m.model_size > 0 else 1e-9
         return (m.req_rate / m.slo) / sz
 
     # Candidate orderings (descending where appropriate)
     orderings = [
         lambda ms: sorted(ms, key=pressure_weight, reverse=True),
         lambda ms: sorted(ms, key=r_over_s, reverse=True),
         lambda ms: sorted(ms, key=lambda m: m.model_size, reverse=True),  # size-desc
         lambda ms: sorted(ms, key=lambda m: m.model_size),                # size-asc
         lambda ms: sorted(ms, key=density, reverse=True),
         lambda ms: sorted(ms, key=lambda m: (r_over_s(m), -m.model_size), reverse=True),
     ]
 
     best_placement = None
     best_score = (float('inf'), float('inf'), float('inf'))
 
     for make_order in orderings:
         try:
             ordered = make_order(models)
             placement, rem_mem, sum_r_over_s = greedy_assign(ordered)
             # Refine with local improvements
             placement, rem_mem, sum_r_over_s = improve(placement, rem_mem, sum_r_over_s)
             # Evaluate via lexicographic scoring
             kvprs = [kvpr(sum_r_over_s[i], rem_mem[i]) for i in range(gpu_num)]
             cand_score = lex_score(kvprs)
             if is_better_score(cand_score, best_score):
                 best_score = cand_score
                 best_placement = placement
         except ValueError:
             # This ordering couldn't place all models due to per-GPU memory constraints
             continue
 
     if best_placement is None:
         # Fall back to an error consistent with greedy behavior
         # Try a final simple size-desc heuristic to produce a clearer error state
         ordered = sorted(models, key=lambda m: m.model_size, reverse=True)
         # This will raise ValueError if infeasible
         best_placement, rem_mem, sum_r_over_s = greedy_assign(ordered)
         # Compute score (not strictly needed later but keeps consistency)
         _ = lex_score([kvpr(sum_r_over_s[i], rem_mem[i]) for i in range(gpu_num)])
 
     # Parametric refinement: binary search on max KVPR target T using transformed weights
     S = GPU_MEM_SIZE
 
     def max_kvpr_of(placement_dict):
         max_v = 0.0
         for gid in range(gpu_num):
             bucket = placement_dict.get(gid, [])
             R = 0.0
             used = 0.0
             for m in bucket:
                 R += (m.req_rate / m.slo)
                 used += m.model_size
             v = kvpr(R, S - used)
             if v > max_v:
                 max_v = v
         return max_v
 
     def bfd_assign_for_T(T):
-        # Hybrid best-fit using transformed weights: enforce memory and minimize
-        # projected global max KVPR, with a slack/memory-aware tie-break.
+        # Try multiple item orderings and selection rules to find a feasible packing at target T.
         capacity = T * S
         if T < 0:
             return None
 
-        # Build items with transformed weights
-        items = []
+        # Precompute items with transformed weights
+        base_items = []
         for m in models:
             dR = (m.req_rate / m.slo)
             w = dR + T * m.model_size
             if w < 0:
                 w = 0.0
-            items.append((w, dR, m.model_size, m))
-        items.sort(key=lambda x: x[0], reverse=True)
-
-        used_w = [0.0] * gpu_num
-        bins_R = [0.0] * gpu_num
-        bins_used_mem = [0.0] * gpu_num
-        assign = {i: [] for i in range(gpu_num)}
-
-        total_used_mem = 0.0
+            base_items.append([w, dR, float(m.model_size), m])
+
         alpha = 0.15
         beta = 0.05
         eps = 1e-12
 
-        for w, dR, sz, m in items:
-            # Compute current top-1 and top-2 KVPRs to evaluate projected global max quickly
-            top1_val = -1.0
-            top2_val = -1.0
-            top1_id = -1
-            for gid in range(gpu_num):
-                rem = S - bins_used_mem[gid]
-                val = kvpr(bins_R[gid], rem)
-                if val > top1_val:
-                    top2_val = top1_val
-                    top1_val = val
-                    top1_id = gid
-                elif val > top2_val:
-                    top2_val = val
-
-            # Global average memory fraction after placing this item (independent of gid)
-            avg_mem_frac_after = (total_used_mem + sz) / (gpu_num * S) if S > 0 else 0.0
-
-            best_gid = None
-            best_proj = float('inf')
-            best_hybrid = float('inf')
-            best_nw = float('inf')
-            best_rem_after = -1.0
-
-            for gid in range(gpu_num):
-                nw = used_w[gid] + w
-                mem_after = bins_used_mem[gid] + sz
-                if nw <= capacity + 1e-9 and mem_after <= S + 1e-9:
+        def order_items(order):
+            items = list(base_items)
+            if order == 'w_desc':
+                items.sort(key=lambda x: x[0], reverse=True)
+            elif order == 'intrinsic_desc':
+                items.sort(key=lambda x: (x[1] / max(S - x[2], 1e-9)), reverse=True)
+            elif order == 'density_desc':
+                items.sort(key=lambda x: (x[1] / (x[2] if x[2] > 0 else 1e-9)), reverse=True)
+            elif order == 'size_desc':
+                items.sort(key=lambda x: x[2], reverse=True)
+            else:
+                items.sort(key=lambda x: x[0], reverse=True)
+            return items
+
+        def try_pack(order, select_rule):
+            items = order_items(order)
+            used_w = [0.0] * gpu_num
+            bins_R = [0.0] * gpu_num
+            bins_used_mem = [0.0] * gpu_num
+            assign = {i: [] for i in range(gpu_num)}
+            total_used_mem = 0.0
+
+            for w, dR, sz, m in items:
+                # If selection uses projected global max, compute current top-1 and top-2 KVPRs
+                if select_rule == 'projected':
+                    top1_val = -1.0
+                    top2_val = -1.0
+                    top1_id = -1
+                    for gid in range(gpu_num):
+                        rem = S - bins_used_mem[gid]
+                        val = kvpr(bins_R[gid], rem)
+                        if val > top1_val:
+                            top2_val = top1_val
+                            top1_val = val
+                            top1_id = gid
+                        elif val > top2_val:
+                            top2_val = val
+                    avg_mem_frac_after = (total_used_mem + sz) / (gpu_num * S) if S > 0 else 0.0
+
+                best_gid = None
+                best_key = None  # tuple for comparison depending on rule
+
+                for gid in range(gpu_num):
+                    nw = used_w[gid] + w
+                    mem_after = bins_used_mem[gid] + sz
+                    if nw > capacity + 1e-9 or mem_after > S + 1e-9:
+                        continue
                     rem_after = S - mem_after
                     if rem_after <= 0:
                         continue
                     new_k = (bins_R[gid] + dR) / rem_after
-                    max_other = top2_val if gid == top1_id else top1_val
-                    projected = new_k if new_k > max_other else max_other
-
-                    # Hybrid tie-break cost
-                    cap = capacity if capacity > 0 else 1.0
-                    k_after_norm = max(0.0, (capacity - nw)) / (cap + eps)
-                    kv_new_norm = new_k / max(T, eps) if T > eps else new_k
-                    mem_imbalance = abs((mem_after / S) - avg_mem_frac_after) if S > 0 else 0.0
-                    hybrid = k_after_norm + alpha * kv_new_norm + beta * mem_imbalance
-
-                    if (projected + 1e-12 < best_proj or
-                        (abs(projected - best_proj) <= 1e-12 and (hybrid < best_hybrid or
-                                                                 (abs(hybrid - best_hybrid) <= 1e-12 and (nw < best_nw or
-                                                                                                         (nw == best_nw and rem_after > best_rem_after)))))):
-                        best_proj = projected
-                        best_hybrid = hybrid
-                        best_nw = nw
-                        best_rem_after = rem_after
+                    # Enforce KVPR<=T feasibility at placement time
+                    if new_k - T > 1e-9:
+                        continue
+
+                    if select_rule == 'projected':
+                        max_other = top2_val if gid == top1_id else top1_val
+                        projected = new_k if new_k > max_other else max_other
+                        cap = capacity if capacity > 0 else 1.0
+                        slack_norm = max(0.0, (capacity - nw)) / (cap + eps)
+                        kv_new_norm = new_k / max(T, eps) if T > eps else new_k
+                        mem_imbalance = abs((mem_after / S) - avg_mem_frac_after) if S > 0 else 0.0
+                        hybrid = slack_norm + alpha * kv_new_norm + beta * mem_imbalance
+                        key = (projected, hybrid, nw, -rem_after)
+                    else:  # 'min_kvpr' local rule
+                        key = (new_k, nw, -rem_after)
+
+                    if best_key is None or key < best_key:
+                        best_key = key
                         best_gid = gid
 
-            if best_gid is None:
-                return None
-
-            used_w[best_gid] += w
-            bins_R[best_gid] += dR
-            bins_used_mem[best_gid] += sz
-            assign[best_gid].append(m)
-            total_used_mem += sz
-
-        # Validate memory and KVPR constraints for this T
-        for gid in range(gpu_num):
-            if bins_used_mem[gid] - S > 1e-6:
-                return None
-            rem = S - bins_used_mem[gid]
-            if rem <= 0:
-                if bins_R[gid] > 1e-12:
+                if best_gid is None:
                     return None
-            else:
-                if (bins_R[gid] / rem) - T > 1e-6:
+
+                used_w[best_gid] += w
+                bins_R[best_gid] += dR
+                bins_used_mem[best_gid] += sz
+                assign[best_gid].append(m)
+                total_used_mem += sz
+
+            # Final validation (should hold due to local checks)
+            for gid in range(gpu_num):
+                if bins_used_mem[gid] - S > 1e-6:
                     return None
-        return assign
+                rem = S - bins_used_mem[gid]
+                if rem <= 0:
+                    if bins_R[gid] > 1e-12:
+                        return None
+                else:
+                    if (bins_R[gid] / rem) - T > 1e-6:
+                        return None
+            return assign
+
+        # Try a small portfolio of strategies
+        for order, rule in [
+            ('w_desc', 'projected'),
+            ('intrinsic_desc', 'projected'),
+            ('w_desc', 'min_kvpr'),
+            ('density_desc', 'projected'),
+            ('size_desc', 'projected'),
+        ]:
+            cand = try_pack(order, rule)
+            if cand is not None:
+                return cand
+        return None
 
     # Compute bounds for the binary search
     total_R = sum((m.req_rate / m.slo) for m in models)
     total_size = sum(m.model_size for m in models)
 
     # Lower bound 1: per-model bound T >= dR / (S - size) for any model
     lb1 = 0.0
     infeasible_single = False
     for m in models:
         dR = (m.req_rate / m.slo)
         denom = S - m.model_size
         if denom <= 0:
             if dR > 0:
                 infeasible_single = True
             continue
         if dR > 0:
             cand = dR / denom
             if cand > lb1:
                 lb1 = cand
 
     # Lower bound 2: global bound from totals
     denom2 = gpu_num * S - total_size
     if denom2 <= 0 and total_R > 0:
         # Not enough aggregate free memory to host any KV capacity
         return best_placement
     lb2 = 0.0 if denom2 <= 0 else (total_R / denom2 if total_R > 0 else 0.0)
 
     # Lower bound 3: pair bound for pairs that cannot co-reside (si + sj > S)
     lb_pair = 0.0
     if models:
         P = min(len(models), 200)
         by_size = sorted(models, key=lambda m: m.model_size, reverse=True)[:P]
         for i in range(len(by_size)):
             mi = by_size[i]
             si = mi.model_size
             ri = (mi.req_rate / mi.slo)
             for j in range(i + 1, len(by_size)):
                 mj = by_size[j]
                 sj = mj.model_size
                 rj = (mj.req_rate / mj.slo)
                 if si + sj > S:
                     denom = 2 * S - (si + sj)
                     if denom > 0:
                         cand = (ri + rj) / denom
                         if cand > lb_pair:
                             lb_pair = cand
 
         # Lower bound 3b: triplet bound for triples with si + sj + sk > 2S
         lb_triplet = 0.0
         if len(by_size) >= 3:
             KTOP = min(10, len(by_size))
             topK = by_size[:KTOP]
             for i in range(len(by_size)):
                 mi = by_size[i]
                 si = mi.model_size
                 ri = (mi.req_rate / mi.slo)
                 for j in range(i + 1, len(by_size)):
                     mj = by_size[j]
                     sj = mj.model_size
                     rj = (mj.req_rate / mj.slo)
                     for mk in topK:
                         if mk is mi or mk is mj:
                             continue
                         sk = mk.model_size
                         rk = (mk.req_rate / mk.slo)
                         ssum = si + sj + sk
                         if ssum > 2 * S:
                             denom = 3 * S - ssum
                             if denom > 0:
                                 cand = (ri + rj + rk) / denom
                                 if cand > lb_triplet:
                                     lb_triplet = cand
         else:
             lb_triplet = 0.0
 
     # Lower bound 4: k-prefix bound (k up to 5 or gpu_num)
     lb_k = 0.0
     if models and gpu_num > 0:
         sorted_by_size = sorted(models, key=lambda m: m.model_size, reverse=True)
         prefix_sizes = []
         prefix_rates = []
         cs = 0.0
         cr = 0.0
         for m in sorted_by_size:
             cs += m.model_size
             cr += (m.req_rate / m.slo)
             prefix_sizes.append(cs)
             prefix_rates.append(cr)
         for k in range(1, min(gpu_num, 6) + 1):
             threshold = (k - 1) * S
             idx = -1
             for t in range(len(prefix_sizes)):
                 if prefix_sizes[t] > threshold:
                     idx = t
                     break
             if idx >= 0:
                 numer = prefix_rates[idx]
                 denom = k * S - prefix_sizes[idx]
                 if denom > 0 and numer > 0:
                     cand = numer / denom
                     if cand > lb_k:
                         lb_k = cand
 
     if infeasible_single:
         return best_placement
 
     lower = max(0.0, lb1, lb2, lb_pair, lb_triplet, lb_k)
     upper = max_kvpr_of(best_placement)
     if not (upper < float('inf')):
         return best_placement
     if lower > upper:
         lower = upper
 
     # Binary search to tighten T
     best_bsearch = None
     lo, hi = lower, upper
     for _ in range(24):  # light and fast
         mid = (lo + hi) / 2.0
         cand = bfd_assign_for_T(mid)
         if cand is not None:
             best_bsearch = cand
             hi = mid
         else:
             lo = mid
 
     # Choose the best between greedy-based and parametric candidate (optionally refine the latter)
     def summarize(placement):
         rem = [S] * gpu_num
         sr = [0.0] * gpu_num
         for gid in range(gpu_num):
             for m in placement.get(gid, []):
                 rem[gid] -= m.model_size
                 sr[gid] += (m.req_rate / m.slo)
         return rem, sr
 
     best_final = best_placement
     # Score the current best using lexicographic metrics
     rem_init, sr_init = summarize(best_final)
     kvprs_init = [kvpr(sr_init[i], rem_init[i]) for i in range(gpu_num)]
     best_score_final = lex_score(kvprs_init)
 
     if best_bsearch is not None:
         # Try a short local improvement on the parametric candidate
         rem_mem_b, sum_r_b = summarize(best_bsearch)
         improved_bsearch, rem_mem_b, sum_r_b = improve({i: list(best_bsearch.get(i, [])) for i in range(gpu_num)},
                                                        rem_mem_b, sum_r_b)
         kvprs_b = [kvpr(sum_r_b[i], rem_mem_b[i]) for i in range(gpu_num)]
         score_b = lex_score(kvprs_b)
         if is_better_score(score_b, best_score_final):
             best_final = improved_bsearch
             best_score_final = score_b
 
     return best_final
 
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")