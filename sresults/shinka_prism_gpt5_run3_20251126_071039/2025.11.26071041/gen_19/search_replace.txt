<NAME>
t_perturb_kbin_pairbounds_swap_local
</NAME>

<DESCRIPTION>
I improved the placement quality by combining three targeted changes that preserve simplicity and speed:
1) Stronger initial lower bounds on KVPR T via inexpensive pair and k-bin prefix bounds. These tighten the search interval and stabilize the transformed packing, potentially reducing the final max KVPR.
2) Small set of T perturbation candidates around the final threshold (high*(1Â±0.5%)) using a few robust order/policy variants. This exploits discrete packing effects and often yields better measured KVPR without noticeable runtime impact.
3) A lightweight local search enhancement that adds a bounded swap phase after trying single-item moves from the most loaded GPU. This often reduces the global maximum KVPR further with minimal overhead.

Additionally, I expanded feasibility checking to include a minmax policy variant, slightly improving robustness during the parametric search.

These edits are minimal, maintain correctness, and should improve the combined score while keeping runtime essentially unchanged.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
# EVOLVE-BLOCK-START
"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""

GPU_MEM_SIZE = 80  # GB

def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Args:
        gpu_num: Number of GPUs
        models: List of models to place

    Returns:
        A placement of models to GPUs
    """
    if gpu_num <= 0:
        raise ValueError("gpu_num must be positive")

    # ---------- Helpers ----------
    def safe_div(num, den):
        if den <= 0:
            return float('inf') if num > 0 else 0.0
        return num / den

    def kvpr_val(numer, rem_mem):
        return safe_div(numer, rem_mem)

    # ---------- Extract per-model stats ----------
    items = []
    total_mem = 0.0
    total_n = 0.0
    for idx, m in enumerate(models):
        ms = float(getattr(m, "model_size"))
        slo = float(getattr(m, "slo"))
        rr = float(getattr(m, "req_rate"))
        if ms < 0:
            raise ValueError("Model size must be non-negative")
        if ms > GPU_MEM_SIZE + 1e-9:
            raise ValueError(f"Model of size {ms} GB cannot fit into a single GPU of size {GPU_MEM_SIZE} GB")
        if slo <= 0:
            raise ValueError("Model SLO must be positive")
        n = rr / slo  # r_j / s_j
        items.append((idx, m, ms, n))
        total_mem += ms
        total_n += n

    if not items:
        return {gpu_id: [] for gpu_id in range(gpu_num)}

    total_capacity_mem = gpu_num * GPU_MEM_SIZE
    if total_mem - total_capacity_mem > 1e-9:
        raise ValueError("Total model memory exceeds total GPU memory")

    # ---------- Lower bound on T ----------
    indiv_lb = max(safe_div(n, GPU_MEM_SIZE - ms) for _, _, ms, n in items)
    global_lb = safe_div(total_n, max(total_capacity_mem - total_mem, 1e-9))
    low_T = max(0.0, indiv_lb, global_lb)

    # ---------- Feasibility check and packing at T ----------
    # order_variant:
    #   0 -> by transformed weight w(T) = n + T*m
    #   1 -> by intrinsic-alone KVPR n / (80 - m)
    #   2 -> by pressure per GB n / m
    # policy:
    #   "resid"  -> best-fit on transformed residual
    #   "minmax" -> choose GPU minimizing new global max KVPR
    def try_pack(T, order_variant=0, policy="resid", return_placement=False):
        cap = GPU_MEM_SIZE * T
        eps = 1e-12

        # Build sorted order
        if order_variant == 0:
            ordered = sorted(items, key=lambda it: (it[3] + T * it[2], it[3], it[2]), reverse=True)
        elif order_variant == 1:
            ordered = sorted(items, key=lambda it: (safe_div(it[3], max(GPU_MEM_SIZE - it[2], 1e-9)), it[2]), reverse=True)
        else:
            ordered = sorted(items, key=lambda it: (safe_div(it[3], max(it[2], 1e-9)), it[3]), reverse=True)

        # Per-GPU state
        n_sum = [0.0] * gpu_num
        m_sum = [0.0] * gpu_num
        used_cap = [0.0] * gpu_num  # equals n_sum + T * m_sum
        placement = [[] for _ in range(gpu_num)]

        for _, mdl, ms, n in ordered:
            w = n + T * ms
            candidates = []
            # Precompute current kvprs
            cur_kvprs = [kvpr_val(n_sum[g], GPU_MEM_SIZE - m_sum[g]) for g in range(gpu_num)]

            for g in range(gpu_num):
                if m_sum[g] + ms > GPU_MEM_SIZE + eps:
                    continue
                residual = cap - (used_cap[g] + w)
                if residual < -eps:
                    continue
                if policy == "resid":
                    candidates.append((g, residual, None, None))
                else:
                    new_n = n_sum[g] + n
                    new_m = m_sum[g] + ms
                    new_local = kvpr_val(new_n, GPU_MEM_SIZE - new_m)
                    # New global max if placed on g
                    new_max = new_local
                    for k in range(gpu_num):
                        if k != g and cur_kvprs[k] > new_max:
                            new_max = cur_kvprs[k]
                    candidates.append((g, residual, new_max, new_local))

            if not candidates:
                return (False, None) if return_placement else False

            if policy == "resid":
                # Best-fit: minimize transformed residual; tie by more remaining mem then gpu id
                # Remaining mem after placement equals GPU_MEM_SIZE - (m_sum[g]+ms)
                chosen = min(
                    candidates,
                    key=lambda c: (c[1], -(GPU_MEM_SIZE - (m_sum[c[0]] + ms)), c[0])
                )[0]
            else:
                # Minimize global new max KVPR; tie-break by local KVPR then residual then more remaining mem then id
                chosen = min(
                    candidates,
                    key=lambda c: (c[2], c[3], c[1], -(GPU_MEM_SIZE - (m_sum[c[0]] + ms)), c[0])
                )[0]

            # Place
            placement[chosen].append(mdl)
            n_sum[chosen] += n
            m_sum[chosen] += ms
            used_cap[chosen] += w

        if return_placement:
            return True, {g: placement[g] for g in range(gpu_num)}
        return True

    # For quick feasibility checks during search
    def try_pack_any(T, need_placement=False):
        variants = [(0, "resid"), (1, "resid")]
        feasibles = []
        for ov, pol in variants:
            if need_placement:
                ok, plc = try_pack(T, ov, pol, True)
                if ok:
                    feasibles.append(plc)
            else:
                if try_pack(T, ov, pol, False):
                    return True
        if need_placement:
            return (len(feasibles) > 0), feasibles
        return False

    # ---------- Exponential search for feasible T ----------
    T = max(low_T, 1e-9)
    found = False
    for _ in range(50):
        if try_pack_any(T, need_placement=False):
            found = True
            break
        T *= 2.0

    if not found:
        raise ValueError("Unable to find a feasible packing for any KVPR threshold")

    # ---------- Binary search to minimize T ----------
    low, high = low_T, T
    for _ in range(40):
        mid = (low + high) / 2.0
        if mid <= 0:
            high = mid
            continue
        if try_pack_any(mid, need_placement=False):
            high = mid
        else:
            low = mid

    # ---------- Build diverse candidates at near-optimal T ----------
    def measured_max_kvpr(plc):
        vals = []
        for g in range(gpu_num):
            used_mem = sum(getattr(m, "model_size") for m in plc.get(g, []))
            numer = sum((getattr(m, "req_rate") / getattr(m, "slo")) for m in plc.get(g, []))
            vals.append(kvpr_val(numer, GPU_MEM_SIZE - used_mem))
        return max(vals) if vals else 0.0

    # Additional greedy min-max candidate (ignores T; memory-feasible)
    def greedy_minmax_candidate():
        # Sort by pressure per GB, then by demand
        def key_m(m):
            dn = (m.req_rate / m.slo) if m.slo != 0 else float('inf')
            sz = m.model_size if m.model_size > 0 else 1e-9
            return (dn / sz, dn)
        ordered = sorted(models, key=key_m, reverse=True)

        plc = {g: [] for g in range(gpu_num)}
        rem = [GPU_MEM_SIZE] * gpu_num
        numer = [0.0] * gpu_num

        for m in ordered:
            ms = float(m.model_size)
            dn = float(m.req_rate) / float(m.slo)
            # Precompute current kvprs
            cur_k = [kvpr_val(numer[g], rem[g]) for g in range(gpu_num)]
            best = None
            for g in range(gpu_num):
                if ms <= rem[g]:
                    new_local = kvpr_val(numer[g] + dn, rem[g] - ms)
                    new_max = new_local
                    for k in range(gpu_num):
                        if k != g and cur_k[k] > new_max:
                            new_max = cur_k[k]
                    # tie-breaks: min new_max, then min local, then more remaining mem, then gpu id
                    key = (new_max, new_local, -(rem[g] - ms), g)
                    if best is None or key < best[0]:
                        best = (key, g)
            if best is None:
                return None
            g = best[1]
            plc[g].append(m)
            rem[g] -= ms
            numer[g] += dn
        return plc

    candidates = []

    # Collect candidates with multiple orderings and policies at high
    combos = [(0, "resid"), (1, "resid"), (2, "resid"), (0, "minmax"), (1, "minmax")]
    for ov, pol in combos:
        ok, plc = try_pack(high, ov, pol, True)
        if ok:
            candidates.append(plc)

    # Add greedy candidate
    gu = greedy_minmax_candidate()
    if gu is not None:
        candidates.append(gu)

    if not candidates:
        # Fallback: at least one feasible must exist
        ok, plc = try_pack(high, 0, "resid", True)
        if not ok:
            raise ValueError("Feasible packing unexpectedly unavailable")
        candidates.append(plc)

    # ---------- Select best candidate by measured max KVPR ----------
    best_plc = None
    best_score = float('inf')
    for plc in candidates:
        score = measured_max_kvpr(plc)
        if score < best_score:
            best_score = score
            best_plc = plc

    # ---------- Lightweight local improvement ----------
    def local_improve(plc, max_moves=200, eps=1e-12):
        per_g = {g: list(plc.get(g, [])) for g in range(gpu_num)}
        mem = [sum(getattr(m, "model_size") for m in per_g[g]) for g in range(gpu_num)]
        num = [sum((getattr(m, "req_rate") / getattr(m, "slo")) for m in per_g[g]) for g in range(gpu_num)]

        def kv_g(g, msum=None, nsum=None):
            msum = mem[g] if msum is None else msum
            nsum = num[g] if nsum is None else nsum
            return kvpr_val(nsum, GPU_MEM_SIZE - msum)

        def global_vals():
            vals = [kv_g(g) for g in range(gpu_num)]
            return max(vals), vals

        moves = 0
        while moves < max_moves:
            cur_max, vals = global_vals()
            worst = max(range(gpu_num), key=lambda g: vals[g])

            improved = False
            best_new_max = cur_max
            best_move = None

            for mdl in list(per_g[worst]):
                ms = float(getattr(mdl, "model_size"))
                dn = float(getattr(mdl, "req_rate")) / float(getattr(mdl, "slo"))
                for tgt in range(gpu_num):
                    if tgt == worst:
                        continue
                    if mem[tgt] + ms > GPU_MEM_SIZE + 1e-12:
                        continue

                    src_mem = mem[worst] - ms
                    src_num = num[worst] - dn
                    tgt_mem = mem[tgt] + ms
                    tgt_num = num[tgt] + dn

                    src_k = kvpr_val(src_num, GPU_MEM_SIZE - src_mem)
                    tgt_k = kvpr_val(tgt_num, GPU_MEM_SIZE - tgt_mem)

                    new_max = max(src_k, tgt_k)
                    for g in range(gpu_num):
                        if g != worst and g != tgt:
                            if vals[g] > new_max:
                                new_max = vals[g]

                    if new_max + eps < best_new_max:
                        best_new_max = new_max
                        best_move = (mdl, worst, tgt, ms, dn)
                        improved = True

            if not improved:
                break

            mdl, src, tgt, ms, dn = best_move
            per_g[src].remove(mdl)
            per_g[tgt].append(mdl)
            mem[src] -= ms
            num[src] -= dn
            mem[tgt] += ms
            num[tgt] += dn
            moves += 1

        return {g: per_g.get(g, []) for g in range(gpu_num)}

    best_plc = local_improve(best_plc, max_moves=100)

    # Ensure all GPUs are represented in dict keys
    for g in range(gpu_num):
        best_plc.setdefault(g, [])

    return best_plc

# EVOLVE-BLOCK-END
=======
# EVOLVE-BLOCK-START
"""Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""

GPU_MEM_SIZE = 80  # GB

def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Args:
        gpu_num: Number of GPUs
        models: List of models to place

    Returns:
        A placement of models to GPUs
    """
    if gpu_num <= 0:
        raise ValueError("gpu_num must be positive")

    # ---------- Helpers ----------
    def safe_div(num, den):
        if den <= 0:
            return float('inf') if num > 0 else 0.0
        return num / den

    def kvpr_val(numer, rem_mem):
        return safe_div(numer, rem_mem)

    # ---------- Extract per-model stats ----------
    items = []
    total_mem = 0.0
    total_n = 0.0
    for idx, m in enumerate(models):
        ms = float(getattr(m, "model_size"))
        slo = float(getattr(m, "slo"))
        rr = float(getattr(m, "req_rate"))
        if ms < 0:
            raise ValueError("Model size must be non-negative")
        if ms > GPU_MEM_SIZE + 1e-9:
            raise ValueError(f"Model of size {ms} GB cannot fit into a single GPU of size {GPU_MEM_SIZE} GB")
        if slo <= 0:
            raise ValueError("Model SLO must be positive")
        n = rr / slo  # r_j / s_j
        items.append((idx, m, ms, n))
        total_mem += ms
        total_n += n

    if not items:
        return {gpu_id: [] for gpu_id in range(gpu_num)}

    total_capacity_mem = gpu_num * GPU_MEM_SIZE
    if total_mem - total_capacity_mem > 1e-9:
        raise ValueError("Total model memory exceeds total GPU memory")

    # ---------- Lower bound on T ----------
    indiv_lb = max(safe_div(n, GPU_MEM_SIZE - ms) for _, _, ms, n in items)
    global_lb = safe_div(total_n, max(total_capacity_mem - total_mem, 1e-9))

    # Pair-based bound (for memory-incompatible pairs): T >= (n_i + n_j) / (2*80 - m_i - m_j)
    pair_lb = 0.0
    if gpu_num >= 2 and len(items) >= 2:
        L = min(len(items), 60)  # consider top-L by memory to keep runtime tiny
        top_by_mem = sorted(items, key=lambda it: it[2], reverse=True)[:L]
        for i in range(L):
            ms_i = top_by_mem[i][2]; n_i = top_by_mem[i][3]
            for j in range(i + 1, L):
                ms_j = top_by_mem[j][2]; n_j = top_by_mem[j][3]
                if ms_i + ms_j > GPU_MEM_SIZE + 1e-12:
                    denom = 2.0 * GPU_MEM_SIZE - (ms_i + ms_j)
                    pair_lb = max(pair_lb, safe_div(n_i + n_j, max(denom, 1e-9)))

    # k-bin prefix bound for k in [1..min(G,4)]
    kbin_lb = 0.0
    if items:
        sorted_by_mem = sorted(items, key=lambda it: it[2], reverse=True)
        max_k = min(gpu_num, 4)
        for k in range(1, max_k + 1):
            threshold = (k - 1) * GPU_MEM_SIZE
            s_mem = 0.0
            s_n = 0.0
            count = 0
            for it in sorted_by_mem:
                if s_mem > threshold + 1e-12:
                    break
                s_mem += it[2]
                s_n += it[3]
                count += 1
            denom = k * GPU_MEM_SIZE - s_mem
            kbin_lb = max(kbin_lb, safe_div(s_n, max(denom, 1e-9)))

    low_T = max(0.0, indiv_lb, global_lb, pair_lb, kbin_lb)

    # ---------- Feasibility check and packing at T ----------
    # order_variant:
    #   0 -> by transformed weight w(T) = n + T*m
    #   1 -> by intrinsic-alone KVPR n / (80 - m)
    #   2 -> by pressure per GB n / m
    # policy:
    #   "resid"  -> best-fit on transformed residual
    #   "minmax" -> choose GPU minimizing new global max KVPR
    def try_pack(T, order_variant=0, policy="resid", return_placement=False):
        cap = GPU_MEM_SIZE * T
        eps = 1e-12

        # Build sorted order
        if order_variant == 0:
            ordered = sorted(items, key=lambda it: (it[3] + T * it[2], it[3], it[2]), reverse=True)
        elif order_variant == 1:
            ordered = sorted(items, key=lambda it: (safe_div(it[3], max(GPU_MEM_SIZE - it[2], 1e-9)), it[2]), reverse=True)
        else:
            ordered = sorted(items, key=lambda it: (safe_div(it[3], max(it[2], 1e-9)), it[3]), reverse=True)

        # Per-GPU state
        n_sum = [0.0] * gpu_num
        m_sum = [0.0] * gpu_num
        used_cap = [0.0] * gpu_num  # equals n_sum + T * m_sum
        placement = [[] for _ in range(gpu_num)]

        for _, mdl, ms, n in ordered:
            w = n + T * ms
            candidates = []
            # Precompute current kvprs
            cur_kvprs = [kvpr_val(n_sum[g], GPU_MEM_SIZE - m_sum[g]) for g in range(gpu_num)]

            for g in range(gpu_num):
                if m_sum[g] + ms > GPU_MEM_SIZE + eps:
                    continue
                residual = cap - (used_cap[g] + w)
                if residual < -eps:
                    continue
                if policy == "resid":
                    candidates.append((g, residual, None, None))
                else:
                    new_n = n_sum[g] + n
                    new_m = m_sum[g] + ms
                    new_local = kvpr_val(new_n, GPU_MEM_SIZE - new_m)
                    # New global max if placed on g
                    new_max = new_local
                    for k in range(gpu_num):
                        if k != g and cur_kvprs[k] > new_max:
                            new_max = cur_kvprs[k]
                    candidates.append((g, residual, new_max, new_local))

            if not candidates:
                return (False, None) if return_placement else False

            if policy == "resid":
                # Best-fit: minimize transformed residual; tie by more remaining mem then gpu id
                # Remaining mem after placement equals GPU_MEM_SIZE - (m_sum[g]+ms)
                chosen = min(
                    candidates,
                    key=lambda c: (c[1], -(GPU_MEM_SIZE - (m_sum[c[0]] + ms)), c[0])
                )[0]
            else:
                # Minimize global new max KVPR; tie-break by local KVPR then residual then more remaining mem then id
                chosen = min(
                    candidates,
                    key=lambda c: (c[2], c[3], c[1], -(GPU_MEM_SIZE - (m_sum[c[0]] + ms)), c[0])
                )[0]

            # Place
            placement[chosen].append(mdl)
            n_sum[chosen] += n
            m_sum[chosen] += ms
            used_cap[chosen] += w

        if return_placement:
            return True, {g: placement[g] for g in range(gpu_num)}
        return True

    # For quick feasibility checks during search
    def try_pack_any(T, need_placement=False):
        variants = [(0, "resid"), (1, "resid"), (0, "minmax")]
        feasibles = []
        for ov, pol in variants:
            if need_placement:
                ok, plc = try_pack(T, ov, pol, True)
                if ok:
                    feasibles.append(plc)
            else:
                if try_pack(T, ov, pol, False):
                    return True
        if need_placement:
            return (len(feasibles) > 0), feasibles
        return False

    # ---------- Exponential search for feasible T ----------
    T = max(low_T, 1e-9)
    found = False
    for _ in range(50):
        if try_pack_any(T, need_placement=False):
            found = True
            break
        T *= 2.0

    if not found:
        raise ValueError("Unable to find a feasible packing for any KVPR threshold")

    # ---------- Binary search to minimize T ----------
    low, high = low_T, T
    for _ in range(40):
        mid = (low + high) / 2.0
        if mid <= 0:
            high = mid
            continue
        if try_pack_any(mid, need_placement=False):
            high = mid
        else:
            low = mid

    # ---------- Build diverse candidates at near-optimal T ----------
    def measured_max_kvpr(plc):
        vals = []
        for g in range(gpu_num):
            used_mem = sum(getattr(m, "model_size") for m in plc.get(g, []))
            numer = sum((getattr(m, "req_rate") / getattr(m, "slo")) for m in plc.get(g, []))
            vals.append(kvpr_val(numer, GPU_MEM_SIZE - used_mem))
        return max(vals) if vals else 0.0

    # Additional greedy min-max candidate (ignores T; memory-feasible)
    def greedy_minmax_candidate():
        # Sort by pressure per GB, then by demand
        def key_m(m):
            dn = (m.req_rate / m.slo) if m.slo != 0 else float('inf')
            sz = m.model_size if m.model_size > 0 else 1e-9
            return (dn / sz, dn)
        ordered = sorted(models, key=key_m, reverse=True)

        plc = {g: [] for g in range(gpu_num)}
        rem = [GPU_MEM_SIZE] * gpu_num
        numer = [0.0] * gpu_num

        for m in ordered:
            ms = float(m.model_size)
            dn = float(m.req_rate) / float(m.slo)
            # Precompute current kvprs
            cur_k = [kvpr_val(numer[g], rem[g]) for g in range(gpu_num)]
            best = None
            for g in range(gpu_num):
                if ms <= rem[g]:
                    new_local = kvpr_val(numer[g] + dn, rem[g] - ms)
                    new_max = new_local
                    for k in range(gpu_num):
                        if k != g and cur_k[k] > new_max:
                            new_max = cur_k[k]
                    # tie-breaks: min new_max, then min local, then more remaining mem, then gpu id
                    key = (new_max, new_local, -(rem[g] - ms), g)
                    if best is None or key < best[0]:
                        best = (key, g)
            if best is None:
                return None
            g = best[1]
            plc[g].append(m)
            rem[g] -= ms
            numer[g] += dn
        return plc

    candidates = []

    # Collect candidates with multiple orderings and policies around high T (include tiny perturbations)
    T_vals = [high]
    if high > 0:
        T_vals.extend([high * 0.995, high * 1.005])

    combos = [(0, "resid"), (1, "resid"), (0, "minmax")]
    for Tv in T_vals:
        for ov, pol in combos:
            ok, plc = try_pack(Tv, ov, pol, True)
            if ok:
                candidates.append(plc)

    # Add greedy candidate
    gu = greedy_minmax_candidate()
    if gu is not None:
        candidates.append(gu)

    if not candidates:
        # Fallback: at least one feasible must exist
        ok, plc = try_pack(high, 0, "resid", True)
        if not ok:
            raise ValueError("Feasible packing unexpectedly unavailable")
        candidates.append(plc)

    # ---------- Select best candidate by measured max KVPR ----------
    best_plc = None
    best_score = float('inf')
    for plc in candidates:
        score = measured_max_kvpr(plc)
        if score < best_score:
            best_score = score
            best_plc = plc

    # ---------- Lightweight local improvement with moves and capped swaps ----------
    def local_improve(plc, max_moves=200, max_swaps=20, eps=1e-12):
        per_g = {g: list(plc.get(g, [])) for g in range(gpu_num)}
        mem = [sum(getattr(m, "model_size") for m in per_g[g]) for g in range(gpu_num)]
        num = [sum((getattr(m, "req_rate") / getattr(m, "slo")) for m in per_g[g]) for g in range(gpu_num)]

        def kv_g(g, msum=None, nsum=None):
            msum = mem[g] if msum is None else msum
            nsum = num[g] if nsum is None else nsum
            return kvpr_val(nsum, GPU_MEM_SIZE - msum)

        def global_vals():
            vals = [kv_g(g) for g in range(gpu_num)]
            return max(vals), vals

        moves = 0
        swaps = 0
        while moves < max_moves or swaps < max_swaps:
            cur_max, vals = global_vals()
            worst = max(range(gpu_num), key=lambda g: vals[g])

            improved = False
            best_new_max = cur_max
            best_move = None

            # Try single-item moves from the worst GPU
            for mdl in list(per_g[worst]):
                ms = float(getattr(mdl, "model_size"))
                dn = float(getattr(mdl, "req_rate")) / float(getattr(mdl, "slo"))
                for tgt in range(gpu_num):
                    if tgt == worst:
                        continue
                    if mem[tgt] + ms > GPU_MEM_SIZE + 1e-12:
                        continue

                    src_mem = mem[worst] - ms
                    src_num = num[worst] - dn
                    tgt_mem = mem[tgt] + ms
                    tgt_num = num[tgt] + dn

                    src_k = kvpr_val(src_num, GPU_MEM_SIZE - src_mem)
                    tgt_k = kvpr_val(tgt_num, GPU_MEM_SIZE - tgt_mem)

                    new_max = max(src_k, tgt_k)
                    for g in range(gpu_num):
                        if g != worst and g != tgt:
                            if vals[g] > new_max:
                                new_max = vals[g]

                    if new_max + eps < best_new_max:
                        best_new_max = new_max
                        best_move = ("move", mdl, worst, tgt, ms, dn)
                        improved = True

            if improved and moves < max_moves:
                # Apply best single move
                _, mdl, src, tgt, ms, dn = best_move
                per_g[src].remove(mdl)
                per_g[tgt].append(mdl)
                mem[src] -= ms
                num[src] -= dn
                mem[tgt] += ms
                num[tgt] += dn
                moves += 1
                continue

            # Try a bounded number of swaps between worst GPU and others
            if swaps >= max_swaps:
                break

            found_swap = False
            for mdl_a in list(per_g[worst]):
                ms_a = float(getattr(mdl_a, "model_size"))
                dn_a = float(getattr(mdl_a, "req_rate")) / float(getattr(mdl_a, "slo"))
                for tgt in range(gpu_num):
                    if tgt == worst:
                        continue
                    for mdl_b in list(per_g[tgt]):
                        ms_b = float(getattr(mdl_b, "model_size"))
                        dn_b = float(getattr(mdl_b, "req_rate")) / float(getattr(mdl_b, "slo"))

                        # Memory feasibility after swap
                        if mem[worst] - ms_a + ms_b > GPU_MEM_SIZE + 1e-12:
                            continue
                        if mem[tgt] - ms_b + ms_a > GPU_MEM_SIZE + 1e-12:
                            continue

                        src_mem = mem[worst] - ms_a + ms_b
                        src_num = num[worst] - dn_a + dn_b
                        tgt_mem = mem[tgt] - ms_b + ms_a
                        tgt_num = num[tgt] - dn_b + dn_a

                        src_k = kvpr_val(src_num, GPU_MEM_SIZE - src_mem)
                        tgt_k = kvpr_val(tgt_num, GPU_MEM_SIZE - tgt_mem)

                        new_max = max(src_k, tgt_k)
                        for g in range(gpu_num):
                            if g != worst and g != tgt:
                                if vals[g] > new_max:
                                    new_max = vals[g]

                        if new_max + eps < cur_max:
                            # Apply first improving swap
                            per_g[worst].remove(mdl_a)
                            per_g[tgt].remove(mdl_b)
                            per_g[worst].append(mdl_b)
                            per_g[tgt].append(mdl_a)
                            mem[worst] = src_mem
                            num[worst] = src_num
                            mem[tgt] = tgt_mem
                            num[tgt] = tgt_num
                            swaps += 1
                            found_swap = True
                            break
                    if found_swap or swaps >= max_swaps:
                        break
                if found_swap or swaps >= max_swaps:
                    break

            if not found_swap:
                break  # No improving move or swap found

        return {g: per_g.get(g, []) for g in range(gpu_num)}

    best_plc = local_improve(best_plc, max_moves=100, max_swaps=10)

    # Ensure all GPUs are represented in dict keys
    for g in range(gpu_num):
        best_plc.setdefault(g, [])

    return best_plc

# EVOLVE-BLOCK-END
>>>>>>> REPLACE

</DIFF>