<NAME>
minimax_greedy_kvpr
</NAME>

<DESCRIPTION>
Replace the local-greedy heuristic with a true minimax greedy step that, for each model, evaluates the resulting maximum KVPR across all GPUs for each feasible placement and chooses the GPU that minimizes this new maximum. This directly targets the objective of minimizing the worst KVPR. Additionally, models are sorted by pressure per GB ((req_rate/slo) per memory), which prioritizes highest-impact models earlier. Safe handling of zero denominators is added to avoid division errors. The change remains simple and self-contained.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Args:
        gpu_num: Number of GPUs
        models: List of models to place

    Returns:
        A placement of models to GPUs
    """

    # Greedy KVPR-minimizing placement based on Algorithm 1 (without Ï„ check)
    # 1) Sort models by r_j / s_j in descending order
    sorted_models = sorted(models, key=lambda m: (m.req_rate / m.slo), reverse=True)

    # 2) Initialize per-GPU states
    placement = {gpu_id: [] for gpu_id in range(gpu_num)}
    shared_kv = [GPU_MEM_SIZE for _ in range(gpu_num)]  # remaining memory per GPU
    weighted_req_rate = [0.0 for _ in range(gpu_num)]   # sum of r_j / s_j per GPU

    # 3) Assign each model to the GPU that minimizes current KVPR while fitting in memory
    for model in sorted_models:
        best_idx = None
        best_ratio = float('inf')

        for gpu_id in range(gpu_num):
            if model.model_size <= shared_kv[gpu_id] and shared_kv[gpu_id] > 0:
                current_ratio = weighted_req_rate[gpu_id] / shared_kv[gpu_id]
                if current_ratio < best_ratio:
                    best_ratio = current_ratio
                    best_idx = gpu_id

        # Failure: if no GPU can fit, raise an error instead of overcommitting
        if best_idx is None:
            raise ValueError(
                f"Unable to place model of size {model.model_size} GB on any GPU. "
                f"Remaining per-GPU memory: {shared_kv}"
            )

        placement[best_idx].append(model)
        weighted_req_rate[best_idx] += model.req_rate / model.slo
        shared_kv[best_idx] -= model.model_size

    return placement
=======
def compute_model_placement(gpu_num, models):
    """
    Compute a model placement that minimizes the maximum KVPR across all GPUs.

    Args:
        gpu_num: Number of GPUs
        models: List of models to place

    Returns:
        A placement of models to GPUs
    """

    # Sort models by pressure per GB: (r_j / s_j) per unit memory, breaking ties by r_j / s_j
    def _pressure_per_gb(m):
        rs = (m.req_rate / m.slo) if m.slo != 0 else float('inf')
        size = m.model_size if m.model_size > 0 else 1e-9
        return (rs / size, rs)

    sorted_models = sorted(models, key=_pressure_per_gb, reverse=True)

    # Initialize per-GPU states
    placement = {gpu_id: [] for gpu_id in range(gpu_num)}
    remaining_mem = [GPU_MEM_SIZE for _ in range(gpu_num)]  # remaining memory per GPU
    weighted_req_rate = [0.0 for _ in range(gpu_num)]       # sum of r_j / s_j per GPU

    def kvpr(numer, denom):
        return (numer / denom) if denom > 0 else float('inf')

    # Assign each model to the GPU that minimizes the maximum KVPR after placement
    for model in sorted_models:
        delta = (model.req_rate / model.slo) if model.slo != 0 else float('inf')
        best_idx = None
        best_max_kvpr = float('inf')
        best_local_kvpr = float('inf')
        best_post_remaining = -1.0

        current_kvprs = [kvpr(weighted_req_rate[g], remaining_mem[g]) for g in range(gpu_num)]

        for gpu_id in range(gpu_num):
            if model.model_size <= remaining_mem[gpu_id]:
                new_numer = weighted_req_rate[gpu_id] + delta
                new_rem = remaining_mem[gpu_id] - model.model_size
                new_kvpr_gpu = kvpr(new_numer, new_rem)

                # Compute new max KVPR across GPUs if we place on this gpu_id
                new_max = new_kvpr_gpu
                for idx, val in enumerate(current_kvprs):
                    if idx != gpu_id and val > new_max:
                        new_max = val

                # Tie-breakers: minimize new_max, then local new_kvpr, then leave more remaining mem, then lower gpu_id
                if (new_max < best_max_kvpr or
                    (new_max == best_max_kvpr and new_kvpr_gpu < best_local_kvpr) or
                    (new_max == best_max_kvpr and new_kvpr_gpu == best_local_kvpr and new_rem > best_post_remaining) or
                    (new_max == best_max_kvpr and new_kvpr_gpu == best_local_kvpr and new_rem == best_post_remaining and (best_idx is None or gpu_id < best_idx))):
                    best_idx = gpu_id
                    best_max_kvpr = new_max
                    best_local_kvpr = new_kvpr_gpu
                    best_post_remaining = new_rem

        # Failure: if no GPU can fit, raise an error instead of overcommitting
        if best_idx is None:
            raise ValueError(
                f"Unable to place model of size {model.model_size} GB on any GPU. "
                f"Remaining per-GPU memory: {remaining_mem}"
            )

        placement[best_idx].append(model)
        weighted_req_rate[best_idx] += delta
        remaining_mem[best_idx] -= model.model_size

    return placement
>>>>>>> REPLACE

</DIFF>