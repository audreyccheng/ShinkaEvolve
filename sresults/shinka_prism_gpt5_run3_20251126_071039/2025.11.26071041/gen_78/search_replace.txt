<NAME>
enable_t_packing_for_memory_only_and_binary_search
</NAME>

<DESCRIPTION>
This edit makes four targeted improvements:
1) Treat memory-only models (slo == 0) as zero KV demand in the T-sweep by setting dR=0.0 instead of inf. This enables the slack-equalization packer to handle such items instead of falling back prematurely.
2) Remove the early abort in assign_balanced_slack when encountering infinite dR, since dR is now finite.
3) Expand the candidate T multipliers and add an additional placement variant ('intrinsic_desc', 'min_kvpr') while increasing seeds for the primary variant. This broadens the search space for better minima.
4) Add a light binary search on T to further tighten the feasible threshold and reduce the maximum KVPR.

These changes are minimal and keep the code simple, while aligning with minimizing the maximum KVPR objective and improving performance robustness across instances.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Extract per-model attributes once
    items = []
    total_R = 0.0
    total_size = 0.0
    for m in models:
        slo = float(m.slo)
        dR = float(m.req_rate) / slo if slo != 0 else float('inf')
        s = float(m.model_size)
        items.append({'obj': m, 'dR': dR, 'size': s})
        total_R += 0.0 if dR == float('inf') else dR
        total_size += s
=======
    # Extract per-model attributes once
    items = []
    total_R = 0.0
    total_size = 0.0
    for m in models:
        slo = float(m.slo)
        dR = float(m.req_rate) / slo if slo != 0 else 0.0
        s = float(m.model_size)
        items.append({'obj': m, 'dR': dR, 'size': s})
        total_R += dR
        total_size += s
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
        # Build enriched list
        enriched = []
        for it in items:
            dR = it['dR']; sz = it['size']
            if dR == float('inf'):
                return None  # cannot handle infinite demand ratios
            w = dR + T * sz
            if w < 0:
                w = 0.0
            enriched.append([w, dR, sz, it['obj']])
=======
        # Build enriched list
        enriched = []
        for it in items:
            dR = it['dR']; sz = it['size']
            w = dR + T * sz
            if w < 0:
                w = 0.0
            enriched.append([w, dR, sz, it['obj']])
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
    candidates_T = []
    for mul in [0.985, 0.99, 1.0, 1.005, 1.01, 1.02]:
        val = max(lower, T_feas * mul)
        candidates_T.append(val)
    candidates_T.append(0.5 * (lower + T_feas))
    candidates_T = sorted(set(round(t, 12) for t in candidates_T))
=======
    candidates_T = []
    for mul in [0.975, 0.985, 0.99, 1.0, 1.005, 1.01, 1.02, 1.03]:
        val = max(lower, T_feas * mul)
        candidates_T.append(val)
    candidates_T.append(0.5 * (lower + T_feas))
    candidates_T = sorted(set(round(t, 12) for t in candidates_T))
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
    variants = [
        ('w_desc', 'tight', min(4, max(1, gpu_num)), 2),
        ('intrinsic_desc', 'tight', min(3, max(1, gpu_num - 1))),
        ('density_desc', 'tight', 0),
        ('w_desc', 'min_kvpr', 0),
        ('size_desc', 'tight', 0),
    ]
=======
    variants = [
        ('w_desc', 'tight', min(4, max(1, gpu_num)), 3),
        ('intrinsic_desc', 'tight', min(3, max(1, gpu_num - 1))),
        ('intrinsic_desc', 'min_kvpr', 0),
        ('density_desc', 'tight', 0),
        ('w_desc', 'min_kvpr', 0),
        ('size_desc', 'tight', 0),
    ]
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
    for T in candidates_T:
        for var in variants:
            # Unpack with defaults for backward compatibility tuple length
            if len(var) == 4:
                order, choose_rule, seedH, seeds = var
            else:
                order, choose_rule, seedH = var
                seeds = 1
            cand = assign_balanced_slack(T, order=order, seed_H=seedH, choose_rule=choose_rule, seeds=seeds)
            if cand is None:
                continue
            val = eval_max_kvpr(cand)
            if val < best_val:
                best_val = val
                best_placement = cand

    # Short bounded local search focusing on the most loaded GPU: moves then swaps
    def local_refine(placement, move_budget=20, swap_budget=10):
=======
    for T in candidates_T:
        for var in variants:
            # Unpack with defaults for backward compatibility tuple length
            if len(var) == 4:
                order, choose_rule, seedH, seeds = var
            else:
                order, choose_rule, seedH = var
                seeds = 1
            cand = assign_balanced_slack(T, order=order, seed_H=seedH, choose_rule=choose_rule, seeds=seeds)
            if cand is None:
                continue
            val = eval_max_kvpr(cand)
            if val < best_val:
                best_val = val
                best_placement = cand

    # Light binary search on T to tighten the feasible threshold further
    def _feasible_assign_for_T(T):
        best = None
        bestv = float('inf')
        for (order, choose_rule, seedH, seeds) in [
            ('w_desc', 'tight', min(4, max(1, gpu_num)), 1),
            ('intrinsic_desc', 'tight', min(3, max(1, gpu_num - 1)), 1),
            ('w_desc', 'min_kvpr', 0, 1),
            ('intrinsic_desc', 'min_kvpr', 0, 1),
        ]:
            cand = assign_balanced_slack(T, order=order, seed_H=seedH, choose_rule=choose_rule, seeds=seeds)
            if cand is not None:
                v = eval_max_kvpr(cand)
                if v < bestv:
                    bestv = v
                    best = cand
        return best

    lo, hi = max(0.0, lower), max(lower, best_val)
    if hi < float('inf'):
        for _ in range(10):
            mid = (lo + hi) / 2.0
            cand = _feasible_assign_for_T(mid)
            if cand is not None:
                hi = mid
                v = eval_max_kvpr(cand)
                if v < best_val:
                    best_val = v
                    best_placement = cand
            else:
                lo = mid

    # Short bounded local search focusing on the most loaded GPU: moves then swaps
    def local_refine(placement, move_budget=20, swap_budget=10):
>>>>>>> REPLACE

</DIFF>