<NAME>
t_neighborhood_minmax_pack
</NAME>

<DESCRIPTION>
I enhance the parametric T-based packing by adding a “minmax” GPU selection policy that chooses the placement minimizing the resulting global max KVPR for each item while still respecting transformed capacity. This directly aligns per-placement decisions with the objective and typically reduces the final max KVPR. Additionally, I diversify candidates by evaluating a small T-neighborhood around the near-optimal T (high), generating placements for multiple T values and both ordering and policy variants. This increases the chance of avoiding local suboptimality due to discrete packing effects with negligible runtime overhead. No external configs are added and all changes are contained within the EVOLVE block.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        def try_pack(T, ordering=0, return_placement=False):
            cap = GPU_MEM_SIZE * T
            eps = 1e-12
            if ordering == 0:
                ordered = sorted(items, key=lambda it: (it[2] + T * it[1], it[2], it[1]), reverse=True)
            else:
                ordered = sorted(items, key=lambda it: (safe_div(it[2], max(GPU_MEM_SIZE - it[1], 1e-9)), it[1]), reverse=True)

            m_sum = [0.0] * gpu_num
            n_sum = [0.0] * gpu_num
            used_cap = [0.0] * gpu_num
            plc = [[] for _ in range(gpu_num)]

            for mdl, ms, n in ordered:
                w = n + T * ms
                best_g = None
                best_resid = float('inf')
                for g in range(gpu_num):
                    if m_sum[g] + ms > GPU_MEM_SIZE + eps:
                        continue
                    resid = cap - (used_cap[g] + w)
                    if resid >= -eps and resid < best_resid - 1e-15:
                        best_resid = resid
                        best_g = g
                if best_g is None:
                    return (False, None) if return_placement else False
                plc[best_g].append(mdl)
                m_sum[best_g] += ms
                n_sum[best_g] += n
                used_cap[best_g] += w

            if return_placement:
                return True, {g: plc[g] for g in range(gpu_num)}
            return True
=======
        def try_pack(T, ordering=0, policy="resid", return_placement=False):
            cap = GPU_MEM_SIZE * T
            eps = 1e-12
            if ordering == 0:
                ordered = sorted(items, key=lambda it: (it[2] + T * it[1], it[2], it[1]), reverse=True)
            else:
                ordered = sorted(items, key=lambda it: (safe_div(it[2], max(GPU_MEM_SIZE - it[1], 1e-9)), it[1]), reverse=True)

            m_sum = [0.0] * gpu_num
            n_sum = [0.0] * gpu_num
            used_cap = [0.0] * gpu_num
            plc = [[] for _ in range(gpu_num)]

            for mdl, ms, n in ordered:
                w = n + T * ms
                best_g = None
                best_key = None

                # Current KVPRs for "minmax" policy
                cur_kvprs = [kvpr(n_sum[g], GPU_MEM_SIZE - m_sum[g]) for g in range(gpu_num)] if policy != "resid" else None

                for g in range(gpu_num):
                    if m_sum[g] + ms > GPU_MEM_SIZE + eps:
                        continue
                    resid = cap - (used_cap[g] + w)
                    if resid < -eps:
                        continue

                    if policy == "resid":
                        key = (resid, g)
                    else:
                        new_local = kvpr(n_sum[g] + n, GPU_MEM_SIZE - (m_sum[g] + ms))
                        # new global max KVPR if placed on g
                        new_max = new_local
                        for k in range(gpu_num):
                            if k != g and cur_kvprs[k] > new_max:
                                new_max = cur_kvprs[k]
                        key = (new_max, new_local, resid, g)

                    if best_key is None or key < best_key:
                        best_key = key
                        best_g = g

                if best_g is None:
                    return (False, None) if return_placement else False

                plc[best_g].append(mdl)
                m_sum[best_g] += ms
                n_sum[best_g] += n
                used_cap[best_g] += w

            if return_placement:
                return True, {g: plc[g] for g in range(gpu_num)}
            return True
>>>>>>> REPLACE
</DIFF>
<DIFF>
<<<<<<< SEARCH
        def try_any(T, need_plc=False):
            if need_plc:
                feas = []
                ok0, p0 = try_pack(T, 0, True)
                ok1, p1 = try_pack(T, 1, True)
                if ok0: feas.append(p0)
                if ok1: feas.append(p1)
                return (len(feas) > 0), feas
            else:
                return try_pack(T, 0, False) or try_pack(T, 1, False)
=======
        def try_any(T, need_plc=False):
            variants = [(0, "resid"), (1, "resid"), (0, "minmax"), (1, "minmax")]
            if need_plc:
                feas = []
                for ov, pol in variants:
                    ok, p = try_pack(T, ov, pol, True)
                    if ok:
                        feas.append(p)
                return (len(feas) > 0), feas
            else:
                for ov, pol in variants:
                    if try_pack(T, ov, pol, False):
                        return True
                return False
>>>>>>> REPLACE
</DIFF>
<DIFF>
<<<<<<< SEARCH
        # Build placements at near-optimal T
        ok, feas_plcs = try_any(high, need_plc=True)
        if not ok:
            return []
        return feas_plcs
=======
        # Build placements at near-optimal T and small neighborhood
        Ts = [high * 0.985, high * 0.99, high, high * 1.005, high * 1.01, (low + high) / 2.0]
        all_plcs = []
        for Tv in Ts:
            ok, feas_plcs = try_any(Tv, need_plc=True)
            if ok:
                all_plcs.extend(feas_plcs)
        if not all_plcs:
            return []
        return all_plcs
>>>>>>> REPLACE
</DIFF>