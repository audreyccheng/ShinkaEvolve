<NAME>
refine_peak_and_tuned_local
</NAME>

<DESCRIPTION>
Add a lightweight donation-first refinement step (refine_peak) after initial local improvements to further reduce the maximum KVPR by moving a few impactful models from the worst GPU to the best targets. Also reduce the iterations of the initial improve_local calls to keep runtime low, and add one short polishing improve_local pass after refine_peak. This typically shaves the peak KVPR at negligible overhead, improving the combined score.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Improve each candidate locally and pick the best by measured max KVPR
    improved_candidates = []
    for plc in candidates:
        improved_candidates.append(improve_local(plc))

    # Tie-break robustly using (max, second-max, average) KVPR tuple
    def score_tuple(plc):
        kvprs = []
        for g in range(gpu_num):
            used_mem = sum(getattr(m, "model_size") for m in plc.get(g, []))
            numer = sum((getattr(m, "req_rate") / getattr(m, "slo")) for m in plc.get(g, []))
            kvprs.append(kvpr(numer, GPU_MEM_SIZE - used_mem))
        if not kvprs:
            return (0.0, 0.0, 0.0)
        kvprs_sorted = sorted(kvprs, reverse=True)
        max_k = kvprs_sorted[0]
        second = kvprs_sorted[1] if len(kvprs_sorted) > 1 else kvprs_sorted[0]
        avg = sum(kvprs_sorted) / len(kvprs_sorted)
        return (max_k, second, avg)

    best_plc = None
    best_score = None
    for plc in improved_candidates:
        st = score_tuple(plc)
        if best_score is None or st < best_score:
            best_score = st
            best_plc = plc

    # Ensure all GPUs are present
    for g in range(gpu_num):
        best_plc.setdefault(g, [])

    return best_plc
=======
    # Improve each candidate locally (bounded iters for speed), then pick the best
    improved_candidates = []
    for plc in candidates:
        improved_candidates.append(improve_local(plc, max_iters=1500))

    # Tie-break robustly using (max, second-max, average) KVPR tuple
    def score_tuple(plc):
        kvprs = []
        for g in range(gpu_num):
            used_mem = sum(getattr(m, "model_size") for m in plc.get(g, []))
            numer = sum((getattr(m, "req_rate") / getattr(m, "slo")) for m in plc.get(g, []))
            kvprs.append(kvpr(numer, GPU_MEM_SIZE - used_mem))
        if not kvprs:
            return (0.0, 0.0, 0.0)
        kvprs_sorted = sorted(kvprs, reverse=True)
        max_k = kvprs_sorted[0]
        second = kvprs_sorted[1] if len(kvprs_sorted) > 1 else kvprs_sorted[0]
        avg = sum(kvprs_sorted) / len(kvprs_sorted)
        return (max_k, second, avg)

    best_plc = None
    best_score = None
    for plc in improved_candidates:
        st = score_tuple(plc)
        if best_score is None or st < best_score:
            best_score = st
            best_plc = plc

    # Final donation-first refinement: try moving top candidates off the worst GPU
    def refine_peak(plc, passes=2, top_k=12, eps=1e-12):
        per_g = {g: list(plc.get(g, [])) for g in range(gpu_num)}
        mem = [sum(getattr(m, "model_size") for m in per_g[g]) for g in range(gpu_num)]
        num = [sum((getattr(m, "req_rate") / getattr(m, "slo")) for m in per_g[g]) for g in range(gpu_num)]

        def kvpr_g(g, msum=None, nsum=None):
            msum = mem[g] if msum is None else msum
            nsum = num[g] if nsum is None else nsum
            return kvpr(nsum, GPU_MEM_SIZE - msum)

        for _ in range(passes):
            vals = [kvpr_g(g) for g in range(gpu_num)]
            worst = max(range(gpu_num), key=lambda g: vals[g])
            # two best (lowest KVPR) targets excluding worst
            best_targets = [g for g in sorted(range(gpu_num), key=lambda x: vals[x]) if g != worst][:2]
            if not best_targets:
                break

            def dn_of(m):
                return float(getattr(m, "req_rate")) / float(getattr(m, "slo"))
            def sz_of(m):
                return float(getattr(m, "model_size"))

            worst_list = list(per_g[worst])
            if not worst_list:
                break
            # Candidate pool: top by demand and top by size
            top_by_dn = sorted(worst_list, key=dn_of, reverse=True)[:max(1, top_k // 2)]
            top_by_sz = sorted(worst_list, key=sz_of, reverse=True)[:max(1, top_k - len(top_by_dn))]
            cand = list({id(m): m for m in (top_by_dn + top_by_sz)}.values())

            improved = False
            best_new_max = max(vals)
            best_action = None

            for mdl in cand:
                ms = sz_of(mdl)
                dn = dn_of(mdl)
                for tgt in best_targets:
                    if mem[tgt] + ms > GPU_MEM_SIZE + 1e-12:
                        continue
                    src_mem = mem[worst] - ms
                    src_num = num[worst] - dn
                    tgt_mem = mem[tgt] + ms
                    tgt_num = num[tgt] + dn
                    src_k = kvpr(src_num, GPU_MEM_SIZE - src_mem)
                    tgt_k = kvpr(tgt_num, GPU_MEM_SIZE - tgt_mem)
                    new_max = max(src_k, tgt_k)
                    for g in range(gpu_num):
                        if g != worst and g != tgt and vals[g] > new_max:
                            new_max = vals[g]
                    if new_max + eps < best_new_max:
                        best_new_max = new_max
                        best_action = (mdl, worst, tgt, ms, dn)
                        improved = True

            if not improved:
                break

            mdl, src, tgt, ms, dn = best_action
            per_g[src].remove(mdl)
            per_g[tgt].append(mdl)
            mem[src] -= ms; num[src] -= dn
            mem[tgt] += ms; num[tgt] += dn

        return {g: per_g.get(g, []) for g in range(gpu_num)}

    # Apply refinement and a short polishing pass
    best_plc = refine_peak(best_plc, passes=2, top_k=12)
    best_plc = improve_local(best_plc, max_iters=1200)

    # Ensure all GPUs are present
    for g in range(gpu_num):
        best_plc.setdefault(g, [])

    return best_plc
>>>>>>> REPLACE

</DIFF>