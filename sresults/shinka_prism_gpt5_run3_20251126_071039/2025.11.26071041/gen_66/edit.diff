--- a/original.py
+++ b/original.py
@@ -1,870 +1,444 @@
 # EVOLVE-BLOCK-START
 """Model placement algorithm for minimizing maximum KV cache pressure across GPUs"""
 
 GPU_MEM_SIZE = 80  # GB
 
 def compute_model_placement(gpu_num, models):
     """
     Compute a model placement that minimizes the maximum KVPR across all GPUs.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         A placement of models to GPUs
     """
-    # Basic checks and trivial return
+    # Trivial cases
     placement_empty = {i: [] for i in range(gpu_num)}
     if gpu_num <= 0 or not models:
         return placement_empty
 
     S = GPU_MEM_SIZE
 
-    # Helper: KVPR safely
+    # KVPR helper: treat zero demand as zero pressure even if memory is full
     def kvpr(R, rem_mem):
+        if R <= 0:
+            return 0.0
         if rem_mem <= 0:
             return float('inf')
         return R / rem_mem
 
-    # Greedy fallback minimizing local max-KVPR increase
-    def _greedy_fallback_kvpr(gpu_num, models, S):
-        # Sort by demand ratio descending; tie by size desc
-        sorted_models = sorted(
-            models,
-            key=lambda m: ((m.req_rate / m.slo) if m.slo != 0 else float('inf'), m.model_size),
-            reverse=True
-        )
+    # Memory-only pre-placement (slo==0)
+    def preplace_memory_only(all_models):
         placement = {i: [] for i in range(gpu_num)}
-        rem_mem = [S] * gpu_num
-        sum_R = [0.0] * gpu_num
-        for m in sorted_models:
-            dR = (m.req_rate / m.slo) if m.slo != 0 else float('inf')
-            size = float(m.model_size)
-            best_gid = None
-            best_val = float('inf')
-            for gid in range(gpu_num):
-                if size <= rem_mem[gid]:
-                    rem = rem_mem[gid] - size
-                    if rem <= 0:
-                        continue
-                    val = (sum_R[gid] + (0.0 if dR == float('inf') else dR)) / rem
-                    if val < best_val:
-                        best_val = val
-                        best_gid = gid
-            if best_gid is None:
-                raise ValueError(
-                    f"Unable to place model of size {m.model_size} GB on any GPU. "
-                    f"Remaining per-GPU memory: {rem_mem}"
-                )
-            placement[best_gid].append(m)
-            sum_R[best_gid] += 0.0 if dR == float('inf') else dR
-            rem_mem[best_gid] -= size
-        return placement
-
-    # Pre-place memory-only (slo==0) models by memory balancing, then build items for slo>0 models
-    def _preplace_memory_only(all_models, S, gpu_num):
-        base = {i: [] for i in range(gpu_num)}
         used = [0.0] * gpu_num
         mem_only = [m for m in all_models if getattr(m, "slo", 0) == 0]
         if mem_only:
-            mem_only = sorted(mem_only, key=lambda m: float(m.model_size), reverse=True)
+            mem_only.sort(key=lambda m: float(m.model_size), reverse=True)
             for m in mem_only:
                 size = float(m.model_size)
-                # Choose GPU with most remaining memory that can fit
+                # place on GPU with most remaining memory that can fit
                 best = None
                 best_rem = -1.0
                 for gid in range(gpu_num):
                     rem = S - used[gid]
                     if size <= rem and rem > best_rem:
                         best_rem = rem
                         best = gid
                 if best is None:
+                    # Cannot place memory-only model -> infeasible
                     raise ValueError(
                         f"Unable to place memory-only model of size {m.model_size} GB on any GPU. "
                         f"Remaining per-GPU memory: {[S - u for u in used]}"
                     )
-                base[best].append(m)
+                placement[best].append(m)
                 used[best] += size
-        return base, used
-
-    base_pre, base_used = _preplace_memory_only(models, S, gpu_num)
-    used0_total = sum(base_used)
-
-    # Extract per-model attributes once (only slo>0)
+        return placement, used
+
+    base_place, base_used = preplace_memory_only(models)
+    # Remaining models: slo>0
     rem_models = [m for m in models if getattr(m, "slo", 0) != 0]
-    items = []
-    total_R = 0.0
-    total_size = 0.0
-    for m in rem_models:
-        slo = float(m.slo)
-        dR = float(m.req_rate) / slo if slo != 0 else 0.0
-        s = float(m.model_size)
-        items.append({'obj': m, 'dR': dR, 'size': s})
-        total_R += dR
-        total_size += s
-
-    # Lower bounds on optimal T
-    def compute_lower_bound():
-        # Per-item bound: T >= dR / (S - s)
-        lb1 = 0.0
-        infeasible_single = False
-        for it in items:
-            dR = it['dR']; s = it['size']
-            denom = S - s
-            if denom <= 0:
-                if (dR != float('inf') and dR > 0) or dR == float('inf'):
-                    infeasible_single = True
-                continue
-            if dR > 0 and dR != float('inf'):
-                cand = dR / denom
-                if cand > lb1:
-                    lb1 = cand
-
-        # Global bound accounting for pre-placed memory-only models: T >= total_R / (gpu_num*S - used0_total - total_size)
-        denom2 = gpu_num * S - used0_total - total_size
-        if denom2 <= 0 and total_R > 0:
-            return float('inf'), True, infeasible_single
-        lb2 = 0.0 if total_R <= 0 or denom2 <= 0 else (total_R / denom2)
-
-        # Pair bound: for pairs that cannot co-reside (s_i + s_j > S)
-        lb_pair = 0.0
-        P = min(len(items), 200)
-        by_size = sorted(items, key=lambda x: x['size'], reverse=True)[:P]
-        for i in range(len(by_size)):
-            si = by_size[i]['size']; ri = by_size[i]['dR']
-            for j in range(i + 1, len(by_size)):
-                sj = by_size[j]['size']; rj = by_size[j]['dR']
-                if si + sj > S:
-                    denom = 2 * S - (si + sj)
-                    if denom > 0:
-                        ri_f = 0.0 if ri == float('inf') else ri
-                        rj_f = 0.0 if rj == float('inf') else rj
-                        cand = (ri_f + rj_f) / denom
-                        if cand > lb_pair:
-                            lb_pair = cand
-
-        # Lightweight triplet bound: for triples with si + sj + sk > 2S
-        lb_triplet = 0.0
-        if len(by_size) >= 3:
-            KTOP = min(8, len(by_size))
-            topK = by_size[:KTOP]
-            for i in range(len(by_size)):
-                si = by_size[i]['size']; ri = by_size[i]['dR']
-                for j in range(i + 1, len(by_size)):
-                    sj = by_size[j]['size']; rj = by_size[j]['dR']
-                    for k in range(min(KTOP, len(by_size))):
-                        mk = topK[k]
-                        if mk is by_size[i] or mk is by_size[j]:
-                            continue
-                        sk = mk['size']; rk = mk['dR']
-                        ssum = si + sj + sk
-                        if ssum > 2 * S:
-                            denom = 3 * S - ssum
-                            if denom > 0:
-                                ri_f = 0.0 if ri == float('inf') else ri
-                                rj_f = 0.0 if rj == float('inf') else rj
-                                rk_f = 0.0 if rk == float('inf') else rk
-                                cand = (ri_f + rj_f + rk_f) / denom
-                                if cand > lb_triplet:
-                                    lb_triplet = cand
-
-        # k-bin prefix bound for k in {1..min(gpu_num,6)}
-        lb_k = 0.0
-        sorted_by_size = sorted(items, key=lambda x: x['size'], reverse=True)
-        prefix_sizes = []
-        prefix_rates = []
-        cs = 0.0; cr = 0.0
-        for it in sorted_by_size:
-            cs += it['size']; cr += (0.0 if it['dR'] == float('inf') else it['dR'])
-            prefix_sizes.append(cs); prefix_rates.append(cr)
-        for k in range(1, min(gpu_num, 6) + 1):
-            threshold = (k - 1) * S
-            idx = -1
-            for t in range(len(prefix_sizes)):
-                if prefix_sizes[t] > threshold:
-                    idx = t
-                    break
-            if idx >= 0:
-                numer = prefix_rates[idx]
-                denom = k * S - prefix_sizes[idx]
-                if denom > 0 and numer > 0:
-                    cand = numer / denom
-                    if cand > lb_k:
-                        lb_k = cand
-
-        lower = max(0.0, lb1, lb2, lb_pair, lb_triplet, lb_k)
-        return lower, False, infeasible_single
-
-    lower, infeasible_global, infeasible_single = compute_lower_bound()
-    if infeasible_single or infeasible_global or lower == float('inf'):
-        # Fallback: simple greedy minimizing local KVPR increase
-        return _greedy_fallback_kvpr(gpu_num, models, S)
-
-    # Deterministic small-random helper
-    import random
-    rng = random.Random(len(items) * 1009 + gpu_num * 9173)
-
-    # Slack-equalization packer for a given T
-    # K_g = T*S - (sumR_g + T*used_mem_g); placing (dR, s) consumes w = dR + T*s
-    def assign_balanced_slack(T, order='w_desc', seed_H=0, choose_rule='tight', seeds=1):
-        if T < 0:
-            return None
-        # Build enriched list
-        enriched = []
-        for it in items:
-            dR = it['dR']; sz = it['size']
-            if dR == float('inf'):
-                return None  # cannot handle infinite demand ratios under bounded T
-            w = dR + T * sz
-            if w < 0:
-                w = 0.0
-            enriched.append([w, dR, sz, it['obj']])
-
-        # Orderings
-        def sort_enriched(arr, Tcur):
-            if order == 'w_desc':
-                arr.sort(key=lambda x: x[0], reverse=True)
-            elif order == 'intrinsic_desc':
-                arr.sort(key=lambda x: (x[1] / max(S - x[2], 1e-9)), reverse=True)
-            elif order == 'size_desc':
-                arr.sort(key=lambda x: x[2], reverse=True)
-            elif order == 'density_desc':
-                arr.sort(key=lambda x: (x[1] / (x[2] if x[2] > 0 else 1e-9)), reverse=True)
-            else:
-                arr.sort(key=lambda x: x[0], reverse=True)
-
-        sort_enriched(enriched, T)
-
-        best_assign = None
-        best_val = float('inf')
-
-        for _ in range(max(1, seeds)):
-            # per-GPU states (start from pre-placed memory-only models)
-            assign = {i: list(base_pre.get(i, [])) for i in range(gpu_num)}
-            used_mem = list(base_used)
-            sum_R = [0.0] * gpu_num
-            for gid in range(gpu_num):
-                for m0 in assign[gid]:
-                    if getattr(m0, "slo", 0) != 0:
-                        sum_R[gid] += float(m0.req_rate) / float(m0.slo)
-            T_current = T
-            K = [T_current * S - (sum_R[g] + T_current * used_mem[g]) for g in range(gpu_num)]  # KV slack
-
-            # Seeding: spread top H intrinsic-pressure models using worst-fit on K
-            H = int(seed_H) if seed_H else 0
-            if H > 0 and len(enriched) > 0:
-                intrinsic = sorted(enriched, key=lambda x: (x[1] / max(S - x[2], 1e-9)), reverse=True)
-                seeds_list = intrinsic[:min(H, len(enriched))]
-                # Remove seeds from enriched while preserving original sequence among the rest
-                seed_ids = set(id(x) for x in seeds_list)
-                remaining = [x for x in enriched if id(x) not in seed_ids]
-                for w, dR, sz, m in seeds_list:
-                    # Choose GPU with largest K that can fit both memory and slack
-                    candidates = []
-                    for gid in range(gpu_num):
-                        if used_mem[gid] + sz <= S + 1e-9 and K[gid] >= w - 1e-9:
-                            candidates.append(gid)
-                    if not candidates:
-                        assign = None
-                        break
-                    # Worst-fit on K; tie-break by larger memory slack, then lower resulting local kvpr
-                    def key_fn(g):
-                        rem = S - (used_mem[g] + sz)
-                        kvnew = kvpr(sum_R[g] + dR, rem) if rem > 0 else float('inf')
-                        return (-K[g], -(rem), kvnew)
-                    candidates.sort(key=lambda g: key_fn(g))
-                    chosen = candidates[0]
-                    if rng.random() < 0.2 and len(candidates) > 1:
-                        chosen = candidates[rng.randrange(min(2, len(candidates)))]
-                    assign[chosen].append(m)
-                    used_mem[chosen] += sz
-                    sum_R[chosen] += dR
-                    K[chosen] -= w
-                if assign is None:
-                    continue
-                enriched = remaining
-
-            # Helper to place a sequence of items with a given T_current
-            def place_sequence(seq_items):
-                nonlocal assign, used_mem, sum_R, K, T_current
-                for w, dR, sz, m in seq_items:
-                    options = []
-                    for gid in range(gpu_num):
-                        if used_mem[gid] + sz <= S + 1e-9 and K[gid] >= w - 1e-9:
-                            K_after = K[gid] - w
-                            if choose_rule in ('min_kvpr', 'hybrid'):
-                                rem = S - (used_mem[gid] + sz)
-                                if rem <= 0:
-                                    continue
-                                kv_new = kvpr(sum_R[gid] + dR, rem)
-                                if choose_rule == 'hybrid':
-                                    cap = T_current * S
-                                    K_after_norm = max(0.0, K_after) / max(cap, 1e-12)
-                                    avg_mem_frac = (sum(used_mem) + sz) / (gpu_num * S) if S > 0 else 0.0
-                                    mem_after_frac = (used_mem[gid] + sz) / S if S > 0 else 0.0
-                                    mem_imbalance = abs(mem_after_frac - avg_mem_frac)
-                                    alpha = 0.15
-                                    beta = 0.05
-                                    kv_new_norm = kv_new / max(T_current, 1e-12) if T_current > 0 else kv_new
-                                    hybrid = K_after_norm + alpha * kv_new_norm + beta * mem_imbalance
-                                    options.append((gid, K_after, kv_new, hybrid))
-                                else:
-                                    options.append((gid, K_after, kv_new, None))
-                            else:
-                                options.append((gid, K_after, None, None))
-                    if not options:
-                        return False
-                    # Selection
-                    if choose_rule == 'min_kvpr':
-                        options.sort(key=lambda t: (t[1], t[2]))
-                    elif choose_rule == 'hybrid':
-                        options.sort(key=lambda t: (t[1], t[3], t[2]))
-                    else:
-                        options.sort(key=lambda t: t[1])
-                    chosen = options[0][0]
-                    # Tiny randomization among top-2 close in K_after
-                    if len(options) > 1:
-                        bestK = options[0][1]
-                        near = [op for op in options if abs(op[1] - bestK) <= max(1e-9, 0.001 * (T_current * S + 1e-9))]
-                        if len(near) >= 2 and rng.random() < 0.25:
-                            chosen = near[rng.randrange(min(2, len(near)))][0]
-                    # Commit
-                    assign[chosen].append(m)
-                    used_mem[chosen] += sz
-                    sum_R[chosen] += dR
-                    K[chosen] -= w
-                return True
-
-            # Two-phase placement with mid-pack T update
-            seq_all = list(enriched)
-            cut = int(0.4 * len(seq_all))
-            ok = True
-            if cut > 0:
-                ok = place_sequence(seq_all[:cut])
-                if not ok:
-                    assign = None
-                else:
-                    # Recompute residual global lower bound using remaining items and residual capacities
-                    rem_items = seq_all[cut:]
-                    if rem_items:
-                        rem_total_R = sum(x[1] for x in rem_items)
-                        rem_total_size = sum(x[2] for x in rem_items)
-                        free_sum = sum(S - used_mem[g] for g in range(gpu_num))
-                        denom = free_sum - rem_total_size
-                        if denom > 0 and rem_total_R > 0:
-                            T2 = rem_total_R / denom
-                            if T2 > T_current:
-                                delta = T2 - T_current
-                                for g in range(gpu_num):
-                                    K[g] += delta * (S - used_mem[g])
-                                T_current = T2
-                        # Rebuild weights and resort remaining according to the (possibly) updated T
-                        rem_enriched = []
-                        for _, dR, sz, m in rem_items:
-                            w2 = dR + T_current * sz
-                            rem_enriched.append([w2, dR, sz, m])
-                        sort_enriched(rem_enriched, T_current)
-                        ok = place_sequence(rem_enriched)
-                        if not ok:
-                            assign = None
-            else:
-                # No cut; place all in one go
-                ok = place_sequence(seq_all)
-                if not ok:
-                    assign = None
-
-            if assign is None:
-                continue
-
-            # Validate constraints strictly
-            ok = True
-            for gid in range(gpu_num):
-                if used_mem[gid] - S > 1e-6:
-                    ok = False; break
-                rem = S - used_mem[gid]
-                if rem <= 0 and sum_R[gid] > 0:
-                    ok = False; break
-                if T_current > 0 and (sum_R[gid] / max(rem, 1e-12)) - T_current > 1e-6:
-                    ok = False; break
-            if not ok:
-                continue
-
-            # Keep best by measured max KVPR
-            val = 0.0
-            for gid in range(gpu_num):
-                rem = S - used_mem[gid]
-                val = max(val, kvpr(sum_R[gid], rem))
-            if val < best_val:
-                best_val = val
-                best_assign = assign
-
-        return best_assign
-
-    # Find the first feasible T by multiplicative sweep from the lower bound
-    def find_first_feasible_T(start_T):
-        T = max(0.0, start_T)
-        growth = 1.08
-        max_steps = 40
-        for _ in range(max_steps):
-            for (order, choose_rule, seedH, seeds) in [
-                ('w_desc', 'tight', min(4, max(1, gpu_num)), 1),
-                ('intrinsic_desc', 'tight', min(3, max(1, gpu_num - 1)), 1),
-                ('w_desc', 'hybrid', min(3, max(1, gpu_num - 1)), 1),
-                ('w_desc', 'min_kvpr', 0, 1),
-            ]:
-                cand = assign_balanced_slack(T, order=order, seed_H=seedH, choose_rule=choose_rule, seeds=1)
-                if cand is not None:
-                    return T, cand
-            T = T * growth if T > 0 else 1e-6
-        return None, None
-
-    # Helper: evaluate max KVPR of a placement
-    def eval_max_kvpr(placement_dict):
-        max_v = 0.0
-        for gid in range(gpu_num):
-            bucket = placement_dict.get(gid, [])
-            used = 0.0
-            R = 0.0
-            for m in bucket:
-                used += float(m.model_size)
-                R += float(m.req_rate / m.slo) if m.slo != 0 else 0.0
-            val = kvpr(R, S - used)
-            if val > max_v:
-                max_v = val
-        return max_v
-
-    # Try to get an initial feasible T-based placement
-    T_feas, placement_at_T = find_first_feasible_T(lower)
-    if placement_at_T is None:
-        # As a safety net
-        return _greedy_fallback_kvpr(gpu_num, models, S)
-
-    # Build a compact set of candidate T values around the first feasible T
-    candidates_T = []
-    for mul in [0.985, 0.99, 1.0, 1.005, 1.01, 1.015, 1.02]:
-        val = max(lower, T_feas * mul)
-        candidates_T.append(val)
-    # Include midpoint between lower and T_feas
-    candidates_T.append(0.5 * (lower + T_feas))
-    candidates_T = sorted(set(round(t, 12) for t in candidates_T))
-
-    # Try a few packing variants per T and pick best by measured KVPR
-    variants = [
-        ('w_desc', 'tight', min(4, max(1, gpu_num)), 2),
-        ('intrinsic_desc', 'tight', min(3, max(1, gpu_num - 1))),
-        ('w_desc', 'hybrid', min(3, max(1, gpu_num - 1)), 2),
-        ('density_desc', 'hybrid', 0),
-        ('density_desc', 'tight', 0),
-        ('w_desc', 'min_kvpr', 0),
-        ('size_desc', 'tight', 0),
-    ]
-
-    best_placement = placement_at_T
-    best_val = eval_max_kvpr(best_placement)
-
-    for T in candidates_T:
-        for var in variants:
-            # Unpack with defaults for backward compatibility tuple length
-            if len(var) == 4:
-                order, choose_rule, seedH, seeds = var
-            else:
-                order, choose_rule, seedH = var
-                seeds = 1
-            cand = assign_balanced_slack(T, order=order, seed_H=seedH, choose_rule=choose_rule, seeds=seeds)
-            if cand is None:
-                continue
-            val = eval_max_kvpr(cand)
-            if val < best_val:
-                best_val = val
-                best_placement = cand
-
-    # Incorporate a greedy multi-ordering candidate with local improvement (from inspiration)
-    def greedy_assign(sorted_models):
-        placement = {gpu_id: list(base_pre.get(gpu_id, [])) for gpu_id in range(gpu_num)}
+
+    # Build greedy min-max placer on top of the pre-placement
+    def greedy_place(order_models):
+        placement = {gid: list(base_place.get(gid, [])) for gid in range(gpu_num)}
         rem_mem = [S - base_used[i] for i in range(gpu_num)]
-        sum_r = [0.0 for _ in range(gpu_num)]
+        sum_R = [0.0 for _ in range(gpu_num)]  # total dR per GPU
+
+        # initialize R with any demand-bearing already in base (should be none, but safe)
         for gid in range(gpu_num):
             for m0 in placement[gid]:
-                if getattr(m0, "slo", 0) != 0:
-                    sum_r[gid] += float(m0.req_rate) / float(m0.slo)
-        for model in sorted_models:
-            slo = float(getattr(model, "slo", 0))
-            dR = (float(model.req_rate) / slo) if slo != 0 else 0.0
-            current_kvprs = [kvpr(sum_r[i], rem_mem[i]) for i in range(gpu_num)]
-            best_gpu = None
-            best_resulting_max = float('inf')
-            best_new_gpu_kvpr = float('inf')
-            best_new_rem = -1.0
+                slo0 = getattr(m0, "slo", 0.0)
+                if slo0:
+                    sum_R[gid] += float(m0.req_rate) / float(slo0)
+
+        def current_kvprs():
+            return [kvpr(sum_R[i], rem_mem[i]) for i in range(gpu_num)]
+
+        for m in order_models:
+            dR = float(m.req_rate) / float(m.slo)
+            size = float(m.model_size)
+
+            # Evaluate best GPU by minimizing resulting global max KVPR
+            kvprs_before = current_kvprs()
+            best_gid = None
+            best_res_max = float('inf')
+            best_local = float('inf')
+            best_rem_after = -1.0
+
             for gid in range(gpu_num):
-                if model.model_size <= rem_mem[gid]:
-                    new_R = sum_r[gid] + dR
-                    new_mem = rem_mem[gid] - model.model_size
-                    new_gpu_kvpr = kvpr(new_R, new_mem)
-                    resulting_max = new_gpu_kvpr
+                if size <= rem_mem[gid]:
+                    new_R = sum_R[gid] + dR
+                    new_rem = rem_mem[gid] - size
+                    new_k = kvpr(new_R, new_rem)
+                    # resulting global max
+                    res_max = new_k
                     for j in range(gpu_num):
                         if j == gid:
                             continue
-                        if current_kvprs[j] > resulting_max:
-                            resulting_max = current_kvprs[j]
-                    if (resulting_max < best_resulting_max or
-                        (resulting_max == best_resulting_max and new_gpu_kvpr < best_new_gpu_kvpr) or
-                        (resulting_max == best_resulting_max and new_gpu_kvpr == best_new_gpu_kvpr and new_mem > best_new_rem)):
-                        best_resulting_max = resulting_max
-                        best_new_gpu_kvpr = new_gpu_kvpr
-                        best_new_rem = new_mem
-                        best_gpu = gid
-            if best_gpu is None:
-                raise ValueError("Greedy ordering cannot place model due to memory.")
-            placement[best_gpu].append(model)
-            rem_mem[best_gpu] -= model.model_size
-            sum_r[best_gpu] += dR
-        return placement, rem_mem, sum_r
-
-    def greedy_improve(placement, rem_mem, sum_r):
-        def all_kvprs():
-            return [kvpr(sum_r[i], rem_mem[i]) for i in range(gpu_num)]
-        eps = 1e-12
-        iters = max(1, min(len(models), 6 * gpu_num))
-        for _ in range(iters):
-            kvprs = all_kvprs()
-            if not kvprs:
+                        if kvprs_before[j] > res_max:
+                            res_max = kvprs_before[j]
+                    # tie-break: minimize resulting max, then the GPU KVPR, then leave more rem
+                    if (res_max < best_res_max or
+                        (res_max == best_res_max and new_k < best_local) or
+                        (res_max == best_res_max and new_k == best_local and new_rem > best_rem_after)):
+                        best_res_max = res_max
+                        best_local = new_k
+                        best_rem_after = new_rem
+                        best_gid = gid
+
+            if best_gid is None:
+                # Unplaceable with this ordering -> signal failure
+                raise ValueError(
+                    f"Unable to place model of size {m.model_size} GB; remaining per-GPU memory: {rem_mem}"
+                )
+
+            # Commit
+            placement[best_gid].append(m)
+            rem_mem[best_gid] -= size
+            sum_R[best_gid] += dR
+
+        return placement
+
+    def eval_max_kvpr(placement):
+        max_v = 0.0
+        for gid in range(gpu_num):
+            used = 0.0
+            R = 0.0
+            for m in placement.get(gid, []):
+                used += float(m.model_size)
+                slo = float(getattr(m, "slo", 0.0))
+                if slo:
+                    R += float(m.req_rate) / slo
+            max_v = max(max_v, kvpr(R, S - used))
+        return max_v
+
+    # Build multiple deterministic orderings for initialization
+    def pressure_weight(m):
+        denom = S - float(m.model_size)
+        if denom <= 0:
+            return float('inf')
+        return (float(m.req_rate) / float(m.slo)) / denom
+
+    def demand(m):
+        return float(m.req_rate) / float(m.slo)
+
+    def density(m):
+        sz = float(m.model_size) if float(m.model_size) > 0 else 1e-9
+        return (float(m.req_rate) / float(m.slo)) / sz
+
+    orderings = [
+        lambda ms: sorted(ms, key=pressure_weight, reverse=True),
+        lambda ms: sorted(ms, key=demand, reverse=True),
+        lambda ms: sorted(ms, key=lambda m: float(m.model_size), reverse=True),
+        lambda ms: sorted(ms, key=lambda m: float(m.model_size)),  # small first
+        lambda ms: sorted(ms, key=density, reverse=True),
+        lambda ms: sorted(ms, key=lambda m: (demand(m), -float(m.model_size)), reverse=True),
+    ]
+
+    best_placement = None
+    best_score = float('inf')
+    # Try each ordering; keep best feasible
+    for make_order in orderings:
+        try:
+            ordered = make_order(rem_models)
+            cand = greedy_place(ordered)
+            score = eval_max_kvpr(cand)
+            if score < best_score:
+                best_score = score
+                best_placement = cand
+        except Exception:
+            continue
+
+    if best_placement is None:
+        # Fallback: place all models greedily by size-desc
+        ordered = sorted(rem_models, key=lambda m: float(m.model_size), reverse=True)
+        best_placement = greedy_place(ordered)
+        best_score = eval_max_kvpr(best_placement)
+
+    # Large Neighborhood Search refinement with donation-first moves and size-focused swaps
+    def lns_refine(placement, max_rounds=6, move_cap=24, swap_cap=12):
+        # Copy mutable structures
+        buckets = {gid: list(placement.get(gid, [])) for gid in range(gpu_num)}
+        used = [0.0] * gpu_num
+        Rsum = [0.0] * gpu_num
+        for gid in range(gpu_num):
+            for m in buckets[gid]:
+                used[gid] += float(m.model_size)
+                slo = float(getattr(m, "slo", 0.0))
+                if slo:
+                    Rsum[gid] += float(m.req_rate) / slo
+
+        def kvprs_all():
+            return [kvpr(Rsum[g], S - used[g]) for g in range(gpu_num)]
+
+        def apply_move(src, dst, m):
+            dR = float(m.req_rate) / float(m.slo)
+            s = float(m.model_size)
+            buckets[src].remove(m); buckets[dst].append(m)
+            Rsum[src] -= dR; Rsum[dst] += dR
+            used[src] -= s; used[dst] += s
+
+        rounds = 0
+        no_improve_passes = 0
+        while rounds < max_rounds:
+            rounds += 1
+            improved_round = False
+            kvs = kvprs_all()
+            cur_max = max(kvs) if kvs else 0.0
+            if gpu_num == 0:
                 break
-            cur_max = max(kvprs)
-            src = max(range(gpu_num), key=lambda i: kvprs[i])
-            improved = False
-            best_move = None
-            # Try moves
-            for mdl in list(placement[src]):
+            worst = max(range(gpu_num), key=lambda g: kvs[g])
+            # Two best recipient GPUs by current KVPR (lowest)
+            best_recips = sorted(range(gpu_num), key=lambda g: kvs[g])[:min(2, gpu_num)]
+            # Donation-first moves from worst
+            moves_left = move_cap
+            best_move = None  # (src,dst,mdl,resulting_max)
+            # Candidate models: largest and highest-demand
+            src_models = list(buckets[worst])
+            src_models.sort(key=lambda mm: (float(mm.req_rate) / float(mm.slo), float(mm.model_size)), reverse=True)
+            # Limit to top 12 by demand and top 12 by size
+            top_by_demand = src_models[:12]
+            top_by_size = sorted(list(buckets[worst]), key=lambda mm: float(mm.model_size), reverse=True)[:12]
+            cand_set = list(dict.fromkeys(top_by_demand + top_by_size))
+
+            for mdl in cand_set:
+                if moves_left <= 0:
+                    break
                 dR = float(mdl.req_rate) / float(mdl.slo)
-                size = float(mdl.model_size)
-                src_R_new = sum_r[src] - dR
-                src_mem_new = rem_mem[src] + size
-                src_kv_new = kvpr(src_R_new, src_mem_new)
-                for dst in range(gpu_num):
-                    if dst == src:
+                s = float(mdl.model_size)
+                # Source after move
+                src_R_new = Rsum[worst] - dR
+                src_used_new = used[worst] - s
+                src_rem_new = S - src_used_new
+                src_kv_new = kvpr(src_R_new, src_rem_new)
+                for dst in best_recips:
+                    if dst == worst:
                         continue
-                    if size <= rem_mem[dst]:
-                        dst_R_new = sum_r[dst] + dR
-                        dst_mem_new = rem_mem[dst] - size
-                        dst_kv_new = kvpr(dst_R_new, dst_mem_new)
-                        resulting = dst_kv_new if dst_kv_new > src_kv_new else src_kv_new
-                        for g in range(gpu_num):
-                            if g == src or g == dst:
-                                continue
-                            if kvprs[g] > resulting:
-                                resulting = kvprs[g]
-                        if resulting + eps < cur_max:
-                            if best_move is None or resulting < best_move[3]:
-                                best_move = (src, dst, mdl, resulting)
+                    if used[dst] + s > S:
+                        continue
+                    dst_R_new = Rsum[dst] + dR
+                    dst_used_new = used[dst] + s
+                    dst_kv_new = kvpr(dst_R_new, S - dst_used_new)
+                    # Resulting global max
+                    resulting = src_kv_new if src_kv_new > dst_kv_new else dst_kv_new
+                    for g in range(gpu_num):
+                        if g == worst or g == dst:
+                            continue
+                        if kvs[g] > resulting:
+                            resulting = kvs[g]
+                    if resulting + 1e-12 < cur_max:
+                        if best_move is None or resulting < best_move[3]:
+                            best_move = (worst, dst, mdl, resulting)
+                moves_left -= 1
+
             if best_move is not None:
-                s, d, m, _ = best_move
-                placement[s].remove(m)
-                placement[d].append(m)
-                dR = float(m.req_rate) / float(m.slo)
-                size = float(m.model_size)
-                sum_r[s] -= dR; sum_r[d] += dR
-                rem_mem[s] += size; rem_mem[d] -= size
-                improved = True
+                src, dst, mdl, _ = best_move
+                apply_move(src, dst, mdl)
+                improved_round = True
             else:
-                # Try limited swaps
-                best_swap = None
-                cap_a = min(10, len(placement[src]))
-                for a in list(placement[src])[:cap_a]:
+                # Try limited swaps between worst and others
+                swaps_left = swap_cap
+                best_swap = None  # (src,dst,a,b,resulting_max)
+                src = worst
+                # Cap candidates
+                A = sorted(list(buckets[src]), key=lambda m: (float(m.req_rate)/float(m.slo), float(m.model_size)), reverse=True)[:12]
+                for a in A:
+                    if swaps_left <= 0:
+                        break
                     aR = float(a.req_rate) / float(a.slo)
                     aS = float(a.model_size)
                     for dst in range(gpu_num):
-                        if dst == src or not placement[dst]:
+                        if dst == src or not buckets[dst]:
                             continue
-                        cap_b = min(10, len(placement[dst]))
-                        for b in list(placement[dst])[:cap_b]:
+                        B = sorted(list(buckets[dst]), key=lambda m: (float(m.req_rate)/float(m.slo), float(m.model_size)), reverse=True)[:12]
+                        for b in B:
+                            if swaps_left <= 0:
+                                break
                             bR = float(b.req_rate) / float(b.slo)
                             bS = float(b.model_size)
-                            mem_src_new = rem_mem[src] + aS - bS
-                            mem_dst_new = rem_mem[dst] + bS - aS
-                            if mem_src_new < 0 or mem_dst_new < 0:
+                            # Feasibility after swap
+                            src_used_new = used[src] - aS + bS
+                            dst_used_new = used[dst] - bS + aS
+                            if src_used_new > S or dst_used_new > S:
+                                swaps_left -= 1
                                 continue
-                            src_R_new = sum_r[src] - aR + bR
-                            dst_R_new = sum_r[dst] - bR + aR
-                            src_kv_new = kvpr(src_R_new, mem_src_new)
-                            dst_kv_new = kvpr(dst_R_new, mem_dst_new)
+                            src_R_new = Rsum[src] - aR + bR
+                            dst_R_new = Rsum[dst] - bR + aR
+                            src_kv_new = kvpr(src_R_new, S - src_used_new)
+                            dst_kv_new = kvpr(dst_R_new, S - dst_used_new)
                             resulting = src_kv_new if src_kv_new > dst_kv_new else dst_kv_new
                             for g in range(gpu_num):
                                 if g == src or g == dst:
                                     continue
-                                if kvprs[g] > resulting:
-                                    resulting = kvprs[g]
-                            if resulting + eps < cur_max:
+                                if kvs[g] > resulting:
+                                    resulting = kvs[g]
+                            if resulting + 1e-12 < cur_max:
                                 if best_swap is None or resulting < best_swap[4]:
                                     best_swap = (src, dst, a, b, resulting)
-                if best_swap is not None:
-                    src_g, dst_g, a, b, _ = best_swap
-                    placement[src_g].remove(a); placement[dst_g].append(a)
-                    placement[dst_g].remove(b); placement[src_g].append(b)
-                    aR = float(a.req_rate) / float(a.slo); bR = float(b.req_rate) / float(b.slo)
-                    aS = float(a.model_size); bS = float(b.model_size)
-                    sum_r[src_g] = sum_r[src_g] - aR + bR
-                    sum_r[dst_g] = sum_r[dst_g] - bR + aR
-                    rem_mem[src_g] = rem_mem[src_g] + aS - bS
-                    rem_mem[dst_g] = rem_mem[dst_g] + bS - aS
-                    improved = True
-            if not improved:
-                break
-        return placement, rem_mem, sum_r
-
-    def build_greedy_candidate():
-        # Only used when all dR are finite (guaranteed here due to earlier check)
-        def pressure_weight(m):
-            denom = S - m.model_size
-            if denom <= 0:
-                return float('inf')
-            return (m.req_rate / m.slo) / denom
-        def r_over_s(m):
-            return (m.req_rate / m.slo)
-        def density(m):
-            sz = m.model_size if m.model_size > 0 else 1e-9
-            return (m.req_rate / m.slo) / sz
-        orderings = [
-            lambda ms: sorted(ms, key=pressure_weight, reverse=True),
-            lambda ms: sorted(ms, key=r_over_s, reverse=True),
-            lambda ms: sorted(ms, key=lambda m: m.model_size, reverse=True),
-            lambda ms: sorted(ms, key=lambda m: m.model_size),
-            lambda ms: sorted(ms, key=density, reverse=True),
-            lambda ms: sorted(ms, key=lambda m: (r_over_s(m), -m.model_size), reverse=True),
-        ]
-        best = None
-        bestv = float('inf')
-        for make_order in orderings:
-            try:
-                ordered = make_order(rem_models)
-                placement, rem_mem, sum_r = greedy_assign(ordered)
-                placement, rem_mem, sum_r = greedy_improve(placement, rem_mem, sum_r)
-                val = eval_max_kvpr(placement)
-                if val < bestv:
-                    bestv = val
-                    best = placement
-            except Exception:
-                continue
-        return best, bestv
-
-    greedy_cand, greedy_val = build_greedy_candidate()
-    if greedy_cand is not None and greedy_val < best_val:
-        best_val = greedy_val
-        best_placement = greedy_cand
-
-    # Short bounded local search focusing on the most loaded GPU: moves then swaps
-    def local_refine(placement, move_budget=20, swap_budget=10):
-        buckets = {gid: list(placement.get(gid, [])) for gid in range(gpu_num)}
-        used_mem = [0.0] * gpu_num
-        sum_R = [0.0] * gpu_num
-        for gid in range(gpu_num):
-            for m in buckets[gid]:
-                used_mem[gid] += float(m.model_size)
-                sum_R[gid] += float(m.req_rate / m.slo) if m.slo != 0 else 0.0
-
-        def current_kvprs():
-            return [kvpr(sum_R[g], S - used_mem[g]) for g in range(gpu_num)]
-
-        def apply_move(src, dst, mdl):
-            dR = float(mdl.req_rate / mdl.slo) if mdl.slo != 0 else 0.0
-            s = float(mdl.model_size)
-            buckets[src].remove(mdl); buckets[dst].append(mdl)
-            sum_R[src] -= dR; sum_R[dst] += dR
-            used_mem[src] -= s; used_mem[dst] += s
-
-        moves_left = move_budget
-        swaps_left = swap_budget
-        while moves_left > 0 or swaps_left > 0:
-            kvprs = current_kvprs()
-            cur_max = max(kvprs)
-            max_gid = max(range(gpu_num), key=lambda g: kvprs[g])
-
-            improved = False
-            best_move = None  # (src,dst,mdl,resulting_max)
-            # Single-item moves
-            for mdl in list(buckets[max_gid]):
-                dR = float(mdl.req_rate / mdl.slo) if mdl.slo != 0 else 0.0
-                s = float(mdl.model_size)
-                R_src_new = sum_R[max_gid] - dR
-                mem_src_new = used_mem[max_gid] - s
-                rem_src_new = S - mem_src_new
-                if rem_src_new <= 0:
-                    continue
-                kv_src_new = kvpr(R_src_new, rem_src_new)
-                for dst in range(gpu_num):
-                    if dst == max_gid:
-                        continue
-                    if used_mem[dst] + s > S:
-                        continue
-                    rem_dst_new = S - (used_mem[dst] + s)
-                    if rem_dst_new <= 0:
-                        continue
-                    R_dst_new = sum_R[dst] + dR
-                    kv_dst_new = kvpr(R_dst_new, rem_dst_new)
-                    resulting = kv_dst_new if kv_dst_new > kv_src_new else kv_src_new
-                    for g in range(gpu_num):
-                        if g == max_gid or g == dst:
-                            continue
-                        if kvprs[g] > resulting:
-                            resulting = kvprs[g]
-                    if resulting + 1e-12 < cur_max:
-                        if best_move is None or resulting < best_move[3]:
-                            best_move = (max_gid, dst, mdl, resulting)
-            if best_move is not None and moves_left > 0:
-                src, dst, mdl, _ = best_move
-                apply_move(src, dst, mdl)
-                moves_left -= 1
-                improved = True
-            else:
-                # Try limited swaps
-                best_swap = None  # (src,dst,a,b,resulting_max)
-                if swaps_left > 0:
-                    cap_a = min(10, len(buckets[max_gid]))
-                    for a in list(buckets[max_gid])[:cap_a]:
-                        aR = float(a.req_rate / a.slo) if a.slo != 0 else 0.0
-                        aS = float(a.model_size)
-                        for dst in range(gpu_num):
-                            if dst == max_gid or not buckets[dst]:
-                                continue
-                            cap_b = min(10, len(buckets[dst]))
-                            for b in list(buckets[dst])[:cap_b]:
-                                bR = float(b.req_rate / b.slo) if b.slo != 0 else 0.0
-                                bS = float(b.model_size)
-                                mem_src_new = used_mem[max_gid] - aS + bS
-                                mem_dst_new = used_mem[dst] - bS + aS
-                                if mem_src_new > S or mem_dst_new > S:
-                                    continue
-                                rem_src = S - mem_src_new
-                                rem_dst = S - mem_dst_new
-                                if rem_src <= 0 or rem_dst <= 0:
-                                    continue
-                                R_src_new = sum_R[max_gid] - aR + bR
-                                R_dst_new = sum_R[dst] - bR + aR
-                                kv_src_new = kvpr(R_src_new, rem_src)
-                                kv_dst_new = kvpr(R_dst_new, rem_dst)
-                                resulting = kv_src_new if kv_src_new > kv_dst_new else kv_dst_new
-                                for g in range(gpu_num):
-                                    if g == max_gid or g == dst:
-                                        continue
-                                    val = kvpr(sum_R[g], S - used_mem[g])
-                                    if val > resulting:
-                                        resulting = val
-                                if resulting + 1e-12 < cur_max:
-                                    if best_swap is None or resulting < best_swap[4]:
-                                        best_swap = (max_gid, dst, a, b, resulting)
+                            swaps_left -= 1
+                        if swaps_left <= 0:
+                            break
                 if best_swap is not None:
                     src, dst, a, b, _ = best_swap
+                    # Apply swap
                     buckets[src].remove(a); buckets[src].append(b)
                     buckets[dst].remove(b); buckets[dst].append(a)
-                    aR = float(a.req_rate / a.slo) if a.slo != 0 else 0.0
-                    bR = float(b.req_rate / b.slo) if b.slo != 0 else 0.0
+                    aR = float(a.req_rate) / float(a.slo)
+                    bR = float(b.req_rate) / float(b.slo)
                     aS = float(a.model_size); bS = float(b.model_size)
-                    sum_R[src] = sum_R[src] - aR + bR
-                    sum_R[dst] = sum_R[dst] - bR + aR
-                    used_mem[src] = used_mem[src] - aS + bS
-                    used_mem[dst] = used_mem[dst] - bS + aS
-                    swaps_left -= 1
-                    improved = True
-            if not improved:
-                break
+                    Rsum[src] = Rsum[src] - aR + bR
+                    Rsum[dst] = Rsum[dst] - bR + aR
+                    used[src] = used[src] - aS + bS
+                    used[dst] = used[dst] - bS + aS
+                    improved_round = True
+                else:
+                    # Optional ruin-and-recreate when stuck: remove a few high-impact items from worst and reinsert
+                    removed = []
+                    if buckets[worst]:
+                        # Score by decrease in worst KVPR if removed
+                        scored = []
+                        for m in buckets[worst]:
+                            dR = float(m.req_rate) / float(m.slo)
+                            s = float(m.model_size)
+                            R_new = Rsum[worst] - dR
+                            rem_new = S - (used[worst] - s)
+                            new_k = kvpr(R_new, rem_new)
+                            gain = kvs[worst] - new_k
+                            scored.append((gain, m))
+                        scored.sort(key=lambda t: t[0], reverse=True)
+                        take = min(3, len(scored))
+                        for i in range(take):
+                            _, m = scored[i]
+                            buckets[worst].remove(m)
+                            Rsum[worst] -= float(m.req_rate) / float(m.slo)
+                            used[worst] -= float(m.model_size)
+                            removed.append(m)
+                    # Reinsert removed greedily by minimizing resulting max
+                    for m in removed:
+                        dR = float(m.req_rate) / float(m.slo)
+                        s = float(m.model_size)
+                        # compute current kvprs
+                        kvs_now = kvprs_all()
+                        best_gid = None
+                        best_res = float('inf')
+                        best_local = float('inf')
+                        best_rem_after = -1.0
+                        for gid in range(gpu_num):
+                            if used[gid] + s <= S:
+                                new_R = Rsum[gid] + dR
+                                new_rem = S - (used[gid] + s)
+                                new_k = kvpr(new_R, new_rem)
+                                res = new_k
+                                for j in range(gpu_num):
+                                    if j == gid:
+                                        continue
+                                    if kvs_now[j] > res:
+                                        res = kvs_now[j]
+                                if (res < best_res or
+                                    (res == best_res and new_k < best_local) or
+                                    (res == best_res and new_k == best_local and new_rem > best_rem_after)):
+                                    best_res = res
+                                    best_local = new_k
+                                    best_rem_after = new_rem
+                                    best_gid = gid
+                        if best_gid is not None:
+                            buckets[best_gid].append(m)
+                            Rsum[best_gid] += dR
+                            used[best_gid] += s
+                    # Accept if improved
+                    new_max = max(kvprs_all()) if gpu_num > 0 else 0.0
+                    if new_max + 1e-12 < cur_max:
+                        improved_round = True
+
+            if improved_round:
+                no_improve_passes = 0
+            else:
+                no_improve_passes += 1
+                if no_improve_passes >= 2:
+                    break  # two consecutive non-improving passes
+
         return buckets
 
-    # Binary search on T using balanced-slack feasibility to further reduce max KVPR
-    def feasible_assign_for_T(T):
-        best = None
-        bestv = float('inf')
-        for (order, choose_rule, seedH) in [
-            ('w_desc', 'tight', min(4, max(1, gpu_num))),
-            ('intrinsic_desc', 'tight', min(3, max(1, gpu_num - 1))),
-            ('w_desc', 'hybrid', min(3, max(1, gpu_num - 1))),
-            ('w_desc', 'min_kvpr', 0),
-            ('density_desc', 'tight', 0),
-        ]:
-            cand = assign_balanced_slack(T, order=order, seed_H=seedH, choose_rule=choose_rule, seeds=1)
-            if cand is not None:
-                val = eval_max_kvpr(cand)
-                if val < bestv:
-                    bestv = val
-                    best = cand
-        return best
-
-    # Start bsearch between lower and current best_val
-    if best_val < float('inf') and lower <= best_val:
-        lo, hi = max(0.0, lower), max(0.0, best_val)
-        best_bsearch = None
-        for _ in range(20):
-            mid = (lo + hi) / 2.0
-            cand = feasible_assign_for_T(mid)
-            if cand is not None:
-                # Feasible at mid, try lower
-                hi = mid
-                best_bsearch = cand
-            else:
-                lo = mid
-        if best_bsearch is not None:
-            val = eval_max_kvpr(best_bsearch)
-            if val < best_val:
-                best_val = val
-                best_placement = best_bsearch
-
-    # Final local refinement
-    refined = local_refine(best_placement, move_budget=20, swap_budget=10)
-
-    # Final check: memory safety
+    refined = lns_refine(best_placement, max_rounds=6, move_cap=24, swap_cap=12)
+
+    # Final safety: ensure no GPU exceeds memory
     for gid in range(gpu_num):
         mem = sum(float(m.model_size) for m in refined.get(gid, []))
         if mem - S > 1e-6:
+            # If ever violated, return best_placement (shouldn't happen)
             return best_placement
 
     return refined
 
 # EVOLVE-BLOCK-END
 
 
 def run_placement(gpu_num, models):
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         gpu_num: Number of GPUs
         models: List of models to place
 
     Returns:
         Dictionary containing GPU placements
     """
     return compute_model_placement(gpu_num, models)
 
 
 if __name__ == "__main__":
     # Test the algorithm
     import os
     import sys
 
     # Add the openevolve_examples directory to the path to import evaluator
     def find_repo_root(start_path):
         """Find the repository root by looking for openevolve_examples directory."""
         current = os.path.abspath(start_path)
         while current != os.path.dirname(current):  # Stop at filesystem root
             if os.path.exists(os.path.join(current, 'openevolve_examples', 'prism')):
                 return current
             current = os.path.dirname(current)
         raise RuntimeError("Could not find openevolve_examples directory")
 
     repo_root = find_repo_root(os.path.dirname(__file__))
     sys.path.insert(0, os.path.join(repo_root, 'openevolve_examples', 'prism'))
 
     from evaluator import generate_test_gpu_models, calculate_kvcache_pressure, safe_float
     import numpy as np
 
     test_cases = generate_test_gpu_models()
     all_kvpr = []
     for i, (gpu_num, gpu_models) in enumerate(test_cases):
         results = compute_model_placement(gpu_num, gpu_models)
         max_kvpr = calculate_kvcache_pressure(results)
         all_kvpr.append(safe_float(max_kvpr))
 
     avg_kvpr = np.mean(all_kvpr)
     if avg_kvpr != 0:
         avg_kvpr = 1.0 / avg_kvpr
 
     print(f"Max KVPR: {avg_kvpr:.3f}")