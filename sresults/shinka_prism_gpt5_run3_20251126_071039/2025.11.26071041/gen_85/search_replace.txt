<NAME>
greedy_minmax_candidate_and_lexitiebreak
</NAME>

<DESCRIPTION>
I added a new greedy min-max KVPR candidate constructor that directly minimizes the projected global maximum KVPR at each step without relying on the parametric T framework. This candidate complements existing strategies (T-based, regret insertion, and memory-pack variants) and can find better placements in cases where regret ordering stalls or T-based residuals misguide. Additionally, I refined the final candidate selection to compare full lexicographic KVPR vectors across GPUs rather than only the top-two and average, providing a stricter tie-break that consistently favors more evenly balanced placements. These changes are lightweight, maintain simplicity, and preserve correctness while potentially reducing the maximum KVPR and improving robustness across instances.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    # Regret-based insertion tailored for min-max KVPR
    def regret_insertion():
        placement = {g: [] for g in range(gpu_num)}
        rem = [GPU_MEM_SIZE] * gpu_num
        numer = [0.0] * gpu_num

        unassigned = list(items)

        # Precompute top1/top2 of current kvprs for O(1) max-except calculations
        def top12(vals):
            top = (-1, -float('inf'))
            second = (-1, -float('inf'))
            for i, v in enumerate(vals):
                if v > top[1]:
                    second = top
                    top = (i, v)
                elif v > second[1]:
                    second = (i, v)
            return top, second

        # Iteratively insert models
        while unassigned:
            current_kvprs = [kvpr(numer[g], rem[g]) for g in range(gpu_num)]
            (top_idx, top_val), (sec_idx, sec_val) = top12(current_kvprs)

            best_model = None
            best_gpu = None
            best_new_max = float('inf')
            best_regret = -float('inf')

            # Evaluate regret for each model
            for (mdl, ms, dn) in unassigned:
                feasible = []
                for g in range(gpu_num):
                    if ms <= rem[g]:
                        new_local = kvpr(numer[g] + dn, rem[g] - ms)
                        base_other = top_val if g != top_idx else sec_val
                        new_max = new_local if new_local > base_other else base_other
                        feasible.append((g, new_max, new_local))
                if not feasible:
                    continue

                feasible.sort(key=lambda x: (x[1], x[2]))  # sort by new_max then local
                best = feasible[0]
                second = feasible[1] if len(feasible) > 1 else (None, float('inf'), float('inf'))
                regret = second[1] - best[1]  # larger regret => more critical

                # Choose the model with largest regret, then smaller best new_max
                if (regret > best_regret or
                    (regret == best_regret and best[1] < best_new_max)):
                    best_regret = regret
                    best_new_max = best[1]
                    best_model = (mdl, ms, dn)
                    best_gpu = best[0]

            if best_model is None:
                # No feasible candidate in this step; fail to allow fallback
                return None

            # Commit placement
            mdl, ms, dn = best_model
            placement[best_gpu].append(mdl)
            rem[best_gpu] -= ms
            numer[best_gpu] += dn
            unassigned.remove(best_model)

        return placement
=======
    # Regret-based insertion tailored for min-max KVPR
    def regret_insertion():
        placement = {g: [] for g in range(gpu_num)}
        rem = [GPU_MEM_SIZE] * gpu_num
        numer = [0.0] * gpu_num

        unassigned = list(items)

        # Precompute top1/top2 of current kvprs for O(1) max-except calculations
        def top12(vals):
            top = (-1, -float('inf'))
            second = (-1, -float('inf'))
            for i, v in enumerate(vals):
                if v > top[1]:
                    second = top
                    top = (i, v)
                elif v > second[1]:
                    second = (i, v)
            return top, second

        # Iteratively insert models
        while unassigned:
            current_kvprs = [kvpr(numer[g], rem[g]) for g in range(gpu_num)]
            (top_idx, top_val), (sec_idx, sec_val) = top12(current_kvprs)

            best_model = None
            best_gpu = None
            best_new_max = float('inf')
            best_regret = -float('inf')

            # Evaluate regret for each model
            for (mdl, ms, dn) in unassigned:
                feasible = []
                for g in range(gpu_num):
                    if ms <= rem[g]:
                        new_local = kvpr(numer[g] + dn, rem[g] - ms)
                        base_other = top_val if g != top_idx else sec_val
                        new_max = new_local if new_local > base_other else base_other
                        feasible.append((g, new_max, new_local))
                if not feasible:
                    continue

                feasible.sort(key=lambda x: (x[1], x[2]))  # sort by new_max then local
                best = feasible[0]
                second = feasible[1] if len(feasible) > 1 else (None, float('inf'), float('inf'))
                regret = second[1] - best[1]  # larger regret => more critical

                # Choose the model with largest regret, then smaller best new_max
                if (regret > best_regret or
                    (regret == best_regret and best[1] < best_new_max)):
                    best_regret = regret
                    best_new_max = best[1]
                    best_model = (mdl, ms, dn)
                    best_gpu = best[0]

            if best_model is None:
                # No feasible candidate in this step; fail to allow fallback
                return None

            # Commit placement
            mdl, ms, dn = best_model
            placement[best_gpu].append(mdl)
            rem[best_gpu] -= ms
            numer[best_gpu] += dn
            unassigned.remove(best_model)

        return placement

    # Greedy min-max KVPR placement: place each model to minimize projected global max KVPR
    def greedy_minmax():
        placement = {g: [] for g in range(gpu_num)}
        rem = [GPU_MEM_SIZE] * gpu_num
        numer = [0.0] * gpu_num

        # Order by intrinsic KVPR priority: n / (80 - m)
        ordered = sorted(items, key=lambda it: (safe_div(it[2], max(GPU_MEM_SIZE - it[1], 1e-9)), it[1]), reverse=True)

        for mdl, ms, dn in ordered:
            best = None
            # Current per-GPU KVPRs
            cur = [kvpr(numer[g], rem[g]) for g in range(gpu_num)]
            for g in range(gpu_num):
                if ms > rem[g]:
                    continue
                new_local = kvpr(numer[g] + dn, rem[g] - ms)
                # Project new global max if placed on g
                new_max = new_local
                for h in range(gpu_num):
                    if h != g and cur[h] > new_max:
                        new_max = cur[h]
                # Prefer smaller new_max, then smaller local KVPR, then more remaining memory, then smaller id
                key = (new_max, new_local, -(rem[g] - ms), g)
                if best is None or key < best[0]:
                    best = (key, g)

            if best is None:
                return None

            g = best[1]
            placement[g].append(mdl)
            rem[g] -= ms
            numer[g] += dn

        return placement
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
    # Candidate A: regret-based insertion (KVPR-aware)
    plc_regret = regret_insertion()
    if plc_regret is not None:
        candidates.append(plc_regret)

    # Candidate B: memory-balanced dual strategy
    plc_dual = memory_pack(order="size_desc", strategy="dual")
    if plc_dual is not None:
        candidates.append(plc_dual)

    # Candidate C: ratio-desc best-fit to diversify memory-rate density packing
    plc_ratio = memory_pack(order="ratio_desc", strategy="bestfit")
    if plc_ratio is not None:
        candidates.append(plc_ratio)
=======
    # Candidate A: regret-based insertion (KVPR-aware)
    plc_regret = regret_insertion()
    if plc_regret is not None:
        candidates.append(plc_regret)

    # Candidate B: memory-balanced dual strategy
    plc_dual = memory_pack(order="size_desc", strategy="dual")
    if plc_dual is not None:
        candidates.append(plc_dual)

    # Candidate C: ratio-desc best-fit to diversify memory-rate density packing
    plc_ratio = memory_pack(order="ratio_desc", strategy="bestfit")
    if plc_ratio is not None:
        candidates.append(plc_ratio)

    # Candidate D: greedy minmax KVPR placement
    plc_greedy = greedy_minmax()
    if plc_greedy is not None:
        candidates.append(plc_greedy)
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
    # Select best by lexicographic (max, second, avg) KVPR to break ties robustly
    def score_tuple(plc):
        kvprs = []
        for g in range(gpu_num):
            used_mem = sum(getattr(m, "model_size") for m in plc.get(g, []))
            numer = sum((getattr(m, "req_rate") / getattr(m, "slo")) for m in plc.get(g, []))
            kvprs.append(kvpr(numer, GPU_MEM_SIZE - used_mem))
        if not kvprs:
            return (0.0, 0.0, 0.0)
        kvprs_sorted = sorted(kvprs, reverse=True)
        max_k = kvprs_sorted[0]
        second = kvprs_sorted[1] if len(kvprs_sorted) > 1 else kvprs_sorted[0]
        avg = sum(kvprs_sorted) / len(kvprs_sorted)
        return (max_k, second, avg)
=======
    # Select best by full lexicographic KVPR vector (descending); finer than using only top-2 and avg
    def score_tuple(plc):
        kvprs = []
        for g in range(gpu_num):
            used_mem = sum(getattr(m, "model_size") for m in plc.get(g, []))
            numer = sum((getattr(m, "req_rate") / getattr(m, "slo")) for m in plc.get(g, []))
            kvprs.append(kvpr(numer, GPU_MEM_SIZE - used_mem))
        if not kvprs:
            return (0.0,)
        kvprs_sorted = tuple(sorted(kvprs, reverse=True))
        return kvprs_sorted
>>>>>>> REPLACE

</DIFF>