<NAME>
wls_router_focus_and_clip
</NAME>

<DESCRIPTION>
I propose three targeted improvements to the router-level projection stage to increase counter accuracy and confidence calibration without destabilizing the system:

1) Adaptive router tolerance check: Skip the weighted projection when a router’s RX/TX sums are already within an adaptive imbalance tolerance. This prevents unnecessary adjustments and preserves correct signals—improving both accuracy and calibration.

2) Confidence-focused weighting: Concentrate corrections on low-confidence capacity. I reintroduce a focus set covering ~70% of the “correctable capacity” and boost weights outside this set, so high-confidence counters move less. This aligns adjustments with the most likely faulty measurements (low confidence), improving repair accuracy and avoiding over-reliance on any single interface.

3) Per-variable relative change clipping (±10%): Apply a uniform scaling to the computed WLS deltas when any variable exceeds the per-variable change cap. This guards against over-correction and keeps changes within realistic bounds, improving stability and confidence calibration. Clip hits are tracked to penalize over-aggressive changes in final confidence, aligning reported confidence with actual repair risk.

These changes are minimal and consistent with the rest of the algorithm. I added a few hyperparameters for the focus and clipping, and modified only the router projection stage. All other stages, including link hardening, bundle smoothing, re-sync with scaling guard, and confidence calibration remain unaffected.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    STRONG_SCALE_GUARD = 0.08   # guard threshold for re-sync skipping
    RESYNC_MAX_F = 0.40         # max fraction for one-sided nudge toward mean
    BUNDLE_DOM_FRAC = 0.60      # bundle dominance threshold on a side’s traffic
    INTRA_BUNDLE_CLIP = 0.05    # ±5% intra-bundle smoothing cap
    UNTOUCHED_BOOST = 0.02      # confidence boost for untouched well-synced counters
    CLIP_HIT_PENALTY = 0.95     # multiplicative penalty when strong scaling/clipping hit
=======
    STRONG_SCALE_GUARD = 0.08   # guard threshold for re-sync skipping
    RESYNC_MAX_F = 0.40         # max fraction for one-sided nudge toward mean
    BUNDLE_DOM_FRAC = 0.60      # bundle dominance threshold on a side’s traffic
    INTRA_BUNDLE_CLIP = 0.05    # ±5% intra-bundle smoothing cap
    UNTOUCHED_BOOST = 0.02      # confidence boost for untouched well-synced counters
    CLIP_HIT_PENALTY = 0.95     # multiplicative penalty when strong scaling/clipping hit
    # Targeted router correction focus and clipping
    WEIGHT_FOCUS = 0.70         # focus router corrections on lowest-confidence 70% capacity
    MID_TIER_BOOST = 1.5        # weight boost for 0.70–0.85 confidence (moves less)
    OUTSIDE_FOCUS_BOOST = 2.0   # weight boost for ≥0.85 confidence (moves least)
    PER_VAR_REL_CLIP = 0.10     # ±10% relative change cap per variable in router stage
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
    for r, ifs in router_ifaces.items():
        up_ifs = [i for i in ifs if status.get(i) == 'up']
        if len(up_ifs) < 2:
            continue

        # Build variable vectors
        vals: List[float] = []
        signs: List[int] = []  # +1 for rx, -1 for tx
        idx_map: List[Tuple[str, str]] = []  # (interface_id, 'rx'/'tx')
        weights: List[float] = []

        # Compose rx variables
        for i in up_ifs:
            v = rx[i]
            vals.append(v)
            signs.append(+1)
            idx_map.append((i, 'rx'))
            # Trust weight: higher for confident and high-rate counters
            c = clamp01(conf_rx.get(i, 0.7))
            # Two-tier adjustment (low and mid confidence make smaller weights => larger correction)
            tier = 1.0
            if c < 0.70:
                tier = 0.6
            elif c < 0.85:
                tier = 0.8
            # Weight proportional to confidence^2 and inversely to magnitude (to make relative changes)
            w = max(1e-6, (c**2) * tier / max(v, ZERO_THRESH))
            # Protect near-zero from taking huge absolute correction
            if v < ZERO_THRESH:
                w *= 3.0
            weights.append(w)

        # Compose tx variables
        for i in up_ifs:
            v = tx[i]
            vals.append(v)
            signs.append(-1)
            idx_map.append((i, 'tx'))
            c = clamp01(conf_tx.get(i, 0.7))
            tier = 1.0
            if c < 0.70:
                tier = 0.6
            elif c < 0.85:
                tier = 0.8
            w = max(1e-6, (c**2) * tier / max(v, ZERO_THRESH))
            if v < ZERO_THRESH:
                w *= 3.0
            weights.append(w)

        # Compute lambda for weighted projection
        # a^T x0 = sum(signs[i] * vals[i])
        ax0 = 0.0
        denom = 0.0  # a^T W^{-1} a = sum( (sign^2)/w_i ) = sum(1/w_i)
        inv_weights = [1.0 / max(1e-12, w) for w in weights]
        for sgn, v, invw in zip(signs, vals, inv_weights):
            ax0 += sgn * v
            denom += invw  # since sgn^2 = 1

        if denom <= 1e-12:
            continue

        # Damped Lagrange multiplier
        lam = 2.0 * ax0 / denom
        lam *= DAMP_ROUTER

        # Initial deltas
        deltas = [-(invw) * sgn * lam * 0.5 for sgn, invw in zip(signs, inv_weights)]
        # Note: derived from x* = x0 - 0.5 W^{-1} λ a, with λ as above (post damping)

        # Dominance cap iteration: avoid one interface taking >50% of correction
        def apply_dominance_cap(deltas_in: List[float], invw_in: List[float]) -> List[float]:
            # Compute shares by absolute delta proportional to invw (since all deltas share same lam)
            for _ in range(3):
                absd = [abs(d) for d in deltas_in]
                total = sum(absd) + 1e-12
                shares = [d / total for d in absd]
                max_share = max(shares)
                if max_share <= DOMINANCE_CAP + 1e-6:
                    break
                k = shares.index(max_share)
                # Increase weight (decrease inv weight) for offender
                invw_in[k] *= (max_share / DOMINANCE_CAP)  # reduce its share
                # Recompute deltas with modified inverse weights
                denom_new = sum(invw_in)
                if denom_new <= 1e-12:
                    break
                lam_new = 2.0 * ax0 / denom_new
                lam_new *= DAMP_ROUTER
                deltas_in = [-(invw) * sgn * lam_new * 0.5 for sgn, invw in zip(signs, invw_in)]
            return deltas_in

        deltas = apply_dominance_cap(deltas, inv_weights[:])

        # Apply deltas
        for (i, side), dv, v_old in zip(idx_map, deltas, vals):
            v_new = max(0.0, v_old + dv)
            if side == 'rx':
                prev = rx[i]
                rx[i] = v_new
                if prev > ZERO_EPS and v_new >= 0.0:
                    scl = v_new / prev if prev > 0 else 1.0
                    scaled_rx_factor[i] *= scl
                    if abs(scl - 1.0) >= 0.10:
                        clip_hit[i] = True
            else:
                prev = tx[i]
                tx[i] = v_new
                if prev > ZERO_EPS and v_new >= 0.0:
                    scl = v_new / prev if prev > 0 else 1.0
                    scaled_tx_factor[i] *= scl
                    if abs(scl - 1.0) >= 0.10:
                        clip_hit[i] = True
=======
    for r, ifs in router_ifaces.items():
        up_ifs = [i for i in ifs if status.get(i) == 'up']
        if len(up_ifs) < 2:
            continue

        # Skip projection if router already close to balanced (adaptive tolerance)
        sum_rx_r = sum(rx[i] for i in up_ifs)
        sum_tx_r = sum(tx[i] for i in up_ifs)
        denom_r = max(1.0, sum_rx_r, sum_tx_r)
        rel_gap_r = abs(sum_rx_r - sum_tx_r) / denom_r
        n_active = len(up_ifs)
        tau_router = min(0.07, max(0.03, 0.05 * math.sqrt(2.0 / max(2, n_active))))
        if rel_gap_r <= tau_router:
            continue

        # Build variable vectors
        vals: List[float] = []
        signs: List[int] = []  # +1 for rx, -1 for tx
        idx_map: List[Tuple[str, str]] = []  # (interface_id, 'rx'/'tx')
        weights: List[float] = []

        # Compose rx variables
        for i in up_ifs:
            v = rx[i]
            vals.append(v)
            signs.append(+1)
            idx_map.append((i, 'rx'))
            # Trust weight: higher for confident and high-rate counters moves less
            c = clamp01(conf_rx.get(i, 0.7))
            tier = 1.0
            if c < 0.70:
                tier = 0.6
            elif c < 0.85:
                tier = 0.8
            # Weight proportional to confidence^2 and inversely to magnitude (relative changes)
            w = max(1e-6, (c**2) * tier / max(v, ZERO_THRESH))
            # Protect near-zero from taking huge absolute correction
            if v < ZERO_THRESH:
                w *= 3.0
            weights.append(w)

        # Compose tx variables
        for i in up_ifs:
            v = tx[i]
            vals.append(v)
            signs.append(-1)
            idx_map.append((i, 'tx'))
            c = clamp01(conf_tx.get(i, 0.7))
            tier = 1.0
            if c < 0.70:
                tier = 0.6
            elif c < 0.85:
                tier = 0.8
            w = max(1e-6, (c**2) * tier / max(v, ZERO_THRESH))
            if v < ZERO_THRESH:
                w *= 3.0
            weights.append(w)

        # Confidence-focused adjustment: boost weights outside the low-confidence focus set
        # so they move less; concentrate corrections on low-confidence/high-rate variables.
        capacities: List[float] = []
        for (i, side), v in zip(idx_map, vals):
            cdir = conf_rx.get(i, 0.7) if side == 'rx' else conf_tx.get(i, 0.7)
            capacities.append((1.0 - clamp01(cdir)) * max(v, ZERO_THRESH) + 1e-12)
        total_cap = sum(capacities)
        order = sorted(range(len(capacities)), key=lambda k: capacities[k], reverse=True)
        focus_set = set()
        acc = 0.0
        for k in order:
            if total_cap > 0.0 and acc / total_cap >= WEIGHT_FOCUS:
                break
            focus_set.add(k)
            acc += capacities[k]
        if not focus_set:
            focus_set = set(range(len(capacities)))

        # Apply boosts: mid-tier and high confidence outside focus move less
        for idx in range(len(weights)):
            if idx not in focus_set:
                i, side = idx_map[idx]
                cdir = conf_rx.get(i, 0.7) if side == 'rx' else conf_tx.get(i, 0.7)
                if cdir >= 0.85:
                    weights[idx] *= OUTSIDE_FOCUS_BOOST
                elif cdir >= 0.70:
                    weights[idx] *= MID_TIER_BOOST

        # Compute lambda for weighted projection
        ax0 = 0.0
        denom = 0.0  # a^T W^{-1} a = sum(1/w_i) since sign^2 = 1
        inv_weights = [1.0 / max(1e-12, w) for w in weights]
        for sgn, v, invw in zip(signs, vals, inv_weights):
            ax0 += sgn * v
            denom += invw

        if denom <= 1e-12:
            continue

        # Damped Lagrange multiplier
        lam = 2.0 * ax0 / denom
        lam *= DAMP_ROUTER

        # Initial deltas
        deltas = [-(invw) * sgn * lam * 0.5 for sgn, invw in zip(signs, inv_weights)]
        # Note: derived from x* = x0 - 0.5 W^{-1} λ a, with λ as above (post damping)

        # Dominance cap iteration: avoid one interface taking >50% of correction
        def apply_dominance_cap(deltas_in: List[float], invw_in: List[float]) -> List[float]:
            for _ in range(3):
                absd = [abs(d) for d in deltas_in]
                total = sum(absd) + 1e-12
                shares = [d / total for d in absd]
                max_share = max(shares)
                if max_share <= DOMINANCE_CAP + 1e-6:
                    break
                k = shares.index(max_share)
                invw_in[k] *= (max_share / DOMINANCE_CAP)  # reduce its share
                denom_new = sum(invw_in)
                if denom_new <= 1e-12:
                    break
                lam_new = 2.0 * ax0 / denom_new
                lam_new *= DAMP_ROUTER
                deltas_in = [-(invw) * sgn * lam_new * 0.5 for sgn, invw in zip(signs, invw_in)]
            return deltas_in

        deltas = apply_dominance_cap(deltas, inv_weights[:])

        # Per-variable relative change clipping (±10%); scale all deltas uniformly if needed
        gamma = 1.0
        for dv, v_old in zip(deltas, vals):
            cap_abs = PER_VAR_REL_CLIP * max(v_old, ZERO_THRESH)
            if abs(dv) > cap_abs:
                gamma = min(gamma, cap_abs / max(abs(dv), 1e-12))
        if gamma < 1.0:
            deltas = [dv * gamma for dv in deltas]

        # Apply deltas
        for (i, side), dv, v_old in zip(idx_map, deltas, vals):
            cap_abs = PER_VAR_REL_CLIP * max(v_old, ZERO_THRESH)
            if abs(dv) >= cap_abs - 1e-12:
                clip_hit[i] = True
            v_new = max(0.0, v_old + dv)
            if side == 'rx':
                prev = rx[i]
                rx[i] = v_new
                if prev > ZERO_EPS and v_new >= 0.0:
                    scl = v_new / prev if prev > 0 else 1.0
                    scaled_rx_factor[i] *= scl
                    if abs(scl - 1.0) >= 0.10:
                        clip_hit[i] = True
            else:
                prev = tx[i]
                tx[i] = v_new
                if prev > ZERO_EPS and v_new >= 0.0:
                    scl = v_new / prev if prev > 0 else 1.0
                    scaled_tx_factor[i] *= scl
                    if abs(scl - 1.0) >= 0.10:
                        clip_hit[i] = True
>>>>>>> REPLACE