--- a/original.py
+++ b/original.py
@@ -1,531 +1,645 @@
 # EVOLVE-BLOCK-START
 """
 Bundle-aware consensus projection for network telemetry repair.
 
 Algorithm summary:
 1) Robust link hardening with adaptive tolerance and soft-zero snapping.
 2) Router flow conservation via targeted, confidence-weighted corrections.
    - Bundle-aware scaling for parallel links (shared factor).
    - Per-interface clipping ±10% and 60% damping.
 3) Confidence-gap-proportional re-sync on links with scaling guard.
 4) Confidence calibrated from measurement residuals, link residuals, router residuals,
    plus a scale-factor term and peer smoothing.
 
 Maintains inputs/outputs of the original function.
 """
 from typing import Dict, Any, Tuple, List
 
 
 def repair_network_telemetry(telemetry: Dict[str, Dict[str, Any]],
                              topology: Dict[str, List[str]]) -> Dict[str, Dict[str, Tuple]]:
     # Hyperparameters
     TAU_H_BASE = 0.02        # ~2% hardening threshold
     ZERO_EPS = 1e-6
     ZERO_THRESH = 1.0        # Mbps near-zero threshold
     DAMP_ROUTER = 0.60       # router damping factor
     PER_LINK_CLIP = 0.10     # per-interface relative change cap (±10%)
     BUNDLE_CLIP = 0.15       # bundle shared factor cap (±15%)
     STRONG_SCALE_GUARD = 0.08  # guard for re-sync when strong router scaling applied
     RESYNC_MAX_F = 0.40      # max one-sided nudge toward mean
     PEER_SMOOTH = 0.10       # 10% peer smoothing
+    WEIGHT_FOCUS = 0.70      # focus router corrections on lowest-confidence 70% weight
+    DOMINANCE_CAP = 0.50     # cap any single interface's share of a pass's correction to ≤50%
+    CLIP_HIT_PENALTY = 0.95  # confidence penalty multiplier when strong scaling/clipping hit
+    UNTOUCHED_BOOST = 0.02   # confidence boost for untouched, well-synced counters
 
     def clamp01(x: float) -> float:
         return max(0.0, min(1.0, x))
 
     def rel_diff(a: float, b: float) -> float:
         return abs(a - b) / max(1.0, abs(a), abs(b))
 
     def adaptive_tau(v1: float, v2: float) -> float:
         # Adaptive symmetry tolerance:
         # tighter for high rates, looser for low/near-zero or low confidence regions.
         if v1 >= 100.0 and v2 >= 100.0:
             return 0.015
         if v1 < ZERO_THRESH or v2 < ZERO_THRESH:
             return 0.03
         return TAU_H_BASE
 
     # Build basic maps
     orig_rx: Dict[str, float] = {}
     orig_tx: Dict[str, float] = {}
     status: Dict[str, str] = {}
     peer_of: Dict[str, str] = {}
     local_router_of: Dict[str, Any] = {}
     remote_router_of: Dict[str, Any] = {}
 
     for iid, d in telemetry.items():
         orig_rx[iid] = float(d.get('rx_rate', 0.0))
         orig_tx[iid] = float(d.get('tx_rate', 0.0))
         status[iid] = d.get('interface_status', 'unknown')
         ct = d.get('connected_to')
         peer_of[iid] = ct if ct in telemetry else None
         local_router_of[iid] = d.get('local_router')
         remote_router_of[iid] = d.get('remote_router')
 
     # Build router->interfaces mapping, prefer provided topology
     router_ifaces: Dict[str, List[str]] = {}
     if topology:
         for r, ifs in topology.items():
             router_ifaces[r] = [i for i in ifs if i in telemetry]
     else:
         # Fallback to local_router fields
         for iid, d in telemetry.items():
             r = d.get('local_router')
             if r is not None:
                 router_ifaces.setdefault(r, []).append(iid)
 
     # Derive missing remote_router via peer's local_router if needed
     for iid in telemetry:
         if not remote_router_of.get(iid):
             p = peer_of.get(iid)
             if p and p in telemetry:
                 remote_router_of[iid] = telemetry[p].get('local_router')
 
     # Build link pairs (unique, undirected)
     link_pairs: List[Tuple[str, str]] = []
     seen = set()
     for a in telemetry:
         b = peer_of.get(a)
         if not b or b not in telemetry or a == b:
             continue
         if (b, a) in seen or (a, b) in seen:
             continue
         seen.add((a, b))
         link_pairs.append((a, b))
 
     # Initialize hardened values with originals
     hardened_rx: Dict[str, float] = {i: max(0.0, orig_rx[i]) for i in telemetry}
     hardened_tx: Dict[str, float] = {i: max(0.0, orig_tx[i]) for i in telemetry}
     # Initialize confidences (will be calibrated later)
     conf_rx: Dict[str, float] = {i: 0.7 for i in telemetry}
     conf_tx: Dict[str, float] = {i: 0.7 for i in telemetry}
 
     # Track cumulative router scaling factors for re-sync guard and confidence
     scaled_rx_factor: Dict[str, float] = {i: 1.0 for i in telemetry}
     scaled_tx_factor: Dict[str, float] = {i: 1.0 for i in telemetry}
+    # Track whether a direction hit a clip bound in router stage
+    clip_hit_rx: Dict[str, bool] = {i: False for i in telemetry}
+    clip_hit_tx: Dict[str, bool] = {i: False for i in telemetry}
 
     # Stage 1: Robust link hardening with adaptive tolerance and soft-zero
     for a, b in link_pairs:
         a_up = (status.get(a) == 'up')
         b_up = (status.get(b) == 'up')
         a_rx, a_tx = orig_rx[a], orig_tx[a]
         b_rx, b_tx = orig_rx[b], orig_tx[b]
 
         # If either side is down: force both to zero on the link with high confidence
         if not a_up or not b_up:
             for i in (a, b):
                 hardened_rx[i] = 0.0
                 hardened_tx[i] = 0.0
                 conf_rx[i] = max(conf_rx[i], 0.85)
                 conf_tx[i] = max(conf_tx[i], 0.85)
             continue
 
         # Soft-zero stabilization for near-zero links
         if max(a_rx, a_tx, b_rx, b_tx) < 2.0 * ZERO_THRESH:
             for i in (a, b):
                 hardened_rx[i] = 0.0
                 hardened_tx[i] = 0.0
                 conf_rx[i] = max(conf_rx[i], 0.95)
                 conf_tx[i] = max(conf_tx[i], 0.95)
             continue
 
         # Direction 1: a.tx vs b.rx
         d1 = rel_diff(a_tx, b_rx)
         tau1 = adaptive_tau(a_tx, b_rx)
         if d1 <= tau1:
             v1 = 0.5 * (a_tx + b_rx)
             hardened_tx[a] = v1
             hardened_rx[b] = v1
             c1 = clamp01(0.9 + 0.1 * (1.0 - d1 / max(tau1, 1e-12)))
             conf_tx[a] = max(conf_tx[a], c1)
             conf_rx[b] = max(conf_rx[b], c1)
         else:
             # Pick the observation closer to both sides' own measurements to avoid bias
             # With two points, robustly choose the one farther from zero and rely on redundancy
             choice = b_rx if abs(b_rx) >= abs(a_tx) else a_tx
             hardened_tx[a] = max(0.0, choice)
             hardened_rx[b] = max(0.0, choice)
             c1 = clamp01(1.0 - d1)
             conf_tx[a] = max(conf_tx[a], c1)
             conf_rx[b] = max(conf_rx[b], c1)
 
         # Direction 2: a.rx vs b.tx
         d2 = rel_diff(a_rx, b_tx)
         tau2 = adaptive_tau(a_rx, b_tx)
         if d2 <= tau2:
             v2 = 0.5 * (a_rx + b_tx)
             hardened_rx[a] = v2
             hardened_tx[b] = v2
             c2 = clamp01(0.9 + 0.1 * (1.0 - d2 / max(tau2, 1e-12)))
             conf_rx[a] = max(conf_rx[a], c2)
             conf_tx[b] = max(conf_tx[b], c2)
         else:
             choice = b_tx if abs(b_tx) >= abs(a_rx) else a_rx
             hardened_rx[a] = max(0.0, choice)
             hardened_tx[b] = max(0.0, choice)
             c2 = clamp01(1.0 - d2)
             conf_rx[a] = max(conf_rx[a], c2)
             conf_tx[b] = max(conf_tx[b], c2)
 
     # Unpaired interfaces: trust local with moderate confidence; zero if down
     in_pairs = {x for pair in link_pairs for x in pair}
     for i in telemetry:
         if i not in in_pairs:
             if status.get(i) != 'up':
                 hardened_rx[i] = 0.0
                 hardened_tx[i] = 0.0
                 conf_rx[i] = max(conf_rx[i], 0.85)
                 conf_tx[i] = max(conf_tx[i], 0.85)
             else:
                 hardened_rx[i] = max(0.0, orig_rx[i])
                 hardened_tx[i] = max(0.0, orig_tx[i])
                 conf_rx[i] = max(conf_rx[i], 0.6)
                 conf_tx[i] = max(conf_tx[i], 0.6)
 
     # Stage 2: Router flow conservation with targeted, bundle-aware corrections
     for r, ifs in router_ifaces.items():
         if not ifs:
             continue
         up_ifs = [i for i in ifs if status.get(i) == 'up']
         if len(up_ifs) < 2:
             continue
 
         sum_rx = sum(hardened_rx[i] for i in up_ifs)
         sum_tx = sum(hardened_tx[i] for i in up_ifs)
         denom = max(1.0, sum_rx, sum_tx)
         imbalance = (sum_rx - sum_tx)  # positive means rx > tx
         rel_gap = abs(imbalance) / denom
 
         # Adaptive router tolerance based on number of active interfaces
         n_active = len(up_ifs)
         tau_router = min(0.07, max(0.03, 0.05 * (2.0 / max(2, n_active)) ** 0.5))
         if rel_gap <= tau_router:
             continue
 
         # Choose side with lower aggregate confidence to adjust
         avg_rx_conf = sum(conf_rx[i] for i in up_ifs) / len(up_ifs)
         avg_tx_conf = sum(conf_tx[i] for i in up_ifs) / len(up_ifs)
         adjust_side = 'rx' if avg_rx_conf < avg_tx_conf else 'tx'
         total_adjust = (-imbalance if adjust_side == 'rx' else imbalance) * DAMP_ROUTER
 
         # Build per-interface values and confidences for the chosen side
         side_vals = {i: (hardened_rx[i] if adjust_side == 'rx' else hardened_tx[i]) for i in up_ifs}
         side_confs = {i: (conf_rx[i] if adjust_side == 'rx' else conf_tx[i]) for i in up_ifs}
 
         # Group interfaces into bundles by (local_router, remote_router)
         # Determine remote router with fallback via peer's local router
         bundle_map: Dict[Tuple[Any, Any], List[str]] = {}
         for i in up_ifs:
             lr = local_router_of.get(i)
             rr = remote_router_of.get(i)
             if not rr:
                 p = peer_of.get(i)
                 if p:
                     rr = local_router_of.get(p)
             key = (lr, rr)
             bundle_map.setdefault(key, []).append(i)
 
         side_total = sum(side_vals.values())
         # Identify majority bundles (>=50% of side traffic)
         majority_bundles = []
         for key, members in bundle_map.items():
             s = sum(side_vals[m] for m in members)
             if side_total > ZERO_EPS and s >= 0.5 * side_total:
                 majority_bundles.append((key, members, s))
 
         # Build weights for distribution: w_i = (1 - conf) * max(val, ZERO_THRESH)
         weights = {i: (1.0 - clamp01(side_confs[i])) * max(side_vals[i], ZERO_THRESH) + 1e-9 for i in up_ifs}
         total_w = sum(weights.values())
         if total_w <= 0:
             weights = {i: 1.0 for i in up_ifs}
             total_w = float(len(up_ifs))
 
+        # Focus adjustments on the lowest-confidence subset covering WEIGHT_FOCUS of total weight
+        sorted_ifs = sorted(up_ifs, key=lambda x: weights[x], reverse=True)
+        focus_set: List[str] = []
+        acc = 0.0
+        for i in sorted_ifs:
+            if acc / max(total_w, 1e-12) >= WEIGHT_FOCUS:
+                break
+            focus_set.append(i)
+            acc += weights[i]
+        if not focus_set:
+            focus_set = list(up_ifs)
+            acc = total_w
+        focus_total_w = max(acc, 1e-9)
+
         # Apply bundle-aware scaling if there is a majority bundle; else per-interface adjustments
         if majority_bundles:
-            # Distribute total_adjust across bundles proportionally to their combined weights
-            # Compute per-bundle weight as sum of member weights
+            # Compute per-bundle weight and focused members
             bundle_weights: Dict[Tuple[Any, Any], float] = {}
+            bundle_members_focused: Dict[Tuple[Any, Any], List[str]] = {}
             for key, members, _ in majority_bundles:
-                bundle_weights[key] = sum(weights[m] for m in members)
-            # Remaining non-majority as one group (per-interface below)
-            major_total_w = sum(bundle_weights.values())
-
-            # First, scale majority bundles with a shared factor per bundle
-            for key, members, s_sum in majority_bundles:
+                focused_members = [m for m in members if m in focus_set]
+                if not focused_members:
+                    continue
+                bundle_members_focused[key] = focused_members
+                bundle_weights[key] = sum(weights[m] for m in focused_members)
+
+            # First, scale majority bundles with a shared factor per bundle on focused members only
+            for key, members, _ in majority_bundles:
+                focused_members = bundle_members_focused.get(key, [])
+                if not focused_members:
+                    continue
                 w_g = bundle_weights.get(key, 0.0)
-                if major_total_w <= 0 or s_sum <= ZERO_EPS:
+                if w_g <= 0.0:
                     continue
-                adj_g = total_adjust * (w_g / total_w)  # use global total_w to preserve proportionality
-                target_sum = max(0.0, s_sum + adj_g)
-                scale_g = target_sum / s_sum
+                adj_g = total_adjust * (w_g / focus_total_w)
+                s_sum_focus = sum(side_vals[m] for m in focused_members)
+                if s_sum_focus <= ZERO_EPS:
+                    continue
+                target_sum = max(0.0, s_sum_focus + adj_g)
+                scale_g = target_sum / s_sum_focus
                 # Clip group scale to [0.85, 1.15]
-                scale_g = max(1.0 - BUNDLE_CLIP, min(1.0 + BUNDLE_CLIP, scale_g))
-                for m in members:
+                clipped = False
+                if scale_g > 1.0 + BUNDLE_CLIP:
+                    scale_g = 1.0 + BUNDLE_CLIP
+                    clipped = True
+                elif scale_g < 1.0 - BUNDLE_CLIP:
+                    scale_g = 1.0 - BUNDLE_CLIP
+                    clipped = True
+                for m in focused_members:
                     old = side_vals[m]
                     new = max(0.0, scale_g * old)
                     if adjust_side == 'rx':
                         if hardened_rx[m] > ZERO_EPS:
                             scaled_rx_factor[m] *= (new / hardened_rx[m])
                         hardened_rx[m] = new
                         # Confidence penalty proportional to relative change
                         relc = abs(new - old) / max(1.0, abs(old))
                         conf_rx[m] = clamp01(conf_rx[m] * (1.0 - 0.6 * relc))
+                        if clipped:
+                            clip_hit_rx[m] = True
                     else:
                         if hardened_tx[m] > ZERO_EPS:
                             scaled_tx_factor[m] *= (new / hardened_tx[m])
                         hardened_tx[m] = new
                         relc = abs(new - old) / max(1.0, abs(old))
                         conf_tx[m] = clamp01(conf_tx[m] * (1.0 - 0.6 * relc))
-
-            # Then, adjust remaining non-majority interfaces individually using leftover weights
-            non_majority = [i for i in up_ifs if all(i not in members for _, members, _ in majority_bundles)]
+                        if clipped:
+                            clip_hit_tx[m] = True
+
+            # Then, adjust remaining focused non-majority interfaces individually
+            all_major_members = [m for _, members, _ in majority_bundles for m in members]
+            non_majority_all = [i for i in up_ifs if i not in all_major_members]
+            non_majority = [i for i in non_majority_all if i in focus_set]
             nm_total_w = sum(weights[i] for i in non_majority)
             if nm_total_w > 0:
-                # Compute remaining adjustment share for non-majority
-                adj_nm = total_adjust * (nm_total_w / total_w)
+                # Apply dominance cap on weight shares within this pass
+                cap_per = DOMINANCE_CAP * nm_total_w
+                eff_weights = {i: min(weights[i], cap_per) for i in non_majority}
+                eff_total_w = max(1e-9, sum(eff_weights.values()))
+                adj_nm = total_adjust * (nm_total_w / focus_total_w)
                 for i in non_majority:
                     v_old = side_vals[i]
-                    w_i = weights[i] / nm_total_w
-                    adj_i = adj_nm * w_i
+                    w_i = eff_weights[i] / eff_total_w
+                    adj_i_raw = adj_nm * w_i
                     # Clip per-interface relative change ±10%
                     cap = PER_LINK_CLIP * v_old
-                    adj_i = min(max(adj_i, -cap), cap)
+                    adj_i = min(max(adj_i_raw, -cap), cap)
+                    if abs(adj_i) >= cap - 1e-12:
+                        if adjust_side == 'rx':
+                            clip_hit_rx[i] = True
+                        else:
+                            clip_hit_tx[i] = True
                     v_new = max(0.0, v_old + adj_i)
                     if adjust_side == 'rx':
                         if hardened_rx[i] > ZERO_EPS:
                             scaled_rx_factor[i] *= (v_new / hardened_rx[i])
                         hardened_rx[i] = v_new
                         relc = abs(adj_i) / max(1.0, abs(v_old))
                         conf_rx[i] = clamp01(conf_rx[i] * (1.0 - 0.6 * relc))
                     else:
                         if hardened_tx[i] > ZERO_EPS:
                             scaled_tx_factor[i] *= (v_new / hardened_tx[i])
                         hardened_tx[i] = v_new
                         relc = abs(adj_i) / max(1.0, abs(v_old))
                         conf_tx[i] = clamp01(conf_tx[i] * (1.0 - 0.6 * relc))
         else:
-            # No dominant bundle: targeted per-interface corrections only
-            for i in up_ifs:
+            # No dominant bundle: targeted per-interface corrections only over focus_set
+            # Apply dominance cap on per-interface weight shares within this pass
+            cap_per = DOMINANCE_CAP * focus_total_w
+            eff_weights = {i: min(weights[i], cap_per) for i in focus_set}
+            eff_total_w = max(1e-9, sum(eff_weights.values()))
+            for i in focus_set:
                 v_old = side_vals[i]
-                w_i = weights[i] / total_w
-                adj_i = total_adjust * w_i
+                w_i = eff_weights[i] / eff_total_w
+                adj_i_raw = total_adjust * w_i
                 cap = PER_LINK_CLIP * v_old
-                adj_i = min(max(adj_i, -cap), cap)
+                adj_i = min(max(adj_i_raw, -cap), cap)
+                if abs(adj_i) >= cap - 1e-12:
+                    if adjust_side == 'rx':
+                        clip_hit_rx[i] = True
+                    else:
+                        clip_hit_tx[i] = True
                 v_new = max(0.0, v_old + adj_i)
                 if adjust_side == 'rx':
                     if hardened_rx[i] > ZERO_EPS:
                         scaled_rx_factor[i] *= (v_new / hardened_rx[i])
                     hardened_rx[i] = v_new
                     relc = abs(adj_i) / max(1.0, abs(v_old))
                     conf_rx[i] = clamp01(conf_rx[i] * (1.0 - 0.6 * relc))
                 else:
                     if hardened_tx[i] > ZERO_EPS:
                         scaled_tx_factor[i] *= (v_new / hardened_tx[i])
                     hardened_tx[i] = v_new
                     relc = abs(adj_i) / max(1.0, abs(v_old))
                     conf_tx[i] = clamp01(conf_tx[i] * (1.0 - 0.6 * relc))
 
-    # Stage 3: Confidence-gap-proportional re-sync with scaling guard
+    # Stage 3: Confidence-gap-proportional re-sync with scaling guard and router attenuation
+    # Compute per-router residuals after router corrections (for attenuation/soft-zero)
+    router_residual_mid: Dict[str, float] = {}
+    for r, ifs in router_ifaces.items():
+        ups = [i for i in ifs if status.get(i) == 'up']
+        if not ups:
+            router_residual_mid[r] = 0.0
+        else:
+            srx = sum(hardened_rx[i] for i in ups)
+            stx = sum(hardened_tx[i] for i in ups)
+            denomr = max(1.0, srx, stx)
+            router_residual_mid[r] = abs(srx - stx) / denomr
+
+    # Soft-zero stabilization after router projection if adjacent routers are balanced
+    for a, b in link_pairs:
+        if status.get(a) != 'up' or status.get(b) != 'up':
+            continue
+        max_link = max(hardened_rx[a], hardened_tx[a], hardened_rx[b], hardened_tx[b])
+        if max_link < 2.0 * ZERO_THRESH:
+            ra = local_router_of.get(a)
+            rb = local_router_of.get(b)
+            # Adaptive router tolerances by active interfaces
+            na = len([i for i in router_ifaces.get(ra, []) if status.get(i) == 'up'])
+            nb = len([i for i in router_ifaces.get(rb, []) if status.get(i) == 'up'])
+            tau_ra = min(0.07, max(0.03, 0.05 * (2.0 / max(2, na)) ** 0.5))
+            tau_rb = min(0.07, max(0.03, 0.05 * (2.0 / max(2, nb)) ** 0.5))
+            if router_residual_mid.get(ra, 0.0) <= tau_ra and router_residual_mid.get(rb, 0.0) <= tau_rb:
+                hardened_rx[a] = hardened_tx[a] = 0.0
+                hardened_rx[b] = hardened_tx[b] = 0.0
+                conf_rx[a] = max(conf_rx[a], 0.95)
+                conf_tx[a] = max(conf_tx[a], 0.95)
+                conf_rx[b] = max(conf_rx[b], 0.95)
+                conf_tx[b] = max(conf_tx[b], 0.95)
+
     def nudge_toward_mean(val_lo: float, val_hi: float, frac: float) -> float:
         target = 0.5 * (val_lo + val_hi)
         return val_lo + frac * (target - val_lo)
 
     for a, b in link_pairs:
         if status.get(a) != 'up' or status.get(b) != 'up':
             continue
+
+        # Attenuation factor based on adjacent routers' imbalance
+        ra = local_router_of.get(a)
+        rb = local_router_of.get(b)
+        att = clamp01(1.0 - max(router_residual_mid.get(ra, 0.0), router_residual_mid.get(rb, 0.0)))
 
         # Direction 1: a.tx vs b.rx
         a_tx, b_rx = hardened_tx[a], hardened_rx[b]
         if max(a_tx, b_rx) > ZERO_EPS:
             d1 = rel_diff(a_tx, b_rx)
             tau1 = adaptive_tau(a_tx, b_rx)
             if d1 > tau1:
                 ca, cb = conf_tx.get(a, 0.6), conf_rx.get(b, 0.6)
                 # Skip if strong router scaling already applied on the target direction
                 if ca >= cb and abs(scaled_rx_factor.get(b, 1.0) - 1.0) <= STRONG_SCALE_GUARD:
-                    f = min(RESYNC_MAX_F, max(0.0, ca - cb))
+                    f = min(RESYNC_MAX_F, max(0.0, ca - cb)) * att
                     if f > 0.0:
                         old = b_rx
                         new = max(0.0, nudge_toward_mean(old, a_tx, f))
                         hardened_rx[b] = new
                         relc = rel_diff(new, old)
                         conf_rx[b] = clamp01(conf_rx[b] * (1.0 - 0.3 * relc))
                 elif cb > ca and abs(scaled_tx_factor.get(a, 1.0) - 1.0) <= STRONG_SCALE_GUARD:
-                    f = min(RESYNC_MAX_F, max(0.0, cb - ca))
+                    f = min(RESYNC_MAX_F, max(0.0, cb - ca)) * att
                     if f > 0.0:
                         old = a_tx
                         new = max(0.0, nudge_toward_mean(old, b_rx, f))
                         hardened_tx[a] = new
                         relc = rel_diff(new, old)
                         conf_tx[a] = clamp01(conf_tx[a] * (1.0 - 0.3 * relc))
 
         # Direction 2: a.rx vs b.tx
         a_rx, b_tx = hardened_rx[a], hardened_tx[b]
         if max(a_rx, b_tx) > ZERO_EPS:
             d2 = rel_diff(a_rx, b_tx)
             tau2 = adaptive_tau(a_rx, b_tx)
             if d2 > tau2:
                 ca, cb = conf_rx.get(a, 0.6), conf_tx.get(b, 0.6)
                 if ca >= cb and abs(scaled_tx_factor.get(b, 1.0) - 1.0) <= STRONG_SCALE_GUARD:
-                    f = min(RESYNC_MAX_F, max(0.0, ca - cb))
+                    f = min(RESYNC_MAX_F, max(0.0, ca - cb)) * att
                     if f > 0.0:
                         old = b_tx
                         new = max(0.0, nudge_toward_mean(old, a_rx, f))
                         hardened_tx[b] = new
                         relc = rel_diff(new, old)
                         conf_tx[b] = clamp01(conf_tx[b] * (1.0 - 0.3 * relc))
                 elif cb > ca and abs(scaled_rx_factor.get(a, 1.0) - 1.0) <= STRONG_SCALE_GUARD:
-                    f = min(RESYNC_MAX_F, max(0.0, cb - ca))
+                    f = min(RESYNC_MAX_F, max(0.0, cb - ca)) * att
                     if f > 0.0:
                         old = a_rx
                         new = max(0.0, nudge_toward_mean(old, b_tx, f))
                         hardened_rx[a] = new
                         relc = rel_diff(new, old)
                         conf_rx[a] = clamp01(conf_rx[a] * (1.0 - 0.3 * relc))
 
     # Enforce status down => zero as a final safety
     for i in telemetry:
         if status.get(i) != 'up':
             hardened_rx[i] = 0.0
             hardened_tx[i] = 0.0
             conf_rx[i] = max(conf_rx[i], 0.85)
             conf_tx[i] = max(conf_tx[i], 0.85)
 
     # Compute router residuals after all adjustments (for confidence calibration)
     router_residual: Dict[str, float] = {}
     for r, ifs in router_ifaces.items():
         up_ifs = [i for i in ifs if i in telemetry]
         if not up_ifs:
             router_residual[r] = 0.0
             continue
         sum_rx = sum(hardened_rx[i] for i in up_ifs)
         sum_tx = sum(hardened_tx[i] for i in up_ifs)
         denom = max(1.0, sum_rx, sum_tx)
         router_residual[r] = abs(sum_rx - sum_tx) / denom
 
     # Stage 4: Confidence calibration with scale penalty and peer smoothing
     def compute_conf(i: str) -> Tuple[float, float]:
         p = peer_of.get(i)
         # Measurement residuals
         r_meas_rx = rel_diff(hardened_rx[i], orig_rx[i])
         r_meas_tx = rel_diff(hardened_tx[i], orig_tx[i])
         # Link residuals
         if p and p in telemetry and status.get(i) == 'up' and status.get(p) == 'up':
             r_link_rx = rel_diff(hardened_rx[i], hardened_tx[p])
             r_link_tx = rel_diff(hardened_tx[i], hardened_rx[p])
         else:
             r_link_rx = 0.2
             r_link_tx = 0.2
         # Router residual
         rtr = router_residual.get(local_router_of.get(i), 0.0)
         # Base confidence blend
         base_rx = 1.0 - (0.55 * r_meas_rx + 0.35 * r_link_rx + 0.10 * rtr)
         base_tx = 1.0 - (0.55 * r_meas_tx + 0.35 * r_link_tx + 0.10 * rtr)
         base_rx = clamp01(base_rx)
         base_tx = clamp01(base_tx)
 
         # Scale-factor term (penalize big routed adjustments)
         alpha_rx = abs(scaled_rx_factor.get(i, 1.0) - 1.0)
         alpha_tx = abs(scaled_tx_factor.get(i, 1.0) - 1.0)
         scale_term_rx = clamp01(1.0 - min(0.5, alpha_rx))
         scale_term_tx = clamp01(1.0 - min(0.5, alpha_tx))
 
         c_rx = clamp01(0.90 * base_rx + 0.10 * scale_term_rx)
         c_tx = clamp01(0.90 * base_tx + 0.10 * scale_term_tx)
 
         # If interface is down, zero is a strong invariant; raise confidence floor
         if status.get(i) != 'up':
             c_rx = max(c_rx, 0.85)
             c_tx = max(c_tx, 0.85)
         return c_rx, c_tx
 
     # Compute confidences
     for i in telemetry:
         cr, ct = compute_conf(i)
         conf_rx[i], conf_tx[i] = cr, ct
+        # Clip-hit penalty when strong scaling occurred (>=10%) or cap hit
+        if abs(scaled_rx_factor.get(i, 1.0) - 1.0) >= 0.10 or clip_hit_rx.get(i, False):
+            conf_rx[i] = clamp01(conf_rx[i] * CLIP_HIT_PENALTY)
+        if abs(scaled_tx_factor.get(i, 1.0) - 1.0) >= 0.10 or clip_hit_tx.get(i, False):
+            conf_tx[i] = clamp01(conf_tx[i] * CLIP_HIT_PENALTY)
+        # Untouched boost when minimal change (<1%) and good final symmetry on link
+        p = peer_of.get(i)
+        if p and p in telemetry and status.get(i) == 'up' and status.get(p) == 'up':
+            # RX direction check against peer TX
+            if abs(scaled_rx_factor.get(i, 1.0) - 1.0) < 0.01:
+                link_res_rx = rel_diff(hardened_rx[i], hardened_tx[p])
+                if link_res_rx <= adaptive_tau(hardened_rx[i], hardened_tx[p]):
+                    conf_rx[i] = min(0.98, conf_rx[i] + UNTOUCHED_BOOST)
+            # TX direction check against peer RX
+            if abs(scaled_tx_factor.get(i, 1.0) - 1.0) < 0.01:
+                link_res_tx = rel_diff(hardened_tx[i], hardened_rx[p])
+                if link_res_tx <= adaptive_tau(hardened_tx[i], hardened_rx[p]):
+                    conf_tx[i] = min(0.98, conf_tx[i] + UNTOUCHED_BOOST)
 
     # Peer smoothing
     for a, b in link_pairs:
         if status.get(a) == 'up' and status.get(b) == 'up':
             conf_tx[a] = clamp01((1.0 - PEER_SMOOTH) * conf_tx[a] + PEER_SMOOTH * conf_rx[b])
             conf_rx[b] = clamp01((1.0 - PEER_SMOOTH) * conf_rx[b] + PEER_SMOOTH * conf_tx[a])
             conf_rx[a] = clamp01((1.0 - PEER_SMOOTH) * conf_rx[a] + PEER_SMOOTH * conf_tx[b])
             conf_tx[b] = clamp01((1.0 - PEER_SMOOTH) * conf_tx[b] + PEER_SMOOTH * conf_rx[a])
 
     # Assemble final result
     result: Dict[str, Dict[str, Tuple]] = {}
     for i, data in telemetry.items():
         my_status = status.get(i, 'unknown')
         peer_id = peer_of.get(i)
         # Status confidence: do not flip, but penalize inconsistent peer or down+nonzero readings
         status_conf = 1.0
         if peer_id and peer_id in telemetry:
             if telemetry[peer_id].get('interface_status', 'unknown') != my_status:
                 status_conf = 0.6
         if my_status == 'down' and (orig_rx.get(i, 0.0) > ZERO_EPS or orig_tx.get(i, 0.0) > ZERO_EPS):
             status_conf = min(status_conf, 0.6)
 
         repaired: Dict[str, Any] = {}
         repaired['rx_rate'] = (orig_rx.get(i, 0.0), max(0.0, hardened_rx.get(i, 0.0)), clamp01(conf_rx.get(i, 0.6)))
         repaired['tx_rate'] = (orig_tx.get(i, 0.0), max(0.0, hardened_tx.get(i, 0.0)), clamp01(conf_tx.get(i, 0.6)))
         repaired['interface_status'] = (my_status, my_status, status_conf)
 
         # Copy metadata unchanged
         repaired['connected_to'] = data.get('connected_to')
         repaired['local_router'] = data.get('local_router')
         repaired['remote_router'] = data.get('remote_router')
 
         result[i] = repaired
 
     return result
 
 # EVOLVE-BLOCK-END
 
 
 def run_repair(telemetry: Dict[str, Dict[str, Any]], topology: Dict[str, List[str]]) -> Dict[str, Dict[str, Tuple]]:
     """
     Main entry point that will be called by the evaluator.
 
     Args:
         telemetry: Network interface telemetry data
         topology: Dictionary where key is router_id and value contains a list of interface_ids
 
     Returns:
         Dictionary containing repaired results with confidence scores
     """
     return repair_network_telemetry(telemetry, topology)
 
 
 if __name__ == "__main__":
     # Simple test case
     test_telemetry = {
         'if1_to_if2': {
             'interface_status': 'up',
             'rx_rate': 100.0,
             'tx_rate': 95.0,
             'connected_to': 'if2_to_if1',
             'local_router': 'router1',
             'remote_router': 'router2'
         },
         'if2_to_if1': {
             'interface_status': 'up',
             'rx_rate': 95.0,  # Should match if1's TX
             'tx_rate': 100.0,  # Should match if1's RX
             'connected_to': 'if1_to_if2',
             'local_router': 'router2',
             'remote_router': 'router1'
         }
     }
 
     test_topology = {
         'router1': ['if1_to_if2'],
         'router2': ['if2_to_if1']
     }
 
     result = run_repair(test_telemetry, test_topology)
 
     print("Repair results:")
     for if_id, data in result.items():
         print(f"\n{if_id}:")
         print(f"  RX: {data['rx_rate']}")
         print(f"  TX: {data['tx_rate']}")
         print(f"  Status: {data['interface_status']}")